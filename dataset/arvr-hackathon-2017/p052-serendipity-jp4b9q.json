{
    "source": "https://devpost.com/software/serendipity-jp4b9q",
    "title": "Serendipity made at MIT's Reality, Virtually Hackathon",
    "blurb": "'x-ray' vision to see through the environment, wherein relevant events at that time are highlighted in space.",
    "awards": [],
    "videos": [],
    "images": [],
    "team": [
        {
            "name": "Muso Fun",
            "about": "",
            "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1475966827/yoz0um09vjx0fkahcgxy.jpg?height=180&width=180"
        },
        {
            "name": "Hanbin Cho",
            "about": "",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/511/datas/profile.jpg"
        },
        {
            "name": "Warren Partridge",
            "about": "",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/652/256/datas/profile.jpg"
        },
        {
            "name": "Marisa Lu",
            "about": "",
            "photo": "https://media.licdn.com/mpr/mprx/0_fvQ1tga1N6MM1P-7hvY1pOf19_nMPlxmhvrgBHU1cXr49T2f5vy1VU_1r_kXzPOG8XyjsWuPVboZvvBP86_fsWSxxboJvvemL6_rleZtNQ0vx_P-WBzYApn_0-rdpv0xu5F099TwLN-?height=180&width=180"
        },
        {
            "name": "Hadi Zayer",
            "about": "",
            "photo": "https://graph.facebook.com/1510171769000167/picture?height=180&width=180"
        }
    ],
    "built_with": [
        "arkit",
        "mapbox",
        "unity"
    ],
    "content_html": "<div>\n<p>Team lead: Big Bin\nTeam lead's phone number 571-499-0280 \nRoom: The warmer roomer with yellow lighting Table 7\nCategory: Other: Closest umbrella we could think of was 'social vr' but ours is probably more like 'mixing' reality for lifestyle/productivity purposes</p>\n<h2>Inspiration</h2>\n<p><strong>Problem Space:</strong>\nThe digital age makes accessing information and doing more \u2018easier\u2019, but that doesn\u2019t mean life is any easier.  Stressing out from the information overload is easy! </p>\n<p>Being in the know about what\u2019s going on and acting on all the information from diverse sources, takes effort because the information given usually isn\u2019t applicable in the moment. So you organize, and you schedule, or just try to remember. </p>\n<p>Can we understand what's going on around us in a delightful, intuitive?</p>\n<p><strong>Goal:</strong>\nWe are inspired to present relevant information when and where it would be actionable for people in a delightful, intuitive way. Let's bring a little more serendipity to daily life and make 'busy' less stressful.</p>\n<p><strong>Interaction Goal:</strong>\nUser goes about their life, and the application lets them know of relevant events according to their profile\u2019s interests and various social medias, when and where the information is immediately actionable (time frame and location range can be adjusted to preferences). The events are shown embedded within a virtual 3D replica of the real environment simulated from your point of view. It is as if your phone was an xray for the world. </p>\n<p>For a more active interaction, if the user happens to realize they have free time, they can just as easily bring up the app and toggle time, space, and interest preferences for discovering events/interesting places. It\u2019d make being a tourist/traveler less stressful.</p>\n<p>(we believed 3D representation of the map would b e a more intuitive way for the user to understand the environment and spatial relationships. At any rate, it should be easier for most people to understand at a glance where something is if they understand the spatial context, rather than glancing at a flat 2D map in a bird's perspective)</p>\n<p><strong>Meta Goal:</strong> \nVR environments generated based off the real world, and informed by the user's live position in time and space, used to expand their understanding of what\u2019s going on around them physically and temporally in order to ultimately augment the social, qualitative reality of their life.</p>\n<h2>What we made</h2>\n<p>A working proof of concept that demonstrates how VR environments might be generated based off real world data and oriented to reflect the users location and orientation. </p>\n<h2>How it was built</h2>\n<p>Our proof of concept uses mapbox\u2019s Unity sdk to generate a virtual model of the real world around the user\u2019s live or inputted location. (Mapbox SDK provided map info like topography, floorplans, building elevations) The generation of the map model was tweaked to allow selection of individual buildings. The unity build for ios and android got the user's location information from the phone system and used compass data to orient the virtual environment (to script the angle and location of the camera in Unity) to match what we believed would be the user's point of view of the real world. We ran out of time to hook up the event spaces highlighted in the map to other API's, so the 'events' you see in the virtual world, aren't actually real.</p>\n<h2>UI / Interaction prototyping</h2>\n<p>Early prototypes to help with ideation and understanding what sort of interaction we want were done with 360 photos (Ricoh Theta Camera), photoshopped, and thrown into Aframe to view on mobile. UI prototype was quickly shaped in Sketch and plopped into Origami.</p>\n<h2>Challenges we ran into</h2>\n<p>It's always the little things that take time to solve. On a high level, the plan for how to execute wasn't particularly convoluted (mapbox, unity, ios/android and possible plumbing with social media API's/other sites to port event info in to a database) Getting started with Mapbox, and crash coursing Unity took a while and then the little bugs when building out for mobile kept us occupied.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I'm pretty excited for the concept, and am proud about the ideating process. Otherwise, I'm very proud of the team;\nwe all sloughed through new material and it was a learning experience all around.</p>\n<h2>What I learned</h2>\n<p>I should make design assets early/before the hackathon so that I can get more hands on coding experience while there are experienced developers around. (or not, it's hard to say; I had fun talking to people before deciding on running with an idea)</p>\n<h2>What's next for Serendipity</h2>\n<p>Make it work! I want this for real to use! Polish the interaction, the Unity shaders, the UX, and the visual language. If we get far enough with it, then start testing for edge cases, and porting real data in, etc, etc.\nOther than that....well the code is open source, so I want to see what other ideas people will have! </p>\n</div>",
    "content_md": "\nTeam lead: Big Bin\nTeam lead's phone number 571-499-0280 \nRoom: The warmer roomer with yellow lighting Table 7\nCategory: Other: Closest umbrella we could think of was 'social vr' but ours is probably more like 'mixing' reality for lifestyle/productivity purposes\n\n\n## Inspiration\n\n\n**Problem Space:**\nThe digital age makes accessing information and doing more \u2018easier\u2019, but that doesn\u2019t mean life is any easier. Stressing out from the information overload is easy! \n\n\nBeing in the know about what\u2019s going on and acting on all the information from diverse sources, takes effort because the information given usually isn\u2019t applicable in the moment. So you organize, and you schedule, or just try to remember. \n\n\nCan we understand what's going on around us in a delightful, intuitive?\n\n\n**Goal:**\nWe are inspired to present relevant information when and where it would be actionable for people in a delightful, intuitive way. Let's bring a little more serendipity to daily life and make 'busy' less stressful.\n\n\n**Interaction Goal:**\nUser goes about their life, and the application lets them know of relevant events according to their profile\u2019s interests and various social medias, when and where the information is immediately actionable (time frame and location range can be adjusted to preferences). The events are shown embedded within a virtual 3D replica of the real environment simulated from your point of view. It is as if your phone was an xray for the world. \n\n\nFor a more active interaction, if the user happens to realize they have free time, they can just as easily bring up the app and toggle time, space, and interest preferences for discovering events/interesting places. It\u2019d make being a tourist/traveler less stressful.\n\n\n(we believed 3D representation of the map would b e a more intuitive way for the user to understand the environment and spatial relationships. At any rate, it should be easier for most people to understand at a glance where something is if they understand the spatial context, rather than glancing at a flat 2D map in a bird's perspective)\n\n\n**Meta Goal:** \nVR environments generated based off the real world, and informed by the user's live position in time and space, used to expand their understanding of what\u2019s going on around them physically and temporally in order to ultimately augment the social, qualitative reality of their life.\n\n\n## What we made\n\n\nA working proof of concept that demonstrates how VR environments might be generated based off real world data and oriented to reflect the users location and orientation. \n\n\n## How it was built\n\n\nOur proof of concept uses mapbox\u2019s Unity sdk to generate a virtual model of the real world around the user\u2019s live or inputted location. (Mapbox SDK provided map info like topography, floorplans, building elevations) The generation of the map model was tweaked to allow selection of individual buildings. The unity build for ios and android got the user's location information from the phone system and used compass data to orient the virtual environment (to script the angle and location of the camera in Unity) to match what we believed would be the user's point of view of the real world. We ran out of time to hook up the event spaces highlighted in the map to other API's, so the 'events' you see in the virtual world, aren't actually real.\n\n\n## UI / Interaction prototyping\n\n\nEarly prototypes to help with ideation and understanding what sort of interaction we want were done with 360 photos (Ricoh Theta Camera), photoshopped, and thrown into Aframe to view on mobile. UI prototype was quickly shaped in Sketch and plopped into Origami.\n\n\n## Challenges we ran into\n\n\nIt's always the little things that take time to solve. On a high level, the plan for how to execute wasn't particularly convoluted (mapbox, unity, ios/android and possible plumbing with social media API's/other sites to port event info in to a database) Getting started with Mapbox, and crash coursing Unity took a while and then the little bugs when building out for mobile kept us occupied.\n\n\n## Accomplishments that I'm proud of\n\n\nI'm pretty excited for the concept, and am proud about the ideating process. Otherwise, I'm very proud of the team;\nwe all sloughed through new material and it was a learning experience all around.\n\n\n## What I learned\n\n\nI should make design assets early/before the hackathon so that I can get more hands on coding experience while there are experienced developers around. (or not, it's hard to say; I had fun talking to people before deciding on running with an idea)\n\n\n## What's next for Serendipity\n\n\nMake it work! I want this for real to use! Polish the interaction, the Unity shaders, the UX, and the visual language. If we get far enough with it, then start testing for edge cases, and porting real data in, etc, etc.\nOther than that....well the code is open source, so I want to see what other ideas people will have! \n\n\n"
}