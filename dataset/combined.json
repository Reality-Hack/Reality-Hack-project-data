{
    "arvr-hackathon-2017": [
        {
            "source": "https://devpost.com/software/speakeasy-5wl7z2",
            "title": "Speakeasy",
            "blurb": "Practice your speech in front of an audience and receive real time feedback on your vocal and physical delivery",
            "awards": [
                "Grand Prize in VR"
            ],
            "videos": [
                "https://www.youtube.com/embed/g6q7rjH1Efc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Kathy practicing her speech",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/880/datas/original.jpg"
                },
                {
                    "title": "Practice a keynote speech in an auditorium",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/925/datas/original.jpg"
                },
                {
                    "title": "Practice interview answers. Do more gestures! Stop saying &quot;just&quot; and &quot;like&quot;!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/920/datas/original.png"
                },
                {
                    "title": "Choose amongst an Auditorium, Classroom, Interview, Elevator Pitch, Wedding, Jury and Dating scenes",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/923/datas/original.png"
                },
                {
                    "title": "Summary screen of how your speech will be received",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/921/datas/original.png"
                },
                {
                    "title": "Main menu options",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/922/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Kathy Wang",
                    "about": "UI/UX and VR Interface Designer, also experienced in 3D Modeling, Motion Graphics and Industrial Design.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/027/datas/profile.jpeg"
                },
                {
                    "name": "paulwehner",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/905420?height=180&v=3&width=180"
                },
                {
                    "name": "ENKAI JI",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/984/datas/profile.jpg"
                },
                {
                    "name": "casper21",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/dbc735e2eb58cbd4f77ebfe60f253dec?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Marlon Fuentes",
                    "about": "",
                    "photo": "https://graph.facebook.com/10155750215229904/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "c#",
                "google-web-speech-api",
                "htc-vive",
                "leap-motion",
                "pyaudio",
                "python",
                "ricoh",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>With VR and AR, we can both mimic the stressful situations that trigger your physical and vocal nervous ticks, and give you real time feedback to help you eliminate them. Additionally, we can put you in the shoes of the great public speakers in history, and teach you the vocal and physical delivery that made their speeches great.</p>\n<h2>What it does</h2>\n<p>With Speakeasy you you can choose the type of public speaking you want to practice, to help you overcome your fears and get hints on how to become a master public speaker. The first step is choosing whether you are delivering a public speech, a classroom lecture, answering interview questions, or asking someone out on a date. After you choose your scene, you are transported to that environment in VR utilizing 360 video that we captured using a Ricoh Theta camera during the Hackathon.</p>\n<p>Once you are transported to the scene of your choosing, you begin delivering your speech. Unlike the commercial solutions which currently exist, you get feedback in real time on your vocal AND physical delivery. </p>\n<p>On the vocal side, Speakeasy immediately alerts you if you use a filler words like \"umm\", \"right\" or \"you know\" and keeps track of how many times you used each filler word. Speakeasy also tracks how many words you are speaking per minute, and lets you know if you are going to fast or too slow. And finally, Speakeasy tracks the tone and loudness of your voice in real time, so you can find out if you are varying your frequency correctly and projecting optimally.</p>\n<p>On the physical side, Speakeasy tracks every gesture you make via a Leap Motion detector, so you can both see your gestures in VR and ensure you are doing enough gestures per minute. The best public speakers tell a story both through their words and their physical movement. If you don't do enough gestures, Speakeasy lets you know. Additionally, Speakeasy can track if you lean from side to side, and don't keep eye contact with the audience.</p>\n<p>The feedback provided by Speakeasy is based on scientific research into communication best practices. Specifically, a survey of all the TED talks posted online found that the speakers of the most viral TED talks, which on average received 7.4 million views, used on average 26 gestures per minute. On the other hand, the least viral TED talks, which on average received only 120,000 views, the speakers did only 15 gestures per minute. When we look at the vocal patterns of the masters, on average they use 130 words per minute, and vary their vocal tone.</p>\n<p>After you finish your speech, you receive a summary screen, letting you know how your speech will be received, and tips to improve your delivery. You can also share you speech with friends or a professional coach, so you can receive feedback on your content and overall performance. These ratings will later be fed back to Speakeasy's machine learning algorithm, so it can learn the unique delivery style that works best for you.</p>\n<p>From the main menu you can also go back into your speech \"History\", in case you said a phrase or did a gesture you want to remember, and go into \"Master\" mode to see how well you can deliver the best speeches in history.</p>\n<h2>How we built it</h2>\n<p>We tackled providing real time feedback of someones vocal delivery first. Eliminating the filler words was our first and primary goal, and accomplishing just that in an immersive VR or AR environment we felt would make our hack a success. We did all our audio processing inside a locally running python server that utilizes pyaudio and the Google Speech to Text API. With the Speech to Text API we can get a full transcript of the words used throughout someones speech, and with a blacklist of the most commonly used filler words, detect when someone said a filler word. In our python server we also track the loudness of the users voice. We calculate loudness using an RMS algorithm which we wrote, where we only track the loudness of each word spoken and ignore the gaps in speech when the user is pausing between words. This was vitally important so that we determine the true vocal variation of the user and not penalize them for taking pauses for dramatic effects, which the speaker is encouraged to do by Speakeasy. Additionally, when calculating Loudness we average the loudness over time, so score how well the user is projecting himself and whether they are varying their tone well. Averaging over time was important so that Speakeasy doesn't give false warnings or praise based on too short of a time frame. Finally our python server sends the information to our Unity app via HTTP requests, updating Unity each time a word is spoken.</p>\n<p>The core of the VR/AR experience was built in Unity utilizing an HTC Vive and Leap Motion hand trackers. With the Leap Motion detector, we were able to determine each time a user did a gesture in the \"strike zone\". Multiple studies have shown that the ideal number of gestures is 26 per minute, and that the gestures should be made in front of the user in an area similar to a baseball \"strike zone\" (<a href=\"http://wapo.st/2z8RZjh\" rel=\"nofollow\">http://wapo.st/2z8RZjh</a>). Only gestures made in this \"strike zone\" are counted as \"Good\" gestuers by Speakeasy, so if a user is making gestures while the hands are very low (so too weak of a gesture) or the hands are too high (too aggressive of a gesture), those \"Bad\" gestures are ignored and don't fall into the \"Good\" category. Additionally, with the HTC Vive headset we are able to track whether a user is making a rocking motion as well as whether they are utilizing the stage effectively if the speaker intends to walk around the stage. Finally, with the HTC Vive hand controller, we mimic a 3D microphone, so a user can practice the delivery of a speech when they know they will be using a microphone.</p>\n<p>To create the scenes that mimic the different public speaking environments that a user may encounter, we utilized a Ricoh Theta camera, and filmed scenes inside the MIT Stata Center Keynote auditorium from the hackathon. Additionally, we asked the audience of hackers inside the Keynote auditorium to give us different reactions, including clapping, booing, laughing, slowly ignoring the speaker and paying attention. With these different 360 video clips, we strung them together inside Unity to give the HTC Vive user a first person perspective of delivering a keynote speech at the Reality Virtually Hackathon. Beyond this, by capturing different audience reactions, we can give the speech giver a realistic audience reaction when they start using filler words and take on other bad habits. In those instances the audience will boo. When they begin hitting all the optimal delivery techniques for physical and vocal delivery, the audience will be focused on them. We ultimately brought the 360 video into Unity with a Sphere object, texture, material, and a skybox.</p>\n<p>To construct the UX of our scenes, the initial mockups were made with Photoshop and then implemented in Unity. </p>\n<p>We also did initial testing of the AR experience with Meta, Samsung Gear and Merge VR headsets, and look forward to bringing the Speakeasy app to those platforms in the future, so that users can receive the real time feedback which Speakeasy provides while practicing their speeches in the real environment and in front of a real, live audience.</p>\n<h2>Challenges we ran into</h2>\n<p>Our team had never used Leap Motion before, stitched together a 360 video inside Unity and had limited experience with the signal processing and machine learning necessary for the vocal feedback.</p>\n<p>The Leap Motion tracking while initially easy to get set up, was challenging to determine the best way to track what we would consider an optimal \"strike zone\" gesture.</p>\n<p>Importing 360 video into Unity, and using it in the context of a HTC Vive scene ended up taking much, much longer than we expected. There was a script that we were missing that we didn't realize we needed, and it took many iterations and multiple tutorials to determine that. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We came together quickly as a team, and were able to immediately focus on a clear goal for a hack and be productive within 5 minutes of forming a team. This was especially impressive as none of us came into the hackathon knowing what we wanted to work on, and we also met for the first time at the hackathon.</p>\n<p>After forming our team, because we were so focused and able to communicate what we wanted to do effectively to each other, we were able to capture 360 video from the tail end of the team building in the Reality Virtually Keynote auditorium so we could get audience reactions to be used in our VR experience.</p>\n<p>Additionally, we are able to accomplish our primary and stretch goals in terms of audio and physical gesture processing!</p>\n<h2>What we learned</h2>\n<p>How to use:</p>\n<ul>\n<li>Leap Motion</li>\n<li>Python sound processing libraries</li>\n<li>Creating a 360 video skybox</li>\n<li>Gesture tracking</li>\n<li>And a lot more!</li>\n</ul>\n<h2>What's next for Speakeasy</h2>\n<p>Now that we can do the Vocal and Gesture tracking, we want to put you in the shoes of master public speakers, and gamify the practice of trying to speak like great speakers in history. Step into the shoes of Steve Job's when he launched the iPhone to the world, and see how well you can mimic his keynote speech by saying the words he said and making the gestures he made.</p>\n<p>Additionally, we want to bring Speakeasy to Meta, Samsung Gear VR and Merge VR so you can get real time feedback while practicing your speech out in the Real World.</p>\n<p>Video submitted by 11pm deadline: <a href=\"https://youtu.be/ykhc6oZYEWg\" rel=\"nofollow\">https://youtu.be/ykhc6oZYEWg</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWith VR and AR, we can both mimic the stressful situations that trigger your physical and vocal nervous ticks, and give you real time feedback to help you eliminate them. Additionally, we can put you in the shoes of the great public speakers in history, and teach you the vocal and physical delivery that made their speeches great.\n\n\n## What it does\n\n\nWith Speakeasy you you can choose the type of public speaking you want to practice, to help you overcome your fears and get hints on how to become a master public speaker. The first step is choosing whether you are delivering a public speech, a classroom lecture, answering interview questions, or asking someone out on a date. After you choose your scene, you are transported to that environment in VR utilizing 360 video that we captured using a Ricoh Theta camera during the Hackathon.\n\n\nOnce you are transported to the scene of your choosing, you begin delivering your speech. Unlike the commercial solutions which currently exist, you get feedback in real time on your vocal AND physical delivery. \n\n\nOn the vocal side, Speakeasy immediately alerts you if you use a filler words like \"umm\", \"right\" or \"you know\" and keeps track of how many times you used each filler word. Speakeasy also tracks how many words you are speaking per minute, and lets you know if you are going to fast or too slow. And finally, Speakeasy tracks the tone and loudness of your voice in real time, so you can find out if you are varying your frequency correctly and projecting optimally.\n\n\nOn the physical side, Speakeasy tracks every gesture you make via a Leap Motion detector, so you can both see your gestures in VR and ensure you are doing enough gestures per minute. The best public speakers tell a story both through their words and their physical movement. If you don't do enough gestures, Speakeasy lets you know. Additionally, Speakeasy can track if you lean from side to side, and don't keep eye contact with the audience.\n\n\nThe feedback provided by Speakeasy is based on scientific research into communication best practices. Specifically, a survey of all the TED talks posted online found that the speakers of the most viral TED talks, which on average received 7.4 million views, used on average 26 gestures per minute. On the other hand, the least viral TED talks, which on average received only 120,000 views, the speakers did only 15 gestures per minute. When we look at the vocal patterns of the masters, on average they use 130 words per minute, and vary their vocal tone.\n\n\nAfter you finish your speech, you receive a summary screen, letting you know how your speech will be received, and tips to improve your delivery. You can also share you speech with friends or a professional coach, so you can receive feedback on your content and overall performance. These ratings will later be fed back to Speakeasy's machine learning algorithm, so it can learn the unique delivery style that works best for you.\n\n\nFrom the main menu you can also go back into your speech \"History\", in case you said a phrase or did a gesture you want to remember, and go into \"Master\" mode to see how well you can deliver the best speeches in history.\n\n\n## How we built it\n\n\nWe tackled providing real time feedback of someones vocal delivery first. Eliminating the filler words was our first and primary goal, and accomplishing just that in an immersive VR or AR environment we felt would make our hack a success. We did all our audio processing inside a locally running python server that utilizes pyaudio and the Google Speech to Text API. With the Speech to Text API we can get a full transcript of the words used throughout someones speech, and with a blacklist of the most commonly used filler words, detect when someone said a filler word. In our python server we also track the loudness of the users voice. We calculate loudness using an RMS algorithm which we wrote, where we only track the loudness of each word spoken and ignore the gaps in speech when the user is pausing between words. This was vitally important so that we determine the true vocal variation of the user and not penalize them for taking pauses for dramatic effects, which the speaker is encouraged to do by Speakeasy. Additionally, when calculating Loudness we average the loudness over time, so score how well the user is projecting himself and whether they are varying their tone well. Averaging over time was important so that Speakeasy doesn't give false warnings or praise based on too short of a time frame. Finally our python server sends the information to our Unity app via HTTP requests, updating Unity each time a word is spoken.\n\n\nThe core of the VR/AR experience was built in Unity utilizing an HTC Vive and Leap Motion hand trackers. With the Leap Motion detector, we were able to determine each time a user did a gesture in the \"strike zone\". Multiple studies have shown that the ideal number of gestures is 26 per minute, and that the gestures should be made in front of the user in an area similar to a baseball \"strike zone\" (<http://wapo.st/2z8RZjh>). Only gestures made in this \"strike zone\" are counted as \"Good\" gestuers by Speakeasy, so if a user is making gestures while the hands are very low (so too weak of a gesture) or the hands are too high (too aggressive of a gesture), those \"Bad\" gestures are ignored and don't fall into the \"Good\" category. Additionally, with the HTC Vive headset we are able to track whether a user is making a rocking motion as well as whether they are utilizing the stage effectively if the speaker intends to walk around the stage. Finally, with the HTC Vive hand controller, we mimic a 3D microphone, so a user can practice the delivery of a speech when they know they will be using a microphone.\n\n\nTo create the scenes that mimic the different public speaking environments that a user may encounter, we utilized a Ricoh Theta camera, and filmed scenes inside the MIT Stata Center Keynote auditorium from the hackathon. Additionally, we asked the audience of hackers inside the Keynote auditorium to give us different reactions, including clapping, booing, laughing, slowly ignoring the speaker and paying attention. With these different 360 video clips, we strung them together inside Unity to give the HTC Vive user a first person perspective of delivering a keynote speech at the Reality Virtually Hackathon. Beyond this, by capturing different audience reactions, we can give the speech giver a realistic audience reaction when they start using filler words and take on other bad habits. In those instances the audience will boo. When they begin hitting all the optimal delivery techniques for physical and vocal delivery, the audience will be focused on them. We ultimately brought the 360 video into Unity with a Sphere object, texture, material, and a skybox.\n\n\nTo construct the UX of our scenes, the initial mockups were made with Photoshop and then implemented in Unity. \n\n\nWe also did initial testing of the AR experience with Meta, Samsung Gear and Merge VR headsets, and look forward to bringing the Speakeasy app to those platforms in the future, so that users can receive the real time feedback which Speakeasy provides while practicing their speeches in the real environment and in front of a real, live audience.\n\n\n## Challenges we ran into\n\n\nOur team had never used Leap Motion before, stitched together a 360 video inside Unity and had limited experience with the signal processing and machine learning necessary for the vocal feedback.\n\n\nThe Leap Motion tracking while initially easy to get set up, was challenging to determine the best way to track what we would consider an optimal \"strike zone\" gesture.\n\n\nImporting 360 video into Unity, and using it in the context of a HTC Vive scene ended up taking much, much longer than we expected. There was a script that we were missing that we didn't realize we needed, and it took many iterations and multiple tutorials to determine that. \n\n\n## Accomplishments that we're proud of\n\n\nWe came together quickly as a team, and were able to immediately focus on a clear goal for a hack and be productive within 5 minutes of forming a team. This was especially impressive as none of us came into the hackathon knowing what we wanted to work on, and we also met for the first time at the hackathon.\n\n\nAfter forming our team, because we were so focused and able to communicate what we wanted to do effectively to each other, we were able to capture 360 video from the tail end of the team building in the Reality Virtually Keynote auditorium so we could get audience reactions to be used in our VR experience.\n\n\nAdditionally, we are able to accomplish our primary and stretch goals in terms of audio and physical gesture processing!\n\n\n## What we learned\n\n\nHow to use:\n\n\n* Leap Motion\n* Python sound processing libraries\n* Creating a 360 video skybox\n* Gesture tracking\n* And a lot more!\n\n\n## What's next for Speakeasy\n\n\nNow that we can do the Vocal and Gesture tracking, we want to put you in the shoes of master public speakers, and gamify the practice of trying to speak like great speakers in history. Step into the shoes of Steve Job's when he launched the iPhone to the world, and see how well you can mimic his keynote speech by saying the words he said and making the gestures he made.\n\n\nAdditionally, we want to bring Speakeasy to Meta, Samsung Gear VR and Merge VR so you can get real time feedback while practicing your speech out in the Real World.\n\n\nVideo submitted by 11pm deadline: <https://youtu.be/ykhc6oZYEWg>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/hovr",
            "title": "HOVR",
            "blurb": "We create rich VR environments to increase mobility and healing for elderly patients",
            "awards": [
                "Second Prize in VR"
            ],
            "videos": [
                "https://www.youtube.com/embed/TANl5bnncHo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Michael A McNair",
                    "about": "Michael put together the idea on paper and allowed it to flow. I was on the part of helping creating a great UX, and gave it the ability to add audio and certain visuals to work smoothly through out the project. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/751/datas/profile.jpg"
                },
                {
                    "name": "Peter Iordanov",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/5401480?height=180&v=3&width=180"
                },
                {
                    "name": "Sam De Lara",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/919/240/datas/profile.png"
                },
                {
                    "name": "David Gu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5fe459918ed6fa59084582d3b4266566?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "hovr",
                "steamvr",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Team Members</h2>\n<ul>\n<li>Micheael A. Mcnair </li>\n<li>Sam De Lara</li>\n<li>David Gu</li>\n<li>Peter Iordanov</li>\n</ul>\n<h2>Category</h2>\n<p>Healthcare and Medicine</p>\n<h2>Environment</h2>\n<ul>\n<li>Platform: HTC Vive</li>\n<li>Development Tools: Unity, Maya (Educational License)</li>\n<li>SDKs: SteamVR</li>\n</ul>\n<h3>Unity Store Packages Used:</h3>\n<p><a href=\"https://www.assetstore.unity3d.com/en/#!/content/814\" rel=\"nofollow\">Two Million User Party Pack</a></p>\n<p><a href=\"https://www.assetstore.unity3d.com/en/#!/content/96371\" rel=\"nofollow\">Free Stylized Nature Environment</a></p>\n<ul>\n<li>Balloon</li>\n<li>Grass, with color modification</li>\n<li>Trees, with color modification</li>\n<li>Foliage, with color modification</li>\n<li>Flowers 1-6, with color modification</li>\n<li>Terrain</li>\n<li>Soccer ball</li>\n</ul>\n<h3>Other Assets (Music/ meshes)</h3>\n<p><a href=\"Turbosquid.com\" rel=\"nofollow\">Turbosquid.com</a></p>\n<ul>\n<li>Deer</li>\n<li>Bunny</li>\n<li>Birds</li>\n<li>Bench</li>\n</ul>\n<p><a href=\"Freesound.org\" rel=\"nofollow\">Freesound.org</a></p>\n<ul>\n<li>Ping sound</li>\n</ul>\n<p><a href=\"Soundbible.com\" rel=\"nofollow\">Soundbible</a></p>\n<ul>\n<li>Birds chirping </li>\n</ul>\n<p><a href=\"FreeMusicArchive.com\" rel=\"nofollow\">FreeMusicArchive.com</a></p>\n<ul>\n<li>Main theme</li>\n</ul>\n<h2>Inspiration</h2>\n<p>We believe in mobilizing every elderly patient in the country. Hospitals have tried a lot of different methods to get the elderly or the sick to get up and move around during long visits to the hospital or long term care. What HOVR wants to accomplish is to utilize VR technology by encompassing their body and movement in a VR experience in other areas of the country. </p>\n<p>We are a group of five strangers that got together to fight a bad and expensive problem for hospitals around the country. We see the potential and scale for a device like this in the healthcare market. There are countless studies that have been documented about mental health issues in hospitals that if changing the state of the patient can accelerate the recovery process. What if you could help the recovery issue of hospital patients and have the confidence that having your elder in long term care is a positive thing. </p>\n<h2>Challenges</h2>\n<p>Some of the challenges that we ran into with this device is the ability of the calibration between connecting the feet and the motion of the device at the same time. Also most of the environments that we like to use we want to make diagnostic to people and where they have been at. Time constraints make that pretty difficult.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Some of the accomplishments we are proud of is the fact that our product does so much for the hospital work flow. We understand that their is a problem in the healthcare industry and this is a solution to the problem. There has been countless tactics on helping elderly patients continuing their mobility in hospitals. Fitbits are a tactic to help track steps, nurses pick patients out of the bed to walk them around the hospital and this could be a potential danger for the patient. </p>\n<h2>What we learned</h2>\n<p>HOVR reduces the danger to patients but allows the individual  to stimulate their brain and not even notice the hovr device. Some of the things that we learned came from growing as people. We are complete strangers that came up with an idea that could help people mentally and physically. We feel that is powerful within itself. As we take a step back at ourselves we asked how could we be more mobile in our own lives. Our project has made us better advocates for our health.</p>\n<h2>What's next for HOVR</h2>\n<p>HOVR has alot to accomplish but we are turning a simple process into a scaleable business. We will add in apple bluetooth connection, and expand the demographic  to help gather data  lower insurance companies by showing the progress of our users. We are also going to create a SAAS model so that hospitals can push it out over all their software. We are building in different 3D environments so that users can experience multiple 360 immersive experiences. We believe that technology is the way to help us change our normalities.  </p>\n</div>",
            "content_md": "\n## Team Members\n\n\n* Micheael A. Mcnair\n* Sam De Lara\n* David Gu\n* Peter Iordanov\n\n\n## Category\n\n\nHealthcare and Medicine\n\n\n## Environment\n\n\n* Platform: HTC Vive\n* Development Tools: Unity, Maya (Educational License)\n* SDKs: SteamVR\n\n\n### Unity Store Packages Used:\n\n\n[Two Million User Party Pack](https://www.assetstore.unity3d.com/en/#!/content/814)\n\n\n[Free Stylized Nature Environment](https://www.assetstore.unity3d.com/en/#!/content/96371)\n\n\n* Balloon\n* Grass, with color modification\n* Trees, with color modification\n* Foliage, with color modification\n* Flowers 1-6, with color modification\n* Terrain\n* Soccer ball\n\n\n### Other Assets (Music/ meshes)\n\n\n<Turbosquid.com>\n\n\n* Deer\n* Bunny\n* Birds\n* Bench\n\n\n<Freesound.org>\n\n\n* Ping sound\n\n\n[Soundbible](Soundbible.com)\n\n\n* Birds chirping\n\n\n<FreeMusicArchive.com>\n\n\n* Main theme\n\n\n## Inspiration\n\n\nWe believe in mobilizing every elderly patient in the country. Hospitals have tried a lot of different methods to get the elderly or the sick to get up and move around during long visits to the hospital or long term care. What HOVR wants to accomplish is to utilize VR technology by encompassing their body and movement in a VR experience in other areas of the country. \n\n\nWe are a group of five strangers that got together to fight a bad and expensive problem for hospitals around the country. We see the potential and scale for a device like this in the healthcare market. There are countless studies that have been documented about mental health issues in hospitals that if changing the state of the patient can accelerate the recovery process. What if you could help the recovery issue of hospital patients and have the confidence that having your elder in long term care is a positive thing. \n\n\n## Challenges\n\n\nSome of the challenges that we ran into with this device is the ability of the calibration between connecting the feet and the motion of the device at the same time. Also most of the environments that we like to use we want to make diagnostic to people and where they have been at. Time constraints make that pretty difficult.\n\n\n## Accomplishments that we're proud of\n\n\nSome of the accomplishments we are proud of is the fact that our product does so much for the hospital work flow. We understand that their is a problem in the healthcare industry and this is a solution to the problem. There has been countless tactics on helping elderly patients continuing their mobility in hospitals. Fitbits are a tactic to help track steps, nurses pick patients out of the bed to walk them around the hospital and this could be a potential danger for the patient. \n\n\n## What we learned\n\n\nHOVR reduces the danger to patients but allows the individual to stimulate their brain and not even notice the hovr device. Some of the things that we learned came from growing as people. We are complete strangers that came up with an idea that could help people mentally and physically. We feel that is powerful within itself. As we take a step back at ourselves we asked how could we be more mobile in our own lives. Our project has made us better advocates for our health.\n\n\n## What's next for HOVR\n\n\nHOVR has alot to accomplish but we are turning a simple process into a scaleable business. We will add in apple bluetooth connection, and expand the demographic to help gather data lower insurance companies by showing the progress of our users. We are also going to create a SAAS model so that hospitals can push it out over all their software. We are building in different 3D environments so that users can experience multiple 360 immersive experiences. We believe that technology is the way to help us change our normalities. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/cosplayar",
            "title": "CosplayAR",
            "blurb": "An easily accessible AR tool for clothing patterns and creation.",
            "awards": [
                "Grand Prize in AR",
                "Best Object Augmentation"
            ],
            "videos": [
                "https://www.youtube.com/embed/9dUAmJhmx0w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Choose from user-created designs",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/823/datas/original.png"
                },
                {
                    "title": "Edit designs to suit your unique you.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/845/datas/original.png"
                },
                {
                    "title": "Never be in the dark about what materials you need again.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/855/datas/original.png"
                },
                {
                    "title": "Position AR trackers...",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/834/datas/original.png"
                },
                {
                    "title": "...and overlay design over your fabric.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/835/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Ben Taylor",
                    "about": "Design",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/651/400/datas/profile.jpg"
                },
                {
                    "name": "Justin Ng",
                    "about": "Lead Engineer (AR)",
                    "photo": "https://media.licdn.com/mpr/mprx/0_5P24iI_lws0B-I94bqIFiWGpIxuRKe94FN4bie3aqsZI723ZdvpqSHvhLA2X17cqkKa62fPuNObP?height=180&width=180"
                },
                {
                    "name": "Dawn Rivers",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_tSg6dNnWaytpdwFUZSLQGc2WfZO8odzRgpde238efJ23oJbs4pdHkQZWdRxpWJVUZpb6eCBdGZgT2DJRjOZV38nLbZgh2DiBZOZLoT_HuMeu6R4Iq7awEzSJFDbr5DNQAgpoS1SVJiC?height=180&width=180"
                },
                {
                    "name": "gfaline",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/24961131?height=180&v=4&width=180"
                },
                {
                    "name": "Lilika Markatou",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/326e21b82e77d651b10ccde87be17c6e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "mergevr",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>CosplayAR - An easily accessible AR tool for clothing patterns and creation.</h2>\n<p>Making clothes is a complicated nightmare. With AR, it can be a breeze! It's time to modernize a method in use since 1860. </p>\n</div>",
            "content_md": "\n## CosplayAR - An easily accessible AR tool for clothing patterns and creation.\n\n\nMaking clothes is a complicated nightmare. With AR, it can be a breeze! It's time to modernize a method in use since 1860. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/cultoure",
            "title": "CulToure ",
            "blurb": "Giving diplaced learners a sense of normalcy",
            "awards": [
                "Grand Prize in web VR"
            ],
            "videos": [
                "https://www.youtube.com/embed/kYz9f1u4p4s?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Johan Ospina",
                    "about": "I worked on Developing the infrastructure for this experience along with Teogenes Moura, it was a fun experience",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/678/datas/profile.jpg"
                },
                {
                    "name": "RichardIsaacs1",
                    "about": "Designed the educational program with gamification, proposed the financial\nmodel and the Cultoure name.",
                    "photo": "https://graph.facebook.com/10215320240665368/picture?height=180&width=180"
                },
                {
                    "name": "Amanda Mattson",
                    "about": "Co-created concept and user experience, storytelling, asset management, design",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/657/412/datas/profile.jpg"
                },
                {
                    "name": "Mischa Downing",
                    "about": "Created concept and designed website, provided educational consultation and project management. ",
                    "photo": "https://avatars3.githubusercontent.com/u/32585776?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "web-vr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Moving is stressful. But what if you were a child forced from your home by a natural disaster or by political unrest? In Syria, thousands of children have lost their homes because of war and have been forced to live in refugee camps as a result. Many are seeking asylum with their families in a new country without knowing the language or customs. We aim to ease the transition of this cultural shock with our multi-platform WebVR application: CulToure.</p>\n<h2>Our Goal</h2>\n<p>To provide a safe virtual space for vulnerable children - a space in which they can experience a new culture and start to become comfortable.</p>\n<h2>What It Does</h2>\n<p>Cultoure enables a refugee child to interact with a child in the new host country. Our two-way virtual sound enabled relationship builder will foster social connections while the children learn words in each other's language. This will prepare them to enter an immersive 360-degree worldview where they can experience each other's environment. We hope this introduction and playful interaction will help refugee children feel more secure when they arrive in their new country. We hope Cultoure will also increase racial tolerance and cultural understanding in the host population. </p>\n<h2>LaunchPad</h2>\n<p><a href=\"https://cultoureblog.wordpress.com/\" rel=\"nofollow\">https://cultoureblog.wordpress.com/</a></p>\n<h2>Team Lead</h2>\n<p>Amanda Mattson</p>\n<h2>Team Lead Phone</h2>\n<p>(339) 223-7492</p>\n<h2>Team Name</h2>\n<p>Cultoure\n(originally Leap Mind)</p>\n<h1>Team Members</h1>\n<p>Mischa Downing\nJohan Ospina\nTeogenes Moura\nRichard Isaacs</p>\n<h2>Platform</h2>\n<p>Web VR</p>\n<h2>Development Tools</h2>\n<p>HTML, Javascript, CSS, A-frame</p>\n<h1>SDKs</h1>\n<p>A-Frame, web VR, </p>\n<h1>APIs</h1>\n<p>SketchFab, Node.js, GoogleTranslate</p>\n<h1>Assets</h1>\n<p>Lobo Loco ambient sound track: <a href=\"http://freemusicarchive.org/genre/Ambient/\" rel=\"nofollow\">http://freemusicarchive.org/genre/Ambient/</a>\nTree of life ambient sound track: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/67920\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/67920</a>\nBed:\n<a href=\"https://sketchfab.com/models/fb044af83e35473e87430119a91345bc\" rel=\"nofollow\">https://sketchfab.com/models/fb044af83e35473e87430119a91345bc</a></p>\n<p>Dresser:\n<a href=\"https://sketchfab.com/models/4121216e285d4e20a0a01a427118859a\" rel=\"nofollow\">https://sketchfab.com/models/4121216e285d4e20a0a01a427118859a</a></p>\n<p>Bookshelf: estante_\n<a href=\"https://sketchfab.com/models/be7edf0fb184411a947e1dc3c2aa317d\" rel=\"nofollow\">https://sketchfab.com/models/be7edf0fb184411a947e1dc3c2aa317d</a> </p>\n<p>Nightstand:\n<a href=\"https://sketchfab.com/models/7c2ffdd7caa74380857d6056584718e9\" rel=\"nofollow\">https://sketchfab.com/models/7c2ffdd7caa74380857d6056584718e9</a> </p>\n<p>Lamp:\n<a href=\"https://sketchfab.com/models/10040b70746743d1bc4823e7eee31c06\" rel=\"nofollow\">https://sketchfab.com/models/10040b70746743d1bc4823e7eee31c06</a> </p>\n<p>Soccer Ball:\n<a href=\"https://sketchfab.com/models/a9ed84b06e6549e486aa585129a1cf48\" rel=\"nofollow\">https://sketchfab.com/models/a9ed84b06e6549e486aa585129a1cf48</a></p>\n<p>American Shakira Poster: <a href=\"https://upload.wikimedia.org/wikipedia/commons/8/8d/Shakira_perfoming_during_the_%22Tour_Anfibio%22.jpg\" rel=\"nofollow\">https://upload.wikimedia.org/wikipedia/commons/8/8d/Shakira_perfoming_during_the_%22Tour_Anfibio%22.jpg</a> </p>\n<p>Syrian Shakira Poster: <a href=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Shakira_-_Live_Paris_-_2010_%2812%29.jpg\" rel=\"nofollow\">https://upload.wikimedia.org/wikipedia/commons/4/41/Shakira_-_Live_Paris_-_2010_%2812%29.jpg</a> </p>\n<p>Nightstand2: <a href=\"https://sketchfab.com/models/54158b1da50748b2b2705ebcc12b2a9d#\" rel=\"nofollow\">https://sketchfab.com/models/54158b1da50748b2b2705ebcc12b2a9d#</a> </p>\n<p>Nighstand2_3: <a href=\"https://sketchfab.com/models/2d75e283b87640f292a44ce08f698844\" rel=\"nofollow\">https://sketchfab.com/models/2d75e283b87640f292a44ce08f698844</a> </p>\n<p>Bed2: <a href=\"https://sketchfab.com/models/f342f50a8eda4e6fafcdb4f057be5792#\" rel=\"nofollow\">https://sketchfab.com/models/f342f50a8eda4e6fafcdb4f057be5792#</a> \nBed2_3: </p>\n<p>Lamp2: <a href=\"https://sketchfab.com/models/6cb326fa47a242cdb9619865986bd660#\" rel=\"nofollow\">https://sketchfab.com/models/6cb326fa47a242cdb9619865986bd660#</a></p>\n<p>Dresser2: <a href=\"https://sketchfab.com/models/15562ed7b1d24a7a842d971f6555216e#\" rel=\"nofollow\">https://sketchfab.com/models/15562ed7b1d24a7a842d971f6555216e#</a> </p>\n<p>Easel1: <a href=\"https://sketchfab.com/models/a2153c35659347879d1ff9ce5b796100#\" rel=\"nofollow\">https://sketchfab.com/models/a2153c35659347879d1ff9ce5b796100#</a> </p>\n<p>Cat: <a href=\"https://sketchfab.com/models/1e7143dfafd04ff4891efcb06949a0b4\" rel=\"nofollow\">https://sketchfab.com/models/1e7143dfafd04ff4891efcb06949a0b4</a> \nCat3: <a href=\"https://sketchfab.com/models/d04d8ac2c1ba47ee9e03351b4f08dd4b\" rel=\"nofollow\">https://sketchfab.com/models/d04d8ac2c1ba47ee9e03351b4f08dd4b</a> </p>\n<p>Syria floor TexturesCom_Floor sCheckerboard005 1_1_seamless_S.jp g textures.com \nUSA floor TexturesCom_Woo dFine0051_1_seaml ess_S.jpg textures.com \nUSA wall 22375516_1015942 8882875716_20917 45414_o.jpg textures.com \nSyria Wall 22375516_1015942 8882875716_20917 45414_o.jpg textures.com \nCubemap of City Tokyo (used to represent Boston) https:// ru.wikipedia.org/ wiki/ %D0%A1%D1%84 %D0%B5%D1%80 %D0%B8%D1%87 %D0%B5%D1%81 %D0%BA%D0%B 0%D1%8F_%D0% BF%D0%B0%D0 %BD%D0%BE%D 1%80%D0%B0%D 0%BC%D0%B0 </p>\n<p>Cubemap of desert (to represent Syria) https:// <a href=\"http://www.pinterest.com\" rel=\"nofollow\">www.pinterest.com</a> /pin/ 3547289081231678 58/ </p>\n<p>Patricio Gonzalez Syria Flag art https:// <a href=\"http://www.etsy.com/\" rel=\"nofollow\">www.etsy.com/</a> listing/535800309/ syria-flag-printsyrian-flag-art-syria </p>\n<p>USA Flag https:// fineartamerica.com/ featured/licenseplate-map-of-theunited-states-onburnt-orange-slabdesignturnpike.html Fine Art America</p>\n<h2>Libraries</h2>\n<p>N/A</p>\n<h2>Components Not Created at Hackathon</h2>\n<p>all except translations</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMoving is stressful. But what if you were a child forced from your home by a natural disaster or by political unrest? In Syria, thousands of children have lost their homes because of war and have been forced to live in refugee camps as a result. Many are seeking asylum with their families in a new country without knowing the language or customs. We aim to ease the transition of this cultural shock with our multi-platform WebVR application: CulToure.\n\n\n## Our Goal\n\n\nTo provide a safe virtual space for vulnerable children - a space in which they can experience a new culture and start to become comfortable.\n\n\n## What It Does\n\n\nCultoure enables a refugee child to interact with a child in the new host country. Our two-way virtual sound enabled relationship builder will foster social connections while the children learn words in each other's language. This will prepare them to enter an immersive 360-degree worldview where they can experience each other's environment. We hope this introduction and playful interaction will help refugee children feel more secure when they arrive in their new country. We hope Cultoure will also increase racial tolerance and cultural understanding in the host population. \n\n\n## LaunchPad\n\n\n<https://cultoureblog.wordpress.com/>\n\n\n## Team Lead\n\n\nAmanda Mattson\n\n\n## Team Lead Phone\n\n\n(339) 223-7492\n\n\n## Team Name\n\n\nCultoure\n(originally Leap Mind)\n\n\n# Team Members\n\n\nMischa Downing\nJohan Ospina\nTeogenes Moura\nRichard Isaacs\n\n\n## Platform\n\n\nWeb VR\n\n\n## Development Tools\n\n\nHTML, Javascript, CSS, A-frame\n\n\n# SDKs\n\n\nA-Frame, web VR, \n\n\n# APIs\n\n\nSketchFab, Node.js, GoogleTranslate\n\n\n# Assets\n\n\nLobo Loco ambient sound track: <http://freemusicarchive.org/genre/Ambient/>\nTree of life ambient sound track: <https://www.assetstore.unity3d.com/en/#!/content/67920>\nBed:\n<https://sketchfab.com/models/fb044af83e35473e87430119a91345bc>\n\n\nDresser:\n<https://sketchfab.com/models/4121216e285d4e20a0a01a427118859a>\n\n\nBookshelf: estante\\_\n<https://sketchfab.com/models/be7edf0fb184411a947e1dc3c2aa317d> \n\n\nNightstand:\n<https://sketchfab.com/models/7c2ffdd7caa74380857d6056584718e9> \n\n\nLamp:\n<https://sketchfab.com/models/10040b70746743d1bc4823e7eee31c06> \n\n\nSoccer Ball:\n<https://sketchfab.com/models/a9ed84b06e6549e486aa585129a1cf48>\n\n\nAmerican Shakira Poster: <https://upload.wikimedia.org/wikipedia/commons/8/8d/Shakira_perfoming_during_the_%22Tour_Anfibio%22.jpg> \n\n\nSyrian Shakira Poster: <https://upload.wikimedia.org/wikipedia/commons/4/41/Shakira_-_Live_Paris_-_2010_%2812%29.jpg> \n\n\nNightstand2: <https://sketchfab.com/models/54158b1da50748b2b2705ebcc12b2a9d#> \n\n\nNighstand2\\_3: <https://sketchfab.com/models/2d75e283b87640f292a44ce08f698844> \n\n\nBed2: <https://sketchfab.com/models/f342f50a8eda4e6fafcdb4f057be5792#> \nBed2\\_3: \n\n\nLamp2: <https://sketchfab.com/models/6cb326fa47a242cdb9619865986bd660#>\n\n\nDresser2: <https://sketchfab.com/models/15562ed7b1d24a7a842d971f6555216e#> \n\n\nEasel1: <https://sketchfab.com/models/a2153c35659347879d1ff9ce5b796100#> \n\n\nCat: <https://sketchfab.com/models/1e7143dfafd04ff4891efcb06949a0b4> \nCat3: <https://sketchfab.com/models/d04d8ac2c1ba47ee9e03351b4f08dd4b> \n\n\nSyria floor TexturesCom\\_Floor sCheckerboard005 1\\_1\\_seamless\\_S.jp g textures.com \nUSA floor TexturesCom\\_Woo dFine0051\\_1\\_seaml ess\\_S.jpg textures.com \nUSA wall 22375516\\_1015942 8882875716\\_20917 45414\\_o.jpg textures.com \nSyria Wall 22375516\\_1015942 8882875716\\_20917 45414\\_o.jpg textures.com \nCubemap of City Tokyo (used to represent Boston) https:// ru.wikipedia.org/ wiki/ %D0%A1%D1%84 %D0%B5%D1%80 %D0%B8%D1%87 %D0%B5%D1%81 %D0%BA%D0%B 0%D1%8F\\_%D0% BF%D0%B0%D0 %BD%D0%BE%D 1%80%D0%B0%D 0%BC%D0%B0 \n\n\nCubemap of desert (to represent Syria) https:// [www.pinterest.com](http://www.pinterest.com) /pin/ 3547289081231678 58/ \n\n\nPatricio Gonzalez Syria Flag art https:// [www.etsy.com/](http://www.etsy.com/) listing/535800309/ syria-flag-printsyrian-flag-art-syria \n\n\nUSA Flag https:// fineartamerica.com/ featured/licenseplate-map-of-theunited-states-onburnt-orange-slabdesignturnpike.html Fine Art America\n\n\n## Libraries\n\n\nN/A\n\n\n## Components Not Created at Hackathon\n\n\nall except translations\n\n\n"
        },
        {
            "source": "https://devpost.com/software/collaborativear",
            "title": "CollaborativeAR",
            "blurb": "For the first time ever, share a space in AR with other friends, on your iPhone",
            "awards": [
                "Best Everyday Mobile AR Hacks - 1st",
                "1st Prize in Architecture, Engineer, Construction"
            ],
            "videos": [
                "https://www.youtube.com/embed/ciSQxYdzqe8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Step 1 - Calibration",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/954/datas/original.png"
                },
                {
                    "title": "Step 2 - Alignment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/953/datas/original.png"
                },
                {
                    "title": "Collaborate!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/955/datas/original.png"
                },
                {
                    "title": "Same World",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/952/datas/original.png"
                },
                {
                    "title": "Post Hackathon",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/547/885/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Avery Lamp",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/530/883/datas/profile.jpg"
                },
                {
                    "name": "Kenneth Friedman",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/321/912/datas/profile.JPG"
                },
                {
                    "name": "Abigail Ayers",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/47b2a67ced850f39dcb81b0ed0234b86?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Joe Crotchett",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/885/datas/profile.JPG"
                },
                {
                    "name": "lingxiao",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/1197405?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "arkit",
                "scenekit",
                "swift"
            ],
            "content_html": "<div>\n<h2>Brief Description of Project</h2>\n<p>Currently, Mobile augmented reality (AR) technology doesn\u2019t provide a solution for creating shared AR experiences.  With our platform, mobile AR developers and designers can create a new class of AR experience where multiple collaborators can engage in a shared space.  This has rich applications in education, collaborative working, prototyping, to name a few.  For the first time ever, we have made a strategy that allows for the aligning of worlds in AR experiences, opening the possibilities for AR experiences of all types.</p>\n<h2>Team Members:</h2>\n<p>Abby Ayers, Joseph Crotchett, Kenny Friedman, Avery Lamp, Xiao Ling</p>\n<ul>\n<li>platform: ARKit</li>\n<li>Development Tools: Maya, XCode, Substance Painter, Git</li>\n<li>SDKs: iOS</li>\n<li>APIs: ARKit</li>\n<li>Assets: None</li>\n<li>Libraries: None</li>\n<li>Components not created in this Hackathon;\n    - Code: None\n    - Assets: \n          - Planet Textures - <a href=\"http://planetpixelemporium.com/planets.html\" rel=\"nofollow\">http://planetpixelemporium.com/planets.html</a>\n          - Chess Board - <a href=\"https://github.com/d-ronnqvist/SCNBook-code/tree/obj-c\" rel=\"nofollow\">https://github.com/d-ronnqvist/SCNBook-code/tree/obj-c</a> MIT license</li>\n</ul>\n<h2>License</h2>\n<p>All sample code has an MIT License, but some assets may have a different license. Such exceptions are mentioned in the license that is included with <em>that specific</em> sample project.</p>\n</div>",
            "content_md": "\n## Brief Description of Project\n\n\nCurrently, Mobile augmented reality (AR) technology doesn\u2019t provide a solution for creating shared AR experiences. With our platform, mobile AR developers and designers can create a new class of AR experience where multiple collaborators can engage in a shared space. This has rich applications in education, collaborative working, prototyping, to name a few. For the first time ever, we have made a strategy that allows for the aligning of worlds in AR experiences, opening the possibilities for AR experiences of all types.\n\n\n## Team Members:\n\n\nAbby Ayers, Joseph Crotchett, Kenny Friedman, Avery Lamp, Xiao Ling\n\n\n* platform: ARKit\n* Development Tools: Maya, XCode, Substance Painter, Git\n* SDKs: iOS\n* APIs: ARKit\n* Assets: None\n* Libraries: None\n* Components not created in this Hackathon;\n - Code: None\n - Assets: \n - Planet Textures - <http://planetpixelemporium.com/planets.html>\n - Chess Board - <https://github.com/d-ronnqvist/SCNBook-code/tree/obj-c> MIT license\n\n\n## License\n\n\nAll sample code has an MIT License, but some assets may have a different license. Such exceptions are mentioned in the license that is included with *that specific* sample project.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/around-x5vjsi",
            "title": "ARound",
            "blurb": "Idea is to augment the world with spatial sounds that change depending on your proximity towards places",
            "awards": [
                "Best Everyday Mobile AR Hacks - 2nd",
                "1st Prize in Education, AR & VR for good"
            ],
            "videos": [
                "https://www.youtube.com/embed/NpU5qcs6Y6g?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/836/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Anandana Kapur",
                    "about": "Creative direction, features input, user validation, project management, Sound and film. ",
                    "photo": "https://www.gravatar.com/avatar/aaa82345f2f0125b4bd6b899ec5083c4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sunish Gupta",
                    "about": "Experience Architect and Strategy; Integrating Usability and Accessibility (Inclusive Design)  ; Function/Feature/Task  Analysis selection and prioritization;,   Scenario creation and User Validation; System Design; Business Model, Marketing and Future Direction. ",
                    "photo": "https://www.gravatar.com/avatar/3a4a0b424eb49465263ff3af0f859471?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dhara Bhavsar",
                    "about": "Worked on the web app and the backend, configuring firebase on web and unity-android app, tech stack decision, managed Git repo, feature suggestions.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/875/datas/profile.jpg"
                },
                {
                    "name": "Maxym Nesmashny",
                    "about": "Product design, research, usability testing, accessibility, spatial audio design, team formation and facilitation",
                    "photo": "https://graph.facebook.com/10210348035205446/picture?height=180&width=180"
                },
                {
                    "name": "Vedant Saran",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/6468d0b21b172061e62cef9a18a37088?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "arcore",
                "css",
                "firebase",
                "google-maps",
                "html5",
                "javascript",
                "unity"
            ],
            "content_html": "<div>\n<p><strong>ARound</strong></p>\n<p><strong>Augmented Reality app for sound-based navigation</strong></p>\n<p><strong>Team ARound</strong></p>\n<p>Anandana Kapur, Dhara Bhavsar, Maxym Nesmashny, Sunish Gupta, Vedant Saran</p>\n<p><strong>User types</strong></p>\n<p>Visually impaired (sound reliant)</p>\n<p>Visually and hearing impaired (haptic feedback reliant)</p>\n<p>Mainstream (information and context reliant)</p>\n<p><strong>Why this app?</strong></p>\n<p>Team *<em>Getting ARound *</em> wants to augment the world with sound. Places of interest emit sounds that alert you of their presence, and that adds another layer of information to the world around us.  </p>\n<p>Spatial audio and ASMR related sounds can help to augment our everyday experiences and be a very powerful tool to navigate the world. </p>\n<p>Contextual audio cues that do not interfere with existing sounds or cause disorientation can be very valuable to navigate cities, forests, etc.</p>\n<p><strong>What does ARound do?</strong></p>\n<p>The  Augmented Reality app  <strong>ARound</strong>  equips all its users to independently navigate spaces by augmenting sounds of places, objects, and activities. In its current state the app functions using three modes:</p>\n<ol>\n<li><p><strong><em>Discovery mode</em></strong>: As you walk down a path, we use GPS data with combination of ARCore camera technology to sense your surroundings and trigger sounds of objects and locations on your path depending on your orientation towards them. The sounds are geofenced to an area so that they start playing just before you approach an object/place of interest, and gain in volume as you get closer. </p></li>\n<li><p><strong><em>Breadcrumb mode</em></strong>: As you walk around,  the app leaves a trail of sound along the path you have taken. If you lose your way or need to retrace your steps, you can simply follow your \"audio breadcrumbs\" to follow the path you took. Since the sound is anchored to the real-world, and we can accurately track the user\u2019s position using ARCore, the sound is spatialized to the user\u2019s orientation relative to the breadcrumbs. That makes it super easy to get back on the path - you just follow the direction the sound appears to be coming from. This feature gives the visually impaired the freedom and confidence to go on walks, hikes, or boating, without assistance. </p></li>\n<li><p><strong><em>Share mode</em></strong>: You can choose sounds from a library of audio files and link them to locations in the real world, through a web application. This creates a sound pin on the map that is triggered when the user approaches that location in the real world. This could be used to set up meetings with a blind person - You\u2019d just drop a pin of an audio note at the meeting location, and they\u2019d know when they arrived. There\u2019s also a social media aspect to this- you could create and share your own \u2018sonic maps\u2019 of your surroundings to experience the unique sonic landscapes that matter to your friends and family. </p></li>\n</ol>\n<p><strong>Why is ARound unique?</strong></p>\n<p>ARound brings a new perspective to the possibilities of Augmented Reality. While most of AR today is centered on augmenting one's visual systems, we think that there's immense unrealized potential in augmented one\u2019s sense of hearing. While current-generation AR systems can only augment your visuals in a tiny field of view, spatial audio enhances the user\u2019s perception in all 360 degrees. </p>\n<p>It takes advantage of the spatialization and contextualization abilities of AR technologies to create a highly responsive and immersive experience. With good computer vision and enough sound pins placed on the map, ARound could build a vibrant \u2018Internet of Sounds\u2019 - an immersive and contextual layer of sound augmented on top of the real world. </p>\n<h2><strong>Current features</strong></h2>\n<ol>\n<li><strong>Learn about their environment</strong></li>\n</ol>\n<p>As the users walk through the world, they\u2019ll become subtly aware of all the sounds and tagged objects around them. That\u2019s important for the blind - objects that are quiet are invisible, so augmented sounds help you orient yourself and learn about objects in your surroundings. Haptic feedback could make this useful for the deaf as well. That\u2019s also interesting to mainstream audiences as a discovery or educational platform. </p>\n<p><strong>2</strong><strong>. Path-finding and **</strong>_Deviation detection _**</p>\n<p>Easily find your way back on hikes and new paths. Hands-free and don\u2019t need to be looking at the screen, so you can navigate yourself which engaging with the real world. If you deviate too much from your path, the app will gently alert you of it, and guide you back in the correct direction. Since the deviation alert is volume-based, you always know exactly how far away you are from your path. </p>\n<p><strong>3</strong><strong>. **</strong><em>Social media</em>**</p>\n<p>Sonic scapes can be created and shared by all. Users can create sound maps for fellow travelers or teachers can make students discover cities and events. </p>\n<h2>*<em>What inspired us *</em></h2>\n<p>Independent living and traveling by people with disabilities have always been a challenge. This is especially true for the visually impaired population. Activities such as walking around or getting to familiar places like cafes, post office, ATM etc. can be challenging. Getting around unfamiliar routes while hiking, sailing or on nature walks can be be even more challenging. </p>\n<h2><strong>Challenges we ran into</strong></h2>\n<p>Designing for app for the blind presents a whole new set of design challenges, many of which we were initially unaware of. Sunish\u2019s experience in accessibility design was crucial in designing a product that would actually be usable by the target audience. However, we learned that the Unity engine was not natively compatible with Google Talkback and the other accessibility features on Android. The plugin to enable talkback was a paid feature, and under the licensing rules of the hackathon, using it would\u2019ve disqualified us. </p>\n<p>We also found out that our phones were not supported by the ARCore platform. Thanks to the ar-for-all guide, and the help of the mentors at the event, we finally managed to get it to work on (some of) our phones. </p>\n<h2>*<em>Accomplishments *</em></h2>\n<h2>We are thrilled to have built a working prototype. We were able to implement directional spatial audio that leads you along a certain path. We are proud about the idea being user-validated by our team member who is visually impaired.</h2>\n<h1><strong>What we learned</strong></h1>\n<p>None of us had a lot of prior experience with sound design or spatial audio, so we learned a lot about that. We got to play with ARCore and Firebase.  Some of us improved our understanding of how to work with Unity better. We brainstormed about translating user experience and feedback into technical solutions and that was extremely rewarding. We even learned how to incorporate accessibility/inclusivity design in our product. Our collaboration skills improved as well! </p>\n<p><strong>What's next for ARound</strong></p>\n<p>We are looking to develop our MVP into a full-scale platform that would help all our user types*  to navigate a given environment as well as be fun experience to use in everyday life. Finally, we want to build a full-scale platform for augmented sound, something like ARcore or ARkit, but with spatial direction in mind. With the help of this platform, many developers would be able to jump in and build all kinds of amazing apps and experiences. We are very excited for the future of ARound.</p>\n</div>",
            "content_md": "\n**ARound**\n\n\n**Augmented Reality app for sound-based navigation**\n\n\n**Team ARound**\n\n\nAnandana Kapur, Dhara Bhavsar, Maxym Nesmashny, Sunish Gupta, Vedant Saran\n\n\n**User types**\n\n\nVisually impaired (sound reliant)\n\n\nVisually and hearing impaired (haptic feedback reliant)\n\n\nMainstream (information and context reliant)\n\n\n**Why this app?**\n\n\nTeam **Getting ARound ** wants to augment the world with sound. Places of interest emit sounds that alert you of their presence, and that adds another layer of information to the world around us. \n\n\nSpatial audio and ASMR related sounds can help to augment our everyday experiences and be a very powerful tool to navigate the world. \n\n\nContextual audio cues that do not interfere with existing sounds or cause disorientation can be very valuable to navigate cities, forests, etc.\n\n\n**What does ARound do?**\n\n\nThe Augmented Reality app **ARound** equips all its users to independently navigate spaces by augmenting sounds of places, objects, and activities. In its current state the app functions using three modes:\n\n\n1. ***Discovery mode***: As you walk down a path, we use GPS data with combination of ARCore camera technology to sense your surroundings and trigger sounds of objects and locations on your path depending on your orientation towards them. The sounds are geofenced to an area so that they start playing just before you approach an object/place of interest, and gain in volume as you get closer.\n2. ***Breadcrumb mode***: As you walk around, the app leaves a trail of sound along the path you have taken. If you lose your way or need to retrace your steps, you can simply follow your \"audio breadcrumbs\" to follow the path you took. Since the sound is anchored to the real-world, and we can accurately track the user\u2019s position using ARCore, the sound is spatialized to the user\u2019s orientation relative to the breadcrumbs. That makes it super easy to get back on the path - you just follow the direction the sound appears to be coming from. This feature gives the visually impaired the freedom and confidence to go on walks, hikes, or boating, without assistance.\n3. ***Share mode***: You can choose sounds from a library of audio files and link them to locations in the real world, through a web application. This creates a sound pin on the map that is triggered when the user approaches that location in the real world. This could be used to set up meetings with a blind person - You\u2019d just drop a pin of an audio note at the meeting location, and they\u2019d know when they arrived. There\u2019s also a social media aspect to this- you could create and share your own \u2018sonic maps\u2019 of your surroundings to experience the unique sonic landscapes that matter to your friends and family.\n\n\n**Why is ARound unique?**\n\n\nARound brings a new perspective to the possibilities of Augmented Reality. While most of AR today is centered on augmenting one's visual systems, we think that there's immense unrealized potential in augmented one\u2019s sense of hearing. While current-generation AR systems can only augment your visuals in a tiny field of view, spatial audio enhances the user\u2019s perception in all 360 degrees. \n\n\nIt takes advantage of the spatialization and contextualization abilities of AR technologies to create a highly responsive and immersive experience. With good computer vision and enough sound pins placed on the map, ARound could build a vibrant \u2018Internet of Sounds\u2019 - an immersive and contextual layer of sound augmented on top of the real world. \n\n\n## **Current features**\n\n\n1. **Learn about their environment**\n\n\nAs the users walk through the world, they\u2019ll become subtly aware of all the sounds and tagged objects around them. That\u2019s important for the blind - objects that are quiet are invisible, so augmented sounds help you orient yourself and learn about objects in your surroundings. Haptic feedback could make this useful for the deaf as well. That\u2019s also interesting to mainstream audiences as a discovery or educational platform. \n\n\n**2****. Path-finding and ****\\_Deviation detection \\_**\n\n\nEasily find your way back on hikes and new paths. Hands-free and don\u2019t need to be looking at the screen, so you can navigate yourself which engaging with the real world. If you deviate too much from your path, the app will gently alert you of it, and guide you back in the correct direction. Since the deviation alert is volume-based, you always know exactly how far away you are from your path. \n\n\n**3****. *****Social media***\n\n\nSonic scapes can be created and shared by all. Users can create sound maps for fellow travelers or teachers can make students discover cities and events. \n\n\n## **What inspired us **\n\n\nIndependent living and traveling by people with disabilities have always been a challenge. This is especially true for the visually impaired population. Activities such as walking around or getting to familiar places like cafes, post office, ATM etc. can be challenging. Getting around unfamiliar routes while hiking, sailing or on nature walks can be be even more challenging. \n\n\n## **Challenges we ran into**\n\n\nDesigning for app for the blind presents a whole new set of design challenges, many of which we were initially unaware of. Sunish\u2019s experience in accessibility design was crucial in designing a product that would actually be usable by the target audience. However, we learned that the Unity engine was not natively compatible with Google Talkback and the other accessibility features on Android. The plugin to enable talkback was a paid feature, and under the licensing rules of the hackathon, using it would\u2019ve disqualified us. \n\n\nWe also found out that our phones were not supported by the ARCore platform. Thanks to the ar-for-all guide, and the help of the mentors at the event, we finally managed to get it to work on (some of) our phones. \n\n\n## **Accomplishments **\n\n\n## We are thrilled to have built a working prototype. We were able to implement directional spatial audio that leads you along a certain path. We are proud about the idea being user-validated by our team member who is visually impaired.\n\n\n# **What we learned**\n\n\nNone of us had a lot of prior experience with sound design or spatial audio, so we learned a lot about that. We got to play with ARCore and Firebase. Some of us improved our understanding of how to work with Unity better. We brainstormed about translating user experience and feedback into technical solutions and that was extremely rewarding. We even learned how to incorporate accessibility/inclusivity design in our product. Our collaboration skills improved as well! \n\n\n**What's next for ARound**\n\n\nWe are looking to develop our MVP into a full-scale platform that would help all our user types* to navigate a given environment as well as be fun experience to use in everyday life. Finally, we want to build a full-scale platform for augmented sound, something like ARcore or ARkit, but with spatial direction in mind. With the help of this platform, many developers would be able to jump in and build all kinds of amazing apps and experiences. We are very excited for the future of ARound.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/luminate",
            "title": "Luminate",
            "blurb": "Luminate elevates the experience of making digital monetary donations.",
            "awards": [
                "Best Everyday Mobile AR Hacks - 3rd"
            ],
            "videos": [
                "https://www.youtube.com/embed/wvW1n66s9KQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Kaitlyn-Irvine",
                    "about": "UI/UX, Brand Strategy, Storytelling, Project Management, Copywriting",
                    "photo": "https://avatars1.githubusercontent.com/u/22640105?height=180&v=4&width=180"
                },
                {
                    "name": "Angela May Nguyen",
                    "about": "Contributed to images, app idea, design, animations, copy.",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xPs-kBv0qWU-PQiHM5CtfqkOzpc-9Fi40Ba-T_wOqVMPPBG5MBaPW_r0BMUP1NiNxBC-dBPxy4V1v_TZPhxau_OPJ4Vtv_QkvhxAbCIY-JfOxvg4JldxF9L7MShCp_Avp_ROhyUe6CY?height=180&width=180"
                },
                {
                    "name": "Jesse Litton",
                    "about": "I worked on the UI Integrations for the app.",
                    "photo": "https://avatars0.githubusercontent.com/u/696100?height=180&v=4&width=180"
                },
                {
                    "name": "Prayash Thapa",
                    "about": "ARKit + SceneKit + Firebase integration.",
                    "photo": "https://www.gravatar.com/avatar/f1750886d1d3a3cd99b0fc9824350348?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "arkit",
                "firebase",
                "scenekit",
                "swift"
            ],
            "content_html": "<div>\n<h2>Team Information</h2>\n<p><strong>Team Name:</strong> Reality.af</p>\n<p><strong>Team Lead:</strong> Prayash Thapa (303) 883-7541</p>\n<p><strong>Full-Team:</strong> Prayash Thapa, Angela Nguyen, Jesse Litton, Kaitlyn Irvine, </p>\n<p><strong>Location, Floor, and Room:</strong>\n(#49) Third floor, past the kitchen, around the purple sofas in the Living Mobile/Civic Media area</p>\n<h2>Project Information</h2>\n<p>Category: Education / AR/VR for Good</p>\n<p><strong>Project Name:</strong> Luminate\n<strong>Project Description:</strong>\nLuminate elevates the experience of making digital monetary donations. Luminate allows anyone to send funds and inspirational messages in a virtual lantern to a geo-tagged  fundraising location. Using Augmented Reality, recipients in that designated area can then explore the lanterns in real time with their mobile device.</p>\n<h2>Development Information</h2>\n<p><strong>Development Tools Used to Build:</strong>\nXcode</p>\n<p><strong>SDKs used:</strong>\nFirebase, ARkit, SceneKit</p>\n<p><strong>APIs used:</strong>\nFirebase</p>\n<p><strong>Any assets used that you did not create:</strong>\n[Water image]\nCC0 License \u2013 Free for personal/commercial use. No attribution required.\n<a href=\"https://www.pexels.com/photo/sea-water-blue-sun-62307/\" rel=\"nofollow\">https://www.pexels.com/photo/sea-water-blue-sun-62307/</a></p>\n<p>[Las Vegas Memorial image]\nJoel Angel Juarez, Las Vegas Review-Journal\n<a href=\"https://www.reviewjournal.com/wp-content/uploads/2017/10/9368727_web1_shootingday5_jj_001.jpg\" rel=\"nofollow\">https://www.reviewjournal.com/wp-content/uploads/2017/10/9368727_web1_shootingday5_jj_001.jpg</a></p>\n<p><strong>Any Libraries used:</strong>\nLBTA components</p>\n<p><strong>Screen capture of the application:</strong>\n<a href=\"https://youtu.be/wvW1n66s9KQ\" rel=\"nofollow\">https://youtu.be/wvW1n66s9KQ</a></p>\n<p><strong>Any components not created at the hackathon:</strong>\nNone.</p>\n</div>",
            "content_md": "\n## Team Information\n\n\n**Team Name:** Reality.af\n\n\n**Team Lead:** Prayash Thapa (303) 883-7541\n\n\n**Full-Team:** Prayash Thapa, Angela Nguyen, Jesse Litton, Kaitlyn Irvine, \n\n\n**Location, Floor, and Room:**\n(#49) Third floor, past the kitchen, around the purple sofas in the Living Mobile/Civic Media area\n\n\n## Project Information\n\n\nCategory: Education / AR/VR for Good\n\n\n**Project Name:** Luminate\n**Project Description:**\nLuminate elevates the experience of making digital monetary donations. Luminate allows anyone to send funds and inspirational messages in a virtual lantern to a geo-tagged fundraising location. Using Augmented Reality, recipients in that designated area can then explore the lanterns in real time with their mobile device.\n\n\n## Development Information\n\n\n**Development Tools Used to Build:**\nXcode\n\n\n**SDKs used:**\nFirebase, ARkit, SceneKit\n\n\n**APIs used:**\nFirebase\n\n\n**Any assets used that you did not create:**\n[Water image]\nCC0 License \u2013 Free for personal/commercial use. No attribution required.\n<https://www.pexels.com/photo/sea-water-blue-sun-62307/>\n\n\n[Las Vegas Memorial image]\nJoel Angel Juarez, Las Vegas Review-Journal\n<https://www.reviewjournal.com/wp-content/uploads/2017/10/9368727_web1_shootingday5_jj_001.jpg>\n\n\n**Any Libraries used:**\nLBTA components\n\n\n**Screen capture of the application:**\n<https://youtu.be/wvW1n66s9KQ>\n\n\n**Any components not created at the hackathon:**\nNone.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/triloka",
            "title": "Triloka",
            "blurb": "Engaging VR simulation to assuage depression by providing diverse range of interactions to address negative feelings",
            "awards": [
                "1st Prize in Healthcare and medicine category"
            ],
            "videos": [
                "https://player.vimeo.com/video/237704061?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Sit around Bon Fire - Listen and Share",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/656/datas/original.png"
                },
                {
                    "title": "Bon Fire",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/844/datas/original.png"
                },
                {
                    "title": "Meditation near the serene sea",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/857/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Rohan Pooniwala",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/17231273?height=180&v=4&width=180"
                },
                {
                    "name": "Raghav Gupta",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/17514444?height=180&v=4&width=180"
                },
                {
                    "name": "Gama Liu",
                    "about": "",
                    "photo": "https://graph.facebook.com/1359194997512920/picture?height=180&width=180"
                },
                {
                    "name": "Yan Liu",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Glx-Bes7nr1U09KCQPItxec7BvbFy6Piah5ytjzmnPIQy9CfGbbPxyUmz_esVzfmGv5PVJNa0vq6sGIxSGsaxyJGsvqbsG6_aGsANg2flN3IKqj0CP2xqdGOVhaB1GltbTOO1ipe7SB?height=180&width=180"
                },
                {
                    "name": "Meenakshi Sivapriya Manikandaswamy",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/22399529?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "ibm-watson",
                "maya",
                "microsoft",
                "oculus-gear-vr",
                "photoshop",
                "sapi-wysabuddy-fbchat-aftereffects",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Losing a friend to the grim ailment of depression a few days back, got us thinking as to why there has not been an effective solution to such a common problem faced by the youth nowadays. Millions of lives could have been saved if there had been at least one person who they could've talked to without worrying about the stigma associated with mental health problems. Though there are multiple helplines and chatbots to help people with depression, the fear of your identity being revealed or being judged causes most victims to hesitate. Moreover, the unfriendly platform on which it is carried out also serves as a hindrance for people to open up.</p>\n<h2>What it does</h2>\n<p>Triloka blends the three realms of the virtual, physical and the spiritual world to help the user address and overcome all the negative feelings that are a result of depression. The user goes through a series of interactions that help him prevail over his loneliness, sadness, guilt, disinterest in daily activities, inability to concentrate, and the feeling of hopelessness. Various environments, including a forest where people share their depression stories around a bonfire, a virtual therapist who suggests activities to help overcome depression, an exciting canoeing encounter that helps bring back the user's interest in fun activities, a peaceful mountain environment to encourage meditation, all the while listening to soothing music and a mood reflecting sky, adds to this therapeutic experience.</p>\n<p><strong>The negative feelings faced and the way they are tackled:</strong></p>\n<p><strong>Loneliness</strong>: Meeting new people around the campfire and opening up without the fear of being judged or having \npreconceptions.\n<strong>Sadness</strong>: Lulling soft music in the background that calms the user\n<strong>Guilt</strong>: Listening to how other people overcame their depression and trying to empathize with them\n<strong>Lack of Interest in daily activities</strong>: Energetic activities like canoeing that can revitalise the mind\n<strong>Difficulty concentrating</strong>: Meditation session under serene conditions\n<strong>Hopelessness</strong>: Motivating and engaging incentives like unlocking more environments if positive thoughts are spoken, slowly giving hope.</p>\n<h2>How we built it</h2>\n<p>Used Microsoft Speech API for conversion of speech to text and vice-versa. We use it for conversing with the specialized (for depression) facebook bot and return the responses in voice using text to speech. IBM Watson's Tone Analyzer is used to track the user's mood and keep a measure of the score to reflect it on the sky colour.\nThe major aspect of this project that we spent most of our time building was the environments and the models associated with it. We imported free 3D models from the online website free3D.com and customised them to our requirements using the student version of Autodesk Maya. All the character animations and terrain systems were built using Maya and Unity engine to bring about a realistic VR experience.</p>\n<h2>Challenges we ran into</h2>\n<p>We brainstormed a lot on how to structure the interactions and environment such that it feels realistic enough to engage the user to take up multiple sessions which will ultimately help assuage depression. We had some trouble trying to stitch together each environment, interfacing them to provide an appealing storyline.\nWe ran out of time implementing the third environment involving the canoeing activity. Our members were inexperienced in character animation and thus spent a lot of time executing it. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are very proud of how we worked as a team and how our skills complemented each other to make this project.\nOur members successfully integrated speech-to-text and text-to-speech conversions for operating the talk-bot in a realistic manner. The environments, which we managed to build, are high-quality renderings that are also structured such that the user finds the environment engaging to return the next time. Enabling the users to have functional conversations with a specially trained bot, Wysa, that offers meaningful advice to get over depression. Learning and implementing character animation in such a short time.</p>\n<h2>What we learned</h2>\n<p>We learnt how to rig and animate humanoid characters, efficient ways to render models with a high degree of detail, to integrate Microsoft speech API to have expressive conversations with a bot interfaced over Facebook.</p>\n<h2>What's next for Triloka</h2>\n<p>The future for Triloka involves real-time implementation of the talk-bots which allows the user to choose, if he prefers, to talk to an actual therapist or a loved one. We are also excited to release this to the real world market and analyse how people react to this interactive experience. This would also give us more ideas to improve upon in Triloka 2.0</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nLosing a friend to the grim ailment of depression a few days back, got us thinking as to why there has not been an effective solution to such a common problem faced by the youth nowadays. Millions of lives could have been saved if there had been at least one person who they could've talked to without worrying about the stigma associated with mental health problems. Though there are multiple helplines and chatbots to help people with depression, the fear of your identity being revealed or being judged causes most victims to hesitate. Moreover, the unfriendly platform on which it is carried out also serves as a hindrance for people to open up.\n\n\n## What it does\n\n\nTriloka blends the three realms of the virtual, physical and the spiritual world to help the user address and overcome all the negative feelings that are a result of depression. The user goes through a series of interactions that help him prevail over his loneliness, sadness, guilt, disinterest in daily activities, inability to concentrate, and the feeling of hopelessness. Various environments, including a forest where people share their depression stories around a bonfire, a virtual therapist who suggests activities to help overcome depression, an exciting canoeing encounter that helps bring back the user's interest in fun activities, a peaceful mountain environment to encourage meditation, all the while listening to soothing music and a mood reflecting sky, adds to this therapeutic experience.\n\n\n**The negative feelings faced and the way they are tackled:**\n\n\n**Loneliness**: Meeting new people around the campfire and opening up without the fear of being judged or having \npreconceptions.\n**Sadness**: Lulling soft music in the background that calms the user\n**Guilt**: Listening to how other people overcame their depression and trying to empathize with them\n**Lack of Interest in daily activities**: Energetic activities like canoeing that can revitalise the mind\n**Difficulty concentrating**: Meditation session under serene conditions\n**Hopelessness**: Motivating and engaging incentives like unlocking more environments if positive thoughts are spoken, slowly giving hope.\n\n\n## How we built it\n\n\nUsed Microsoft Speech API for conversion of speech to text and vice-versa. We use it for conversing with the specialized (for depression) facebook bot and return the responses in voice using text to speech. IBM Watson's Tone Analyzer is used to track the user's mood and keep a measure of the score to reflect it on the sky colour.\nThe major aspect of this project that we spent most of our time building was the environments and the models associated with it. We imported free 3D models from the online website free3D.com and customised them to our requirements using the student version of Autodesk Maya. All the character animations and terrain systems were built using Maya and Unity engine to bring about a realistic VR experience.\n\n\n## Challenges we ran into\n\n\nWe brainstormed a lot on how to structure the interactions and environment such that it feels realistic enough to engage the user to take up multiple sessions which will ultimately help assuage depression. We had some trouble trying to stitch together each environment, interfacing them to provide an appealing storyline.\nWe ran out of time implementing the third environment involving the canoeing activity. Our members were inexperienced in character animation and thus spent a lot of time executing it. \n\n\n## Accomplishments that we're proud of\n\n\nWe are very proud of how we worked as a team and how our skills complemented each other to make this project.\nOur members successfully integrated speech-to-text and text-to-speech conversions for operating the talk-bot in a realistic manner. The environments, which we managed to build, are high-quality renderings that are also structured such that the user finds the environment engaging to return the next time. Enabling the users to have functional conversations with a specially trained bot, Wysa, that offers meaningful advice to get over depression. Learning and implementing character animation in such a short time.\n\n\n## What we learned\n\n\nWe learnt how to rig and animate humanoid characters, efficient ways to render models with a high degree of detail, to integrate Microsoft speech API to have expressive conversations with a bot interfaced over Facebook.\n\n\n## What's next for Triloka\n\n\nThe future for Triloka involves real-time implementation of the talk-bots which allows the user to choose, if he prefers, to talk to an actual therapist or a loved one. We are also excited to release this to the real world market and analyse how people react to this interactive experience. This would also give us more ideas to improve upon in Triloka 2.0\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rufus",
            "title": "Rufus",
            "blurb": "Opening a window into the universality of human relationships with cinematic VR.",
            "awards": [
                "1st Prize in Film & Advertisement"
            ],
            "videos": [
                "https://www.youtube.com/embed/jV_mWFT8bzE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "mikehaojiang",
                    "about": "As the only Software/Unity developer in this team, I developed, tested and deployed all code. I made sure that the visual content (3D models, 2D art) and audio content (sound/music) was integrated properly into the virtual world (Unity). Changing of lightings, shaders, skyboxes, activation/deactivation of GameObjects/Lights/Sound\n Sources was programmed/managed by me. I also made sure that the application ran smoothly on a HTC Vive. In fact, the application runs at 100 FPS. In order to manage the enormous amount of content generated by the film director, sound expert, graphics designer and tiltbrush artist, I wrote a framework from scratch to handle/manage this data.",
                    "photo": "https://avatars3.githubusercontent.com/u/16158407?height=180&v=4&width=180"
                },
                {
                    "name": "Haiden McGill",
                    "about": "",
                    "photo": "https://graph.facebook.com/v3.3/10208958771884851/picture?height=180&width=180"
                },
                {
                    "name": "wyattroy",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32600128?height=180&v=4&width=180"
                },
                {
                    "name": "Yanting Vanessa Cao",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Cc2LxDC8IQrEgSJrIc9LORH8IcrEg2UpC19wJZQ8oKyIgpoKacNkBsH8ecPIZxUuHQNHl0Fh_F_wUeESmkdJBJE26F_oUHmymkd6y4x3D6nQlOvhGzxepulrQtMV-HJSFFSdRozd-_9?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe-creative-suite",
                "htc-vive",
                "maya",
                "oculus",
                "protools",
                "soundly",
                "steam",
                "tiltbrush",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our country\u2019s nuclear arsenal is hair-triggered to a Tweet. We think the future of humanity relies on inventing a new way to communicate; one that allows people to share nuanced, empathetic, honest and vulnerable experiences, not short sharp words. We\u2019ve built the first step: a cinematic VR story that lets the viewer hear what others leave unsaid, and see what others hide inside. We were inspired by previous works including Dear Angelica and Pearl, which challenged traditional filmmaking techniques and began to build a new set of rules. Unlike those pieces, we are experimenting with ways to show a two-person conversation in 3D space, visualizing the characters\u2019 emotional states that their words cover up.</p>\n<h2>What it does</h2>\n<p>The viewer is a voyeur in the story of two lifelong friends sharing intimate memories of the house dog who almost tore them apart, but ultimately brought them closer together.</p>\n<h2>How we built it</h2>\n<p>We use Tiltbrush and Maya to render our characters\u2019 messy, intimate emotions and memories in physical metaphors, like color (red/blue), space (the milky way/a bedroom) and soundscapes. We used Unity and Vive to let the viewer explore this world in three dimensions, with six degrees of freedom. And we use a 40,000 year old form of virtual reality \u2014 story \u2014 to make you care. </p>\n<h2>Challenges we ran into</h2>\n<p>Our creative director wrote a screenplay on Friday night; however refining the story arc was a team effort that lasted until the afternoon on Saturday, eating up much of the time we had to execute it. Our developer is skilled at building applications in Unity; however this is the first time he is doing creative animation work, meaning he had to learn by doing. We were unable to find free acting talent on such late notice, so we had to act the piece ourselves. Ultimately, the biggest challenge was that our Tilt brush artist got sick on Sunday morning and he missed the whole day. This made it impossible to finish the piece. Both developer and sound designer were working simultaneously on the same scene but it was difficult to coordinate timing the audio asset. Because it\u2019s a linear film, not a game, implementing the spatial audio was challenging. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are amazed that we went from not knowing each other on Thursday to developing a full character-driven narrative together, and creating a piece of VR cinema in two days. To make it happen we needed expertise in color theory, audio engineering, fiction writing, computer programming, acting, 3D modelling, animation, how to operate and debug both Vive and Rift, and dozens of other skills that someone on the team had.</p>\n<h2>What we learned</h2>\n<p>Having a diversified team allowed us to work seamlessly and creatively -- however it also meant that there was only one expert in each domain, which was challenging when our tiltbrush artist got sick. Various team members learned basic tiltbrush skills, how to design 3D spaces, Vive setup, navigating Unity, how to collaborate to create authentic stories, and what VR gear is best suited for each purpose.</p>\n<h2>What's next for Rufus</h2>\n<p>We want to integrate real, photorealistic humans into these stories, mix illustration with reality; and craft longer stories that address a broad range of human experiences. Our goal is to develop a cinematic grammar for this new art form that allows anyone to communicate their internal experience empathetically and honestly. </p>\n<h2>Assets Resources</h2>\n<p>Pexel, freemusicarchive.org, Freepik, Soundly</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur country\u2019s nuclear arsenal is hair-triggered to a Tweet. We think the future of humanity relies on inventing a new way to communicate; one that allows people to share nuanced, empathetic, honest and vulnerable experiences, not short sharp words. We\u2019ve built the first step: a cinematic VR story that lets the viewer hear what others leave unsaid, and see what others hide inside. We were inspired by previous works including Dear Angelica and Pearl, which challenged traditional filmmaking techniques and began to build a new set of rules. Unlike those pieces, we are experimenting with ways to show a two-person conversation in 3D space, visualizing the characters\u2019 emotional states that their words cover up.\n\n\n## What it does\n\n\nThe viewer is a voyeur in the story of two lifelong friends sharing intimate memories of the house dog who almost tore them apart, but ultimately brought them closer together.\n\n\n## How we built it\n\n\nWe use Tiltbrush and Maya to render our characters\u2019 messy, intimate emotions and memories in physical metaphors, like color (red/blue), space (the milky way/a bedroom) and soundscapes. We used Unity and Vive to let the viewer explore this world in three dimensions, with six degrees of freedom. And we use a 40,000 year old form of virtual reality \u2014 story \u2014 to make you care. \n\n\n## Challenges we ran into\n\n\nOur creative director wrote a screenplay on Friday night; however refining the story arc was a team effort that lasted until the afternoon on Saturday, eating up much of the time we had to execute it. Our developer is skilled at building applications in Unity; however this is the first time he is doing creative animation work, meaning he had to learn by doing. We were unable to find free acting talent on such late notice, so we had to act the piece ourselves. Ultimately, the biggest challenge was that our Tilt brush artist got sick on Sunday morning and he missed the whole day. This made it impossible to finish the piece. Both developer and sound designer were working simultaneously on the same scene but it was difficult to coordinate timing the audio asset. Because it\u2019s a linear film, not a game, implementing the spatial audio was challenging. \n\n\n## Accomplishments that we're proud of\n\n\nWe are amazed that we went from not knowing each other on Thursday to developing a full character-driven narrative together, and creating a piece of VR cinema in two days. To make it happen we needed expertise in color theory, audio engineering, fiction writing, computer programming, acting, 3D modelling, animation, how to operate and debug both Vive and Rift, and dozens of other skills that someone on the team had.\n\n\n## What we learned\n\n\nHaving a diversified team allowed us to work seamlessly and creatively -- however it also meant that there was only one expert in each domain, which was challenging when our tiltbrush artist got sick. Various team members learned basic tiltbrush skills, how to design 3D spaces, Vive setup, navigating Unity, how to collaborate to create authentic stories, and what VR gear is best suited for each purpose.\n\n\n## What's next for Rufus\n\n\nWe want to integrate real, photorealistic humans into these stories, mix illustration with reality; and craft longer stories that address a broad range of human experiences. Our goal is to develop a cinematic grammar for this new art form that allows anyone to communicate their internal experience empathetically and honestly. \n\n\n## Assets Resources\n\n\nPexel, freemusicarchive.org, Freepik, Soundly\n\n\n"
        },
        {
            "source": "https://devpost.com/software/limbpossibleproject",
            "title": "Limbpossible Project",
            "blurb": "Action Bear wants to be human and has to overcome some puzzles to open the box w/ a heart by finding useful limbs.",
            "awards": [
                "1st Prize in Gaming/Entertainment"
            ],
            "videos": [
                "https://www.youtube.com/embed/TNFyZJJC58s?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/933/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/935/datas/original.jpg"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/936/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/937/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/938/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/939/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/940/datas/original.png"
                },
                {
                    "title": "Limbpossible Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/941/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "anselmhook",
                    "about": "i do the things to the things",
                    "photo": "https://graph.facebook.com/10155554830751539/picture?height=180&width=180"
                },
                {
                    "name": "Lucas Rizzotto",
                    "about": "i did things",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/457/250/datas/profile.jpg"
                },
                {
                    "name": "Dulce Baerga",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Og-UycNHXN7SfWOgNgEUBNedhCOAuJuDB2HUViddh3gKudYANgHVcigHGkSAdd8Al2EVl8teeiprEj1T-acbNi0kDiplEjVAqacZxGoW6Gdj8I3SZxT90nXZSn6TijShgSPB4yGdKQi?height=180&width=180"
                },
                {
                    "name": "Sam Barnes",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9edf852846386bace64197a7c3823431?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "blender",
                "oculus",
                "photoshop",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>The Limbpossible Project</h2>\n<p>This is the story of Action Bear created by Mego in 1949, a forward-thinking but now defunct toy company. Action Bear was a new toy intended to make children feel safe in their homes in light of cold war hysteria.</p>\n<p>As Action Bear you puzzle your way out of a toy factory to the real world, adding and removing body parts as you go. Pick limbs up and try out their powers.  Hold your eyeball in your hand to peek around a corner.  Discover if you have a heart. Explore identity, cybernetics, body dysmorphia and what it means to be embodied.</p>\n<p><strong>Do the limbpossible</strong>\nLimb swapping gameplaymechanics\nThrowing eyeball to stick to a wall to view passwords\nAttaching tools to your arm \nLaunch hand off to reach levers </p>\n<p><strong>Back story</strong>\nIn post-WW2 United States, a foward-thinking toy manufacturer named Mego tries to build a new toy named \"Action Bear\" to make children feel safe at their homes.  You play as a prototype Action Bear, being tested through numerous scenarios where you must switch limbs and use them in creative ways to progress. Players can remove eyeballs from their skull to see around corners, amputate their hands and throw them in the distance to reach far away levers, and avoid certain death with a number of h\u0336i\u0336g\u0336h\u0336l\u0336y\u0336 \u0336d\u0336a\u0336n\u0336g\u0336e\u0336r\u0336o\u0336u\u0336s\u0336  family friendly tools.  At the end of the level players must perform a heart transplant on themselves. Because toys aren't much if they don't have a little bit of heart in them.</p>\n<p>This was inspired by a REAL TOY MANUFACTURER COMPANY named Mego who went bankrupt in the 70's due to some very creepy toys. Each level contains bits of the company's history so players can soak in the creepiness.  We also have what we believe is this hackathon's top thematic tracklist, with hit old tracks such as \"I've Got You Under My Skin\", \"Jeepers Creepers\", \"I don't want to set the world on fire\", \"If I give my heart to you\" and more to accompany you in your limb-bending adventure.</p>\n<p><strong>Design Goals</strong>\nWe feel there isn't enough experimentation in VR and new media. Developers have been hesitant to put everything on the line to explore risky game mechanics. We are admonitioned against exploring or playing with changing the players view or changing their appearance suddenly. There are new interaction methods waiting to be discovered that have yet to be attempted. The rules of the real world do not have to apply to VR. We are more flexible and less bound to our conventional identities than we may think.</p>\n<p><strong>What we've done</strong>\nA novel game mechanic\nCreated 3 levels\nCreated a heart retrieval system \nAwesome story and sound\nA hook system for changing body parts\nA cute robot model \nEye separation in Unity - breaking the normal VR paradigm.</p>\n<p><strong>Challenges and Wins</strong>\nTurns out that taking control of the camera from the Vive is tricky.\nBuilding a complete experience to explore the thesis was challenging.\nThe game play fell naturally out of the deep narrative.\nWe found the game fun to play ourselves and surprisingly addictive.\nWe got to know each other!</p>\n<p><strong>What's next</strong>\nMulti-participant support and team challenges.\nDeeper interrogations of identity; changing every body part.\nWe learned VRTK which is a great VR helper toolkit.</p>\n<p><strong>Freely redistributable assets we used:</strong>\nPost Processing Stack: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/83912\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/83912</a>\nCardboard Boxes Pack: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/30695\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/30695</a>\nCannon on a Platform: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/57534\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/57534</a>\nHammer PBR: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/66110\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/66110</a>\nOld Radio: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/72923\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/72923</a>\nSteamVR Plugin: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/32647\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/32647</a>\nVRTK: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/64131\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/64131</a>\nWooden Chair: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/848\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/848</a>\nBrain Meal: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/89596\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/89596</a>\nTanto Knife: <a href=\"https://www.assetstore.unity3d.com/en/#!/content/83268\" rel=\"nofollow\">https://www.assetstore.unity3d.com/en/#!/content/83268</a></p>\n<p>The rest of the 3D &amp; 2D art was created by Dulce Baerga &amp; Lucas Rizzotto</p>\n<p><strong>Tools that we used:</strong>\nPrograms used:\nUnity3D\nAdobe Photoshop\nBlender\nHTC Vive\nOculus Rift</p>\n</div>",
            "content_md": "\n## The Limbpossible Project\n\n\nThis is the story of Action Bear created by Mego in 1949, a forward-thinking but now defunct toy company. Action Bear was a new toy intended to make children feel safe in their homes in light of cold war hysteria.\n\n\nAs Action Bear you puzzle your way out of a toy factory to the real world, adding and removing body parts as you go. Pick limbs up and try out their powers. Hold your eyeball in your hand to peek around a corner. Discover if you have a heart. Explore identity, cybernetics, body dysmorphia and what it means to be embodied.\n\n\n**Do the limbpossible**\nLimb swapping gameplaymechanics\nThrowing eyeball to stick to a wall to view passwords\nAttaching tools to your arm \nLaunch hand off to reach levers \n\n\n**Back story**\nIn post-WW2 United States, a foward-thinking toy manufacturer named Mego tries to build a new toy named \"Action Bear\" to make children feel safe at their homes. You play as a prototype Action Bear, being tested through numerous scenarios where you must switch limbs and use them in creative ways to progress. Players can remove eyeballs from their skull to see around corners, amputate their hands and throw them in the distance to reach far away levers, and avoid certain death with a number of h\u0336i\u0336g\u0336h\u0336l\u0336y\u0336 \u0336d\u0336a\u0336n\u0336g\u0336e\u0336r\u0336o\u0336u\u0336s\u0336 family friendly tools. At the end of the level players must perform a heart transplant on themselves. Because toys aren't much if they don't have a little bit of heart in them.\n\n\nThis was inspired by a REAL TOY MANUFACTURER COMPANY named Mego who went bankrupt in the 70's due to some very creepy toys. Each level contains bits of the company's history so players can soak in the creepiness. We also have what we believe is this hackathon's top thematic tracklist, with hit old tracks such as \"I've Got You Under My Skin\", \"Jeepers Creepers\", \"I don't want to set the world on fire\", \"If I give my heart to you\" and more to accompany you in your limb-bending adventure.\n\n\n**Design Goals**\nWe feel there isn't enough experimentation in VR and new media. Developers have been hesitant to put everything on the line to explore risky game mechanics. We are admonitioned against exploring or playing with changing the players view or changing their appearance suddenly. There are new interaction methods waiting to be discovered that have yet to be attempted. The rules of the real world do not have to apply to VR. We are more flexible and less bound to our conventional identities than we may think.\n\n\n**What we've done**\nA novel game mechanic\nCreated 3 levels\nCreated a heart retrieval system \nAwesome story and sound\nA hook system for changing body parts\nA cute robot model \nEye separation in Unity - breaking the normal VR paradigm.\n\n\n**Challenges and Wins**\nTurns out that taking control of the camera from the Vive is tricky.\nBuilding a complete experience to explore the thesis was challenging.\nThe game play fell naturally out of the deep narrative.\nWe found the game fun to play ourselves and surprisingly addictive.\nWe got to know each other!\n\n\n**What's next**\nMulti-participant support and team challenges.\nDeeper interrogations of identity; changing every body part.\nWe learned VRTK which is a great VR helper toolkit.\n\n\n**Freely redistributable assets we used:**\nPost Processing Stack: <https://www.assetstore.unity3d.com/en/#!/content/83912>\nCardboard Boxes Pack: <https://www.assetstore.unity3d.com/en/#!/content/30695>\nCannon on a Platform: <https://www.assetstore.unity3d.com/en/#!/content/57534>\nHammer PBR: <https://www.assetstore.unity3d.com/en/#!/content/66110>\nOld Radio: <https://www.assetstore.unity3d.com/en/#!/content/72923>\nSteamVR Plugin: <https://www.assetstore.unity3d.com/en/#!/content/32647>\nVRTK: <https://www.assetstore.unity3d.com/en/#!/content/64131>\nWooden Chair: <https://www.assetstore.unity3d.com/en/#!/content/848>\nBrain Meal: <https://www.assetstore.unity3d.com/en/#!/content/89596>\nTanto Knife: <https://www.assetstore.unity3d.com/en/#!/content/83268>\n\n\nThe rest of the 3D & 2D art was created by Dulce Baerga & Lucas Rizzotto\n\n\n**Tools that we used:**\nPrograms used:\nUnity3D\nAdobe Photoshop\nBlender\nHTC Vive\nOculus Rift\n\n\n"
        },
        {
            "source": "https://devpost.com/software/barricade",
            "title": "Barricade",
            "blurb": "The classic game rampart gets a modern interpretation with spacial mapping and 3d awareness in the Hololens.",
            "awards": [
                "Best Microsoft Mixed Reality project"
            ],
            "videos": [
                "https://www.youtube.com/embed/yq26Cb4TND0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Level generation based on your environment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/873/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Scott Niejadlik",
                    "about": "Original concept creation, team management, world space angle and position shader coding,  extraction of the meshes after spatial understanding is complete in offer to turn them into game object locations. helping concept the algorithms and helped code for the grid based solution solving tiles, modelled the simple structures presented, textured a few of them.",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GiY3WX8vYskRERZ8mqDEx8TqszGipo1hZzuyH3R4A?height=180&width=180"
                },
                {
                    "name": "Tin Chag",
                    "about": "Spatial Mapping + Understanding,\nNetworking, \nMenu UI.",
                    "photo": "https://www.gravatar.com/avatar/de2a1e57672f86438dbae1a0c307ad45?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "F O",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/980/054/datas/profile.jpg"
                },
                {
                    "name": "Sean Nealon",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/18402909?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>On the night of January 3rd, before any of us ever knew each other we all woke up in a cold sweat.  The image of castles on a majestic peak overlooking a mighty ocean.  On the horizon, pirate ships emerged from the briney deep.  We were all shocked to realize that we all shared this same vision.  We knew we had to build this experience, like the misty morning ocean breeze, it stirred out spirits and inspired us to take on this mighty challenge.</p>\n<h2>What it does</h2>\n<p>Quick to the Baracades!  Scan your immediate environment and watch it transform into a lush and tropical archipelago, your furniture transforms into mighty cliffs rising out of the oceans. Prime territory for your defensive positions from the the fearsome pirates that want to destroy your lovely tropical paradise. Construct your Barricades and fortify your towns in a strategic race to construct.  Then prepare your defenses and destroy the enemy.  Rinse and repeat till you are either dead or king of the oceans.</p>\n<h2>How we built it</h2>\n<p>Red bull, Unity, C#, Hololens, SMiles, Free food, Swag, and Charisma.  Also a healthy dose of human adrenal glands.</p>\n<h2>Challenges we ran into</h2>\n<p>Tile based placement and algorithms to determine when our constructions are complete.  Shader complexity for painting the environment based on angles and postions.  Spatial mapping and understanding to help identify prime locations for placement of assets.  Sharing services is very hard, we were able to get a basic version of it working which is impressive in it's own right. </p>\n<p>More to come tomorrow.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Spatial mapping used to generate the playing level, level calculations in order to determine castle placement viability on the grid, sharing services in order to allow a very rudimentary coop mode. </p>\n<h2>What we learned</h2>\n<p>Spatial understanding queries and extracting meshes from the mixed reality toolkit.  Networked sharing of holograms and interfaces.  Explored complex shader algorithms in order to create adaptive environments.  Learning Fill hole algorithms and custom building a prototype to mimic the original rampart style castle build phases. The the hololens continues to amaze us with its capabilities and feeling of living in the future.</p>\n<h2>What's next for Barricade</h2>\n<p>Expanding the scope of the game engine, allowing for full placement of castle pieces and ship spawning, full combat mode, sharing coop more, and network play.  </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOn the night of January 3rd, before any of us ever knew each other we all woke up in a cold sweat. The image of castles on a majestic peak overlooking a mighty ocean. On the horizon, pirate ships emerged from the briney deep. We were all shocked to realize that we all shared this same vision. We knew we had to build this experience, like the misty morning ocean breeze, it stirred out spirits and inspired us to take on this mighty challenge.\n\n\n## What it does\n\n\nQuick to the Baracades! Scan your immediate environment and watch it transform into a lush and tropical archipelago, your furniture transforms into mighty cliffs rising out of the oceans. Prime territory for your defensive positions from the the fearsome pirates that want to destroy your lovely tropical paradise. Construct your Barricades and fortify your towns in a strategic race to construct. Then prepare your defenses and destroy the enemy. Rinse and repeat till you are either dead or king of the oceans.\n\n\n## How we built it\n\n\nRed bull, Unity, C#, Hololens, SMiles, Free food, Swag, and Charisma. Also a healthy dose of human adrenal glands.\n\n\n## Challenges we ran into\n\n\nTile based placement and algorithms to determine when our constructions are complete. Shader complexity for painting the environment based on angles and postions. Spatial mapping and understanding to help identify prime locations for placement of assets. Sharing services is very hard, we were able to get a basic version of it working which is impressive in it's own right. \n\n\nMore to come tomorrow.\n\n\n## Accomplishments that we're proud of\n\n\nSpatial mapping used to generate the playing level, level calculations in order to determine castle placement viability on the grid, sharing services in order to allow a very rudimentary coop mode. \n\n\n## What we learned\n\n\nSpatial understanding queries and extracting meshes from the mixed reality toolkit. Networked sharing of holograms and interfaces. Explored complex shader algorithms in order to create adaptive environments. Learning Fill hole algorithms and custom building a prototype to mimic the original rampart style castle build phases. The the hololens continues to amaze us with its capabilities and feeling of living in the future.\n\n\n## What's next for Barricade\n\n\nExpanding the scope of the game engine, allowing for full placement of castle pieces and ship spawning, full combat mode, sharing coop more, and network play. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/anonymousraccoon",
            "title": "Kolideo by AnonymousRaccoon ",
            "blurb": "Augmented Reality Educational Content Platform With Real-Time Analytics On Student Learning",
            "awards": [
                "Best use of Merge VR"
            ],
            "videos": [
                "https://player.vimeo.com/video/237345198?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Example of insights generated by app",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/797/datas/original.png"
                },
                {
                    "title": "Logo ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/810/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Jacqueline Assar",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/014/datas/profile.jpg"
                },
                {
                    "name": "Kat Schneider",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/015/datas/profile.jpg"
                },
                {
                    "name": "Kachina Studer",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/f5fb119e0b0ac4f2c374b05ee9105a96?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jacqueline Assar",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/693/datas/profile.jpeg"
                },
                {
                    "name": "Anish Dhesikan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/213/datas/profile.jpg"
                }
            ],
            "built_with": [
                "arcore",
                "blender",
                "merge-cube",
                "rhino",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>In life outside of school, we rarely succeed based on our ability to output a single correct answer, we persist in a gray area and there is more than one \u201ccorrect\u201d answer or outcome to arrive at. However, our education system holds the standardized multiple choice assessment exams on a pedestal. This is not only detrimental to the students but it has also been documented to be discouraging to different types of learners that display their \u201cintelligence\u201d and \u201ccompetence\u201d in non-classical ways. </p>\n<p>Kolideo seeks to be the first-ever AR learning platform that is based on and can run real-time analytics on the learners themselves - not on their \u201canswers\u201d but on their process to their answers. We were inspired by the fact that through mobile AR, our phones can observe our behavior and make us better. We want to use this power for education. </p>\n<h2>What it does</h2>\n<p>We focused on building Kolideo for the Merge Cube with mobile AR because we want to make sure that it is accessible cost-point and hardware wise to students from all socioeconomic backgrounds. Our platform includes  educational puzzles in the frontend and a machine learning based backend to run analytics on the student as they progress through the puzzle. Our app can let a teacher know items of interest like how long a student took, how many tries it took, how they went about solving the puzzle itself, which parts of the puzzle they excelled at etc. Ultimately, these observations can lead to insights for the learner and the teacher. </p>\n<h2>How we built it</h2>\n<p>We used the Merge Cube SDK, Unity, Photon, Vuforia, AR Core and the Samsung S8. We spent a lot of time ideating on the best demo for a puzzle. We settled on a geometry puzzle that teaches students about spatial learning and that requires trial and error based problem solving. This was done intentionally so that we could generate insights for the teacher/learner based on how the student progresses through the puzzle. We built all the assets ourselves. </p>\n<h2>Challenges we ran into</h2>\n<p>Originally, we wanted to create a version for the Meta (so we could demo \"low-end AR\" and \"high-end VR\"  but we ran into issues since they do not have Vuforia support. Therefore, we decided to focus on the Merge for the purposes of the hackathon. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're proud of our experimentation into running real-time analytics on AR experiences. We believe that mobile AR is a powerful tool for observing human behavior and we are excited about applying this tool to education. </p>\n<h2>What we learned</h2>\n<p>This was all of our first times developing for mobile AR so we learned about the pros/cons compared to the HoloLens which we usually develop with. Ultimately, we believe these tradeoffs were necessary because our ultimate goal with Kolideo is accessibility. </p>\n<h2>What's next for AnonymousRaccoon</h2>\n<p>We would like to add more educational puzzles, fully buildout our analytics backend and add functionality into the app that suggests next learning steps. We would also like to build a function that links students to other students who have complimentary learning techniques to facilitate peer-peer group learning. We would like to partner with experts in the education space to refine and fully build out our metric. We want this \u201cmetric\u201d to be different from the one size fits all of the traditional skills assessment since this metric is based on how the student processes information or thinks as opposed to only being about what they \u201cknow\u201d. We believe that observational AR is uniquely poised for figuring out this metric. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nIn life outside of school, we rarely succeed based on our ability to output a single correct answer, we persist in a gray area and there is more than one \u201ccorrect\u201d answer or outcome to arrive at. However, our education system holds the standardized multiple choice assessment exams on a pedestal. This is not only detrimental to the students but it has also been documented to be discouraging to different types of learners that display their \u201cintelligence\u201d and \u201ccompetence\u201d in non-classical ways. \n\n\nKolideo seeks to be the first-ever AR learning platform that is based on and can run real-time analytics on the learners themselves - not on their \u201canswers\u201d but on their process to their answers. We were inspired by the fact that through mobile AR, our phones can observe our behavior and make us better. We want to use this power for education. \n\n\n## What it does\n\n\nWe focused on building Kolideo for the Merge Cube with mobile AR because we want to make sure that it is accessible cost-point and hardware wise to students from all socioeconomic backgrounds. Our platform includes educational puzzles in the frontend and a machine learning based backend to run analytics on the student as they progress through the puzzle. Our app can let a teacher know items of interest like how long a student took, how many tries it took, how they went about solving the puzzle itself, which parts of the puzzle they excelled at etc. Ultimately, these observations can lead to insights for the learner and the teacher. \n\n\n## How we built it\n\n\nWe used the Merge Cube SDK, Unity, Photon, Vuforia, AR Core and the Samsung S8. We spent a lot of time ideating on the best demo for a puzzle. We settled on a geometry puzzle that teaches students about spatial learning and that requires trial and error based problem solving. This was done intentionally so that we could generate insights for the teacher/learner based on how the student progresses through the puzzle. We built all the assets ourselves. \n\n\n## Challenges we ran into\n\n\nOriginally, we wanted to create a version for the Meta (so we could demo \"low-end AR\" and \"high-end VR\" but we ran into issues since they do not have Vuforia support. Therefore, we decided to focus on the Merge for the purposes of the hackathon. \n\n\n## Accomplishments that we're proud of\n\n\nWe're proud of our experimentation into running real-time analytics on AR experiences. We believe that mobile AR is a powerful tool for observing human behavior and we are excited about applying this tool to education. \n\n\n## What we learned\n\n\nThis was all of our first times developing for mobile AR so we learned about the pros/cons compared to the HoloLens which we usually develop with. Ultimately, we believe these tradeoffs were necessary because our ultimate goal with Kolideo is accessibility. \n\n\n## What's next for AnonymousRaccoon\n\n\nWe would like to add more educational puzzles, fully buildout our analytics backend and add functionality into the app that suggests next learning steps. We would also like to build a function that links students to other students who have complimentary learning techniques to facilitate peer-peer group learning. We would like to partner with experts in the education space to refine and fully build out our metric. We want this \u201cmetric\u201d to be different from the one size fits all of the traditional skills assessment since this metric is based on how the student processes information or thinks as opposed to only being about what they \u201cknow\u201d. We believe that observational AR is uniquely poised for figuring out this metric. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/playarworld",
            "title": "playARworld - Levels",
            "blurb": "An active learning AR game that encourages users to move through space. Like Guitar Hero x Temple Run x Pokemon Go.",
            "awards": [
                "Merge VR sponsored 2nd prize"
            ],
            "videos": [
                "https://www.youtube.com/embed/F2AaQBxVGos?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Playar World Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/679/datas/original.png"
                },
                {
                    "title": "Spatial UI",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/689/datas/original.png"
                },
                {
                    "title": "Progression Logic",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/687/datas/original.png"
                },
                {
                    "title": "Pass Fail Logic",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/686/datas/original.png"
                },
                {
                    "title": "Prototype Screenshot",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/697/datas/original.png"
                },
                {
                    "title": "Collaboration",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/758/datas/original.JPG"
                },
                {
                    "title": "Ideation",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/759/datas/original.JPG"
                },
                {
                    "title": "Whiteboard",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/748/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Kyle Greenberg",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/5355055?height=180&v=4&width=180"
                },
                {
                    "name": "Hayoun Oh",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/18452224?height=180&v=4&width=180"
                },
                {
                    "name": "Marcel .",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/959/177/datas/profile.png"
                },
                {
                    "name": "Samuel Brewton",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/564/datas/profile.png"
                }
            ],
            "built_with": [
                "arcore",
                "blood",
                "creativity",
                "merge",
                "merge3d",
                "mergevr",
                "sweat",
                "tears",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Seeing learning as central to vocational and self growth, we set out to elevate education. We asked \u201cWhat if we take bring curriculum beyond the edges of books and into physical space?\u201d Through a mind meld hotter than lava, team PlayARWorld is ecstatic to announce the first prototype of this platform \u201cLevels v1.0\u201d</p>\n<h2>What it does</h2>\n<p>PlayARWorld\u2019s Levels is an educational AR game that encourages users to move through space to learn new topics and test themselves.\nPeople are products of their environment and we are empowering people to reach new Levels of learning by upgrading their environment and exploring the possibilities with active and augmented learning. \nThis is also the first step in building an educational game engine where users can upload curriculum to generate tests for themselves or others. Making this open source, we hope people will author their own Levels.</p>\n<p>We see this being used in the education space, training for industry and government, and even for entertainment in social settings like parties.\nPlease take the  first step into the next way of learning with Levels by PlayARWorld.\nFor ages 5 up.</p>\n<h2>How we built it</h2>\n<ul>\n<li>Using Unity as our game development engine</li>\n<li>ARCore on Google Pixel</li>\n<li>Sweat</li>\n<li>Iteration</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Deploying to ARCore to Samsung Galaxy S8</li>\n<li>Examining Mapbox as a Unity plugin that we eventually decided was not core to our minimum viable experience</li>\n<li>Concept solidification</li>\n</ul>\n<h2>Accomplishments that we\u2019re proud of</h2>\n<ul>\n<li>Deploying a build to ARCore</li>\n<li>Collaborating at a high level on a new form of media</li>\n<li>Pushing education and interaction</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Unity networking limitations and options</li>\n<li>Vuforia and Merge Cube SDKs </li>\n<li>Mapbox SDK in conjunction with Unity</li>\n<li>Ideation to development</li>\n<li>Deeper knowledge about AR tracking</li>\n<li>Better understanding of the game development process</li>\n</ul>\n<h2>What\u2019s next for playARworld</h2>\n<ol>\n<li>Making Levels an educational game engine by allowing user to upload their own questions and answers</li>\n<li>Implementing a multiplayer setting to make the educational experience social, bringing net-zero sum competition to education in AR</li>\n</ol>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSeeing learning as central to vocational and self growth, we set out to elevate education. We asked \u201cWhat if we take bring curriculum beyond the edges of books and into physical space?\u201d Through a mind meld hotter than lava, team PlayARWorld is ecstatic to announce the first prototype of this platform \u201cLevels v1.0\u201d\n\n\n## What it does\n\n\nPlayARWorld\u2019s Levels is an educational AR game that encourages users to move through space to learn new topics and test themselves.\nPeople are products of their environment and we are empowering people to reach new Levels of learning by upgrading their environment and exploring the possibilities with active and augmented learning. \nThis is also the first step in building an educational game engine where users can upload curriculum to generate tests for themselves or others. Making this open source, we hope people will author their own Levels.\n\n\nWe see this being used in the education space, training for industry and government, and even for entertainment in social settings like parties.\nPlease take the first step into the next way of learning with Levels by PlayARWorld.\nFor ages 5 up.\n\n\n## How we built it\n\n\n* Using Unity as our game development engine\n* ARCore on Google Pixel\n* Sweat\n* Iteration\n\n\n## Challenges we ran into\n\n\n* Deploying to ARCore to Samsung Galaxy S8\n* Examining Mapbox as a Unity plugin that we eventually decided was not core to our minimum viable experience\n* Concept solidification\n\n\n## Accomplishments that we\u2019re proud of\n\n\n* Deploying a build to ARCore\n* Collaborating at a high level on a new form of media\n* Pushing education and interaction\n\n\n## What we learned\n\n\n* Unity networking limitations and options\n* Vuforia and Merge Cube SDKs\n* Mapbox SDK in conjunction with Unity\n* Ideation to development\n* Deeper knowledge about AR tracking\n* Better understanding of the game development process\n\n\n## What\u2019s next for playARworld\n\n\n1. Making Levels an educational game engine by allowing user to upload their own questions and answers\n2. Implementing a multiplayer setting to make the educational experience social, bringing net-zero sum competition to education in AR\n\n\n"
        },
        {
            "source": "https://devpost.com/software/cocare",
            "title": "Homeward",
            "blurb": "We're using two Thetas to rapidly capture the geometry of a space in order to view that space as a 3D model in AR.",
            "awards": [
                "Best use of Theta camera"
            ],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Catherine Eng \u4f0d\u5029\u96ef",
                    "about": "I did research and preliminary drafts of iconography that correlated with the FEMA task force X-codes and that could be used in an AR or VR environment. https://docs.google.com/presentation/d/1xFEOIUTWhvj1Cvcr8kFtoq8UzSJBEoF2lwfqp92U72k/edit?usp=sharing",
                    "photo": "https://media.licdn.com/mpr/mprx/0_10oZdXrBQZ0KTyOR1RynFPvBbuP_b2Ccxm-92lqnbjTGbpPcxmrnIhJnLsuKTy-9PRynehs9fjlCXHyQyufk3rUVEjliXHcB0ufUoAmcTpk7aO2IPjMzE586oew1SH8QnfwNSiPXyDZ?height=180&width=180"
                },
                {
                    "name": "mrdr86",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32602888?height=180&v=4&width=180"
                },
                {
                    "name": "Hisham Bedri",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8356b048c3efac6746982ff1e0a6e593?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "YUNMENGDAI0130",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/32598109?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "photoshop",
                "ricoh-theta-360",
                "solidworks",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We're inspired by the resilience of hurricane survivors and the courage of search and rescue teams. </p>\n<p>Specifically, we were drawn to the story of <a href=\"https://www.washingtonpost.com/news/the-fix/wp/2015/08/29/the-hurricane-katrina-x-codes-art-politics-controversy-and-now-reform/?utm_term=.518f4a680b7d\" rel=\"nofollow\">\"X-codes\"</a> - also called \"Katrina crosses,\" but officially called \"search codes\" - that were spray painted on home after home in New Orleans by FEMA's Search and Rescue teams to communicate important information in the wake of Hurricane Katrina. Each quadrant of the X contained letters and numbers that explained the date the home was searched, which team conducted the search, the types of hazards present, and the number of bodies found inside -- alive and dead. </p>\n<p>For many in New Orleans, the X's have become controversial and political symbols that tell the tragic story of an entire city. Some people view them as reminders of the government's failure - and the people's will to survive. And others, their trauma and loss. But for all New Orleanians who returned home, the X-codes remain a visceral reminder of Hurricane Katrina's unimaginable toll taken on block-after-block of communities.</p>\n<p>Reading their stories, we wanted to create an AR tool that could help first-responders - like FEMA's Search and Rescue Teams - quickly capture geometric data of the damage done to the city's streets, neighborhoods, and homes and share it with displaced survivors. This way, we might be able to improve the quality of data being captured while also reduce survivors' anxiety by allowing them to experience and anticipate what to expect when they return home.</p>\n<p>We also imagined other use cases for the technology, including journalism, education, real estate, and tourism.</p>\n<p>Here's our video! <a href=\"https://youtu.be/aOhv9TxXIv8\" rel=\"nofollow\">https://youtu.be/aOhv9TxXIv8</a></p>\n<h2>What it does</h2>\n<p>Homeward uses two Ricoh Thetas (or a single vuze) to rapidly capture the geometry of a space in order to view that space as a 3D model in AR. We developed a novel processing pipeline to generate low-poly 3D meshes from stereo panoramas. </p>\n<p>The user takes two panoramas, separated by a certain distance. We built a slider to make this process easier. In the future, this can be replaced by two relatively cheap panoramic cameras (each would cost $80-$200). After which, the user can upload the photos (or video frames!) to the algorithm.</p>\n<p>We start with finding the stereo disparity between images using a block matching algorithm (implemented in matlab). From the disparity, we perform filtering to produce a better depth map. Note that the depth map is now in an flat 360 equirectangular projection format. </p>\n<p>We fixed erroneous points in the depth map by running a standard deviation filter on one o the color images. The system requres texture to work, so we weight lower areas with low texture.</p>\n<p>From there, we could unwrap it to generate a dense point cloud. It was really challenging generating a satisfying mesh through standard poisson surface reconstruction. Instead, we used a method by sampling the vertex indices in the flat equirectangular depth image. Then we found a standard grid for the mesh indices. From there, we transformed the vertex positions. this led to a much more satisfying mesh.</p>\n<p>We then displayed it all using Vuforia on Unity on a phone. We implemented pinch to zoom, as well as a left and right button for rotation.</p>\n<p>In addition, we implemented a pointer animation where if you click on the screen, we allow a pin-point to fall from the sky. this was fun!</p>\n<p>The system is limited when there's not enough texture. We have ideas of solving that by doing segmentation of regions of the image first, or by using deep learning. Stay tuned.</p>\n<p>Many of the memers of our team were completely new to unity/coding. At first we tried to use the mapbox api, but found it very complicated, and scrapped showing a terrain. However, team members did learn the basics and how to implement simple yet awesome animations through scripting.</p>\n<p>We made it!</p>\n<h2>Challenges we ran into</h2>\n<p>Lots!</p>\n<p>We discussed who our end user was - whether first responders or family's wanting to capture the condition of their home.</p>\n<p>We critiqued on offline situations.</p>\n<p>We critiqued on scales.</p>\n<p>We tried to create elevation maps on MapBox but ditched the program (which we loved!).</p>\n<p>We tried to incorporate the use of tools.</p>\n<p>We tried to set accurate locations in our code.</p>\n<p>We tried to generate patterns.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were a group that began walking from table to table on Saturday assembling a team and looking for design help, and we got to meet two awesome designers in the process!</p>\n<p>We're really proud that it even worked at all. We really weren't sure if we could do it in 48 hours. In fact, we weren't sure this morning we could do it. But this was super dope, and we had a great time meeting, learning, and teaching. </p>\n<p>We're proud that we bring our idea out in 48 hours.</p>\n<p>We're super proud of the code that was created.</p>\n<p>We're proud that our team learned Unity.</p>\n<p>We're proud that we built something for AR Good!</p>\n<h2>What we learned</h2>\n<p>We learned how to use multiple programs.</p>\n<p>We learned how is the potential of the code that we created.</p>\n<p>We learned many from other teams.</p>\n<p>We learned that everyone in the team is important.</p>\n<h2>What's next for Homeward</h2>\n<p>Next up, we're fine-tuning our algorithm, and doing more research into the needs of hurricane victims and search-and-rescue first-responders.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe're inspired by the resilience of hurricane survivors and the courage of search and rescue teams. \n\n\nSpecifically, we were drawn to the story of [\"X-codes\"](https://www.washingtonpost.com/news/the-fix/wp/2015/08/29/the-hurricane-katrina-x-codes-art-politics-controversy-and-now-reform/?utm_term=.518f4a680b7d) - also called \"Katrina crosses,\" but officially called \"search codes\" - that were spray painted on home after home in New Orleans by FEMA's Search and Rescue teams to communicate important information in the wake of Hurricane Katrina. Each quadrant of the X contained letters and numbers that explained the date the home was searched, which team conducted the search, the types of hazards present, and the number of bodies found inside -- alive and dead. \n\n\nFor many in New Orleans, the X's have become controversial and political symbols that tell the tragic story of an entire city. Some people view them as reminders of the government's failure - and the people's will to survive. And others, their trauma and loss. But for all New Orleanians who returned home, the X-codes remain a visceral reminder of Hurricane Katrina's unimaginable toll taken on block-after-block of communities.\n\n\nReading their stories, we wanted to create an AR tool that could help first-responders - like FEMA's Search and Rescue Teams - quickly capture geometric data of the damage done to the city's streets, neighborhoods, and homes and share it with displaced survivors. This way, we might be able to improve the quality of data being captured while also reduce survivors' anxiety by allowing them to experience and anticipate what to expect when they return home.\n\n\nWe also imagined other use cases for the technology, including journalism, education, real estate, and tourism.\n\n\nHere's our video! <https://youtu.be/aOhv9TxXIv8>\n\n\n## What it does\n\n\nHomeward uses two Ricoh Thetas (or a single vuze) to rapidly capture the geometry of a space in order to view that space as a 3D model in AR. We developed a novel processing pipeline to generate low-poly 3D meshes from stereo panoramas. \n\n\nThe user takes two panoramas, separated by a certain distance. We built a slider to make this process easier. In the future, this can be replaced by two relatively cheap panoramic cameras (each would cost $80-$200). After which, the user can upload the photos (or video frames!) to the algorithm.\n\n\nWe start with finding the stereo disparity between images using a block matching algorithm (implemented in matlab). From the disparity, we perform filtering to produce a better depth map. Note that the depth map is now in an flat 360 equirectangular projection format. \n\n\nWe fixed erroneous points in the depth map by running a standard deviation filter on one o the color images. The system requres texture to work, so we weight lower areas with low texture.\n\n\nFrom there, we could unwrap it to generate a dense point cloud. It was really challenging generating a satisfying mesh through standard poisson surface reconstruction. Instead, we used a method by sampling the vertex indices in the flat equirectangular depth image. Then we found a standard grid for the mesh indices. From there, we transformed the vertex positions. this led to a much more satisfying mesh.\n\n\nWe then displayed it all using Vuforia on Unity on a phone. We implemented pinch to zoom, as well as a left and right button for rotation.\n\n\nIn addition, we implemented a pointer animation where if you click on the screen, we allow a pin-point to fall from the sky. this was fun!\n\n\nThe system is limited when there's not enough texture. We have ideas of solving that by doing segmentation of regions of the image first, or by using deep learning. Stay tuned.\n\n\nMany of the memers of our team were completely new to unity/coding. At first we tried to use the mapbox api, but found it very complicated, and scrapped showing a terrain. However, team members did learn the basics and how to implement simple yet awesome animations through scripting.\n\n\nWe made it!\n\n\n## Challenges we ran into\n\n\nLots!\n\n\nWe discussed who our end user was - whether first responders or family's wanting to capture the condition of their home.\n\n\nWe critiqued on offline situations.\n\n\nWe critiqued on scales.\n\n\nWe tried to create elevation maps on MapBox but ditched the program (which we loved!).\n\n\nWe tried to incorporate the use of tools.\n\n\nWe tried to set accurate locations in our code.\n\n\nWe tried to generate patterns.\n\n\n## Accomplishments that we're proud of\n\n\nWe were a group that began walking from table to table on Saturday assembling a team and looking for design help, and we got to meet two awesome designers in the process!\n\n\nWe're really proud that it even worked at all. We really weren't sure if we could do it in 48 hours. In fact, we weren't sure this morning we could do it. But this was super dope, and we had a great time meeting, learning, and teaching. \n\n\nWe're proud that we bring our idea out in 48 hours.\n\n\nWe're super proud of the code that was created.\n\n\nWe're proud that our team learned Unity.\n\n\nWe're proud that we built something for AR Good!\n\n\n## What we learned\n\n\nWe learned how to use multiple programs.\n\n\nWe learned how is the potential of the code that we created.\n\n\nWe learned many from other teams.\n\n\nWe learned that everyone in the team is important.\n\n\n## What's next for Homeward\n\n\nNext up, we're fine-tuning our algorithm, and doing more research into the needs of hurricane victims and search-and-rescue first-responders.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/arsports",
            "title": "ARSports",
            "blurb": "An AR-based sports entertainment experience for the end user",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/cDR55JzGJ5E?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "ARSports experience",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/908/datas/original.png"
                },
                {
                    "title": "ARSports - Ads on blimp",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/909/datas/original.jpg"
                },
                {
                    "title": "ARSports - Player position view 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/911/datas/original.jpg"
                },
                {
                    "title": "ARSports - Player position view 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/912/datas/original.jpg"
                },
                {
                    "title": "ARSports - Ordering food",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/913/datas/original.jpg"
                },
                {
                    "title": "ARSports - Scoreboard view",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/914/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Roman Jaquez",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/543/184/datas/profile.JPG"
                },
                {
                    "name": "Lionel Aaron Dsouza",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/1692096?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "microsoft-hololens",
                "unity",
                "visual-studio",
                "windows-10"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The fact that the sports-watching experience is still constrained to just watching it on TV - what if the user can be also made part of the game and experience it in a more immersive way, where he is interacting with the game and making decisions on how to view it, get information from it - not just limited to what the game broadcast offers, but beyond.</p>\n<h2>What it does</h2>\n<p>Allows the user to have a more immersive experience when it comes to watching any type of sports</p>\n<h2>How we built it</h2>\n<p>Using Unity SDK for Windows 10 as an Augmented Reality application for the Microsoft Hololens.</p>\n<h2>Challenges we ran into</h2>\n<p>Spatial Mapping was a bit challenging, as well as updating to the latest version of the HoloToolkit, integrating voice calls and AI to the food ordering system</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Getting as much as we did in 2 days, creating cool 3d assets from scratch and getting voice commands in a short period of time</p>\n<h2>What we learned</h2>\n<p>Architecting and structuring an augmented reality, having a deeper understanding in Unity</p>\n<h2>What's next for ARSports</h2>\n<p>Improve on the spatial mapping and space awareness, integrate AI for enhancing the food ordering experience, consuming the realtime, play-by-play sports data api to link it to actual data</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe fact that the sports-watching experience is still constrained to just watching it on TV - what if the user can be also made part of the game and experience it in a more immersive way, where he is interacting with the game and making decisions on how to view it, get information from it - not just limited to what the game broadcast offers, but beyond.\n\n\n## What it does\n\n\nAllows the user to have a more immersive experience when it comes to watching any type of sports\n\n\n## How we built it\n\n\nUsing Unity SDK for Windows 10 as an Augmented Reality application for the Microsoft Hololens.\n\n\n## Challenges we ran into\n\n\nSpatial Mapping was a bit challenging, as well as updating to the latest version of the HoloToolkit, integrating voice calls and AI to the food ordering system\n\n\n## Accomplishments that we're proud of\n\n\nGetting as much as we did in 2 days, creating cool 3d assets from scratch and getting voice commands in a short period of time\n\n\n## What we learned\n\n\nArchitecting and structuring an augmented reality, having a deeper understanding in Unity\n\n\n## What's next for ARSports\n\n\nImprove on the spatial mapping and space awareness, integrate AI for enhancing the food ordering experience, consuming the realtime, play-by-play sports data api to link it to actual data\n\n\n"
        },
        {
            "source": "https://devpost.com/software/supershopper",
            "title": "SuperShopper",
            "blurb": "We are building VR and AR shopping application that people can shop at any category of stores around their home",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/qc9sFdwWqmU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Praveen Chukka",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_GfgoT0qIaw5Nwe5WCI5E6dzoE2IbSdHeCIIkFEGoESC6SJRWIY5whZnoeWCsIV9HIfboWszEG7eQaDzeSyZ9WIqQb7eFaDxWwyZdCo3wumgwXRkzFDabGYfsFVtcbDHZG0p6LqxPE08?height=180&width=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>I moved to Hyderabad over 2 years ago to work for Microsoft. We were then recently introduced to food delivery applications like Foodpanda and Swiggy. We used it multiple times and we loved it. We continue to use it even today. It beat ordering pizza all the time. Later on as the months passed by I was introduced to grocery delivery apps like BigBasket, Groffers, PepperTap. We ordered once or twice but later on, we never did. We always sort to going to the store and picking up our weekly needs. I wondered why did we do this. Even though it was saving us time and travel costs we still resorted to go to the store and purchase. I then realized it was the experience that the store provided us. We can look at 50 items at a time just by looking around. We never prepared shopping lists so when we looked at a product we would remember that we would need it. This gave me the idea of building SuperShopper.\n\u00a0\u00a0\u00a0 There was a friend of mine who owned a VR startup called CUSMAT. He gave me the gist of the technical aspects to build a VR application. As I already had good experience with coding in C# in Microsoft, it was fairly easy to build a working prototype in 20 days. \n\u00a0\u00a0\u00a0 There was huge traction over this application all over the company when I showcased it. There was even a line in the booth that we made. There was no doubt that this was something totally new and exciting that everybody wanted a piece of.\n\u00a0\u00a0\u00a0 We went back to the drawing boards and dreamt the next revolution of online/mobile shopping industry. SuperShopper was born.</p>\n<h2>What it does</h2>\n<p>SuperShopper is a mobile app that is designed to empower brick and mortar businesses who are being threatened by the new and powerful E-Commerce applications that is taking over the market. Super shopper provides hyperlocal stores to be virtually available using technologies like Virtual Reality and Augmented Reality, using nothing else other than the customer\u2019s smartphone. \u00a0SuperShopper App will be powered by a unique algorithm with features configured and optimized for a smooth virtual experience. The app is designed for various businesses to capture their target customers to patronize them right from their comfortable locations. The application will be delivered in a format that is convenient and useful for both businesses and their target customers; The customers get an immersive shopping experience and the store owners would increase their sales</p>\n<h2>How I built it</h2>\n<p>I built it using Unity. </p>\n<h2>Challenges I ran into</h2>\n<h2>Accomplishments that I'm proud of</h2>\n<h2>What I learned</h2>\n<h2>What's next for SuperShopper</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nI moved to Hyderabad over 2 years ago to work for Microsoft. We were then recently introduced to food delivery applications like Foodpanda and Swiggy. We used it multiple times and we loved it. We continue to use it even today. It beat ordering pizza all the time. Later on as the months passed by I was introduced to grocery delivery apps like BigBasket, Groffers, PepperTap. We ordered once or twice but later on, we never did. We always sort to going to the store and picking up our weekly needs. I wondered why did we do this. Even though it was saving us time and travel costs we still resorted to go to the store and purchase. I then realized it was the experience that the store provided us. We can look at 50 items at a time just by looking around. We never prepared shopping lists so when we looked at a product we would remember that we would need it. This gave me the idea of building SuperShopper.\n\u00a0\u00a0\u00a0 There was a friend of mine who owned a VR startup called CUSMAT. He gave me the gist of the technical aspects to build a VR application. As I already had good experience with coding in C# in Microsoft, it was fairly easy to build a working prototype in 20 days. \n\u00a0\u00a0\u00a0 There was huge traction over this application all over the company when I showcased it. There was even a line in the booth that we made. There was no doubt that this was something totally new and exciting that everybody wanted a piece of.\n\u00a0\u00a0\u00a0 We went back to the drawing boards and dreamt the next revolution of online/mobile shopping industry. SuperShopper was born.\n\n\n## What it does\n\n\nSuperShopper is a mobile app that is designed to empower brick and mortar businesses who are being threatened by the new and powerful E-Commerce applications that is taking over the market. Super shopper provides hyperlocal stores to be virtually available using technologies like Virtual Reality and Augmented Reality, using nothing else other than the customer\u2019s smartphone. \u00a0SuperShopper App will be powered by a unique algorithm with features configured and optimized for a smooth virtual experience. The app is designed for various businesses to capture their target customers to patronize them right from their comfortable locations. The application will be delivered in a format that is convenient and useful for both businesses and their target customers; The customers get an immersive shopping experience and the store owners would increase their sales\n\n\n## How I built it\n\n\nI built it using Unity. \n\n\n## Challenges I ran into\n\n\n## Accomplishments that I'm proud of\n\n\n## What I learned\n\n\n## What's next for SuperShopper\n\n\n"
        },
        {
            "source": "https://devpost.com/software/soundar",
            "title": "SoundAR",
            "blurb": "AR and Sound Experience ",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/237344655?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "aliamohamed",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/32604238?height=180&v=4&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>The Video: <a href=\"https://vimeo.com/237344655\" rel=\"nofollow\">https://vimeo.com/237344655</a></p>\n</div>",
            "content_md": "\nThe Video: <https://vimeo.com/237344655>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/defocusvr",
            "title": "defocusVR",
            "blurb": "An immersive experience to help manage chronic pain through ambient media, machine learning and an innovative UI.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Alec Ostrander",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/28493454?height=180&v=4&width=180"
                },
                {
                    "name": "GB Phares",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4850811?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "glsl",
                "hlsl",
                "javascript",
                "shaderlab",
                "smalltalk"
            ],
            "content_html": "<div>\n<p>Team lead: Brett Phares\nTeam name: defocus VR\nTeam lead mobile: 2129040257</p>\n<p>Team members:\nJon Cohrs\nAlec Ostrander\nBrett Phares</p>\n<p>Category:\nHealthcare</p>\n<p>Brief description:\nDefocus is an immersive, surreal experience that helps bed-bound patients manage their chronic pain through the combination of ambient media, biometric profile building and an eyegaze-based UI.</p>\n<p>Location: third floor, E15-358, table number 50</p>\n<p>Environment:\n  Platform: Windows Universal Platform MR Headset, Android\n  Development Tools: Unity 2017.2.0f2-MFRT, Ableton, WaveLab\n  SDKs: HoloToolkit (Microsoft)\n  APIs: Unity API, </p>\n<p>Assets:\nAudio samples from Jon Cohrs;\nEnvironment assets built out of Unity with particle effects and post processing component or out of Unity Asset Store;\nGoogle Blocks models: \u201cPurple Panda\u201d by\u00a0Diekus Gon; \u201cbear\u201d by Kazuya Noshiro; \u201cTrees\u201d by Kyle Dettman;</p>\n<p>Video link: <a href=\"https://vimeo.com/237321748/ba2db0bf6f\" rel=\"nofollow\">https://vimeo.com/237321748/ba2db0bf6f</a></p>\n</div>",
            "content_md": "\nTeam lead: Brett Phares\nTeam name: defocus VR\nTeam lead mobile: 2129040257\n\n\nTeam members:\nJon Cohrs\nAlec Ostrander\nBrett Phares\n\n\nCategory:\nHealthcare\n\n\nBrief description:\nDefocus is an immersive, surreal experience that helps bed-bound patients manage their chronic pain through the combination of ambient media, biometric profile building and an eyegaze-based UI.\n\n\nLocation: third floor, E15-358, table number 50\n\n\nEnvironment:\n Platform: Windows Universal Platform MR Headset, Android\n Development Tools: Unity 2017.2.0f2-MFRT, Ableton, WaveLab\n SDKs: HoloToolkit (Microsoft)\n APIs: Unity API, \n\n\nAssets:\nAudio samples from Jon Cohrs;\nEnvironment assets built out of Unity with particle effects and post processing component or out of Unity Asset Store;\nGoogle Blocks models: \u201cPurple Panda\u201d by\u00a0Diekus Gon; \u201cbear\u201d by Kazuya Noshiro; \u201cTrees\u201d by Kyle Dettman;\n\n\nVideo link: <https://vimeo.com/237321748/ba2db0bf6f>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/clovercoders",
            "title": "CloverCoders",
            "blurb": "Visualize 3D models before printing with Augmented Reality",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/ktrIa6h_uwo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Tushar Gupta",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8e437550756edb1abeb57c702291891f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Rohan Sapre",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/442/407/datas/profile.jpg"
                },
                {
                    "name": "Rakesh Ravi Shankar",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_0k9zITCg8gox-Ihezz2Uf5hykUTxBVPJnzCzavcy5O0O-o8Nx88v_bfy_xyGA4PZnkC459hpWVhYlOfkzc1div3r2Vh0lOXZ1c19HNqjFRzPUH0XxQ5ZehV2u7Vf4OPWsNBJ765hIQh?height=180&width=180"
                },
                {
                    "name": "Bharat Vaidhyanathan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/442/459/datas/profile.jpg"
                }
            ],
            "built_with": [
                "arcore",
                "c#",
                "unet",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Tired of wasting money on printing 3D artifacts that looked good on paper but not in reality? Want to see your projects in real-life before going all in. We are here to help!</p>\n<h2>What it does</h2>\n<p>Presents your 3D models using Augmented Reality, on objects/terrains around you. Visualize it right where you want it to be. Removes unknowns out of the equation. Now you can get up close and inspect your model from awkward angles.</p>\n<h2>How we built it</h2>\n<p>Used Unity and ARCore. Scripts written in C#. Used library UNet. </p>\n<h2>Challenges we ran into</h2>\n<p>All of us are newbies in the world of VR/AR. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We got out first VR project working. Includes Networking with devices.</p>\n<h2>What we learned</h2>\n<p>A lot about Unity and ARCore.</p>\n<h2>What's next for CloverCoders</h2>\n<p>Many more cool VR/AR </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTired of wasting money on printing 3D artifacts that looked good on paper but not in reality? Want to see your projects in real-life before going all in. We are here to help!\n\n\n## What it does\n\n\nPresents your 3D models using Augmented Reality, on objects/terrains around you. Visualize it right where you want it to be. Removes unknowns out of the equation. Now you can get up close and inspect your model from awkward angles.\n\n\n## How we built it\n\n\nUsed Unity and ARCore. Scripts written in C#. Used library UNet. \n\n\n## Challenges we ran into\n\n\nAll of us are newbies in the world of VR/AR. \n\n\n## Accomplishments that we're proud of\n\n\nWe got out first VR project working. Includes Networking with devices.\n\n\n## What we learned\n\n\nA lot about Unity and ARCore.\n\n\n## What's next for CloverCoders\n\n\nMany more cool VR/AR \n\n\n"
        },
        {
            "source": "https://devpost.com/software/meta-data",
            "title": "Meta Data",
            "blurb": "Visualize multi-layer, geo-located data to gain insights into causes and consequences, and to humanize the data.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Z90_pk7__2Q?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Gururaj Sridhar",
                    "about": "I worked on the backend processing of data. Data was processed and read into structures. This was further transformed into presentable formats in Augumented Reality using the Meta hardware. Used Unity SDK for Meta. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/467/518/datas/profile.jpg"
                },
                {
                    "name": "Tyler C. Roach",
                    "about": "I worked on the user interface of the application which allowed the user to spin the globe around, select a place, and then view the data for that place in 3D.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/146/301/datas/profile.jpg"
                },
                {
                    "name": "Glyn Anderson",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/065c2c1ef02ee1324cf2e0039361a372?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "mapbox",
                "meta-sdk",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The recent mass shooting in Las Vegas and the resulting reporting about mass shootings and homicides in the USA motivated the team to create an experiential way to visualize and explore data, and to reveal nuances of the factors underlying and resulting from crime in America's cities.</p>\n<h2>What it does</h2>\n<p>Meta Data allows the user to explore collections of data about incidents of homicide and other capital crimes in a geo-located environment.  Drilling down into a specific incident gives the user a street-level perspective of the scene of the crime and additional information about the event drawn from many publicly available sources.</p>\n<h2>How I built it</h2>\n<p>With Unity and the Meta2 AR platform, with the addition of MapBox for mapping and street-level geometry.</p>\n<h2>Challenges I ran into</h2>\n<p>Various challenges around the hardware platform, and design challenges around how to present data and what data to present at the different levels of the experience.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>The product achieved a significant part of the vision and serves as a good proof of concept.</p>\n<h2>What I learned</h2>\n<p>We learned how to work with the Meta; some of us increased our Unity skills; and we all learned a lot about the ideas around presenting and correlating data, and making data meaningful.</p>\n<h2>What's next for Meta Data</h2>\n<p>To fully realize the vision of Meta Data, the product should be able to draw from additional data sources in real time and present a fully realized street view experience.</p>\n<p>Our Github: <a href=\"https://github.com/Reality-Virtually-Hackathon/Meta-Data\" rel=\"nofollow\">https://github.com/Reality-Virtually-Hackathon/Meta-Data</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe recent mass shooting in Las Vegas and the resulting reporting about mass shootings and homicides in the USA motivated the team to create an experiential way to visualize and explore data, and to reveal nuances of the factors underlying and resulting from crime in America's cities.\n\n\n## What it does\n\n\nMeta Data allows the user to explore collections of data about incidents of homicide and other capital crimes in a geo-located environment. Drilling down into a specific incident gives the user a street-level perspective of the scene of the crime and additional information about the event drawn from many publicly available sources.\n\n\n## How I built it\n\n\nWith Unity and the Meta2 AR platform, with the addition of MapBox for mapping and street-level geometry.\n\n\n## Challenges I ran into\n\n\nVarious challenges around the hardware platform, and design challenges around how to present data and what data to present at the different levels of the experience.\n\n\n## Accomplishments that I'm proud of\n\n\nThe product achieved a significant part of the vision and serves as a good proof of concept.\n\n\n## What I learned\n\n\nWe learned how to work with the Meta; some of us increased our Unity skills; and we all learned a lot about the ideas around presenting and correlating data, and making data meaningful.\n\n\n## What's next for Meta Data\n\n\nTo fully realize the vision of Meta Data, the product should be able to draw from additional data sources in real time and present a fully realized street view experience.\n\n\nOur Github: <https://github.com/Reality-Virtually-Hackathon/Meta-Data>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/mind-state",
            "title": "Mind State",
            "blurb": "VR and brain-computer interface for exploring visual and musical harmonies reflective of one's mind state.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Stas Rutkowski",
                    "about": "Concepting and art creation ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_zVjRUCh9Ibw0h7R8c0pXUhizd3ZA82R8BZdkU31k9Xuhxe2hMpDczTbEF74fiIVuqJ0LNQS2URcH?height=180&width=180"
                },
                {
                    "name": "Suzanne Hillman",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/18105933?height=180&v=4&width=180"
                },
                {
                    "name": "Hajimex",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/1407293?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "eeg",
                "simmetri",
                "vibe"
            ],
            "content_html": "<div>\n<p>Cell phone for Stas (because my screen isn't working): 631-680-1825</p>\n<h2>Inspiration</h2>\n<p>Existing EEG hardware device and focus on \"coherence\" from Masahiro Kahata, plus thoughts on how one might incorporate the EEG signals in a way to create an engaging VR interface. </p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<p>Coming to agreement on the interface. I think a few people had some very strong ideas of what they wanted to do before they got here, and we ended up going with the one that was more clearly fleshed out and possible (but perhaps ambitious) over the one that might have been an interesting idea but was less clear.</p>\n<p>Getting the interaction between the EEG signals and the VR world to work.</p>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for Mind State</h2>\n</div>",
            "content_md": "\nCell phone for Stas (because my screen isn't working): 631-680-1825\n\n\n## Inspiration\n\n\nExisting EEG hardware device and focus on \"coherence\" from Masahiro Kahata, plus thoughts on how one might incorporate the EEG signals in a way to create an engaging VR interface. \n\n\n## What it does\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\nComing to agreement on the interface. I think a few people had some very strong ideas of what they wanted to do before they got here, and we ended up going with the one that was more clearly fleshed out and possible (but perhaps ambitious) over the one that might have been an interesting idea but was less clear.\n\n\nGetting the interaction between the EEG signals and the VR world to work.\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for Mind State\n\n\n"
        },
        {
            "source": "https://devpost.com/software/city-whisper-hau0og",
            "title": "City Whisper ",
            "blurb": "geolocation-based app for sharing location-associated digital object or messages in use of multi-users' communication",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/MsBuahj0fq0,?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Xingyang Cai",
                    "about": "The application of City Whisper is my idea. I focus on building concept and defining the direction of the application. Also, I am the presenter and leader.",
                    "photo": "https://graph.facebook.com/1493038910788905/picture?height=180&width=180"
                },
                {
                    "name": "sqluo123",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/32603329?height=180&v=4&width=180"
                },
                {
                    "name": "shadypark",
                    "about": "",
                    "photo": "https://graph.facebook.com/360273977757899/picture?height=180&width=180"
                },
                {
                    "name": "Meichun Cai",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/134/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We want to achieve GPS based Augmented Reality apps to create a new social media to rebond people with our physical environment under the digital age.</p>\n<h2>What it does</h2>\n<p>It is geolocation-based app which encourges people to seek new or memorable places where they would like to leave messages, pictures, or creative works to others in order to inspire their friends, families and the collective by sharing their love and ideas. It is practically achievable to have location-associated messages which people have to take physical efforts to go to the designated place to uncover them. In the other way the delivered messages make the place become more meaningful between people.\u00a0</p>\n<h2>How we built it</h2>\n<p>Since this app is a geolocation app for multiuser\u2019 communication, we learnt from Vuforia and GPS for the geolocation based purpose and send bird for the messaging purpose.</p>\n<h2>Challenges we ran into</h2>\n<p>To figure how to redesign the messaging system between different users to communicate the location of digital objects and words  to figure how to convert the 3D local space coordinate </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are able to place digital object at any location and share it to other users through messaging system, especially how to place the digital object up the the accuracy under one meter.</p>\n<h2>What we learned</h2>\n<p>Coding in unity, AR and team working. </p>\n<h2>What's next for City Whisper</h2>\n<p>Can be improved to richer the designed digital objects or effects, and have more user considerate options for messaging. \nCan be further developed as a treasure seeking game.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe want to achieve GPS based Augmented Reality apps to create a new social media to rebond people with our physical environment under the digital age.\n\n\n## What it does\n\n\nIt is geolocation-based app which encourges people to seek new or memorable places where they would like to leave messages, pictures, or creative works to others in order to inspire their friends, families and the collective by sharing their love and ideas. It is practically achievable to have location-associated messages which people have to take physical efforts to go to the designated place to uncover them. In the other way the delivered messages make the place become more meaningful between people.\u00a0\n\n\n## How we built it\n\n\nSince this app is a geolocation app for multiuser\u2019 communication, we learnt from Vuforia and GPS for the geolocation based purpose and send bird for the messaging purpose.\n\n\n## Challenges we ran into\n\n\nTo figure how to redesign the messaging system between different users to communicate the location of digital objects and words to figure how to convert the 3D local space coordinate \n\n\n## Accomplishments that we're proud of\n\n\nWe are able to place digital object at any location and share it to other users through messaging system, especially how to place the digital object up the the accuracy under one meter.\n\n\n## What we learned\n\n\nCoding in unity, AR and team working. \n\n\n## What's next for City Whisper\n\n\nCan be improved to richer the designed digital objects or effects, and have more user considerate options for messaging. \nCan be further developed as a treasure seeking game.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/webvr-checkout",
            "title": "WebVR Checkout",
            "blurb": "Seamless WebVR checkout experience through authentification while immersed in VR",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/bkqCLEqe2Xw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Roland Dubois",
                    "about": "Team Lead. Concept, Design, Code & Production (A-Frame)",
                    "photo": "https://avatars.githubusercontent.com/u/347570?height=180&v=3&width=180"
                },
                {
                    "name": "pennyz",
                    "about": " Project Management, UX, Business Research, Production (Video), Q/A",
                    "photo": "https://avatars1.githubusercontent.com/u/30180346?height=180&v=4&width=180"
                },
                {
                    "name": "Sawan Ruparel",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2724c50d2eb2cd5abd48d9d6faff2aac?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Maximiliano Madrid",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fdca6ead77468b7b0260d9765c413f2c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "manokhinv Manokhin",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/4396449?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "a-frame",
                "ibm-watson",
                "javascript",
                "node.js",
                "payment-processing",
                "webvr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>VR provides an escape from reality. VR is a new reality. When immersed in device agnostic web-based VR shopping experiences, one of the biggest user pain points is breaking out of the VR environment in order authenticate and process payments. </p>\n<h2>What it does</h2>\n<p>Tied to an immersive webVR experience, WebVR Checkout frees you from going back to the reality to finish the checkout authentication. Instead of finishing the payment information through mobile phones, computers, iPads, or pos machines, WebVRcheckout secures your E-commerce payment through biometric voice recognition, trustZone questions, and IBM Watson AI quiz. </p>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<p>Mobile web browsers are not up to date with supporting latest web standards such as speech recognition of the Web Audio API or the Payment Request API. A few of our team members were only present for a day. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Learned about limitations of WebRTC and potential new web standards allow our project to become real in the near future.</p>\n<h2>What we learned</h2>\n<p>Doing research and figuring out workarounds can take up a long time.</p>\n<h2>What's next for WebVR Checkout</h2>\n<p>Becoming a component of the A-Frame.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nVR provides an escape from reality. VR is a new reality. When immersed in device agnostic web-based VR shopping experiences, one of the biggest user pain points is breaking out of the VR environment in order authenticate and process payments. \n\n\n## What it does\n\n\nTied to an immersive webVR experience, WebVR Checkout frees you from going back to the reality to finish the checkout authentication. Instead of finishing the payment information through mobile phones, computers, iPads, or pos machines, WebVRcheckout secures your E-commerce payment through biometric voice recognition, trustZone questions, and IBM Watson AI quiz. \n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\nMobile web browsers are not up to date with supporting latest web standards such as speech recognition of the Web Audio API or the Payment Request API. A few of our team members were only present for a day. \n\n\n## Accomplishments that we're proud of\n\n\nLearned about limitations of WebRTC and potential new web standards allow our project to become real in the near future.\n\n\n## What we learned\n\n\nDoing research and figuring out workarounds can take up a long time.\n\n\n## What's next for WebVR Checkout\n\n\nBecoming a component of the A-Frame.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/collabar",
            "title": "CollabAR",
            "blurb": "Imagine the ability to share a real office space with your team members across the world. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/QPNxozWyrQQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "bassholio Bassler",
                    "about": "I am the UX/UI designer",
                    "photo": "https://avatars1.githubusercontent.com/u/22763359?height=180&v=4&width=180"
                },
                {
                    "name": "kamal-mk Khraisheh",
                    "about": "I worked on the design elements of the app. I did most of the UI backend work for our features.",
                    "photo": "https://avatars2.githubusercontent.com/u/32181787?height=180&v=4&width=180"
                },
                {
                    "name": "Fatma Ozen",
                    "about": "I modeled and textured the 3D assets .",
                    "photo": "https://media.licdn.com/mpr/mprx/0_1IbP-_Q9cqLfG-Ox1fx1M6bcNvYfGPrxnfxPNBEcN6YmG_S19fA-vbX9P_o-Xih7rfxYrTbnMXkaTBTxyZC7VBQspXk7TBd7yZCKAnPBcLliwT-1rdqOlGgQylYxHBYYzR6xnXoTT1n?height=180&width=180"
                },
                {
                    "name": "Mauricio Coen",
                    "about": "I worked on the network development and communicating the theoretical aspects of AR to team members. ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_1AB4wBJpLH1CqZxNvzi46-4jTxpilMuNrz34m_ujTgxCqWtNzz748lNpG7gO-WOzMAfM7PUydxxGBj_kyCtX__sKuxx_Bg9zJCtReC7gbYEfgwDWv1XvH9Tu2o5POgTXK39qfyA0CZo?height=180&width=180"
                }
            ],
            "built_with": [
                "android-studio",
                "invision",
                "maya",
                "mudbox",
                "photoshop",
                "sketch",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are all inspired by the following spaces: Education, Health, Engineering, Art</p>\n<h2>What it does</h2>\n<p>Our native AR app developed in Unity for Android devices allows multiple users to collaboratively share high fidelity 3d models with each other in real time in augmented space on their phone. </p>\n<p>This can be used for: </p>\n<ul>\n<li>Education where users use this to share models to teach about specific details of historic events, elements, people, art, etc.<br/></li>\n<li>Health, to share details visually for doctors, care providers, manufacturers, etc heart, lungs, cells to explain or teach findings in regards to particular elements of the body.</li>\n<li>Engineering to share models and interactions of models together for building, prototyping complex assemblies, annotating components.</li>\n<li>Art to visually learn from 3d models that appear in front of you in real time. E.g. Statues, paintings, buildings.\nGaming, augmented reality games!</li>\n</ul>\n<h2>How we built it</h2>\n<p>Built within Unity, Maya, Visual Studio,  Android Studio, Photoshop, Sketch, Invision, Mudbox as a collaborative effort between 3 developers, 1 3d modeler, and 1 UX/UI designer</p>\n<h2>Challenges we ran into</h2>\n<p>Learning Unity as a team. Version control for a big project with many different file types.\nNetworking for online collaboration, basic solutions for Unity and Android hard to implement on ARCore technology because of the camera setup, and we ran into a lot of dead ends in development due to lack of experience. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Learning Unity as a team and creating something potentially groundbreaking. </p>\n<h2>What we learned</h2>\n<p>How powerful 5 people can be when we put our heads together!</p>\n<h2>What's next for CollabAR</h2>\n<p>Exploring other iterations that will emphasize the needs of other industries. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are all inspired by the following spaces: Education, Health, Engineering, Art\n\n\n## What it does\n\n\nOur native AR app developed in Unity for Android devices allows multiple users to collaboratively share high fidelity 3d models with each other in real time in augmented space on their phone. \n\n\nThis can be used for: \n\n\n* Education where users use this to share models to teach about specific details of historic events, elements, people, art, etc.\n* Health, to share details visually for doctors, care providers, manufacturers, etc heart, lungs, cells to explain or teach findings in regards to particular elements of the body.\n* Engineering to share models and interactions of models together for building, prototyping complex assemblies, annotating components.\n* Art to visually learn from 3d models that appear in front of you in real time. E.g. Statues, paintings, buildings.\nGaming, augmented reality games!\n\n\n## How we built it\n\n\nBuilt within Unity, Maya, Visual Studio, Android Studio, Photoshop, Sketch, Invision, Mudbox as a collaborative effort between 3 developers, 1 3d modeler, and 1 UX/UI designer\n\n\n## Challenges we ran into\n\n\nLearning Unity as a team. Version control for a big project with many different file types.\nNetworking for online collaboration, basic solutions for Unity and Android hard to implement on ARCore technology because of the camera setup, and we ran into a lot of dead ends in development due to lack of experience. \n\n\n## Accomplishments that we're proud of\n\n\nLearning Unity as a team and creating something potentially groundbreaking. \n\n\n## What we learned\n\n\nHow powerful 5 people can be when we put our heads together!\n\n\n## What's next for CollabAR\n\n\nExploring other iterations that will emphasize the needs of other industries. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/augmented-fifth",
            "title": "Augmented Fifth",
            "blurb": "Augmented Reality Interactive Character Soundscape",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/gx8RDOu5F1k?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Louis DeScioli",
                    "about": "I wrote a lot of the code for handling the placement and tracking of the characters within the augmented reality scene, and made the basic UI for choosing characters, adjusting the tempo, and resetting the scene.",
                    "photo": "https://www.gravatar.com/avatar/4272e3cb52eac9d4105917246b8d71a7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Colin Greenhill",
                    "about": "I modeled, rigged and animated the characters in Blender, imported them into Unity and wrote the script to control the animations.",
                    "photo": "https://www.gravatar.com/avatar/85b769fac153e8a3ccf4197b209023fb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Emily Salvador",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_t35SR8jAMrsgqi6-NL1SrLqrgqA8tKFxZ3xigzJrjvx3c8NPg31_nAZAYq73-i5YgK1SrLBlNP8TANZmYt8l1NngAP8hANrYjt8m4v_KVtvuRCIGli9hZ3STl6srZNb7NAXTxbPPTBO?height=180&width=180"
                },
                {
                    "name": "Yichao Guan",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_PotKeBaHtpBfuXDkPaXOfPFHnV7mdN-ZraXKa_WH14Ofukhk1aXrhqwHlSI-EzgvMooYQ6HeUUp7fK3kxMBu8_5kOUpafKkNMMBPwCKWzsdCLQxHzH3jI9YZYa6OQK15rV1y2y3LQro?height=180&width=180"
                }
            ],
            "built_with": [
                "arkit",
                "blender",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>When assessing the affordances of augmented reality, we wanted to take advantage of how sound changes temporally in a physical environment.  Our team wanted to explore character development, musical composition, and phone position to create a unique experience for many user types.  We wanted our app to appeal to children and adults, novices and experts.  Our team comes from very diverse backgrounds and brought different levels of skills to the hackathon that helped us create something better than any of us could create on our own.  Our team members included:</p>\n<p><strong>Emily Salvador</strong> (Team Lead, Synthesizer Lead, 2D Asset Designer)\nEmily is currently a first year masters student at the MIT Media Lab, in the Object Based Media group.  She majored in computer science and music in undergrad.  She wanted to create something musical that would be simple and quick to create interesting compositions and spacial soundscapes.  During her time working at Walt Disney Imagineering and Universal Creative, she wanted to explore how personal devices can take advantage of information embedded in their physical environment.  In her free time, she loves trying all the new filters on Snapchat.</p>\n<p><strong>Yichao Guan</strong> (Sound Designer)\nYichao is currently a graduate from Berklee college of music, with a double major in Electronic Production &amp; Design and Film Scoring. He aspires to be a composer and sound designer for games and films, but is now just a starving, struggling freelancer. Sometimes at night, he often dreams of cheap Chinese takeouts.</p>\n<p><strong>Vik Parthiban</strong> (Hardware Engineer, learning more about the software)\nVik is a first year masters student at the MIT Media Lab also in the Object Based Media group. He did his bachelors and masters in Electrical Engineering and worked at Magic Leap building light-field displays for mixed reality. He's currently researching new holographic media for glass-less 3D displays and hopes to bring a holo-display to everyone's home in the near future. On the side, he is an advisor to for a student research group working on the Hyperloop Transportation System with SpaceX and Elon Musk.</p>\n<p><strong>Colin Greenhill</strong> (Software Developer, 3D Modeler and Animator)\nColin is a software developer who specializes in educational games, tools and simulations. His experience teaching animation and game development at The Center for Digital Arts and the Boston Museum of Science, and his work at the Education Arcade at MIT has given him the opportunity to explore and further develop the power overlap of games and education.</p>\n<p><strong>Louis DeScioli</strong> (Software Engineer &amp; Designer)\nLouis is a software engineer whose made everything from Internet-connected aquaponic gardens to educational apps for autistic families. He studied Electrical Engineering and Computer Science at MIT. He enjoys creating software experiences that delight and inspire. He is an aspiring augmented reality developer, making an mobile augmented reality game, Out Here Archery. In his spare time he enjoys playing his cello and cooking.</p>\n<h2>What it does</h2>\n<p>We've created an iPad/iPhone app that allows you to place 3D characters into your physical environment.  You can interact with those characters via a sequencer to tell your characters when they should play their instrument.  At the character level, you can toggle between different chords and notations to quickly create beautiful compositions.  As you add more characters, your composition has more layers which increases the complexity and musicality of the piece.  Because the characters are locked at a physical position, as you move around the room, the balance of the instrumentation adjusts depending on how close you are to each character.  At the global level, you can change the tempo of your instruments (which also informs how fast the characters animate).  Additionally, at the global level, you can enable the randomizer which will automatically create melodies and rhythmic variation for you.</p>\n<h2>How we built it</h2>\n<p>We built the app in three parts.  Those were character animation, synthesizer and sound design, and augmented reality tracking.</p>\n<p><strong>Character Animation</strong>\nThe characters were modeled, rigged and animated in Blender, and imported to Unity3D. Each character has a distinct animation to go along with the soundscapes that the user can create.</p>\n<p><strong>Synthesizer and Sound Design</strong>\nFor the synthesizer, we had three main scripts.  The BeatHandler was a global script, that had a delegate called BeatAction.  The tempo of game was maintained in this script, as well as calls based on the tempo.  The Synthesizer script kept track of several variables for each individual track.  For example, it instantiated how many cubes were in each synthesizer and how many individual sounds files could be called.  The CubeController script kept track of the state of each cube and maintained what color the cube should be along with whether or not that cube should play a note (and which note it should play).</p>\n<p><strong>AR Tracking</strong>\nFor the Augmented Reality tracking we used Apple's ARKit framework with the Unity ARKit plugin. The main challenge was tracking the discovered planes in the 3D scene and showing a square indicating to the user whether or not a model could be placed at that spot in the scene. Other considerations were creating shadows for the virtual objects and figuring out how to scale assets to appear at reasonable sizes.</p>\n<h2>Challenges we ran into</h2>\n<p>We struggled the first night to nail down our concept, especially when trying to create modalities that benefited from being in the 3D environment.  The next day, we all collaborated to discover this final concept that takes advantage of composition in a 3D space.  We had some issues with setting the character animations to match the state of the synthesizer, especially because the animations didn't look as nice if they moved too quickly.  We solved this by simplifying the animation.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're super proud of how well we were able to integrate each of the separate components on the second day of the hackathon.  The sequencer is associated with each individual character and adjusts balance in the 3D environment.  We were pleasantly surprised and impressed by Lou's ability to integrate ARKit and by the improvements he made to make the tracking work more seamlessly in our environment.</p>\n<h2>What we learned</h2>\n<p>We learned that it's a lot of fun to do a hackathon with new friends.  We learned about some of the challenges of iterating quickly while developing for Augmented Reality using ARKit and we learned from each other best practices for developing in Unity.</p>\n<h2>What's next for Augmented Fifth</h2>\n<p>Some of us are still students, so we'll get back to pset-ing while the adults will go back to the real world I guess. :)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWhen assessing the affordances of augmented reality, we wanted to take advantage of how sound changes temporally in a physical environment. Our team wanted to explore character development, musical composition, and phone position to create a unique experience for many user types. We wanted our app to appeal to children and adults, novices and experts. Our team comes from very diverse backgrounds and brought different levels of skills to the hackathon that helped us create something better than any of us could create on our own. Our team members included:\n\n\n**Emily Salvador** (Team Lead, Synthesizer Lead, 2D Asset Designer)\nEmily is currently a first year masters student at the MIT Media Lab, in the Object Based Media group. She majored in computer science and music in undergrad. She wanted to create something musical that would be simple and quick to create interesting compositions and spacial soundscapes. During her time working at Walt Disney Imagineering and Universal Creative, she wanted to explore how personal devices can take advantage of information embedded in their physical environment. In her free time, she loves trying all the new filters on Snapchat.\n\n\n**Yichao Guan** (Sound Designer)\nYichao is currently a graduate from Berklee college of music, with a double major in Electronic Production & Design and Film Scoring. He aspires to be a composer and sound designer for games and films, but is now just a starving, struggling freelancer. Sometimes at night, he often dreams of cheap Chinese takeouts.\n\n\n**Vik Parthiban** (Hardware Engineer, learning more about the software)\nVik is a first year masters student at the MIT Media Lab also in the Object Based Media group. He did his bachelors and masters in Electrical Engineering and worked at Magic Leap building light-field displays for mixed reality. He's currently researching new holographic media for glass-less 3D displays and hopes to bring a holo-display to everyone's home in the near future. On the side, he is an advisor to for a student research group working on the Hyperloop Transportation System with SpaceX and Elon Musk.\n\n\n**Colin Greenhill** (Software Developer, 3D Modeler and Animator)\nColin is a software developer who specializes in educational games, tools and simulations. His experience teaching animation and game development at The Center for Digital Arts and the Boston Museum of Science, and his work at the Education Arcade at MIT has given him the opportunity to explore and further develop the power overlap of games and education.\n\n\n**Louis DeScioli** (Software Engineer & Designer)\nLouis is a software engineer whose made everything from Internet-connected aquaponic gardens to educational apps for autistic families. He studied Electrical Engineering and Computer Science at MIT. He enjoys creating software experiences that delight and inspire. He is an aspiring augmented reality developer, making an mobile augmented reality game, Out Here Archery. In his spare time he enjoys playing his cello and cooking.\n\n\n## What it does\n\n\nWe've created an iPad/iPhone app that allows you to place 3D characters into your physical environment. You can interact with those characters via a sequencer to tell your characters when they should play their instrument. At the character level, you can toggle between different chords and notations to quickly create beautiful compositions. As you add more characters, your composition has more layers which increases the complexity and musicality of the piece. Because the characters are locked at a physical position, as you move around the room, the balance of the instrumentation adjusts depending on how close you are to each character. At the global level, you can change the tempo of your instruments (which also informs how fast the characters animate). Additionally, at the global level, you can enable the randomizer which will automatically create melodies and rhythmic variation for you.\n\n\n## How we built it\n\n\nWe built the app in three parts. Those were character animation, synthesizer and sound design, and augmented reality tracking.\n\n\n**Character Animation**\nThe characters were modeled, rigged and animated in Blender, and imported to Unity3D. Each character has a distinct animation to go along with the soundscapes that the user can create.\n\n\n**Synthesizer and Sound Design**\nFor the synthesizer, we had three main scripts. The BeatHandler was a global script, that had a delegate called BeatAction. The tempo of game was maintained in this script, as well as calls based on the tempo. The Synthesizer script kept track of several variables for each individual track. For example, it instantiated how many cubes were in each synthesizer and how many individual sounds files could be called. The CubeController script kept track of the state of each cube and maintained what color the cube should be along with whether or not that cube should play a note (and which note it should play).\n\n\n**AR Tracking**\nFor the Augmented Reality tracking we used Apple's ARKit framework with the Unity ARKit plugin. The main challenge was tracking the discovered planes in the 3D scene and showing a square indicating to the user whether or not a model could be placed at that spot in the scene. Other considerations were creating shadows for the virtual objects and figuring out how to scale assets to appear at reasonable sizes.\n\n\n## Challenges we ran into\n\n\nWe struggled the first night to nail down our concept, especially when trying to create modalities that benefited from being in the 3D environment. The next day, we all collaborated to discover this final concept that takes advantage of composition in a 3D space. We had some issues with setting the character animations to match the state of the synthesizer, especially because the animations didn't look as nice if they moved too quickly. We solved this by simplifying the animation.\n\n\n## Accomplishments that we're proud of\n\n\nWe're super proud of how well we were able to integrate each of the separate components on the second day of the hackathon. The sequencer is associated with each individual character and adjusts balance in the 3D environment. We were pleasantly surprised and impressed by Lou's ability to integrate ARKit and by the improvements he made to make the tracking work more seamlessly in our environment.\n\n\n## What we learned\n\n\nWe learned that it's a lot of fun to do a hackathon with new friends. We learned about some of the challenges of iterating quickly while developing for Augmented Reality using ARKit and we learned from each other best practices for developing in Unity.\n\n\n## What's next for Augmented Fifth\n\n\nSome of us are still students, so we'll get back to pset-ing while the adults will go back to the real world I guess. :)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/achievar",
            "title": "AchieVAR",
            "blurb": "AchieVAR gives you a virtual teacher to help you learn, train, or do physical therapy on mixed reality devices.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Giri S",
                    "about": "Worked on permanently recording events to an immutable unimpeachable blockchain, NLP for guiding the cognitive experience and sensing the sentiment(s) of the scenario(s).",
                    "photo": "https://media.licdn.com/mpr/mprx/0_1NOWt-euo5n1Owc7vK6HV5oSobj7ZW6fqn6FqbYSE5HajMUiJL6byhWSWvja4VEiMLXW1XoD_GwmRSv-sFRvs6I36GwfRS0aJFRElFO2DiY_AUXtvnSXAPrjQBl0tSe0Kkx59JQB2tX?height=180&width=180"
                },
                {
                    "name": "Nima Zeighami",
                    "about": "I was the Project Manager, I did the audio design and voice acting for the application, and I helped execute the teams vision for immersive and experimental technologies, including Windows Mixed Reality and HoloLens.  In addition, I did a small portion of the software development.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/413/592/datas/profile.JPG"
                },
                {
                    "name": "Andres Cuervo",
                    "about": "I made the logo, demoed animation for the logo, and cleaned up some 3D model assets.",
                    "photo": "https://avatars1.githubusercontent.com/u/5826638?height=180&v=4&width=180"
                },
                {
                    "name": "Jacob Shepherd",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/190/datas/profile.jpg"
                },
                {
                    "name": "Khanh Le",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_-ev3kqfdK2vjXlpRyfQ-knxbKE3PH-SRKWTYkBg977zCcnsUYm5Docd4YXTSwzuc1wB0wr_XrHAq?height=180&width=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Team Info</h2>\n<p>Floor: 6</p>\n<p>Location: Table near door to room.</p>\n<p>Room: E14-674</p>\n<p>Table Number: 4</p>\n<p>Team Leader: Jacob Shepherd, phone number (913)626-4777</p>\n<p>Platform: Windows Mixed Reality, HoloLens</p>\n<p>Dev Tools: Unity, Visual Studio Community, Mixamo, Blender, Structure Sensor, iOS Voice Memos, </p>\n<h2>Inspiration</h2>\n<p>Jacob was inspired by the need of healthcare providers to validate the completion of physical therapy and the need for employers to verify a candidates education credentials.  This lead to an end-to-end design in which software facilitates the teaching and recording of a learning experience.  </p>\n<h2>What it does</h2>\n<p>AchieVAR is an app that gives you a mixed reality learning experience, allowing you to make selections using your voice.  It then records your achievement on a blockchain.</p>\n<h2>How I built it</h2>\n<p>We built the application in Unity and HoloToolkit.  Assets were acquired from SketchFab and Mixamo, and 3D scanned using the Occipital Structure Sensor.  Animations were created using Mixamo.  3D assets were modified using Simplygon, Blender and Meshlab.  Audio assets were recorded on iPhone using Voice Recorder.</p>\n<h2>Challenges I ran into</h2>\n<p>We ran into challenges with compatibility with different versions of Unity, mismatched versions of Unity on our development machines, HoloToolkit and MRToolkit versions and support on our devices.  In addition, the 3D scans we did using the Structure Sensor were too high-poly, and thus we were unable to use them without huge performance issues.  So we tried to use Simplygon to decimate them, but there were huge issues with Simplygon's support as they'd recently been bought by Microsoft, thus, it is unsupported as a plugin in the newest version of Unity, 2017.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We are proud of creating an AR app that has limitless real-world application, that is controlled by your voice.</p>\n<h2>What I learned</h2>\n<p>We learned to absolutely not use experimental software stacks without good documentation when time is a factor.</p>\n<h2>What's next for AchieVAR</h2>\n<p>AchieVAR may be developed further, specifically in the healthcare space.</p>\n</div>",
            "content_md": "\n## Team Info\n\n\nFloor: 6\n\n\nLocation: Table near door to room.\n\n\nRoom: E14-674\n\n\nTable Number: 4\n\n\nTeam Leader: Jacob Shepherd, phone number (913)626-4777\n\n\nPlatform: Windows Mixed Reality, HoloLens\n\n\nDev Tools: Unity, Visual Studio Community, Mixamo, Blender, Structure Sensor, iOS Voice Memos, \n\n\n## Inspiration\n\n\nJacob was inspired by the need of healthcare providers to validate the completion of physical therapy and the need for employers to verify a candidates education credentials. This lead to an end-to-end design in which software facilitates the teaching and recording of a learning experience. \n\n\n## What it does\n\n\nAchieVAR is an app that gives you a mixed reality learning experience, allowing you to make selections using your voice. It then records your achievement on a blockchain.\n\n\n## How I built it\n\n\nWe built the application in Unity and HoloToolkit. Assets were acquired from SketchFab and Mixamo, and 3D scanned using the Occipital Structure Sensor. Animations were created using Mixamo. 3D assets were modified using Simplygon, Blender and Meshlab. Audio assets were recorded on iPhone using Voice Recorder.\n\n\n## Challenges I ran into\n\n\nWe ran into challenges with compatibility with different versions of Unity, mismatched versions of Unity on our development machines, HoloToolkit and MRToolkit versions and support on our devices. In addition, the 3D scans we did using the Structure Sensor were too high-poly, and thus we were unable to use them without huge performance issues. So we tried to use Simplygon to decimate them, but there were huge issues with Simplygon's support as they'd recently been bought by Microsoft, thus, it is unsupported as a plugin in the newest version of Unity, 2017.\n\n\n## Accomplishments that I'm proud of\n\n\nWe are proud of creating an AR app that has limitless real-world application, that is controlled by your voice.\n\n\n## What I learned\n\n\nWe learned to absolutely not use experimental software stacks without good documentation when time is a factor.\n\n\n## What's next for AchieVAR\n\n\nAchieVAR may be developed further, specifically in the healthcare space.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rainbow-s-beginning",
            "title": "Rainbow | Seamless Mixed Reality Begins Here",
            "blurb": "Rainbow organizes virtual information for real space through shareable mixed reality layers.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/HsBf6FyjB1I?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Mixed Reality Network Diagram",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/717/datas/original.png"
                },
                {
                    "title": "Circuit diagram for IOT with Arduino",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/757/datas/original.png"
                },
                {
                    "title": "Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/767/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Rogue Fong",
                    "about": "I integrated the Arduino with Unity using serial communication, and programmed logic and functionality for the interactions. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/455/740/datas/profile.JPG"
                },
                {
                    "name": "Adam Sauer",
                    "about": "I set up the environment/scene in Unity, integrated Mapbox SDK with live traffic data and other UI layers, and set up Meta SDK with Hand tracking/UI interaction.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/544/762/datas/profile.jpg"
                },
                {
                    "name": "Doc Martens",
                    "about": "I worked on U/X design, asset creation, pitch and demo video and built/coded all the Internet of things connections using Arduino and sensors.",
                    "photo": "https://avatars1.githubusercontent.com/u/13619262?height=180&v=4&width=180"
                },
                {
                    "name": "yunxin",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/863/datas/profile.JPG"
                },
                {
                    "name": "fpunjwani",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/22136612?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "c#",
                "mapbox",
                "meta",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We already live in augmented reality. But the user experience is sorely lacking. In order to access information or connect with others virtually, we rely on smartphones, hiding behind our screens and disconnecting from the real world. We believe that accessing virtual information should be effortless, allowing us to interact naturally with the world around us. As A/R technology improves, this reality is moving closer and closer. But how might we ensure that today's information overload doesn't pollute our physical reality? How do we maintain what is true and what is real in a landscape of ubiquitous digital augmentation? To begin to answer these questions we created Rainbow, a design pattern for shareable mixed reality.</p>\n<h2>What it does</h2>\n<p>Rainbow is an A/R solution for organizing virtual information in real spaces. By collecting digital content into layers, Rainbow helps users navigate between several different augmented experiences for one location. </p>\n<p><strong>User Scenario</strong> \nIf Ben needed to get to the T Station today, he\u2019d have to take out his phone, find the right app, get directions, and keeps his nose pressed to the screen while mentally navigating between the image on his phone and world around him. He\u2019d have to switch apps to check the train schedule or see if there might be a better transit option.\nBut with Rainbow, Ben could simply turn on the City Layer published by the municipal government, containing all the transit and tourist information he would need to navigate the space. This information would sync up with actual landmarks and transit hubs, providing a contextual navigation experience that makes sense. If he needed to interact with other information around him, he could easily switch the layer in his A/R headset.</p>\n<p><strong>What is layerable reality?</strong>\nLayers of mixed and augmented reality that can be programmed separately. They act as a virtual layer over a physical space, harnessing web APIs, geo-location, and local IOT devices to enhance one or more objects in that space with virtual elements. </p>\n<ul>\n<li>One space can have many Layers. </li>\n<li>Unless programmed specifically, only one pLayer can be viewed by one person at one time. </li>\n<li>Each Layer has several types of rights: content-production and delivery, interactivity, purchasing, editing, administration etc.</li>\n<li>Layers can be owned and controlled by private companies/ventures, individuals, governments or NGOs. </li>\n<li>There can also be common Layers, virtual layers where any user can freely edit, add, subtract, or content or interactivity. Typically these sandbox type layers are controlled through a distributed form of moderation and governance, although anarcho-varieties can exist as well.</li>\n</ul>\n<p><strong>Examples of Layer Types</strong></p>\n<ul>\n<li>Public Layer: Controlled by a municipal government. Provides basic directions to subways, common attractions, public bathrooms, and interactivity with municipal data streams (311, 911 etc.).</li>\n<li>Public/Private Layer owned: Controlled by local Kendall Square district. Provides information about local businesses (promotions, coupons, hours of operation) and events happening in the area</li>\n<li>Private Layer owned by MIT: Controlled privately by a local university. This layer can include tips/tricks from MIT students about places, restaurants etc. This pLayer can have separate sub-layers that allow for ad-hoc group communication, private messaging, urban gaming etc.</li>\n<li>Pop-up Layer: Controlled privately and temporarily by Blizzard Entertainment. A temporary (\u2018pop-up\u2019) pLayer for an urban gaming experience for users subscribed to Blizzard\u2019s World of Warcraft</li>\n</ul>\n<h2>How we built it</h2>\n<p>Rainbow was built using mixed reality technology, Mapbox, and Unity, for Meta\u2019s mixed reality technology. For our prototype, we built an experience that allows users to see different views of an IOT devices recording environmental information and providing an NFC login panel for Kendall Square in Cambridge, MA. The user is able to access two layers of reality: the first one being \u2018municipal\u2019, with weather information streaming from the sensors and the second one being \u2018game\u2019, with a specific event that only select users can access through the NFC.</p>\n<p>First, Rainbow required an understanding of our physical space. We then combined this with geolocation and mapbox to map out space in Unity, including specific markers for smart objects\nSecond, we create a smart object, an internet-of-things box, that shows multiple type of information. What information the user views is dependent on the layer the user is in.\nThird, we built two layers: a Municipal Layer and a Game Layer\nFor the Municipal Layer, we used real-time weather information to share visually with the user. We created a smart box that works with Arduino to get real-time temperature and humidity information as well as the log-in state of unique NFC chips. This information is then threaded into Unity and can be interacted with in A/R.</p>\n<p>For the Game Layer, we created an experience, unlocked only once a specific number of users sign in. We used RFID sensors o that can be accessed by select users\nFinally, we created seamless switching between layers using C# in Unity as well as icons and the user interface using Adobe Illustrator.</p>\n<h2>Challenges we ran into</h2>\n<p>Familiarizing ourselves with all the VR/AR hardwares and using them to create great mixed reality experiences was challenging, but in the end everything worked out well.\nCreating a believable and powerful demo that captures the product features precisely was hard.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are most proud that we can envision a seamlessly mixed reality infrastructure using Meta Vision and Mapbox while implementing IoT devices and blockchain concepts in especially such a short time. Our team members have a background in design, engineering, and business, and all firmly believe in a future where mixed reality would generate more personalized daily experiences and genuine social connections. \nIn two days, we have tested various sensors, experimented with Microsoft HoloLens, Meta Vision, and various geospatial mapping tools to bring on the best-mixed reality experience. It took us a while to figure out all the hardware settings, but it worked out well!\nWe are also proud of the support and trust within our team. From the very start, we\u2019ve set our goal to build an awesome product and to challenge ourselves. And we are so proud we\u2019ve done it!</p>\n<h2>What's next for Rainbow's Beginning</h2>\n<p>With Rainbow, we are the creating infrastructure for ubiquitous augmented reality using the Internet of Things, an infrastructure that enables seamless mixed reality experience. </p>\n<p><strong>Our top priorities include:</strong>\n*Pushing feasibility</p>\n<ul>\n<li>How can users interact with each other in mixed reality?</li>\n<li>How do permissions (read, write access) works in different Layers?</li>\n<li>How might we integrate blockchain for user identification and authentication and/or registering digital asset intellectual property?</li>\n<li>How might integrate with other connected devices and enable devices to get smart in the city?\n** Further testing desirability and viability **</li>\n<li>How do different users interact with spaces in mixed reality?</li>\n<li>What does a revenue generation model?</li>\n<li>How can we scale our idea through leveraging the existing ecosystem?</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe already live in augmented reality. But the user experience is sorely lacking. In order to access information or connect with others virtually, we rely on smartphones, hiding behind our screens and disconnecting from the real world. We believe that accessing virtual information should be effortless, allowing us to interact naturally with the world around us. As A/R technology improves, this reality is moving closer and closer. But how might we ensure that today's information overload doesn't pollute our physical reality? How do we maintain what is true and what is real in a landscape of ubiquitous digital augmentation? To begin to answer these questions we created Rainbow, a design pattern for shareable mixed reality.\n\n\n## What it does\n\n\nRainbow is an A/R solution for organizing virtual information in real spaces. By collecting digital content into layers, Rainbow helps users navigate between several different augmented experiences for one location. \n\n\n**User Scenario** \nIf Ben needed to get to the T Station today, he\u2019d have to take out his phone, find the right app, get directions, and keeps his nose pressed to the screen while mentally navigating between the image on his phone and world around him. He\u2019d have to switch apps to check the train schedule or see if there might be a better transit option.\nBut with Rainbow, Ben could simply turn on the City Layer published by the municipal government, containing all the transit and tourist information he would need to navigate the space. This information would sync up with actual landmarks and transit hubs, providing a contextual navigation experience that makes sense. If he needed to interact with other information around him, he could easily switch the layer in his A/R headset.\n\n\n**What is layerable reality?**\nLayers of mixed and augmented reality that can be programmed separately. They act as a virtual layer over a physical space, harnessing web APIs, geo-location, and local IOT devices to enhance one or more objects in that space with virtual elements. \n\n\n* One space can have many Layers.\n* Unless programmed specifically, only one pLayer can be viewed by one person at one time.\n* Each Layer has several types of rights: content-production and delivery, interactivity, purchasing, editing, administration etc.\n* Layers can be owned and controlled by private companies/ventures, individuals, governments or NGOs.\n* There can also be common Layers, virtual layers where any user can freely edit, add, subtract, or content or interactivity. Typically these sandbox type layers are controlled through a distributed form of moderation and governance, although anarcho-varieties can exist as well.\n\n\n**Examples of Layer Types**\n\n\n* Public Layer: Controlled by a municipal government. Provides basic directions to subways, common attractions, public bathrooms, and interactivity with municipal data streams (311, 911 etc.).\n* Public/Private Layer owned: Controlled by local Kendall Square district. Provides information about local businesses (promotions, coupons, hours of operation) and events happening in the area\n* Private Layer owned by MIT: Controlled privately by a local university. This layer can include tips/tricks from MIT students about places, restaurants etc. This pLayer can have separate sub-layers that allow for ad-hoc group communication, private messaging, urban gaming etc.\n* Pop-up Layer: Controlled privately and temporarily by Blizzard Entertainment. A temporary (\u2018pop-up\u2019) pLayer for an urban gaming experience for users subscribed to Blizzard\u2019s World of Warcraft\n\n\n## How we built it\n\n\nRainbow was built using mixed reality technology, Mapbox, and Unity, for Meta\u2019s mixed reality technology. For our prototype, we built an experience that allows users to see different views of an IOT devices recording environmental information and providing an NFC login panel for Kendall Square in Cambridge, MA. The user is able to access two layers of reality: the first one being \u2018municipal\u2019, with weather information streaming from the sensors and the second one being \u2018game\u2019, with a specific event that only select users can access through the NFC.\n\n\nFirst, Rainbow required an understanding of our physical space. We then combined this with geolocation and mapbox to map out space in Unity, including specific markers for smart objects\nSecond, we create a smart object, an internet-of-things box, that shows multiple type of information. What information the user views is dependent on the layer the user is in.\nThird, we built two layers: a Municipal Layer and a Game Layer\nFor the Municipal Layer, we used real-time weather information to share visually with the user. We created a smart box that works with Arduino to get real-time temperature and humidity information as well as the log-in state of unique NFC chips. This information is then threaded into Unity and can be interacted with in A/R.\n\n\nFor the Game Layer, we created an experience, unlocked only once a specific number of users sign in. We used RFID sensors o that can be accessed by select users\nFinally, we created seamless switching between layers using C# in Unity as well as icons and the user interface using Adobe Illustrator.\n\n\n## Challenges we ran into\n\n\nFamiliarizing ourselves with all the VR/AR hardwares and using them to create great mixed reality experiences was challenging, but in the end everything worked out well.\nCreating a believable and powerful demo that captures the product features precisely was hard.\n\n\n## Accomplishments that we're proud of\n\n\nWe are most proud that we can envision a seamlessly mixed reality infrastructure using Meta Vision and Mapbox while implementing IoT devices and blockchain concepts in especially such a short time. Our team members have a background in design, engineering, and business, and all firmly believe in a future where mixed reality would generate more personalized daily experiences and genuine social connections. \nIn two days, we have tested various sensors, experimented with Microsoft HoloLens, Meta Vision, and various geospatial mapping tools to bring on the best-mixed reality experience. It took us a while to figure out all the hardware settings, but it worked out well!\nWe are also proud of the support and trust within our team. From the very start, we\u2019ve set our goal to build an awesome product and to challenge ourselves. And we are so proud we\u2019ve done it!\n\n\n## What's next for Rainbow's Beginning\n\n\nWith Rainbow, we are the creating infrastructure for ubiquitous augmented reality using the Internet of Things, an infrastructure that enables seamless mixed reality experience. \n\n\n**Our top priorities include:**\n*Pushing feasibility\n\n\n* How can users interact with each other in mixed reality?\n* How do permissions (read, write access) works in different Layers?\n* How might we integrate blockchain for user identification and authentication and/or registering digital asset intellectual property?\n* How might integrate with other connected devices and enable devices to get smart in the city?\n** Further testing desirability and viability **\n* How do different users interact with spaces in mixed reality?\n* What does a revenue generation model?\n* How can we scale our idea through leveraging the existing ecosystem?\n\n\n"
        },
        {
            "source": "https://devpost.com/software/algoar",
            "title": "AlgoAR",
            "blurb": "Immersive AR real time learning platform ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/hqjmS1M4Tlo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Siddharth Kumar",
                    "about": "I worked on designing.",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Plbbb71Cdi8MDGqOq8U8AkCGVg3pDmNjBRsTBAxkdiy0DCBgBiskFciGwK2KZ8RBGZsGnGCbZq8yfxnt6V9HKNGXfq8pfxQOlV9Hiv1CsquyfxMzKfRilTfaJfyrYdcvX3oeLmPSf_AzuRntmfd4tl?height=180&width=180"
                },
                {
                    "name": "Hairuo0118",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32548072?height=180&v=4&width=180"
                },
                {
                    "name": "Mona Kim",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/22137972?height=180&v=4&width=180"
                },
                {
                    "name": "yunizoo",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/24235510?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "cinema-4d",
                "photoshop",
                "photoview-360",
                "solidworks",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We were inspired by difficult classes about algorithms in computing and engineering. These classes often require students to study complicated pseudo code to understand algorithms, however we found a better way to have fun while learning. </p>\n<h2>What it does</h2>\n<p>It teaches user to learn computer science algorithm by rearranging the order of physical cards. The user will see an array of real cards through the camera, and virtual numbers will be generated on the card through the camera by recognizing different cards. Then, the user will rearrange the order of the cards to learn algorithm.</p>\n<p>To make learning fun, we have created a system that helps a person build a mental model of an algorithm by following the steps of the algorithm using real objects. The real objects are tracked using computer vision and the </p>\n<h2>How we built it</h2>\n<p>We used Unity, Vuforia, a webcam, and Vuforia VuMark cards. We worked in unity to integrate VuMark Cards with an Augmented Reality Overlay.</p>\n<h2>Challenges we ran into</h2>\n<p>Initially we worked on integrating Vuforia and the Hololens. The latest software for each was not compatible and unfortunately we had to pivot to a webcam and laptop.\nWe also have difficulty displaying different number on different cards at the same time.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Working together to learn the Vuforia API and to think about the future of education through Augmented Reality.</p>\n<h2>What we learned</h2>\n<p>We learned how to pivot and work together! </p>\n<h2>What's next for AlgoAR</h2>\n<p>To create more procedure lessons in the virtual classroom, and the content includes engineering, computer science and other lessons. For example, we want to create a maker classroom where you can disassemble or assemble a drone/robots, or learn about different robots and other maker projects for free in a procedure steps.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe were inspired by difficult classes about algorithms in computing and engineering. These classes often require students to study complicated pseudo code to understand algorithms, however we found a better way to have fun while learning. \n\n\n## What it does\n\n\nIt teaches user to learn computer science algorithm by rearranging the order of physical cards. The user will see an array of real cards through the camera, and virtual numbers will be generated on the card through the camera by recognizing different cards. Then, the user will rearrange the order of the cards to learn algorithm.\n\n\nTo make learning fun, we have created a system that helps a person build a mental model of an algorithm by following the steps of the algorithm using real objects. The real objects are tracked using computer vision and the \n\n\n## How we built it\n\n\nWe used Unity, Vuforia, a webcam, and Vuforia VuMark cards. We worked in unity to integrate VuMark Cards with an Augmented Reality Overlay.\n\n\n## Challenges we ran into\n\n\nInitially we worked on integrating Vuforia and the Hololens. The latest software for each was not compatible and unfortunately we had to pivot to a webcam and laptop.\nWe also have difficulty displaying different number on different cards at the same time.\n\n\n## Accomplishments that we're proud of\n\n\nWorking together to learn the Vuforia API and to think about the future of education through Augmented Reality.\n\n\n## What we learned\n\n\nWe learned how to pivot and work together! \n\n\n## What's next for AlgoAR\n\n\nTo create more procedure lessons in the virtual classroom, and the content includes engineering, computer science and other lessons. For example, we want to create a maker classroom where you can disassemble or assemble a drone/robots, or learn about different robots and other maker projects for free in a procedure steps.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-dark-room-blackout",
            "title": "The Dark Room (Blackout)",
            "blurb": "Blackout is Virtual Reality Horror Game where you fight monster creeping up on you in a dark morgue.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/SP1JtvDkJTs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Cover Page",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/719/datas/original.png"
                },
                {
                    "title": "One of the Main Enemies We Use",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/740/datas/original.png"
                },
                {
                    "title": "The Flashlight that is controlled Merge Cube Ar",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/742/datas/original.png"
                },
                {
                    "title": "A Birds Eye View of the Entire Environment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/744/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Kabeer -",
                    "about": "Creative Development +  UI/UX Design + 3D Modeling & Asset Creation",
                    "photo": "https://media.licdn.com/mpr/mprx/0_18kiD3D6eCXNp3sW8lMTM42LR_W44_NfA-M8jxDh4YLZ47MuhGMLSn25J_Xqp7V2iQMLpnS5obXZwtqiQIzuVVD6gbX4wtefiIzuWRp8HbHZwtBXGO4bRfh2wy4qVXJknrd27hy7grvAH9Z_KCyyhy?height=180&width=180"
                },
                {
                    "name": "Jheel Mehta",
                    "about": "Implementation of Vuforia and Merge Cube's AR tracking in Unity",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Of6NctlB8hclkfCO-YYMJLxBLlnlTyGiNYpV-FOn8TFrT2i7qYYsJQKnGvWCbjmfZIONpAx9d9wKG4Bxlyie0Q-Vu9wAG4baqyiBqbFcbcYgHaytZDvJN1d62_l3w4r0g0bZPzrkn90?height=180&width=180"
                },
                {
                    "name": "OscarCoen",
                    "about": "Sound Design + Implementation",
                    "photo": "https://graph.facebook.com/10214252120839453/picture?height=180&width=180"
                },
                {
                    "name": "Noah Senzel",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/275b1915ce391028ce9762ab390f2c1d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "maya",
                "merge-vr",
                "unity",
                "vuforia",
                "wwise"
            ],
            "content_html": "<div>\n<p>Blackout is Virtual Reality Horror Game which puts you into a pitch black VR environment where you can see nothing and only hear the scary sounds of the monsters creeping up on you. The only source of light you have is a flashlight which you can control using Merge Cube\u2019s AR tracking. </p>\n<p>This game is inspired from the classic Hitchcockian McGuffin technique where the constant sound triggers keep you engaged and the intentional lack of visuals amps up the scare factor. These aspects keep you from easily achieving the main goal of the game; which is to survive as long as you can and kill all the creatures.</p>\n</div>",
            "content_md": "\nBlackout is Virtual Reality Horror Game which puts you into a pitch black VR environment where you can see nothing and only hear the scary sounds of the monsters creeping up on you. The only source of light you have is a flashlight which you can control using Merge Cube\u2019s AR tracking. \n\n\nThis game is inspired from the classic Hitchcockian McGuffin technique where the constant sound triggers keep you engaged and the intentional lack of visuals amps up the scare factor. These aspects keep you from easily achieving the main goal of the game; which is to survive as long as you can and kill all the creatures.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/future-exhibition",
            "title": "Future Exhibition ",
            "blurb": "A public AR frame that provide magical interaction with art pieces, exhibition informations and young artist works.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/-BUhZYr3ln8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "https://vimeo.com/237323723",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/876/datas/original.png"
                },
                {
                    "title": "https://vimeo.com/237323627",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/877/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "chloecg",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/25011670?height=180&v=4&width=180"
                },
                {
                    "name": "Jina Jung",
                    "about": "",
                    "photo": "https://graph.facebook.com/1405470209566109/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "arjs",
                "artoolkit",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration: Contemporary Art that needs site specific to embrace it's full meaning</h2>\n<h2>What it does: Provide magical AR interaction with art pieces, exhibition informations and young artist works.</h2>\n<h2>How we built it: Using AR Toolkit, Ar.js, Unity, Vuforia</h2>\n<h2>Challenges we ran into: Why we need it to be AR? And some technical issue with ar.js since it is a very new library.</h2>\n<h2>Accomplishments that we're proud of: Successfully host another web-server for Ar.js library to run, explore playful interaction with amazing art pieces, and we are proud of ourselves.</h2>\n<h2>What we learned:how to manage a project, the most important thing is not to make it perfect, it is to make progress and make end to end prototype version.</h2>\n<h2>What's next for Future Exhibition:</h2>\n<p>Create social network for this platform, for example when two people watching at the frame from different cities, there will be a way for them to communicate. \nWe want to make this product available for people to purchase, as a new trend for art collection.\nLast but not the least, this platform will have a session for young artist, which will help them start their exposure by share their work at our platform. </p>\n</div>",
            "content_md": "\n## Inspiration: Contemporary Art that needs site specific to embrace it's full meaning\n\n\n## What it does: Provide magical AR interaction with art pieces, exhibition informations and young artist works.\n\n\n## How we built it: Using AR Toolkit, Ar.js, Unity, Vuforia\n\n\n## Challenges we ran into: Why we need it to be AR? And some technical issue with ar.js since it is a very new library.\n\n\n## Accomplishments that we're proud of: Successfully host another web-server for Ar.js library to run, explore playful interaction with amazing art pieces, and we are proud of ourselves.\n\n\n## What we learned:how to manage a project, the most important thing is not to make it perfect, it is to make progress and make end to end prototype version.\n\n\n## What's next for Future Exhibition:\n\n\nCreate social network for this platform, for example when two people watching at the frame from different cities, there will be a way for them to communicate. \nWe want to make this product available for people to purchase, as a new trend for art collection.\nLast but not the least, this platform will have a session for young artist, which will help them start their exposure by share their work at our platform. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/centiment-47dczp",
            "title": "Centiment",
            "blurb": "Brain Powered Advertising ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/TIvxK0tYvNw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jason Azayev",
                    "about": "I worked on the Machine Learning part of the application (pred.py), along with helping the system architecture, RESTful, and visualizations. ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_tBVQ933OEQ6DrRvSUBme4nj0m39uvEqhUBmQ1z0OIK9DrsQTPvuHjzk0HtdDvsZKAPGHj1FxGn5S9mYrZX0sOzEPbn529mGuYX05zqxYuBK8pZJr-vWIv8l7FixpxmvgcbUEtEHdSDM?height=180&width=180"
                },
                {
                    "name": "Benjamin Reichman",
                    "about": "I worked on the mid-tier/system architecture, the data visualizations, and the Unity REST API",
                    "photo": "https://media.licdn.com/mpr/mprx/0_YVqxvNbJACM3LLA8qw8xUtm4zLMyizrDqwCx18h4vnnji5lrtwuxgNm4ttWjQXfTj2axRcTZMt4gLlltAWA_g82Npt4pLlzhNWAynTJMcPDKfbay44b-9zceyFG2ulCAjovPA1VfzR7?height=180&width=180"
                },
                {
                    "name": "Centiment",
                    "about": "",
                    "photo": "https://graph.facebook.com/1458330554288092/picture?height=180&width=180"
                },
                {
                    "name": "Satyam",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/17713264?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "convnet",
                "css3",
                "emotiv",
                "html5",
                "matplot",
                "neo4j",
                "python",
                "python-package-index",
                "r",
                "react",
                "sas",
                "unity"
            ],
            "content_html": "<div>\n<p>Advertising is broken - Pepsi, Facebook, the russian hacking - its all wrong.</p>\n<p>Its an industry lacking in human emotion.</p>\n<p>The answer is Centiment.</p>\n<p>At the intersection of neuroscience and sentiment analysis for advertising, Centiment is focused on equality and ethics, we bring these to the world of advertising in the form of human emotion, intertwined with artificial intelligence.</p>\n<p>We use these things to resolve a problem we are all facing:</p>\n<p>The serving of irrelevant, and sometimes detrimental ads and content to the wrong audience.</p>\n<p>We have all been victim to that online ad or pre-roll video that follows a user around the internet, just because you happened to visit a website once. For some this extends beyond just annoyance to being squeezed into online echo chambers, which limit economically, psychologically and emotionally.</p>\n<p>Poor experiences like this have led to the unprecedented rise in ad blockers.</p>\n<p>Add the limited contextualized targeting capabilities that brands can use for their video advertising campaigns, coupled with the lack of emotional insights available to ad content creators and marketers, it's not surprising that campaign managers are struggling to justify their video ad budgets.</p>\n<p>we have created an AI-driven product to help brands and agencies to:</p>\n<p>understand their audience better\nemotionally optimize written and visual content before release\ntarget the right audience at the right time.</p>\n<p>How we help brands improve their video ad targeting capabilities</p>\n<p>Neurodata + sentiment  = neurosentiment personas</p>\n<p>We have built this first in unity, having created neural data adjusted interruption of unity content based on emotiv EEG scans.</p>\n<p>in the future when we get the resources we would like to deploy this within a real-time bidding environment, and apply further machine learning so that these emotional personas are refined over time.</p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Limited number of training subjects in a short period of time</li>\n<li>time</li>\n<li>lack of access to real time feed from Emotiv</li>\n<li>Unity</li>\n<li>Database </li>\n<li>getting signed in </li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<p>reading peoples minds and creating machine learning in one day that analyses </p>\n<h2>What we learned</h2>\n<p>how to grit</p>\n<h2>What's next for Centiment</h2>\n<p>dominate the world</p>\n<h1>required rule details</h1>\n<p>Micah Brown + Centiment + 929 377 0648\nCarlos Villette Jason Azayev Benjamin Reichman Satyam Sharma\nFilm + Advertising  + EEG Data Driven VR Content Interruption to Advertise\n6 + Table 9 + Table 9\nEnvironment:\nGear VR\nNumpy, Matpotlib, Emotiv, Neo4j (cipher) Unity, C#, HTML, Pyhton, Native (RESTful API)\nEmotiv SDK, Unity SDK,<br/>\nBrain API (on the spot)<br/>\nAssets - N/A \nLibraries - Matplotlib, Numpy\nAll done here\nA link to a video (11:00 pm)\nAll projects should be submitted to the hackathon GitHub account (TBA) before judging begins \nAll projects submitted will be subject to a code-review</p>\n</div>",
            "content_md": "\nAdvertising is broken - Pepsi, Facebook, the russian hacking - its all wrong.\n\n\nIts an industry lacking in human emotion.\n\n\nThe answer is Centiment.\n\n\nAt the intersection of neuroscience and sentiment analysis for advertising, Centiment is focused on equality and ethics, we bring these to the world of advertising in the form of human emotion, intertwined with artificial intelligence.\n\n\nWe use these things to resolve a problem we are all facing:\n\n\nThe serving of irrelevant, and sometimes detrimental ads and content to the wrong audience.\n\n\nWe have all been victim to that online ad or pre-roll video that follows a user around the internet, just because you happened to visit a website once. For some this extends beyond just annoyance to being squeezed into online echo chambers, which limit economically, psychologically and emotionally.\n\n\nPoor experiences like this have led to the unprecedented rise in ad blockers.\n\n\nAdd the limited contextualized targeting capabilities that brands can use for their video advertising campaigns, coupled with the lack of emotional insights available to ad content creators and marketers, it's not surprising that campaign managers are struggling to justify their video ad budgets.\n\n\nwe have created an AI-driven product to help brands and agencies to:\n\n\nunderstand their audience better\nemotionally optimize written and visual content before release\ntarget the right audience at the right time.\n\n\nHow we help brands improve their video ad targeting capabilities\n\n\nNeurodata + sentiment = neurosentiment personas\n\n\nWe have built this first in unity, having created neural data adjusted interruption of unity content based on emotiv EEG scans.\n\n\nin the future when we get the resources we would like to deploy this within a real-time bidding environment, and apply further machine learning so that these emotional personas are refined over time.\n\n\n## Challenges we ran into\n\n\n* Limited number of training subjects in a short period of time\n* time\n* lack of access to real time feed from Emotiv\n* Unity\n* Database\n* getting signed in\n\n\n## Accomplishments that we're proud of\n\n\nreading peoples minds and creating machine learning in one day that analyses \n\n\n## What we learned\n\n\nhow to grit\n\n\n## What's next for Centiment\n\n\ndominate the world\n\n\n# required rule details\n\n\nMicah Brown + Centiment + 929 377 0648\nCarlos Villette Jason Azayev Benjamin Reichman Satyam Sharma\nFilm + Advertising + EEG Data Driven VR Content Interruption to Advertise\n6 + Table 9 + Table 9\nEnvironment:\nGear VR\nNumpy, Matpotlib, Emotiv, Neo4j (cipher) Unity, C#, HTML, Pyhton, Native (RESTful API)\nEmotiv SDK, Unity SDK,  \n\nBrain API (on the spot)  \n\nAssets - N/A \nLibraries - Matplotlib, Numpy\nAll done here\nA link to a video (11:00 pm)\nAll projects should be submitted to the hackathon GitHub account (TBA) before judging begins \nAll projects submitted will be subject to a code-review\n\n\n"
        },
        {
            "source": "https://devpost.com/software/neurolearn",
            "title": "NeuroLearn",
            "blurb": "Informative fMRI Brain network visualization tool ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "paigehf",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/9815305?height=180&v=4&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Making fMRI data sensible </p>\n<h2>What it does</h2>\n<p>Provides an informative visualization for functional brain network data.</p>\n<h2>How I built it</h2>\n<p>With Unity</p>\n<h2>Challenges I ran into</h2>\n<p>Developing in Unity and heavy data processing</p>\n<h2>Accomplishments that I'm proud of</h2>\n<h2>What I learned</h2>\n<h2>What's next for NeuroLearn</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMaking fMRI data sensible \n\n\n## What it does\n\n\nProvides an informative visualization for functional brain network data.\n\n\n## How I built it\n\n\nWith Unity\n\n\n## Challenges I ran into\n\n\nDeveloping in Unity and heavy data processing\n\n\n## Accomplishments that I'm proud of\n\n\n## What I learned\n\n\n## What's next for NeuroLearn\n\n\n"
        },
        {
            "source": "https://devpost.com/software/cat-match-ar",
            "title": "Cat Match AR",
            "blurb": "Cat Match AR is a game that helps match players with their purrfect furry friend through AR kitty interaction.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/JmVBJ4YQ9vA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "in process image showing a kitty avatar in AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/574/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "shylo shepherd",
                    "about": "I worked on art assets, design and general team/project management during the hackathon. Great job everyone!!!",
                    "photo": "https://avatars3.githubusercontent.com/u/5191443?height=180&v=4&width=180"
                },
                {
                    "name": "Meredith Wilson",
                    "about": "I helped with the technical game design and Unity development, especially dealing with the UI. The biggest problem I solved was getting a 3D particle system to track properly with a 2D camera overlay UI canvas. This component is an essential part of the Cat Match AR game play feedback mechanism. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/429/240/datas/profile.png"
                },
                {
                    "name": "Leonard Wedderburn",
                    "about": "I was the programmer that worked with ARcore SDK to work and also place the objects in virtual space",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/958/datas/profile.jpg"
                },
                {
                    "name": "Joe Marchuk",
                    "about": "I took care of sound design, music, and audio implementation for Cat Match AR! I created cat sounds from my own voice as well as a cohesive UI soundscape based on modulating my own whistling. The music adapts to the player's interaction with the cat by changing instrumentation and harmony when the player's affection with the cat reaches a certain level.",
                    "photo": "https://www.gravatar.com/avatar/ed38ebc19147d1af840bcc5bdcd8fb89?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "affinity-designer",
                "arcore",
                "photoshop",
                "pro-tools",
                "reaper",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We all love cats and wanted to make a fun experience in AR interacting with cats. The idea of a cat matching app lent to the opportunity to also help real animals find families. </p>\n<h2>What it does</h2>\n<p>Cat Match AR is an idle game with 5 inputs: pet, feed, play, swap cat to left, and swap cat to right. The player is timed while they interact with a variety of adorable animated cats in the comfort of their own homes through Augmented Reality by petting the kitties, giving the kitties treats and playing with the kitties. Depending on the cat's profile, each kitty's approval meter will fill or empty accordingly to how much they enjoy the activity. Player's can visit with multiple kitties to find the kitty that they get along with the best. At the end of their time, the player is given their \"purrfect match\" and their stats so they can see what kind of interactions the player prefers. This is to help them understand their kitty preferences for actual adoption. </p>\n<h2>How we built it</h2>\n<p>We used Unity 3D and the ARCore SDK. </p>\n<h2>Challenges we ran into</h2>\n<p>Learning how to use the ARCore SDK with Unity in such a short period of time was the big challenge this weekend. That and keeping it working throughout our modifications. It's a new technology and is still being developed, so documentation and stability are still in the beginning stages.  Mentors were a great resource and helped immensely.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are honestly proud of getting it working at all. It's the little achievements throughout the process that make it so rewarding. All of those little aha-moments give you a boost of energy and help you realize how important the small things are in technology. We're also proud of managing our time and scope effectively during the hackathon so that we were well rested, fed and minimized stress.</p>\n<h2>What we learned</h2>\n<p>We got a jumpstart on AR and ARCore, which will be hugely helpful as we continue to pursue AR development in the future.</p>\n<h2>What's next for Cat Match AR</h2>\n<p>The Big Picture is to connect the game to animal shelters and allow them to create avatars for real cats in the game, as well as improve the profiles to incorporate more personality attributes of the cats for more accurate results. An \"idle observation mode\" allowing the kitty avatar to imitate the personality of their real-life cat would be helpful to allow players a chance to see how the real cats they're interacting with might act in their home. The avatars would give cats that are \"less attractive\" a better chance at being adopted, and players a better idea of the types of cat personalities that suit them best so both cat and adopted family are better matched and happier together. We would also like to add mechanics to generate paid content to earn donations for cat rescues and shelters. In the future we would like to develop the same kind of game with dogs. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe all love cats and wanted to make a fun experience in AR interacting with cats. The idea of a cat matching app lent to the opportunity to also help real animals find families. \n\n\n## What it does\n\n\nCat Match AR is an idle game with 5 inputs: pet, feed, play, swap cat to left, and swap cat to right. The player is timed while they interact with a variety of adorable animated cats in the comfort of their own homes through Augmented Reality by petting the kitties, giving the kitties treats and playing with the kitties. Depending on the cat's profile, each kitty's approval meter will fill or empty accordingly to how much they enjoy the activity. Player's can visit with multiple kitties to find the kitty that they get along with the best. At the end of their time, the player is given their \"purrfect match\" and their stats so they can see what kind of interactions the player prefers. This is to help them understand their kitty preferences for actual adoption. \n\n\n## How we built it\n\n\nWe used Unity 3D and the ARCore SDK. \n\n\n## Challenges we ran into\n\n\nLearning how to use the ARCore SDK with Unity in such a short period of time was the big challenge this weekend. That and keeping it working throughout our modifications. It's a new technology and is still being developed, so documentation and stability are still in the beginning stages. Mentors were a great resource and helped immensely.\n\n\n## Accomplishments that we're proud of\n\n\nWe are honestly proud of getting it working at all. It's the little achievements throughout the process that make it so rewarding. All of those little aha-moments give you a boost of energy and help you realize how important the small things are in technology. We're also proud of managing our time and scope effectively during the hackathon so that we were well rested, fed and minimized stress.\n\n\n## What we learned\n\n\nWe got a jumpstart on AR and ARCore, which will be hugely helpful as we continue to pursue AR development in the future.\n\n\n## What's next for Cat Match AR\n\n\nThe Big Picture is to connect the game to animal shelters and allow them to create avatars for real cats in the game, as well as improve the profiles to incorporate more personality attributes of the cats for more accurate results. An \"idle observation mode\" allowing the kitty avatar to imitate the personality of their real-life cat would be helpful to allow players a chance to see how the real cats they're interacting with might act in their home. The avatars would give cats that are \"less attractive\" a better chance at being adopted, and players a better idea of the types of cat personalities that suit them best so both cat and adopted family are better matched and happier together. We would also like to add mechanics to generate paid content to earn donations for cat rescues and shelters. In the future we would like to develop the same kind of game with dogs. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/kinetica-svyxrw",
            "title": "kinetica",
            "blurb": "Pioneering the future of exercise with social virtual reality",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/XnJcjMpAOUw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Debra Do",
                    "about": "I worked on the backend. Created a RESTful Node web service to interface with MongoDB database for persisting sensor data. ",
                    "photo": "https://avatars3.githubusercontent.com/u/858998?height=180&v=4&width=180"
                },
                {
                    "name": "Cameron Feng",
                    "about": "Networking / Multiplayer integration, Game mechanics, Data management",
                    "photo": "https://media.licdn.com/mpr/mprx/0_iXEyCak-qVZHP8mNhhtjopO1AwBW93mWb6OyXVI1lVZH9ApX7XKpL451cO6WP8GN2vKt2M6PgSberLGzfB78EVFx4SbdrLZHfB7xTR-t1ut5V1_N_6JA37p_Z4gJMLfJQqIK65Z4UZW?height=180&width=180"
                },
                {
                    "name": "Jacob Hamman",
                    "about": "Environment Design, Kinect Integration, Game Mechanics",
                    "photo": "https://www.gravatar.com/avatar/f367da7853266d3c6db7c68b6ac049d7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ralston Louie",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/50455d6065b1388e3fed4d2fe4929aa3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "express.js",
                "google-app-engine",
                "html5",
                "javascript",
                "jquery",
                "kinect",
                "mlab",
                "mongodb",
                "node.js",
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration:  Fitness is a key part of life which can be difficult to maintain, but new technologies open the door for immersive, engaging, and insightful new means of exercising.</h2>\n<h2>What it does:</h2>\n<p>Kinetica gamifies different aspects of fitness within a virtual, social context.  Individuals can exercise alone, with many others, and with a specialist (trainer, therapist, clinician).  Our Hackathon build allows for two people to enter into a beautiful environment where one user(the Trainer) guides the other user (Trainee) through different body movements.  These movements could be traditional workout moves (arm raises, curls, squats), sports-specific mechanics(golf swing, yoga, karate maneuvers), dance moves, or physical therapy exercises to promote targeted mobility.  The Microsoft Kinect allows for non-intrusive full body tracking with high accuracy and low latency, allowing the user to see a direct overlay of the trainer along with multiple perspectives of themselves.  </p>\n<h2>How I built it</h2>\n<p>Built with Unity, we are utilizing Oculus Rift for HMD and controller tracking, and Kinect for full body skeletal tracking.\nWe have created a custom backend service running on Google Cloud Services that interfaces with our Unity client to provide realtime data updates about user performance metrics and biosignals.</p>\n<h2>Challenges I ran into:</h2>\n<p>-Figuring out how to deploy web server to Google Cloud Platform\n-Writing Unity scripts to send performance data to web server\n-Implementing networking and interaction logic between two users\n-Hooking the Oculus HMD to Kinect - tracked skeleton \n-Maintaining acceptable frame rates with kinect enabled networked scene.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>-Excellent Team Dynamic\n-Wrangling different hardware/ecosystems into an integrated end product:\nKinect+Oculus+Networking(Multiplayer)+Connection to Database</p>\n<h2>What's next for Kinetica</h2>\n<p>Continue to explore the realm of social, virtual fitness and build towards a healthier future. </p>\n</div>",
            "content_md": "\n## Inspiration: Fitness is a key part of life which can be difficult to maintain, but new technologies open the door for immersive, engaging, and insightful new means of exercising.\n\n\n## What it does:\n\n\nKinetica gamifies different aspects of fitness within a virtual, social context. Individuals can exercise alone, with many others, and with a specialist (trainer, therapist, clinician). Our Hackathon build allows for two people to enter into a beautiful environment where one user(the Trainer) guides the other user (Trainee) through different body movements. These movements could be traditional workout moves (arm raises, curls, squats), sports-specific mechanics(golf swing, yoga, karate maneuvers), dance moves, or physical therapy exercises to promote targeted mobility. The Microsoft Kinect allows for non-intrusive full body tracking with high accuracy and low latency, allowing the user to see a direct overlay of the trainer along with multiple perspectives of themselves. \n\n\n## How I built it\n\n\nBuilt with Unity, we are utilizing Oculus Rift for HMD and controller tracking, and Kinect for full body skeletal tracking.\nWe have created a custom backend service running on Google Cloud Services that interfaces with our Unity client to provide realtime data updates about user performance metrics and biosignals.\n\n\n## Challenges I ran into:\n\n\n-Figuring out how to deploy web server to Google Cloud Platform\n-Writing Unity scripts to send performance data to web server\n-Implementing networking and interaction logic between two users\n-Hooking the Oculus HMD to Kinect - tracked skeleton \n-Maintaining acceptable frame rates with kinect enabled networked scene.\n\n\n## Accomplishments that I'm proud of\n\n\n-Excellent Team Dynamic\n-Wrangling different hardware/ecosystems into an integrated end product:\nKinect+Oculus+Networking(Multiplayer)+Connection to Database\n\n\n## What's next for Kinetica\n\n\nContinue to explore the realm of social, virtual fitness and build towards a healthier future. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/peekn-ar-reality-virtually-hackathon-mit",
            "title": "PeekN- AR ",
            "blurb": "Foobar",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/nMidO4T0iyw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jenny Hu",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xCohPsJ3S0jEE0ZlpFc_1jeXaPgEEAvASFcF6kHTaY32EjZNxkcL1re3j2lCJjUNVPc_3fdkadj7ehLOHdU7kTWFMdjfehWcVdU7c8ZT2dA7ehnpRpnk5NkD7-3iJn4y7-bfAU4mMYu6IkMULFyxU6?height=180&width=180"
                },
                {
                    "name": "soonhokwn",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/25261260?height=180&v=4&width=180"
                },
                {
                    "name": "Thomas P.",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/540/620/datas/profile.png"
                },
                {
                    "name": "Andy Chen",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_iXEyCaF1lHZW1iCeQhp-oMd1qUeWP3SHfXO-XUe1lw8HPApHG6pyfSd1BHAzv8CkEvKpQ46PgSberLUdEB78EVFx4SbdrLZHfB7xTR-t1ut5V1_N_6JA37p_Z4gJMLfJQqIK6LTUJZW?height=180&width=180"
                }
            ],
            "built_with": [
                "arkit",
                "xcode"
            ],
            "content_html": "<div>\n<p>Team: Beacon AR\nLead: Soonho Kwon, 4088073739\nMembers: Soonho Kwon, Rosanne Hu (former member), Thomas Pintaric, Andy Chen\nCategory: Architecture\nBrief Statement: Holding your phone allows you to identify the precise location of your site of interest in your actual visual field. Upon physically reaching your site, you can immersively, without physically obstructing or distracting your field of vision, visualize into the interior of your site.  A final option to examine the 360-degree internal environment of the site is also available.</p>\n<p>Video: <a href=\"https://www.youtube.com/watch?v=nMidO4T0iyw&amp;feature=em-upload_owner\" rel=\"nofollow\">link</a></p>\n<p>Location: Floor 3, Room E15-341, Table Number 57\nEnvironment:\nPlatform: ARKit, Unity, Vuforia\nDevelopment tools: Xcode, Photoscan Pro, Photoshop, iMovie\nSDKs: ARKit\nAPIs: Ricoh API\nAssets: Photogrammetry of MIT Great Dome, 360 Imagery of Great Dome Standard &amp; HDR, 360 Video of Great Dome, MIT locator pin, Video assets, \"Good Starts\" from Youtube Audio Library \nLibraries: CoreLocation and ARKit on ios, and <a href=\"https://github.com/ProjectDent/ARKit-CoreLocation\" rel=\"nofollow\">https://github.com/ProjectDent/ARKit-CoreLocation</a>\nNo components created outside the hackathon.</p>\n<h2>Overview</h2>\n<p>We are using Augmented Reality to provide people with an immersive perspective into the interior environments and dynamics of the physical places around them.  By doing so, we are able to bring immediate glimpses of history, performance, and other experiences right to the moment of discovery. We want to appeal to the wonderment of experiencing something for the first time in a new space, and are excited about scalability to other cultural heritage sites. Our app combines photogrammetry, 360-degree imagery, and augmented reality frameworks to augment a user's visual field with indicators of site location (for example, to identify location), visualization of the artistic essence of a space, and provision of a platform for experiencing that space. </p>\n<h2>Challenges we faced and what we learned</h2>\n<p>Photogrammetry is extremely difficult to perform well in glass, high-glare, and homogeneously-textured environments, and it was difficult to acquire a good quality textured mesh of large interiors. Deciding on a common topic and focus for the team necessitated an ability to compromise as well as a measured level of enthusiasm. In addition, optimizing the geographical overlay of Google Sketchup files of buildings in Cambridge to AR platforms required substantial tweaking and careful measurements\u2014this proved to be too difficult to implement in our given time frame, so we attempted a different approach. Overall, maintaining focus in the face of social and technical adversity was difficult.</p>\n<h2>Accomplishments of which we are proud</h2>\n<p>We divided our project into three distinct aspects: AR for site location, AR of building's interior exhibit on exterior facades, and VR of building internals.  We were able to develop functional versions of each of these goals, which is exciting, although we still need to polish how they tie together.</p>\n</div>",
            "content_md": "\nTeam: Beacon AR\nLead: Soonho Kwon, 4088073739\nMembers: Soonho Kwon, Rosanne Hu (former member), Thomas Pintaric, Andy Chen\nCategory: Architecture\nBrief Statement: Holding your phone allows you to identify the precise location of your site of interest in your actual visual field. Upon physically reaching your site, you can immersively, without physically obstructing or distracting your field of vision, visualize into the interior of your site. A final option to examine the 360-degree internal environment of the site is also available.\n\n\nVideo: [link](https://www.youtube.com/watch?v=nMidO4T0iyw&feature=em-upload_owner)\n\n\nLocation: Floor 3, Room E15-341, Table Number 57\nEnvironment:\nPlatform: ARKit, Unity, Vuforia\nDevelopment tools: Xcode, Photoscan Pro, Photoshop, iMovie\nSDKs: ARKit\nAPIs: Ricoh API\nAssets: Photogrammetry of MIT Great Dome, 360 Imagery of Great Dome Standard & HDR, 360 Video of Great Dome, MIT locator pin, Video assets, \"Good Starts\" from Youtube Audio Library \nLibraries: CoreLocation and ARKit on ios, and <https://github.com/ProjectDent/ARKit-CoreLocation>\nNo components created outside the hackathon.\n\n\n## Overview\n\n\nWe are using Augmented Reality to provide people with an immersive perspective into the interior environments and dynamics of the physical places around them. By doing so, we are able to bring immediate glimpses of history, performance, and other experiences right to the moment of discovery. We want to appeal to the wonderment of experiencing something for the first time in a new space, and are excited about scalability to other cultural heritage sites. Our app combines photogrammetry, 360-degree imagery, and augmented reality frameworks to augment a user's visual field with indicators of site location (for example, to identify location), visualization of the artistic essence of a space, and provision of a platform for experiencing that space. \n\n\n## Challenges we faced and what we learned\n\n\nPhotogrammetry is extremely difficult to perform well in glass, high-glare, and homogeneously-textured environments, and it was difficult to acquire a good quality textured mesh of large interiors. Deciding on a common topic and focus for the team necessitated an ability to compromise as well as a measured level of enthusiasm. In addition, optimizing the geographical overlay of Google Sketchup files of buildings in Cambridge to AR platforms required substantial tweaking and careful measurements\u2014this proved to be too difficult to implement in our given time frame, so we attempted a different approach. Overall, maintaining focus in the face of social and technical adversity was difficult.\n\n\n## Accomplishments of which we are proud\n\n\nWe divided our project into three distinct aspects: AR for site location, AR of building's interior exhibit on exterior facades, and VR of building internals. We were able to develop functional versions of each of these goals, which is exciting, although we still need to polish how they tie together.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/within-dhrvgj",
            "title": "Within MemARy",
            "blurb": "MemARy is a visualisation tool that allows users to build Memory Palaces effortlessly. Never forget anything again!",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Our user",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/724/datas/original.png"
                },
                {
                    "title": "Memory Palace",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/726/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Nathan Yu",
                    "about": "I built the entire software side of the application using Unity and C#.",
                    "photo": "https://www.gravatar.com/avatar/053dfbb8d9339f0ef068d3fe93084ab9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Alla Vovk",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/4f1de54a9d02d884e859936a2509e6b6?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Qingyue99",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/31904741?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-creative-sdk",
                "c#",
                "holotoolkit",
                "maya",
                "microsoft-hololens",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Everyone has moments where we need to recall long lists of information or ideas whether it is studying for a course, remembering concepts, or even the items on a long shopping list. The Ancient Greeks used <strong>Memory Palaces</strong>, an imaginary place in the mind that together with the surrounding environment acts a storage for knowledge. </p>\n<p>The Memory Palace is a concept that allows a user to walk through a familiar place in mind (like your room) and recall different elements of the surroundings that you associate with an object. Along the way, the user can use these different elements to remember a list of concepts that are hard to recall. The idea is that people generally have a good memory for objects in a place that they know that they have for ideas or words. </p>\n<h2>What it does</h2>\n<p>We introduce a concept of learning with a space you live in. In our application, the user can become Sherlock Holmes and evolve the evolution of the brain training, removing the cognitive burden of creating this map in your head. We are using AR and Microsoft HoloLens _ to build this knowledge map. _</p>\n<p>Using spatial mapping \u2014 one of the key features of the Microsoft Hololens \u2014 <strong>we let people use the surrounding physical environment as a memory palace</strong> (a living room, office etc.). Imagine your room as a whiteboard, where all the things you want to remember are augmented objects (3d models, voice recordings, text annotations, images) and you can learn them together within your personalised space. </p>\n<p>Our application allows you to build a map using different objects from a library and attach them to the point where you want to associate them with the physical world. This allows the user to associate the location or object with the specific concept that the user is trying to remember. </p>\n<p>To illustrate this idea, we are using the _ periodic table of elements. _  For example, when the user needs to remember the facts about potassium, he or she can select the object associated with that item, e.g. a banana, and place it in a chosen place such as on a pillow and so on. Once the object is placed, the user can also add <strong>annotations</strong> to these objects.  At the end, you can complete a <strong>quiz</strong> and try to recall all the objects. </p>\n<h2>How I built it</h2>\n<p>We build it with Microsoft HoloLens, C#, HolotoolKit, Unity, VisualStudio, Maya and Adobe CC</p>\n<h2>Challenges I ran into</h2>\n<p>The Hololens is a challenging technology to use. Since it is expensive, it is hard to obtain one and use it for experimentations. Also, there were compatibility issues with the version of Visual Studio working with a specific version of Unity. Another challenge that we had was Hololens picking up movements of people on the space as objects which interfered with the interactions that we wanted to add to the objects in our application.  </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Getting something finally to work. It is hands-free real-world interaction! We made a good representation of the idea, but it is reasonably far away from what we have in our heads (we would want to add more interaction, more guidance for the user and definitely more interactivity)</p>\n<h2>What I learned</h2>\n<p>Good things take time. We learned a lot about integration, cooperation and collaboration. \nWe deepen your understanding in spatial mapping, gaze and voice inputs, learned more about the workflow between Unity and Maya ( the structure is a key). </p>\n<h2>What's next for Within</h2>\n<p>We would continue to work on this project. Spatial mapping is a key thing for AR, and while waiting for an upcoming version of HoloLens, we want to have this app as a working tool. In future, this application can be used not only for memorising things and learning complex concepts but also as a mood board for collecting ideas, inspirations, learning languages (new Italian verbs are on the left wall and some of the new French adjectives near my staircase). </p>\n<h2>Random</h2>\n<p>Team lead - Alla Vovk, <a href=\"mailto:16022839@brookes.ac.uk\" rel=\"nofollow\">16022839@brookes.ac.uk</a>, +4477278778797\nMembers - Nathan Yu, Richard Hsu, Qinqyue Li, Elio Pajares\nCaterogy - Education, AR for Good\nPlace - 3rd floor, table 43</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEveryone has moments where we need to recall long lists of information or ideas whether it is studying for a course, remembering concepts, or even the items on a long shopping list. The Ancient Greeks used **Memory Palaces**, an imaginary place in the mind that together with the surrounding environment acts a storage for knowledge. \n\n\nThe Memory Palace is a concept that allows a user to walk through a familiar place in mind (like your room) and recall different elements of the surroundings that you associate with an object. Along the way, the user can use these different elements to remember a list of concepts that are hard to recall. The idea is that people generally have a good memory for objects in a place that they know that they have for ideas or words. \n\n\n## What it does\n\n\nWe introduce a concept of learning with a space you live in. In our application, the user can become Sherlock Holmes and evolve the evolution of the brain training, removing the cognitive burden of creating this map in your head. We are using AR and Microsoft HoloLens \\_ to build this knowledge map. \\_\n\n\nUsing spatial mapping \u2014 one of the key features of the Microsoft Hololens \u2014 **we let people use the surrounding physical environment as a memory palace** (a living room, office etc.). Imagine your room as a whiteboard, where all the things you want to remember are augmented objects (3d models, voice recordings, text annotations, images) and you can learn them together within your personalised space. \n\n\nOur application allows you to build a map using different objects from a library and attach them to the point where you want to associate them with the physical world. This allows the user to associate the location or object with the specific concept that the user is trying to remember. \n\n\nTo illustrate this idea, we are using the \\_ periodic table of elements. \\_ For example, when the user needs to remember the facts about potassium, he or she can select the object associated with that item, e.g. a banana, and place it in a chosen place such as on a pillow and so on. Once the object is placed, the user can also add **annotations** to these objects. At the end, you can complete a **quiz** and try to recall all the objects. \n\n\n## How I built it\n\n\nWe build it with Microsoft HoloLens, C#, HolotoolKit, Unity, VisualStudio, Maya and Adobe CC\n\n\n## Challenges I ran into\n\n\nThe Hololens is a challenging technology to use. Since it is expensive, it is hard to obtain one and use it for experimentations. Also, there were compatibility issues with the version of Visual Studio working with a specific version of Unity. Another challenge that we had was Hololens picking up movements of people on the space as objects which interfered with the interactions that we wanted to add to the objects in our application. \n\n\n## Accomplishments that I'm proud of\n\n\nGetting something finally to work. It is hands-free real-world interaction! We made a good representation of the idea, but it is reasonably far away from what we have in our heads (we would want to add more interaction, more guidance for the user and definitely more interactivity)\n\n\n## What I learned\n\n\nGood things take time. We learned a lot about integration, cooperation and collaboration. \nWe deepen your understanding in spatial mapping, gaze and voice inputs, learned more about the workflow between Unity and Maya ( the structure is a key). \n\n\n## What's next for Within\n\n\nWe would continue to work on this project. Spatial mapping is a key thing for AR, and while waiting for an upcoming version of HoloLens, we want to have this app as a working tool. In future, this application can be used not only for memorising things and learning complex concepts but also as a mood board for collecting ideas, inspirations, learning languages (new Italian verbs are on the left wall and some of the new French adjectives near my staircase). \n\n\n## Random\n\n\nTeam lead - Alla Vovk, [16022839@brookes.ac.uk](mailto:16022839@brookes.ac.uk), +4477278778797\nMembers - Nathan Yu, Richard Hsu, Qinqyue Li, Elio Pajares\nCaterogy - Education, AR for Good\nPlace - 3rd floor, table 43\n\n\n"
        },
        {
            "source": "https://devpost.com/software/neon-skateboarding",
            "title": "Neon Skateboarding",
            "blurb": "Real skateboarding in VR",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/nUhMSpGBJXM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "ildar iakubov",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/14405387?height=180&v=4&width=180"
                },
                {
                    "name": "beaumanmusic",
                    "about": "",
                    "photo": "https://graph.facebook.com/1875261192720692/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "max/msp",
                "touchdesigner"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are a team of two - digital artist and a musician, we share old passion - skateboarding. Our new passion is VR, which we think lacks tangible and physical experiences. Why dont bring two passions together? The skateboard becomes an interface, a musical instrumeng still being a sports experience - which could never happen in \"real\" world</p>\n<h2>What it does</h2>\n<p>You have a mindblowing audiovisual adventure in VR. And a skateboard which is precisely tracked helps you to go through it. We use generative sound and stunning visuals to create an open, though directed, narration which any viewer could experience environment in int own path. Scenes include skate shoes photogrammetry, LiDAR scans of Russian suburbs, grey cube setup and more, each has its unique sound, which develops according to your motion and skateboard tracking.</p>\n<h2>How we built it</h2>\n<p>Using TouchDesigner and Max/Msp. We paid a lot of attention to interaction nuances to create an experience that would be specifically meaningful in VR. (The scene where you have to put your skateboard to a glitchy cloud and loose it from your sight, but than you suddenly grab it back and see that it radicalyy changed while being out of your sight could be a good example)</p>\n<h2>Challenges we ran into</h2>\n<p>Precise tracking is hard, but doable\nVibration kills tracking. If you need precise tracking using Vive - you should take care of it.\nLow-latency networking between two machines</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>good tracking in the end\ntangible physical experiences\nengaging interactions\na solid audiovisual experience</p>\n<h2>What we learned</h2>\n<p>how to track good\nsome fresh VR interaction tips\nnetworking issues\nparallell sound and visual production</p>\n<h2>What's next for Neon Skateboarding</h2>\n<ul>\n<li>a miniramp (halfpipe) version</li>\n<li>VIVE tracking 2.0 that is releasing in December would allow us stitch multiple trackers, to cover a full skatepark</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are a team of two - digital artist and a musician, we share old passion - skateboarding. Our new passion is VR, which we think lacks tangible and physical experiences. Why dont bring two passions together? The skateboard becomes an interface, a musical instrumeng still being a sports experience - which could never happen in \"real\" world\n\n\n## What it does\n\n\nYou have a mindblowing audiovisual adventure in VR. And a skateboard which is precisely tracked helps you to go through it. We use generative sound and stunning visuals to create an open, though directed, narration which any viewer could experience environment in int own path. Scenes include skate shoes photogrammetry, LiDAR scans of Russian suburbs, grey cube setup and more, each has its unique sound, which develops according to your motion and skateboard tracking.\n\n\n## How we built it\n\n\nUsing TouchDesigner and Max/Msp. We paid a lot of attention to interaction nuances to create an experience that would be specifically meaningful in VR. (The scene where you have to put your skateboard to a glitchy cloud and loose it from your sight, but than you suddenly grab it back and see that it radicalyy changed while being out of your sight could be a good example)\n\n\n## Challenges we ran into\n\n\nPrecise tracking is hard, but doable\nVibration kills tracking. If you need precise tracking using Vive - you should take care of it.\nLow-latency networking between two machines\n\n\n## Accomplishments that we're proud of\n\n\ngood tracking in the end\ntangible physical experiences\nengaging interactions\na solid audiovisual experience\n\n\n## What we learned\n\n\nhow to track good\nsome fresh VR interaction tips\nnetworking issues\nparallell sound and visual production\n\n\n## What's next for Neon Skateboarding\n\n\n* a miniramp (halfpipe) version\n* VIVE tracking 2.0 that is releasing in December would allow us stitch multiple trackers, to cover a full skatepark\n\n\n"
        },
        {
            "source": "https://devpost.com/software/surgvr",
            "title": "SurgVR",
            "blurb": "A VR surgery education and practicing platform. used VR headset and provides haptic feedback using Vibration.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Sarfraz Ahmad",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b37ce38af182aec3d4e86d2d23039036?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "orochi663",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/2004233?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "android",
                "samsung-gear-vr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Improve quality of learning for medical professionals working in the field of surgery and making it cost effective for individual to learn anywhere using the commonly available VR platforms. We wanted to make sure the surgeon is well focused in application so we want to measure the performance of user by measuring precision and attention level of user. </p>\n<h2>What it does</h2>\n<p>Surgeons or future Surgeons can use this App to explore organs and practice surgery on them, in VR environment, using the commonly available VR platforms and/or controllers. It provides haptic feedback using vibration motor of smartphone and/or watch. The App can read EEG headset data to measure attention/concentration level of user which becomes as input to performance measurement.</p>\n<h2>How we built it</h2>\n<ul>\n<li>We used Unity 3D v2017 to design models of stomach and patient model. On phone side we worked on communicating with Android watch for vibrator.</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>The main challenge was to do everything in short time. So we have to drop techniques that require lot of time and instead focused on prototyping a basic demo of the idea/concept.</li>\n<li>There was no EEG headset hardware available for hackathon this year, so we had to work without it and make it optional part.</li>\n<li>We wanted to use operation theater props to create a close to real environment but unity asset for that was paid and finally after talking with mentors we have to let it go.</li>\n<li>We struggled with Oculus App loading on Samsung Galaxy 8 phone. It wasted our 8 hours, finally after multiple tries we were able to load it.\n## Accomplishments that we're proud of</li>\n<li>We are happy that we were able to give our idea a try on this platform. We got chance to utilize our time and resources as a team.\n## What we learned</li>\n<li>working with Oculus platform using Samsung Gear VR.</li>\n<li>We learned about some of the evolving startups working in related space, even had a chance to talk to one of the mentor who is leading a startup in this space.</li>\n</ul>\n<h2>What's next for SurgVR</h2>\n<ul>\n<li>We would like to improve the App and incorporate features we could not. Since it is open source so we would like to  invite community to contribute in it so it can fulfill its purpose.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nImprove quality of learning for medical professionals working in the field of surgery and making it cost effective for individual to learn anywhere using the commonly available VR platforms. We wanted to make sure the surgeon is well focused in application so we want to measure the performance of user by measuring precision and attention level of user. \n\n\n## What it does\n\n\nSurgeons or future Surgeons can use this App to explore organs and practice surgery on them, in VR environment, using the commonly available VR platforms and/or controllers. It provides haptic feedback using vibration motor of smartphone and/or watch. The App can read EEG headset data to measure attention/concentration level of user which becomes as input to performance measurement.\n\n\n## How we built it\n\n\n* We used Unity 3D v2017 to design models of stomach and patient model. On phone side we worked on communicating with Android watch for vibrator.\n\n\n## Challenges we ran into\n\n\n* The main challenge was to do everything in short time. So we have to drop techniques that require lot of time and instead focused on prototyping a basic demo of the idea/concept.\n* There was no EEG headset hardware available for hackathon this year, so we had to work without it and make it optional part.\n* We wanted to use operation theater props to create a close to real environment but unity asset for that was paid and finally after talking with mentors we have to let it go.\n* We struggled with Oculus App loading on Samsung Galaxy 8 phone. It wasted our 8 hours, finally after multiple tries we were able to load it.\n## Accomplishments that we're proud of\n* We are happy that we were able to give our idea a try on this platform. We got chance to utilize our time and resources as a team.\n## What we learned\n* working with Oculus platform using Samsung Gear VR.\n* We learned about some of the evolving startups working in related space, even had a chance to talk to one of the mentor who is leading a startup in this space.\n\n\n## What's next for SurgVR\n\n\n* We would like to improve the App and incorporate features we could not. Since it is open source so we would like to invite community to contribute in it so it can fulfill its purpose.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/wearablewerewolves",
            "title": "WearableWerewolves",
            "blurb": "Exploring AR interfaces and interactions, as wolves. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/6aAbWrYN0ek?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Albert Schweitzer",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c5d805c5862bac69139d705e75f789b3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "raspberry-pi",
                "unity",
                "vuforia",
                "watchos"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The inspiration for our Augmented Wearables project was based out of frustration of navigating and understanding the screen based interactions on current smart watches. We wanted to use Augmented Reality to overcome the difficulty many users face when using smart watches. </p>\n<h2>What it does</h2>\n<p>We present a proof of concept interface design which represents the potential future of Apple Watch when used in conjunction with mixed/augmented reality head-mounted displays. Our applications features 3 menu designs which could be used on the watch; a main watch face, a health/workout app, and a Spotify music player. The watch also connects with smart home devices, and can monitor data coming from these devices to give the user updates about their home. We use the Raspberry Pi for this IoT feature. </p>\n<h2>How we built it</h2>\n<p>We used Vuforia, Unity for iOS, and a Raspberry Pi in order to generate our vision. The Unity application is sending and receiving data between the Raspberry Pi for the IoT portion of the concept, and the UI is completely custom built. Because we could not get access to an AR head mounted display such as Hololens or Meta, we are using an iPhone 8Plus to present our concept with the idea being that it is viewed from AR/MR glasses. </p>\n<h2>Challenges we ran into</h2>\n<p>Networking. We also had trouble with Apple's ARKit as it does not use image tracking in its functionality. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Getting connection between our application and an IoT device, have a clear, intuitive interface. </p>\n<h2>What we learned</h2>\n<p>There are still many difficulties ahead in terms of user interaction with gesture, however we demonstrate that still being able to use a screen or touch pad to navigate an AR interface is extremely promising for future application designs. </p>\n<h2>What's next for WearableWerewolves</h2>\n<p>We will continue to explore the connected device realm and how augmented interfaces can play a part in this future. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe inspiration for our Augmented Wearables project was based out of frustration of navigating and understanding the screen based interactions on current smart watches. We wanted to use Augmented Reality to overcome the difficulty many users face when using smart watches. \n\n\n## What it does\n\n\nWe present a proof of concept interface design which represents the potential future of Apple Watch when used in conjunction with mixed/augmented reality head-mounted displays. Our applications features 3 menu designs which could be used on the watch; a main watch face, a health/workout app, and a Spotify music player. The watch also connects with smart home devices, and can monitor data coming from these devices to give the user updates about their home. We use the Raspberry Pi for this IoT feature. \n\n\n## How we built it\n\n\nWe used Vuforia, Unity for iOS, and a Raspberry Pi in order to generate our vision. The Unity application is sending and receiving data between the Raspberry Pi for the IoT portion of the concept, and the UI is completely custom built. Because we could not get access to an AR head mounted display such as Hololens or Meta, we are using an iPhone 8Plus to present our concept with the idea being that it is viewed from AR/MR glasses. \n\n\n## Challenges we ran into\n\n\nNetworking. We also had trouble with Apple's ARKit as it does not use image tracking in its functionality. \n\n\n## Accomplishments that we're proud of\n\n\nGetting connection between our application and an IoT device, have a clear, intuitive interface. \n\n\n## What we learned\n\n\nThere are still many difficulties ahead in terms of user interaction with gesture, however we demonstrate that still being able to use a screen or touch pad to navigate an AR interface is extremely promising for future application designs. \n\n\n## What's next for WearableWerewolves\n\n\nWe will continue to explore the connected device realm and how augmented interfaces can play a part in this future. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/rupert",
            "title": "Rupert",
            "blurb": "Relive your childhoods worst nightmares in VR and fight the monsters by using your teddy bear as a tangible interface",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/KSpsgJqZchY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Oh no, be careful! There is also a monster under the bed!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/418/datas/original.PNG"
                },
                {
                    "title": "Lock the doors! There is something out there, coming to get you!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/419/datas/original.PNG"
                },
                {
                    "title": "Interaction with the environment is only possible by interaction with the toy in reality",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/555/datas/original.JPG"
                },
                {
                    "title": "The incorporated Vive controller enabled the user to interact with the teddy bear and also indirectly with the environment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/521/datas/original.jpg"
                },
                {
                    "title": "This is the teddy bear toy we used to build a tangible interface which can be touched in reality and also comes alive in VR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/567/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Fin Ambsdorf",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/32569726?height=180&v=4&width=180"
                },
                {
                    "name": "Dorothee Schmidt",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b51bceffdee3ece5f4e25c35672fd334?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Charlie Pennington",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9830a49a8f08e29994c8c8e9df0b74c0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tushar Purang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/768/datas/profile.png"
                },
                {
                    "name": "Maximegau",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/26672876?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "htc-vive",
                "maya",
                "photoshop",
                "steam-vr",
                "unity",
                "wwise",
                "zbrush"
            ],
            "content_html": "<div>\n<h2>The Project</h2>\n<p><strong>What we were going for:</strong>\nDrawing from our own experience with VR applications we wanted to create the most immersive VR experience, leaving the user emotionally invested. Therefore we asked ourselves how a <em>tangible interface</em> could change the way we interact with the virtual world. Therefore we turned a real life teddy bear into a controlling device for VR, by cutting open the back and incorporating the Vive controller into the toy and animating a version of it in VR. This offers the user a variety of possible interactions the teddy bear itself by picking him up, shaking him, hugging him, lifting him up to objects he can not reach on his own or squeezing him.  </p>\n<p><strong>What it looks like:</strong>\nThunderstorms and intense music are setting the mood in the surreal, dark bedroom of a child. the infinite ceiling and towering furniture are scaled to make the user feel like they are a child again. Under the bed and outside monsters are formed out of the shadows. Fighting them is only possible by interaction with the teddy bear. Completing these tasks and comforting the toy, which reacts to both the environment and the user\u2019s actions, forms an <em>emotional bond that outlasts the duration of the experience itself</em>. The biggest challenges we were facing were animating the toy\u2019s states, reactions and syncing it to the storyline as well as the accompanying sounds and visuals. This could easily be solved by spending more time on development. </p>\n<p><strong>What's next for Rupert:</strong>\nIn the future our approach could be used for educational purposes, to teach empathy and responsibility, recapture childhood fears, engage user more easily with VR by tangible interfaces or simply for fun. </p>\n<h2>Team Information</h2>\n<p><strong>Team Lead:</strong> Charlie Pennington</p>\n<p><strong>Team Name:</strong> Ruperts Friends</p>\n<p><strong>Team Lead's mobile number:</strong> 8133734212</p>\n<p><strong>Location:</strong> 6th Floor, E14 674, Table 12</p>\n<h2>Environment</h2>\n<p><strong>Plattform:</strong> PC,HTC Vive</p>\n<p><strong>Development tools:</strong> Unity, Maya, Blender, Photoshop, Mono, Visual Studio  </p>\n<p><strong>SDKs:</strong> Wwise (Audio)</p>\n<p><strong>APIs:</strong> SteamVR</p>\n<p><strong>Assets:</strong> 3D Warehouse Assets ( see 3D Asset List) </p>\n<p><strong>Libraries:</strong> ( SteamVR) </p>\n<p><strong>We used the following Third Party Assets:</strong></p>\n<p>3D Models under Creative Common from SketchUp 3d Warehouse Terms of Use <a href=\"https://3dwarehouse.sketchup.com/tos.html\" rel=\"nofollow\">link</a></p>\n<p>Bed <a href=\"https://3dwarehouse.sketchup.com/model/655dcd675ca7da8fcaf22871626d9a34/Bed-Frame\" rel=\"nofollow\">link</a></p>\n<p>Door <a href=\"https://3dwarehouse.sketchup.com/model/u4727fa09-70b8-4f8d-a9ee-2fabf36b9e3d/30-Six-Panel-Door-30x80-With-Hinges-and-Latch\" rel=\"nofollow\">link</a></p>\n<p>Window <a href=\"https://3dwarehouse.sketchup.com/model/d95728d1-aa98-4619-a97e-569d7d9bbbf3/Quad-Pane-Sash-Window\" rel=\"nofollow\">link</a></p>\n<p>Wardrobe <a href=\"https://3dwarehouse.sketchup.com/model/7f50a6bb1827b9f582bfcd34215ccfc7/eclectic-wardrobe\" rel=\"nofollow\">link</a></p>\n<p>Chest <a href=\"https://3dwarehouse.sketchup.com/model/u3284e43b-d512-4fe8-a541-8e29234b6ea8/Teak-Chest\" rel=\"nofollow\">link</a></p>\n<p>Tree <a href=\"https://3dwarehouse.sketchup.com/model/38c6cbdb7f75874cc52adcbe8dab860b/Dead-Tree\" rel=\"nofollow\">link</a></p>\n</div>",
            "content_md": "\n## The Project\n\n\n**What we were going for:**\nDrawing from our own experience with VR applications we wanted to create the most immersive VR experience, leaving the user emotionally invested. Therefore we asked ourselves how a *tangible interface* could change the way we interact with the virtual world. Therefore we turned a real life teddy bear into a controlling device for VR, by cutting open the back and incorporating the Vive controller into the toy and animating a version of it in VR. This offers the user a variety of possible interactions the teddy bear itself by picking him up, shaking him, hugging him, lifting him up to objects he can not reach on his own or squeezing him. \n\n\n**What it looks like:**\nThunderstorms and intense music are setting the mood in the surreal, dark bedroom of a child. the infinite ceiling and towering furniture are scaled to make the user feel like they are a child again. Under the bed and outside monsters are formed out of the shadows. Fighting them is only possible by interaction with the teddy bear. Completing these tasks and comforting the toy, which reacts to both the environment and the user\u2019s actions, forms an *emotional bond that outlasts the duration of the experience itself*. The biggest challenges we were facing were animating the toy\u2019s states, reactions and syncing it to the storyline as well as the accompanying sounds and visuals. This could easily be solved by spending more time on development. \n\n\n**What's next for Rupert:**\nIn the future our approach could be used for educational purposes, to teach empathy and responsibility, recapture childhood fears, engage user more easily with VR by tangible interfaces or simply for fun. \n\n\n## Team Information\n\n\n**Team Lead:** Charlie Pennington\n\n\n**Team Name:** Ruperts Friends\n\n\n**Team Lead's mobile number:** 8133734212\n\n\n**Location:** 6th Floor, E14 674, Table 12\n\n\n## Environment\n\n\n**Plattform:** PC,HTC Vive\n\n\n**Development tools:** Unity, Maya, Blender, Photoshop, Mono, Visual Studio \n\n\n**SDKs:** Wwise (Audio)\n\n\n**APIs:** SteamVR\n\n\n**Assets:** 3D Warehouse Assets ( see 3D Asset List) \n\n\n**Libraries:** ( SteamVR) \n\n\n**We used the following Third Party Assets:**\n\n\n3D Models under Creative Common from SketchUp 3d Warehouse Terms of Use [link](https://3dwarehouse.sketchup.com/tos.html)\n\n\nBed [link](https://3dwarehouse.sketchup.com/model/655dcd675ca7da8fcaf22871626d9a34/Bed-Frame)\n\n\nDoor [link](https://3dwarehouse.sketchup.com/model/u4727fa09-70b8-4f8d-a9ee-2fabf36b9e3d/30-Six-Panel-Door-30x80-With-Hinges-and-Latch)\n\n\nWindow [link](https://3dwarehouse.sketchup.com/model/d95728d1-aa98-4619-a97e-569d7d9bbbf3/Quad-Pane-Sash-Window)\n\n\nWardrobe [link](https://3dwarehouse.sketchup.com/model/7f50a6bb1827b9f582bfcd34215ccfc7/eclectic-wardrobe)\n\n\nChest [link](https://3dwarehouse.sketchup.com/model/u3284e43b-d512-4fe8-a541-8e29234b6ea8/Teak-Chest)\n\n\nTree [link](https://3dwarehouse.sketchup.com/model/38c6cbdb7f75874cc52adcbe8dab860b/Dead-Tree)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/teleport-to-wonder",
            "title": "VR the World",
            "blurb": "A WebVR experience to teach children about the 7 Wonders of the World",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/237329206?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Mario Dcunha",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/694/datas/profile.jpg"
                }
            ],
            "built_with": [
                "aframe",
                "css3",
                "html5",
                "jquery",
                "webvr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We did a project in school to vary the idea in different forms. I started off with an idea to creatively teach people about the 7 wonders because many of them are ignorant about it. Finally, I ended up making my first VR product, and that too on my most preferred platform the web. I was intrigued by AFrame when I landed at the RealityVirtually Hackathon at MIT Media Lab.\nAlso, I'm one of the craziest persons you will find about the 7 wonders of the world. I have visited three so far.</p>\n<h2>What it does</h2>\n<p>This provides a WebVR experience and gives you the impression and feel to study the basic history and understand the new 7 wonders of the world by teleporting you virtually there, with visuals and audio. This can be expanded to rebuild a feel of the ancient 7 wonders of the world as well.</p>\n<h2>How I built it</h2>\n<p>I built this with all the basic info that I learned in the first 2 days at the RealityVirtually Hackathon at MIT Media Lab.</p>\n<h2>Challenges I ran into</h2>\n<p>It was difficult to understand the difference in tags with the a-frame semantics and the regular HTML but with a little trial and error, I was able to work around it. With more time, I could add CSS animations, transitions, focussing the clicks on certain areas and also build the 3D models of at least the ancient wonders that do not exist today.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>This is my first venture into VR and I'm so happy about it. What makes me glad is that I managed to build this within two days on my own, inside Web, because web is one of my favorite platforms! It's just fun to know that there's something like Aframe out there and I can't wait to dig in more. Also I hope this serves as an inspiration to those who want to build quick VR apps. This is a good start to help children get virtually the real experience so that they can understand stuff better.</p>\n<h2>What I learned</h2>\n<p>Just make it! Get down and dirty and just make it, no matter what people say. </p>\n<h2>What's next for Teleport-to-Wonder</h2>\n<p>Adding better and focussed interactions, expanding it to the ancient 7 wonders and giving a try to build 3D models with Aframes to replicate the extinct ancient wonders.</p>\n<p><a href=\"http://mariodcunha.com/7wotwvr/\" rel=\"nofollow\">http://mariodcunha.com/7wotwvr/</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe did a project in school to vary the idea in different forms. I started off with an idea to creatively teach people about the 7 wonders because many of them are ignorant about it. Finally, I ended up making my first VR product, and that too on my most preferred platform the web. I was intrigued by AFrame when I landed at the RealityVirtually Hackathon at MIT Media Lab.\nAlso, I'm one of the craziest persons you will find about the 7 wonders of the world. I have visited three so far.\n\n\n## What it does\n\n\nThis provides a WebVR experience and gives you the impression and feel to study the basic history and understand the new 7 wonders of the world by teleporting you virtually there, with visuals and audio. This can be expanded to rebuild a feel of the ancient 7 wonders of the world as well.\n\n\n## How I built it\n\n\nI built this with all the basic info that I learned in the first 2 days at the RealityVirtually Hackathon at MIT Media Lab.\n\n\n## Challenges I ran into\n\n\nIt was difficult to understand the difference in tags with the a-frame semantics and the regular HTML but with a little trial and error, I was able to work around it. With more time, I could add CSS animations, transitions, focussing the clicks on certain areas and also build the 3D models of at least the ancient wonders that do not exist today.\n\n\n## Accomplishments that I'm proud of\n\n\nThis is my first venture into VR and I'm so happy about it. What makes me glad is that I managed to build this within two days on my own, inside Web, because web is one of my favorite platforms! It's just fun to know that there's something like Aframe out there and I can't wait to dig in more. Also I hope this serves as an inspiration to those who want to build quick VR apps. This is a good start to help children get virtually the real experience so that they can understand stuff better.\n\n\n## What I learned\n\n\nJust make it! Get down and dirty and just make it, no matter what people say. \n\n\n## What's next for Teleport-to-Wonder\n\n\nAdding better and focussed interactions, expanding it to the ancient 7 wonders and giving a try to build 3D models with Aframes to replicate the extinct ancient wonders.\n\n\n<http://mariodcunha.com/7wotwvr/>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/mindful-testing-virtual-reality",
            "title": "Mindful Testing Virtual Reality (MTVR)",
            "blurb": "Problem: Test taking anxiety. Solution: Reinventing the testing process with a VR experience based on mindfulness.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/WdM2IXyH_H0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Mindful Testing Virtual Reality",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/543/303/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Andy Ng",
                    "about": "First time working on a VR project! Led UX/UI design and content research.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/543/280/datas/profile.JPG"
                },
                {
                    "name": "thebobak",
                    "about": "I worked on UI/UX design, content research and assisted with VR programming",
                    "photo": "https://avatars0.githubusercontent.com/u/1463923?height=180&v=4&width=180"
                },
                {
                    "name": "nx306",
                    "about": "Primary developer on this project, implement most of it under John and others leadership :)",
                    "photo": "https://avatars0.githubusercontent.com/u/7034258?height=180&v=4&width=180"
                },
                {
                    "name": "John Kendall",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0__RLv0aEc53md3ySyfEgqYow963uekg3K7EgRMUL9Qcrd3SSS50PvcSFc3NrqGxplfElvNakBokTWFM-T7ehWcVdU7kTHFMdg7ehnpRpnk5NkD7-3iJn4y7-bfAU4mMYu6IkMUWu8ZV6?height=180&width=180"
                },
                {
                    "name": "Gabby Phan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/6bf9465a98f56fde37f1b1a88e6057ad?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "oculus",
                "unreal-engine"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Research has shown that test-taking anxiety can affect kids starting in kindergarten right up through graduate school. Tests are a fundamental part of education, yet there has been little innovation in (1) re-imagining what testing looks like through technology, and more importantly, how technology might help to address test-taking anxiety. According to NYU, 49% of students surveyed reported feeling a great deal of stress on a daily basis. A substantial minority, 26 percent of participants, reported symptoms of depression at a clinically significant level.</p>\n<h2>What it does</h2>\n<p>Rather than mimicking a traditional classroom, Mindful Testing Virtual Reality (MTVR) places the user into a more relaxing, virtual beach environment. Before testing begins, MTVR guides the user through a simple meditation known as square breathing. After this is completed, the formal assessment is initiated. In this current demo, the course subject is geometry and is explored via 3D object identification. Placed along the beach, the user is asked to correctly identify figures such as cubes, cones, and cylinders.</p>\n<h2>How we built it</h2>\n<p>Technical:</p>\n<ul>\n<li>We utilized Unreal Engine to build the experience</li>\n<li>Audio was self-recorded and made via GarageBand</li>\n</ul>\n<p>Non-technical:\nWe first conducted preliminary research on a wide range of topics including test-taking, anxiety, meditation, virtual environments, and course content. Next was constructing an experience roadmap of what the user would see sequentially, along with visual mockups of the shapes, breathing exercise, and environment.</p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Timing</li>\n<li>Getting acquainted with the software</li>\n<li>Translating curricular material into something that can be assessed tactically via VR</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<ul>\n<li>Without any prior experience, two members of the team learned how to use Unreal Engine</li>\n<li>Building an aesthetically pleasing experience in less than two days</li>\n<li>All five members of the team contributing their unique skillsets to the project (ideation, education, mindfulness, Unreal Engine, UX, project management, voice acting)</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>How to use Unreal Engine</li>\n<li>How effective gamifying the experience contributes to enjoyment and engagement</li>\n<li>Limitations of Oculus Hardware</li>\n<li>Importance of clear instructions and how to optimize that with audio and visual instructions for VR</li>\n</ul>\n<h2>What's next for Mindful Testing Virtual Reality</h2>\n<ul>\n<li>Further experimentation with porting this to WebVR to obtain more adoption</li>\n<li>Potential Collaboration/integration with Seaing Breath VR: <a href=\"https://seaingbreath.com/\" rel=\"nofollow\">https://seaingbreath.com/</a> </li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nResearch has shown that test-taking anxiety can affect kids starting in kindergarten right up through graduate school. Tests are a fundamental part of education, yet there has been little innovation in (1) re-imagining what testing looks like through technology, and more importantly, how technology might help to address test-taking anxiety. According to NYU, 49% of students surveyed reported feeling a great deal of stress on a daily basis. A substantial minority, 26 percent of participants, reported symptoms of depression at a clinically significant level.\n\n\n## What it does\n\n\nRather than mimicking a traditional classroom, Mindful Testing Virtual Reality (MTVR) places the user into a more relaxing, virtual beach environment. Before testing begins, MTVR guides the user through a simple meditation known as square breathing. After this is completed, the formal assessment is initiated. In this current demo, the course subject is geometry and is explored via 3D object identification. Placed along the beach, the user is asked to correctly identify figures such as cubes, cones, and cylinders.\n\n\n## How we built it\n\n\nTechnical:\n\n\n* We utilized Unreal Engine to build the experience\n* Audio was self-recorded and made via GarageBand\n\n\nNon-technical:\nWe first conducted preliminary research on a wide range of topics including test-taking, anxiety, meditation, virtual environments, and course content. Next was constructing an experience roadmap of what the user would see sequentially, along with visual mockups of the shapes, breathing exercise, and environment.\n\n\n## Challenges we ran into\n\n\n* Timing\n* Getting acquainted with the software\n* Translating curricular material into something that can be assessed tactically via VR\n\n\n## Accomplishments that we're proud of\n\n\n* Without any prior experience, two members of the team learned how to use Unreal Engine\n* Building an aesthetically pleasing experience in less than two days\n* All five members of the team contributing their unique skillsets to the project (ideation, education, mindfulness, Unreal Engine, UX, project management, voice acting)\n\n\n## What we learned\n\n\n* How to use Unreal Engine\n* How effective gamifying the experience contributes to enjoyment and engagement\n* Limitations of Oculus Hardware\n* Importance of clear instructions and how to optimize that with audio and visual instructions for VR\n\n\n## What's next for Mindful Testing Virtual Reality\n\n\n* Further experimentation with porting this to WebVR to obtain more adoption\n* Potential Collaboration/integration with Seaing Breath VR: <https://seaingbreath.com/>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-or-8xpmrv",
            "title": "Affordable Haptic Feedback Tool for Surgical Training",
            "blurb": "VR\u2019s ability to \u201ctouch us back\u201d can be the difference between life and death in surgical training",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/pYo-0m0_K2I?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Schematic of how the Scalpel could work, including the rotational base",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/550/datas/original.png"
                },
                {
                    "title": "Robotic arm",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/531/datas/original.jpg"
                },
                {
                    "title": "Schematic of how feedback is provided by friction servos",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/551/datas/original.png"
                },
                {
                    "title": "Schematic detailing how the servo provides feedback to the user",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/552/datas/original.png"
                },
                {
                    "title": "User interacting with the 3D environment. Project of the 3D environment in a big screen is also possible",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/903/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Helena Deus",
                    "about": "I built a virtual model of the \"arm\" in Unity to simulate force in each potentiometer. I worked on the documentation. ",
                    "photo": "https://avatars2.githubusercontent.com/u/770297?height=180&v=4&width=180"
                },
                {
                    "name": "Uwe Gruenefeld",
                    "about": "Development of Software components.",
                    "photo": "https://avatars2.githubusercontent.com/u/6775963?height=180&v=4&width=180"
                },
                {
                    "name": "Mo Kakwan",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/315474?height=180&v=4&width=180"
                },
                {
                    "name": "MichalTL",
                    "about": "",
                    "photo": "https://graph.facebook.com/1847290348631592/picture?height=180&width=180"
                },
                {
                    "name": "HarrisAli1291",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32585935?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "c#",
                "node.js",
                "openscad",
                "unity"
            ],
            "content_html": "<div>\n<h1>Who we are</h1>\n<p>Team name: VR-OR<br/>\nTeam Lead: Mo Kakwan<br/>\nOther Members: Helena Deus, Uwe Gruenefeld, Alisha Harris, Michal Leszczynski<br/>\nCategory: Healthcare &amp; Medicine<br/></p>\n<h2>Inspiration</h2>\n<h3>Inspiration was a personal story for each one of us</h3>\n<p>AH: One of my best friends is a surgeon. She shared with me that she was appalled by the lack of quality of surgical procedures performed in developing countries. There is lack of resources to buy sophisticated and highly accurate robotic surgical and simulation surgical equipment. It must be possible to circumvent these problems in the digital age with a simpler way to provide high-level affordable training to those highly passionate doctors.<br/>\nMK: My cousin uses the Da Vinci machine for most of his surgical procedures. I asked him \u201cwhat kind of haptic feedback does it give?\u201d He said: \u201cnone whatsoever\u201d. And it is so expensive they do not even publish price ranges on the website.<br/>\nML: At my previous company a surgeon came to us with a vision to provide a more consistent cardiac surgery training. Why? 5-10% fewer patients experience severe complications if the surgeon has frequent interaction with the procedure.<br/>\nUG: The best thing is that opportunities are limitless \u2013 no need for material engineering to simulate new surgical procedures. In fact, any skill that requires delicate manual work can be honed with this approach!<br/>\nHD: It is startling that the amazing advances in VR were not nearly matched by feedback mechanisms that could come with them. Joystick vibration was hip in the 90s when I got my first Nintendo\u2026 We need a \u201cGoogle Cardboard for Haptic Devices\u201d<br/></p>\n<p>In summary, we are inspired to offer a solution to VR-aided surgery training that is:<br/>\nAffordable \u2013 it should be the \"google cardboard\" for haptic devices<br/>\nOriginal and Innovative \u2013 even super-expensive surgical gear does not give haptic feedback today<br/>\nMorally necessary \u2013 can save people\u2019s life and health through better training of surgeons<br/>\nWidely applicable \u2013 should be easy to adapt to other applications such as paintbrushes or other handheld devices<br/>\nAddressing a niche \u2013 visuals progressed at crazy pace since 90s, feedback mechanisms have not. It is time to change that.<br/></p>\n<h2>What it does</h2>\n<p>We have developed a piece of hardware \u2013 a robotic arm \u2013 that is able to provide haptic feedback and thus provide a much more immersive AR/VR experience that can be help develop muscle memory and even help students and others empathize with surgeons.</p>\n<h2>How we built it</h2>\n<p>The architecture for the proposed solution included hardware components such as sensors and 3D printed joints and software components, including Unity and an input/output module interfaced with arduino via node.js.</p>\n<p>Platforms: Arduino IDE, Blender, Unity<br/>\nDevelopment tools: MonoDeveloper, SublimeText, Atom<br/>\nSDKs &amp; APIs: None<br/>\nAssets: CAD models, copyright free mp3, Unity pre-fab sphere, Blender model of a scalpel (all assets in github)<br/>\nLibraries: Vuforia, nodejs-websocket, serialport<br/>\n<a href=\"https://github.com/Reality-Virtually-Hackathon/VR-OR/tree/master/unity/Assets\" rel=\"nofollow\">Github Link</a></p>\n<p>Hardware: For the robotic arm, we used 3 potentiometers, 2 common wood skewers (\u201cbones\u201d) and 3D printed wheels and \u201csockets\u201d that acted as the joints between the various components. The \u201cscalpel\u201d consisted of another skewer connected to the tip of the arm using a spherical magnet. Wiring was also used to connect the potentiometers to an arduino platform (model Uno R3), which in turn connected to a computer via a USB port. The robotic arm as a whole <a href=\"https://github.com/Reality-Virtually-Hackathon/VR-OR/blob/master/documentation/pictures/robotic_arm.jpg\" rel=\"nofollow\">(see picture)</a> was allowed to freely rotate around a base (up to 270\u00b0). The \u201cscalpel\u201d itself includes a multi-target - a 2D printed cube, with a different texture in each face, optimized to capture orientation of the blade. The haptic feedback is achieved through the use of servo motors - these are connected to the wheels at the joints and, upon feedback from the 3D virtual model, prevent the rotation of the potentiometers (holding them in place), the effect of which is felt as tension on the \u201cscalpel\u201d when an object in the virtual world is touched. The models for all 3D printed objects were created using OpenScad.</p>\n<p>Software: Each potentiometer generates input information in the form of serials. These values are integrated into Unity via a websocket which connects to a node.js server listening to the input available via a USB port  <a href=\"https://github.com/Reality-Virtually-Hackathon/VR-OR/blob/master/documentation/diagrams/architecture.pdf\" rel=\"nofollow\">(see architecture)</a>. Each of the 3 inputs are transformed into proxies for motion in a 3D vortex space. Blender was used to create a 3D model of the \u201cscalpel\u201d and imported into unity. Unity\u2019s physics engine was then used to represent the mesh deformation and the force associated  \u2013 a sphere mesh is generated on the fly representing the objects that the \u201cscalpel\u201d is cutting. The distance between the scalpel and the center of the sphere is used to compute a value for \u201cforce\u201d, which reflects the resistance of the interaction between the scalpel and the skin. Finally, the force is transmitted to the servo motor controlling the forward/backward potentiometer motion. </p>\n<h2>Challenges we ran into</h2>\n<p>Hardware:<br/>\n1) The \u201cbones\u201d were too heavy to be held together only by our 3D printed joints. This was a challenge early on. Our solution was to add three wheels, one in each joint, which were connected to each other by a string, offering structural support. This solution turned out to also allow a nice mechanism to connect the servo engine.<br/>\n2) At first we connected the wheels with rubber bands, but then it turned out that they are too elastic, i.e. even if a wheel is blocked by a servo motor, the arm can move some more before the band is stretched and starts to give feedback. Substituting rubber bands with dental floss, which is not elastic, helped with this. It also can be cut at any length desired.<br/>\n3) Precisely detecting in the 3D world each of the 6 degrees of freedom (3 axis x 2 direction in each) in our system. We solved this with 3 potentiometers, held together with 3D printed pieces and stabilized by servo motors.<br/>\n4) Friction provided by the arm itself. In an ideal world, there would be zero feedback from the arm when the pen / scalpel is not touching anything (i.e. only friction provided by air), which is not the case for us (friction of the wheels at the base and joints). Moreover, the \u201cnatural\u201d feedback of the arm is different depending on its position and direction of pen / scalpel movement. Solving / optimizing this as a next step post-hackathon.<br/>\n5) We \"burned\" our first arduino board soldering to the wrong pins. The only solution was to buy a new board<br/></p>\n<p>Software:<br/>\n1) Input/Output: one problem we needed to solve well, given the need for very accurate tracking of the tip of the scalpel in 3D space. Errors in sensor Input: the sensor input should in principle work but the sensors kept emitting numbers when the object was stable. This caused some difficulties as it made the virtual object vibrate when in fact it should be static.<br/>\n2) Detection of scalpel direction: in real life motion, the scalpel doesn\u2019t just move along one axis at a time, there is a 6-degree of freedom movement that we needed to capture. To do this, we implemented a mechanism of multi-targets. Our initial solution was to add a single image target, which only worked for one degree of freedom. In the next iteration, we create a paper cube and attached that to the back of the scalpel. Each face of the cube was printed with a natural pattern (e.g. pebbles), which significantly improved detection. To know exactly what is the rotation of the scalpel regardless of tracking we took this even one step further and printed a cube with a different pattern on each face. The faces were 60mm in size. <br/>\n3) Calculation of force: In our system, we needed to represent how force associated with each of 6 degrees of freedom was transferred to each of the 3 robotic arm: for forward/backward motion resistance, up/down motion resistance and left/right motion resistance. As a first step, we can only distinguish between presence or absence of resistance. <br/></p>\n<h2>Accomplishments that we're proud of</h2>\n<p>1) The complexity of the interaction with a robotic arm was reduced to 3 simple numbers which could be easily manipulated to extrapolate to a virtual reality and then again used as feedback to the haptic device.<br/>\n2) The elegance of the mechanism for transforming deformation of an object in Unity into a force or tension value that can be applied in the real world<br/>\n3) Extraordinarily good and productive interaction between curious and passionate teammates who will likely become good friends<br/></p>\n<h2>What we learned</h2>\n<p>1) That open source tools make it relatively straightforward to connect the real world to the virtual world - in both directions<br/>\n2) To not try to work with hardware when you\u2019re tired, boards can get fried :-)<br/>\n3) Unity is a great tool for coders and non-coders alike who want to get jump started into VR as it offers flexibility and power without loss of intuitiveness<br/>\n4) Plan, be realistic about deadlines, solve the full problem first without falling into rabbit holes and then iterate over the solution to perfect it. <br/>\n5) That anything is possible with hard work, determination and when we're all aligned with one goal and vision in mind.<br/></p>\n<h2>What's next for VR-OR</h2>\n<p>We are not going to rock the world in two days, but we can start building a bridge to something that will. The next stage has three objectives: <br/>\n1) to implement and test/increase robustness of the approach, in particular the hardware component - to bring it to a point where it delivers a satisfactory experience without compromising simplicity and affordability (e.g. replacing dental floss with a chain, optimizing size of elements, improving sturdiness);  <br/>\n2) to increase the size of the components used in the robotic arm, use more robust connections or at least larger pieces; <br/>\n3) to apply the solution to as many use cases as possible and prepare a DIY guide and videos to inspire others (e.g. school teachers) and support its growth via the open source community. <br/></p>\n<p>At the end of this Hackaton, we were not too far from a solution that could become the <strong>\u201cGoogle Cardboard for Heptic Devices\u201d</strong> but more testing and documenting is needed so that other are able to contribute. Open sourcing the 3D models will get other smart and curious developers involved in improving the whole system. </p>\n</div>",
            "content_md": "\n# Who we are\n\n\nTeam name: VR-OR  \n\nTeam Lead: Mo Kakwan  \n\nOther Members: Helena Deus, Uwe Gruenefeld, Alisha Harris, Michal Leszczynski  \n\nCategory: Healthcare & Medicine  \n\n\n\n## Inspiration\n\n\n### Inspiration was a personal story for each one of us\n\n\nAH: One of my best friends is a surgeon. She shared with me that she was appalled by the lack of quality of surgical procedures performed in developing countries. There is lack of resources to buy sophisticated and highly accurate robotic surgical and simulation surgical equipment. It must be possible to circumvent these problems in the digital age with a simpler way to provide high-level affordable training to those highly passionate doctors.  \n\nMK: My cousin uses the Da Vinci machine for most of his surgical procedures. I asked him \u201cwhat kind of haptic feedback does it give?\u201d He said: \u201cnone whatsoever\u201d. And it is so expensive they do not even publish price ranges on the website.  \n\nML: At my previous company a surgeon came to us with a vision to provide a more consistent cardiac surgery training. Why? 5-10% fewer patients experience severe complications if the surgeon has frequent interaction with the procedure.  \n\nUG: The best thing is that opportunities are limitless \u2013 no need for material engineering to simulate new surgical procedures. In fact, any skill that requires delicate manual work can be honed with this approach!  \n\nHD: It is startling that the amazing advances in VR were not nearly matched by feedback mechanisms that could come with them. Joystick vibration was hip in the 90s when I got my first Nintendo\u2026 We need a \u201cGoogle Cardboard for Haptic Devices\u201d  \n\n\n\nIn summary, we are inspired to offer a solution to VR-aided surgery training that is:  \n\nAffordable \u2013 it should be the \"google cardboard\" for haptic devices  \n\nOriginal and Innovative \u2013 even super-expensive surgical gear does not give haptic feedback today  \n\nMorally necessary \u2013 can save people\u2019s life and health through better training of surgeons  \n\nWidely applicable \u2013 should be easy to adapt to other applications such as paintbrushes or other handheld devices  \n\nAddressing a niche \u2013 visuals progressed at crazy pace since 90s, feedback mechanisms have not. It is time to change that.  \n\n\n\n## What it does\n\n\nWe have developed a piece of hardware \u2013 a robotic arm \u2013 that is able to provide haptic feedback and thus provide a much more immersive AR/VR experience that can be help develop muscle memory and even help students and others empathize with surgeons.\n\n\n## How we built it\n\n\nThe architecture for the proposed solution included hardware components such as sensors and 3D printed joints and software components, including Unity and an input/output module interfaced with arduino via node.js.\n\n\nPlatforms: Arduino IDE, Blender, Unity  \n\nDevelopment tools: MonoDeveloper, SublimeText, Atom  \n\nSDKs & APIs: None  \n\nAssets: CAD models, copyright free mp3, Unity pre-fab sphere, Blender model of a scalpel (all assets in github)  \n\nLibraries: Vuforia, nodejs-websocket, serialport  \n\n[Github Link](https://github.com/Reality-Virtually-Hackathon/VR-OR/tree/master/unity/Assets)\n\n\nHardware: For the robotic arm, we used 3 potentiometers, 2 common wood skewers (\u201cbones\u201d) and 3D printed wheels and \u201csockets\u201d that acted as the joints between the various components. The \u201cscalpel\u201d consisted of another skewer connected to the tip of the arm using a spherical magnet. Wiring was also used to connect the potentiometers to an arduino platform (model Uno R3), which in turn connected to a computer via a USB port. The robotic arm as a whole [(see picture)](https://github.com/Reality-Virtually-Hackathon/VR-OR/blob/master/documentation/pictures/robotic_arm.jpg) was allowed to freely rotate around a base (up to 270\u00b0). The \u201cscalpel\u201d itself includes a multi-target - a 2D printed cube, with a different texture in each face, optimized to capture orientation of the blade. The haptic feedback is achieved through the use of servo motors - these are connected to the wheels at the joints and, upon feedback from the 3D virtual model, prevent the rotation of the potentiometers (holding them in place), the effect of which is felt as tension on the \u201cscalpel\u201d when an object in the virtual world is touched. The models for all 3D printed objects were created using OpenScad.\n\n\nSoftware: Each potentiometer generates input information in the form of serials. These values are integrated into Unity via a websocket which connects to a node.js server listening to the input available via a USB port [(see architecture)](https://github.com/Reality-Virtually-Hackathon/VR-OR/blob/master/documentation/diagrams/architecture.pdf). Each of the 3 inputs are transformed into proxies for motion in a 3D vortex space. Blender was used to create a 3D model of the \u201cscalpel\u201d and imported into unity. Unity\u2019s physics engine was then used to represent the mesh deformation and the force associated \u2013 a sphere mesh is generated on the fly representing the objects that the \u201cscalpel\u201d is cutting. The distance between the scalpel and the center of the sphere is used to compute a value for \u201cforce\u201d, which reflects the resistance of the interaction between the scalpel and the skin. Finally, the force is transmitted to the servo motor controlling the forward/backward potentiometer motion. \n\n\n## Challenges we ran into\n\n\nHardware:  \n\n1) The \u201cbones\u201d were too heavy to be held together only by our 3D printed joints. This was a challenge early on. Our solution was to add three wheels, one in each joint, which were connected to each other by a string, offering structural support. This solution turned out to also allow a nice mechanism to connect the servo engine.  \n\n2) At first we connected the wheels with rubber bands, but then it turned out that they are too elastic, i.e. even if a wheel is blocked by a servo motor, the arm can move some more before the band is stretched and starts to give feedback. Substituting rubber bands with dental floss, which is not elastic, helped with this. It also can be cut at any length desired.  \n\n3) Precisely detecting in the 3D world each of the 6 degrees of freedom (3 axis x 2 direction in each) in our system. We solved this with 3 potentiometers, held together with 3D printed pieces and stabilized by servo motors.  \n\n4) Friction provided by the arm itself. In an ideal world, there would be zero feedback from the arm when the pen / scalpel is not touching anything (i.e. only friction provided by air), which is not the case for us (friction of the wheels at the base and joints). Moreover, the \u201cnatural\u201d feedback of the arm is different depending on its position and direction of pen / scalpel movement. Solving / optimizing this as a next step post-hackathon.  \n\n5) We \"burned\" our first arduino board soldering to the wrong pins. The only solution was to buy a new board  \n\n\n\nSoftware:  \n\n1) Input/Output: one problem we needed to solve well, given the need for very accurate tracking of the tip of the scalpel in 3D space. Errors in sensor Input: the sensor input should in principle work but the sensors kept emitting numbers when the object was stable. This caused some difficulties as it made the virtual object vibrate when in fact it should be static.  \n\n2) Detection of scalpel direction: in real life motion, the scalpel doesn\u2019t just move along one axis at a time, there is a 6-degree of freedom movement that we needed to capture. To do this, we implemented a mechanism of multi-targets. Our initial solution was to add a single image target, which only worked for one degree of freedom. In the next iteration, we create a paper cube and attached that to the back of the scalpel. Each face of the cube was printed with a natural pattern (e.g. pebbles), which significantly improved detection. To know exactly what is the rotation of the scalpel regardless of tracking we took this even one step further and printed a cube with a different pattern on each face. The faces were 60mm in size.   \n\n3) Calculation of force: In our system, we needed to represent how force associated with each of 6 degrees of freedom was transferred to each of the 3 robotic arm: for forward/backward motion resistance, up/down motion resistance and left/right motion resistance. As a first step, we can only distinguish between presence or absence of resistance.   \n\n\n\n## Accomplishments that we're proud of\n\n\n1) The complexity of the interaction with a robotic arm was reduced to 3 simple numbers which could be easily manipulated to extrapolate to a virtual reality and then again used as feedback to the haptic device.  \n\n2) The elegance of the mechanism for transforming deformation of an object in Unity into a force or tension value that can be applied in the real world  \n\n3) Extraordinarily good and productive interaction between curious and passionate teammates who will likely become good friends  \n\n\n\n## What we learned\n\n\n1) That open source tools make it relatively straightforward to connect the real world to the virtual world - in both directions  \n\n2) To not try to work with hardware when you\u2019re tired, boards can get fried :-)  \n\n3) Unity is a great tool for coders and non-coders alike who want to get jump started into VR as it offers flexibility and power without loss of intuitiveness  \n\n4) Plan, be realistic about deadlines, solve the full problem first without falling into rabbit holes and then iterate over the solution to perfect it.   \n\n5) That anything is possible with hard work, determination and when we're all aligned with one goal and vision in mind.  \n\n\n\n## What's next for VR-OR\n\n\nWe are not going to rock the world in two days, but we can start building a bridge to something that will. The next stage has three objectives:   \n\n1) to implement and test/increase robustness of the approach, in particular the hardware component - to bring it to a point where it delivers a satisfactory experience without compromising simplicity and affordability (e.g. replacing dental floss with a chain, optimizing size of elements, improving sturdiness);   \n\n2) to increase the size of the components used in the robotic arm, use more robust connections or at least larger pieces;   \n\n3) to apply the solution to as many use cases as possible and prepare a DIY guide and videos to inspire others (e.g. school teachers) and support its growth via the open source community.   \n\n\n\nAt the end of this Hackaton, we were not too far from a solution that could become the **\u201cGoogle Cardboard for Heptic Devices\u201d** but more testing and documenting is needed so that other are able to contribute. Open sourcing the 3D models will get other smart and curious developers involved in improving the whole system. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/dental-vr-training-simulator",
            "title": "Dental VR Training Simulator",
            "blurb": "Provide a sandbox environment to young dental students and out of practice dental practitioners to hone their skills",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/KvyMooPAvTs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ardacam",
                    "about": "I worked on creating the 3D models for the simulation. Also worked on sound for simulation. ",
                    "photo": "https://graph.facebook.com/10212508410923941/picture?height=180&width=180"
                },
                {
                    "name": "preetish Kakkar",
                    "about": "- Ideation \n- Application flow/Mockup design\n- Tools interaction with tooth & semantics for same\n- Haptic feedback \n- Programming\n- Design introduction scenes",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/888/datas/profile.jpg"
                },
                {
                    "name": "Kelvin Mei",
                    "about": "Created the human character using makehuman. Added armatures, poses, and teeth separation through Blender. Modified sounds through Audacity.",
                    "photo": "https://graph.facebook.com/100003577782216/picture?height=180&width=180"
                },
                {
                    "name": "Nitish Jain",
                    "about": "- Idea brainstorming\n- Idea finalizing\n- Project division into procedural steps\n- Unity model interaction scripting\n- Project management\n- Bookkeeping to keep the project submission on track",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Pj_hS-J6JoSieAvMA0ch8qmF4xtCeKz4yjchI6nFZOqie8IZN0caL6UFO73CHiM4pjMaW94bqWa_INCkVmQp56JwlWaGIN7MMmQimF2QseRm3CNLz0rSaPGBApq-CN4erDG2dMKurY2?height=180&width=180"
                },
                {
                    "name": "Makivic",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/9288823?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "3ds-max",
                "audiocity",
                "autodesk-fusion-360",
                "blender",
                "c#",
                "makehuman",
                "steamvr-sdk",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Learning theoretically about dental procedures does not usually provide a convincing picture to the students new to the field of dentistry. Our VR training simulator will provide them a platform to practice their craft without the fear of making mistakes. We believe that true learning is always achieved when one learns from his/her mistakes. But, the stakes are too high when dealing with real life patients. Our VR training sim will allow budding future dentists to train in an environment where they are free to make mistakes and learn through them, all while making sure no actual patients get hurt!</p>\n<h2>What it does</h2>\n<p>Simulates a dental office where the user acts as a practicing dentist. In this environment the user has to follow a particular set of steps to perform a dental cavity filling on an infected tooth.</p>\n<h2>How we built it</h2>\n<ul>\n<li>Consulted with a dentist to lay out the procedure for dental cavity filling procedure.</li>\n<li>Models generated using Blender, Fusion 360, Makehuman and 3dsMax</li>\n<li>Scripting done using C# in Unity</li>\n</ul>\n<h2>Assets</h2>\n<ul>\n<li>Dental Chair (Downloaded from Free3D)</li>\n<li>Room</li>\n<li>Table</li>\n<li>Monitor</li>\n<li>Dental Drill</li>\n<li>Dental spoon excavator</li>\n<li>Human Jaw with teeth</li>\n<li>Menu scene from VRSamples (Free)</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Fitting all the work in the given time</li>\n<li>Interference from other VIVE setups in the room</li>\n<li>Precision required for dental models is challenging to replicate with current VR tools</li>\n<li>Creating good quality low poly models for VR</li>\n</ul>\n<h2>Accomplishments that we are proud of</h2>\n<ul>\n<li>Team formation to bring in the right folks for the project</li>\n<li>A fun environment within the team</li>\n<li>Project moving along a steady clip, realizing milestones as we planned</li>\n<li>Haptic interactions modeled for dental tools</li>\n<li>Realistic dental drill sound incorporated</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Team effort required to realize a creative project</li>\n<li>No work is too small!</li>\n<li>Learning from the guys who are great at the skills they bring to the table</li>\n<li>VR is awesome but there is still some ways to go for it to really mimic real life scenarios</li>\n</ul>\n<h2>What's next for Dental VR Training Simulator</h2>\n<ul>\n<li>More complex dental procedures</li>\n<li>Integration with leap motion sensor to provide precise hand movements required to practice dexterity</li>\n<li>Integration with precise haptic tools to mimic the actual dental tools in VR environment</li>\n<li>Provide more realistic collision detection to warn the user when he/she affects non-infected teeth</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nLearning theoretically about dental procedures does not usually provide a convincing picture to the students new to the field of dentistry. Our VR training simulator will provide them a platform to practice their craft without the fear of making mistakes. We believe that true learning is always achieved when one learns from his/her mistakes. But, the stakes are too high when dealing with real life patients. Our VR training sim will allow budding future dentists to train in an environment where they are free to make mistakes and learn through them, all while making sure no actual patients get hurt!\n\n\n## What it does\n\n\nSimulates a dental office where the user acts as a practicing dentist. In this environment the user has to follow a particular set of steps to perform a dental cavity filling on an infected tooth.\n\n\n## How we built it\n\n\n* Consulted with a dentist to lay out the procedure for dental cavity filling procedure.\n* Models generated using Blender, Fusion 360, Makehuman and 3dsMax\n* Scripting done using C# in Unity\n\n\n## Assets\n\n\n* Dental Chair (Downloaded from Free3D)\n* Room\n* Table\n* Monitor\n* Dental Drill\n* Dental spoon excavator\n* Human Jaw with teeth\n* Menu scene from VRSamples (Free)\n\n\n## Challenges we ran into\n\n\n* Fitting all the work in the given time\n* Interference from other VIVE setups in the room\n* Precision required for dental models is challenging to replicate with current VR tools\n* Creating good quality low poly models for VR\n\n\n## Accomplishments that we are proud of\n\n\n* Team formation to bring in the right folks for the project\n* A fun environment within the team\n* Project moving along a steady clip, realizing milestones as we planned\n* Haptic interactions modeled for dental tools\n* Realistic dental drill sound incorporated\n\n\n## What we learned\n\n\n* Team effort required to realize a creative project\n* No work is too small!\n* Learning from the guys who are great at the skills they bring to the table\n* VR is awesome but there is still some ways to go for it to really mimic real life scenarios\n\n\n## What's next for Dental VR Training Simulator\n\n\n* More complex dental procedures\n* Integration with leap motion sensor to provide precise hand movements required to practice dexterity\n* Integration with precise haptic tools to mimic the actual dental tools in VR environment\n* Provide more realistic collision detection to warn the user when he/she affects non-infected teeth\n\n\n"
        },
        {
            "source": "https://devpost.com/software/in-your-skin",
            "title": "In Your Skin",
            "blurb": "IYS is a VR experience that helps improve your empathy by seeing the world from a different skin than your own.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Barrie24",
                    "about": "I developed the 3D models, design, and the scripting for the following of the avatars. ",
                    "photo": "https://graph.facebook.com/1732642697044162/picture?height=180&width=180"
                },
                {
                    "name": "Miranda Monroe",
                    "about": "I developed 3d architectural models, edited skins, and implemented follow scripting.",
                    "photo": "https://media.licdn.com/mpr/mprx/0__NBxO7ijA-eWq527oLSl1glpv6aHqzKiEL3ARVjpNheWq5gOuN7xKoQpPvqHqqiPkn7OxHWyMGCdBApaoFt_9UeKpGCeBlJx2FtyjsggciBXgbGi8nX-gaPuyBZZOlmm5k9PsXNk29p?height=180&width=180"
                },
                {
                    "name": "Elaine Harris",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/32585506?height=180&v=4&width=180"
                },
                {
                    "name": "Imen Maaroufi",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_-m7HL3c6fJRjeSoBj0JHwXgFERETmuNB40RQ_NR6wj6hmpqw-EB6Sz9FHYvha0MFY0BWenvbiY68DHvcOjEqDzNwQY63DHKIOjEI6qTQSx1SFOwBtup5Q87B6IpAkHFVBYaXTQyfcV-?height=180&width=180"
                },
                {
                    "name": "Dan Feng",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3f67876796b5fe38abfcc6185b39db80?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "freesound.org",
                "htcunity-plug-in",
                "instantvr",
                "steamvr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>In a diverse world, where humans are tending to be global citizens, minorities still face discrimination. Through In Your skin immersive experience, we help individuals get our of their comfort zone to experience what others feel, see and hear. The objective of this experience is to build empathy, eliminate discrimination and change the world for the better thanks to VR technology.</p>\n<h2>What it does</h2>\n<p>In Your Skin, provides a virtual experience in a different body than yours to help you feel what another feels. Different scenarios, based on real life experiences, are projected to establish connection between the user and their alternate reality.  </p>\n<h2>How we built it</h2>\n<p>With a team of 5 women from different backgrounds, we were able to allocate design, development, UX, storytelling, and business resources to build a store environment and shopping scenarios, where the user is a shopper who sees himself in the mirror, decides on his avatar/experience, leaves the fitting room, and meets the outside world that will treat him differently every time.</p>\n<p>We used 3D modeling to design and create the assets, setting on Cinema4D, and programming the models on Unity.</p>\n<h2>Challenges we ran into</h2>\n<p>The problem of bias that In Your Skin is trying to address is sensitive. We had to be thoughtful and mindful of others\u2019 feelings and not perpetuate stereotypes, but also be honest in exposing the problem.  The time constraints limited the possible scenarios we could develop, so we chose a single setting- a store that sells apparel- and we limited  avatar variables to race.  We also faced technical challenges as motion tracking with the 3D models, transferring shaders from C4D to Unity, and hardware problems using HTC Vive.  The base stations did not function consistently.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The most relevant outcome of this hackathon was to be able to quickly bond and work as a team with women from different backgrounds who had never met each other.  Each readily embraced the concept,worked hard, and did their best to make it happen.</p>\n<p>The motivation we were able to keep, even with some technical issues, was outstanding. We are also proud of the willingness to do, and learn by doing.  </p>\n<p>We were pleasantly surprised with the number of other hackathon participants who sought us out to tell us how much they liked our idea.  We were interviewed by a freelance reporter and our journey has been documented in photography and video by a reporter from a Japanese magazine.</p>\n<h2>What we learned</h2>\n<p>We learned a lot about the components of VR, developing for VR, and working under pressure to deliver on time. \nAnd perhaps, most importantly, we learned to work as a team with members who were previously strangers but became united around shared values and striving together for excellence. </p>\n<h2>What's next for In Your Skin</h2>\n<p>We see In Your skin as a project that will be turned into a real business, where we use VR for Good and social justice. In Your Skin could be a training program for multiple institutions, enabling them to create more inclusive cultures that effectively utilize diverse talent.  Data shows that diverse organizations are the most successful. \nVariables of gender, ethnicity, physical impairment, and others can be explored.  Multiple scenarios can be developed and presented in sequences customized to users, AI interactivity can be added, more extensive experiences, such as a whole day in a different skin at work, a first day in school, a day in a new country, and more. In Your Skin could also be used in research and data collection for sociology, psychology and anthropology reasons, or for cultural studies. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nIn a diverse world, where humans are tending to be global citizens, minorities still face discrimination. Through In Your skin immersive experience, we help individuals get our of their comfort zone to experience what others feel, see and hear. The objective of this experience is to build empathy, eliminate discrimination and change the world for the better thanks to VR technology.\n\n\n## What it does\n\n\nIn Your Skin, provides a virtual experience in a different body than yours to help you feel what another feels. Different scenarios, based on real life experiences, are projected to establish connection between the user and their alternate reality. \n\n\n## How we built it\n\n\nWith a team of 5 women from different backgrounds, we were able to allocate design, development, UX, storytelling, and business resources to build a store environment and shopping scenarios, where the user is a shopper who sees himself in the mirror, decides on his avatar/experience, leaves the fitting room, and meets the outside world that will treat him differently every time.\n\n\nWe used 3D modeling to design and create the assets, setting on Cinema4D, and programming the models on Unity.\n\n\n## Challenges we ran into\n\n\nThe problem of bias that In Your Skin is trying to address is sensitive. We had to be thoughtful and mindful of others\u2019 feelings and not perpetuate stereotypes, but also be honest in exposing the problem. The time constraints limited the possible scenarios we could develop, so we chose a single setting- a store that sells apparel- and we limited avatar variables to race. We also faced technical challenges as motion tracking with the 3D models, transferring shaders from C4D to Unity, and hardware problems using HTC Vive. The base stations did not function consistently.\n\n\n## Accomplishments that we're proud of\n\n\nThe most relevant outcome of this hackathon was to be able to quickly bond and work as a team with women from different backgrounds who had never met each other. Each readily embraced the concept,worked hard, and did their best to make it happen.\n\n\nThe motivation we were able to keep, even with some technical issues, was outstanding. We are also proud of the willingness to do, and learn by doing. \n\n\nWe were pleasantly surprised with the number of other hackathon participants who sought us out to tell us how much they liked our idea. We were interviewed by a freelance reporter and our journey has been documented in photography and video by a reporter from a Japanese magazine.\n\n\n## What we learned\n\n\nWe learned a lot about the components of VR, developing for VR, and working under pressure to deliver on time. \nAnd perhaps, most importantly, we learned to work as a team with members who were previously strangers but became united around shared values and striving together for excellence. \n\n\n## What's next for In Your Skin\n\n\nWe see In Your skin as a project that will be turned into a real business, where we use VR for Good and social justice. In Your Skin could be a training program for multiple institutions, enabling them to create more inclusive cultures that effectively utilize diverse talent. Data shows that diverse organizations are the most successful. \nVariables of gender, ethnicity, physical impairment, and others can be explored. Multiple scenarios can be developed and presented in sequences customized to users, AI interactivity can be added, more extensive experiences, such as a whole day in a different skin at work, a first day in school, a day in a new country, and more. In Your Skin could also be used in research and data collection for sociology, psychology and anthropology reasons, or for cultural studies. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/ghost-simulator",
            "title": "Ghost Simulator",
            "blurb": "You're a ghost and kids are in your house! You need to scare them away!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/xWNcAiG3BDY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Kathleen Kamali",
                    "about": "I was team lead, idea person and Unity dev. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/537/549/datas/profile.jpg"
                },
                {
                    "name": "Joe Marchuk",
                    "about": "I took care of sound design, music, and audio implementation! I created a music system that layers instruments as the player scares more children. There's also a theremin melody that plays while the player is using their mic input to move around the level. In addition, I created a sound system that creates a more immersive environment by having dynamic sound effects that randomize and react to player actions. ",
                    "photo": "https://www.gravatar.com/avatar/ed38ebc19147d1af840bcc5bdcd8fb89?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dan Donato",
                    "about": "Code, and designed the \"spooky\" object interactions. My favorite was the hands reaching out of the walls.",
                    "photo": "https://avatars2.githubusercontent.com/u/10944648?height=180&v=4&width=180"
                },
                {
                    "name": "Sherry Huai-Hsuan Shih",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_0Bi5Z6uCe930yDo1rBUw0T9CIXt0yjoPyTJ5g-JCfbRYJSbxrBRw0XaCD530MxVxyPJ5vhSG6qKOsJEazX6MK-ua_qKxsMitKX6QU1Miez51K74apvKHRbB-iTE71MN_JbCWp3m3zSe?height=180&width=180"
                }
            ],
            "built_with": [
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Halloween's just around the corner! It's time to scare children and be festive!</p>\n<h2>What it does</h2>\n<p>It's a first person horror game, where you are the horror. You also control your ghostly corporeal form with your voice. Say \"boooooooo\" and watch yourself fly. </p>\n<h2>How we built it</h2>\n<p>We used Unity and the available Oculus SDK to build this game. </p>\n<h2>Challenges we ran into</h2>\n<p>Slight differences between the two dev computers led to strange bugs. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Alternate control schemes are the best!</p>\n<h2>What we learned</h2>\n<p>The right source control is important. Microphone input isn't that bad to set up! </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nHalloween's just around the corner! It's time to scare children and be festive!\n\n\n## What it does\n\n\nIt's a first person horror game, where you are the horror. You also control your ghostly corporeal form with your voice. Say \"boooooooo\" and watch yourself fly. \n\n\n## How we built it\n\n\nWe used Unity and the available Oculus SDK to build this game. \n\n\n## Challenges we ran into\n\n\nSlight differences between the two dev computers led to strange bugs. \n\n\n## Accomplishments that we're proud of\n\n\nAlternate control schemes are the best!\n\n\n## What we learned\n\n\nThe right source control is important. Microphone input isn't that bad to set up! \n\n\n"
        },
        {
            "source": "https://devpost.com/software/produce-ar",
            "title": "Produce-AR",
            "blurb": "An immersive AR musical production app that connects to a peripheral to register sounds.  User can arrange/play.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/I-zGXmtL3Mk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "peripheral that hooks onto shoe",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/586/datas/original.jpg"
                },
                {
                    "title": "app screenshot",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/587/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Ankush Gola",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/597/datas/profile.jpg"
                }
            ],
            "built_with": [
                "adafruit-accelerometer",
                "adafruit-bluetooth",
                "arduino",
                "arkit",
                "ios-avaudio",
                "ios-corebluetooth",
                "ios-vision-framework",
                "scenekit"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Mobile AR is big these days.  However, the user is usually limited to screen gestures to interact with the augmented world.  I wanted to build a mobile AR application that can talk to a hardware peripheral to make the experience more interactive.</p>\n<h2>What it does</h2>\n<p>An iOS augmented reality music production app created with ARKit that connects to a Bluetooth shoe peripheral to register sound hits.  In Recording mode you can register hits.  Hover over the shoe to change instruments.  Enter playback mode to walk through your arrangement and listen to it as you walk (both forwards and backwards), or simply press play.  Built over one weekend at Reality Virtually Hackathon 2017 at MIT.</p>\n<h2>How I built it</h2>\n<p>The iOS app is built with ARKit.  It connects to a hardware peripheral (a shoe) via bluetooth.  The app sends the shoe a notification to vibrate when the instrument selection menu pops up.  The shoe sends the app sound hits.  An Arduino on the controls the bluetooth shield, accelerometer, and vibration motor. </p>\n<p>The app runs animations, listens to bluetooth input, and runs computer vision on separate threads to avoid main thread latency.</p>\n<h2>Challenges I ran into</h2>\n<p>The system has a ton of different parts (bluetooth, sound, animation, computer vision), so getting them all to work/run simultaneously without race conditions was a challenge.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Got it working!</p>\n<h2>What I learned</h2>\n<p>A lot about ARKit, SceneKit, iOS development in general, how to communicate between an Arduino and an iOS App. </p>\n<h2>What's next for Produce-AR</h2>\n<p>Making some sick beats, yo. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMobile AR is big these days. However, the user is usually limited to screen gestures to interact with the augmented world. I wanted to build a mobile AR application that can talk to a hardware peripheral to make the experience more interactive.\n\n\n## What it does\n\n\nAn iOS augmented reality music production app created with ARKit that connects to a Bluetooth shoe peripheral to register sound hits. In Recording mode you can register hits. Hover over the shoe to change instruments. Enter playback mode to walk through your arrangement and listen to it as you walk (both forwards and backwards), or simply press play. Built over one weekend at Reality Virtually Hackathon 2017 at MIT.\n\n\n## How I built it\n\n\nThe iOS app is built with ARKit. It connects to a hardware peripheral (a shoe) via bluetooth. The app sends the shoe a notification to vibrate when the instrument selection menu pops up. The shoe sends the app sound hits. An Arduino on the controls the bluetooth shield, accelerometer, and vibration motor. \n\n\nThe app runs animations, listens to bluetooth input, and runs computer vision on separate threads to avoid main thread latency.\n\n\n## Challenges I ran into\n\n\nThe system has a ton of different parts (bluetooth, sound, animation, computer vision), so getting them all to work/run simultaneously without race conditions was a challenge.\n\n\n## Accomplishments that I'm proud of\n\n\nGot it working!\n\n\n## What I learned\n\n\nA lot about ARKit, SceneKit, iOS development in general, how to communicate between an Arduino and an iOS App. \n\n\n## What's next for Produce-AR\n\n\nMaking some sick beats, yo. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/bought-broken",
            "title": "Bought/Broken",
            "blurb": "This is a social-emotional learning VR platform to help survivors of domestic violence reconcile with their past.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/qIW6OT2JMBE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Tejas Shroff",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/17838397?height=180&v=4&width=180"
                },
                {
                    "name": "Monica Chan",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/31901174?height=180&v=4&width=180"
                },
                {
                    "name": "Theresa Loong",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9ee6fe96360815e1fd41c92001772192?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "htc-vive",
                "unity",
                "wayfair"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>This project was inspired by Theresa Loong's research into a multi-platform interactive installation about how to raise awareness about domestic violence issues. Her installation examines relationships and domestic violence through the lens of objects - bought and broken.\nPlease check out this video for background into Theresa's project. [<a href=\"https://vimeo.com/115780797\" rel=\"nofollow\">https://vimeo.com/115780797</a>]</p>\n<p>by Theresa Loong, Monica Chan, Tejas Shroff, Joshua Widdicombe, Roseena Hussain</p>\n<h2>What it does</h2>\n<p>Bought/Broken is a social-emotional learning Virtual Reality platform that helps a survivor of domestic violence reconcile with their traumatic past experiences. We envision that this tool could be used by a licensed psychologist or therapist to work with domestic violence survivors in their route to regaining their social-emotional confidence. This demonstration allows the VR user, which in this case, is the survivor of domestic violence, to practice techniques of transforming emotions into image motifs (such as in Visual Arts Therapy and Dialectical Behavior Therapy); and mindfulness meditation in a virtually created environment.</p>\n<p>We decided to use VR, and more specifically the HTC Vive, for this project, because we wanted to execute the highest quality of user interactivity, full immersion and control of attention. We created a comfortable virtual environment with relaxing music and voiceover suggestions, where the user picks up broken pieces of a vase to mend it and then picks up a dead phone to give life to it. The lighting of the home also brightens as the user completes each \u201cmend\u201d/challenge, and these metaphors serve as a conversation-starter for guided self-healing, too.</p>\n<h2>How we built it</h2>\n<p>We rendered our home environment on Unity, and used 3D objects/furniture from Wayfair and Unity's Asset Store. We created the interactions with the objects by programming in C# on Unity (eg. piecing together the broken vase, lighting up the iPhone). We also recorded our voiceover using omni-directional VR-specific sound equipment.</p>\n<h2>Accomplishments, Challenges and Lessons Learned</h2>\n<p>Time management and effective delegation of tasks is always a challenge. We managed to group sub-tasks based on skill and speed of learning so that everyone can do something that they are comfortable and learn new skills at the same time. </p>\n<p>Communication is also so important! Effective communication within the team, with the organizers and with the mentors all makes a project more fluid. </p>\n<h2>What's next for Bought/Broken</h2>\n<p>We would love to test our prototype with domestic violence survivors, mental health professionals, psychologists, therapists and learning experience designers, who can give us best advice about the user experience and effectiveness of the product.</p>\n<h2>Additional Information</h2>\n<p>Team Lead: Theresa Loong; 718-496-4964\nAll team members: Monica Chan, Tejas Shroff, Joshua Widdicombe, Roseena Hussain\nCategory: Education VR / VR For Good \nLocation: 3rd floor, Rm 393, Table No. 46</p>\n<h1>Environment:</h1>\n<p>Platform: HTC Vive\nDevelopment tools: Unity, C#\nSDKs: SteamVR \nAssets: Unity Asset Store; Wayfair; SketchFab</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThis project was inspired by Theresa Loong's research into a multi-platform interactive installation about how to raise awareness about domestic violence issues. Her installation examines relationships and domestic violence through the lens of objects - bought and broken.\nPlease check out this video for background into Theresa's project. [<https://vimeo.com/115780797>]\n\n\nby Theresa Loong, Monica Chan, Tejas Shroff, Joshua Widdicombe, Roseena Hussain\n\n\n## What it does\n\n\nBought/Broken is a social-emotional learning Virtual Reality platform that helps a survivor of domestic violence reconcile with their traumatic past experiences. We envision that this tool could be used by a licensed psychologist or therapist to work with domestic violence survivors in their route to regaining their social-emotional confidence. This demonstration allows the VR user, which in this case, is the survivor of domestic violence, to practice techniques of transforming emotions into image motifs (such as in Visual Arts Therapy and Dialectical Behavior Therapy); and mindfulness meditation in a virtually created environment.\n\n\nWe decided to use VR, and more specifically the HTC Vive, for this project, because we wanted to execute the highest quality of user interactivity, full immersion and control of attention. We created a comfortable virtual environment with relaxing music and voiceover suggestions, where the user picks up broken pieces of a vase to mend it and then picks up a dead phone to give life to it. The lighting of the home also brightens as the user completes each \u201cmend\u201d/challenge, and these metaphors serve as a conversation-starter for guided self-healing, too.\n\n\n## How we built it\n\n\nWe rendered our home environment on Unity, and used 3D objects/furniture from Wayfair and Unity's Asset Store. We created the interactions with the objects by programming in C# on Unity (eg. piecing together the broken vase, lighting up the iPhone). We also recorded our voiceover using omni-directional VR-specific sound equipment.\n\n\n## Accomplishments, Challenges and Lessons Learned\n\n\nTime management and effective delegation of tasks is always a challenge. We managed to group sub-tasks based on skill and speed of learning so that everyone can do something that they are comfortable and learn new skills at the same time. \n\n\nCommunication is also so important! Effective communication within the team, with the organizers and with the mentors all makes a project more fluid. \n\n\n## What's next for Bought/Broken\n\n\nWe would love to test our prototype with domestic violence survivors, mental health professionals, psychologists, therapists and learning experience designers, who can give us best advice about the user experience and effectiveness of the product.\n\n\n## Additional Information\n\n\nTeam Lead: Theresa Loong; 718-496-4964\nAll team members: Monica Chan, Tejas Shroff, Joshua Widdicombe, Roseena Hussain\nCategory: Education VR / VR For Good \nLocation: 3rd floor, Rm 393, Table No. 46\n\n\n# Environment:\n\n\nPlatform: HTC Vive\nDevelopment tools: Unity, C#\nSDKs: SteamVR \nAssets: Unity Asset Store; Wayfair; SketchFab\n\n\n"
        },
        {
            "source": "https://devpost.com/software/drama-club",
            "title": "Drama Club",
            "blurb": "A \"Guitar Hero\" style game for actors and performers.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Harish",
                    "about": "I worked on UX design with Gloria Chow. I also worked on testing and tweaking the gameplay scoring code. ",
                    "photo": "https://avatars2.githubusercontent.com/u/362292?height=180&v=4&width=180"
                },
                {
                    "name": "AsierT",
                    "about": "I\u2019m working on art, brainstorming,  3d content creations and illustration",
                    "photo": "https://avatars3.githubusercontent.com/u/31211802?height=180&v=4&width=180"
                },
                {
                    "name": "Becky Lane",
                    "about": "",
                    "photo": "https://graph.facebook.com/10154715855227471/picture?height=180&width=180"
                },
                {
                    "name": "Jacob Madden",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/704/859/datas/profile.jpg"
                },
                {
                    "name": "gloria chow",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/780/004/datas/profile.JPG"
                }
            ],
            "built_with": [
                "3ds-max",
                "ibm-watson",
                "merge",
                "photoshop",
                "samsung-gear-vr",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Team Members</h2>\n<p>Becky Lane - \nHarish Tella\nJacob Madden\nGloria Chow\nAsier Taberna</p>\n<h2>Location</h2>\n<p>6th Floor, Table 11</p>\n<h2>Category</h2>\n<p>Gaming and Entertainment</p>\n<h2>Platform</h2>\n<p>GearVR</p>\n<h2>Description</h2>\n<p>Do you need a partner to help you memorize a speech or lines for a play or film? Do you want to act out your favorite scenes with friends or a computer-generated reader? Do want to practice performing your speech in front of an audience? Drama Club is the VR experience for you.</p>\n<h2>What it does</h2>\n<p>Like  <em>Guitar Hero</em> for performers. <strong>Drama Club</strong> is a VR acting and performing experience where participants can either choose a script from a popular film or play or load their own material. Then, users can memorize lines, perform scenes or speeches, and receive accuracy feedback from a virtual \u201cdirector\u201d powered by IBM Watson.  A virtual actor will run lines with you, or grab friends to act out a scene. There's also Merge Cube integration to incorporate virtual tactile props. Users can grab a skull when reciting Hamlet's soliloquy or a sword when acting out the scene from <em>The Princess Bride</em>! </p>\n<h2>Who it's for</h2>\n<p><strong>Drama Club</strong> is for actors, public speakers, and VR game enthusiasts. It's a fun and exciting way to commit text to memory and challenge friends in a gameplay format.</p>\n<p>Use Drama Club as a memorization tool, or enter <strong>Performance Mode</strong> that rewards accurate line performance and penalizes you for taking a peek at  your script. As mistakes stack up, the stage lights begin to dim. Too many mistakes? The lights go down and the curtain closes.  Watch out for tomatoes! Flawless performance? Watch the awards statues pile up and read rave reviews in the media.</p>\n<h2>How we built it</h2>\n<p>Unity, Merge Cube, 3ds, IBM Watson</p>\n<h2>GitHub</h2>\n<p><a href=\"https://github.com/Reality-Virtually-Hackathon/Drama-Club\" rel=\"nofollow\">https://github.com/Reality-Virtually-Hackathon/Drama-Club</a></p>\n</div>",
            "content_md": "\n## Team Members\n\n\nBecky Lane - \nHarish Tella\nJacob Madden\nGloria Chow\nAsier Taberna\n\n\n## Location\n\n\n6th Floor, Table 11\n\n\n## Category\n\n\nGaming and Entertainment\n\n\n## Platform\n\n\nGearVR\n\n\n## Description\n\n\nDo you need a partner to help you memorize a speech or lines for a play or film? Do you want to act out your favorite scenes with friends or a computer-generated reader? Do want to practice performing your speech in front of an audience? Drama Club is the VR experience for you.\n\n\n## What it does\n\n\nLike *Guitar Hero* for performers. **Drama Club** is a VR acting and performing experience where participants can either choose a script from a popular film or play or load their own material. Then, users can memorize lines, perform scenes or speeches, and receive accuracy feedback from a virtual \u201cdirector\u201d powered by IBM Watson. A virtual actor will run lines with you, or grab friends to act out a scene. There's also Merge Cube integration to incorporate virtual tactile props. Users can grab a skull when reciting Hamlet's soliloquy or a sword when acting out the scene from *The Princess Bride*! \n\n\n## Who it's for\n\n\n**Drama Club** is for actors, public speakers, and VR game enthusiasts. It's a fun and exciting way to commit text to memory and challenge friends in a gameplay format.\n\n\nUse Drama Club as a memorization tool, or enter **Performance Mode** that rewards accurate line performance and penalizes you for taking a peek at your script. As mistakes stack up, the stage lights begin to dim. Too many mistakes? The lights go down and the curtain closes. Watch out for tomatoes! Flawless performance? Watch the awards statues pile up and read rave reviews in the media.\n\n\n## How we built it\n\n\nUnity, Merge Cube, 3ds, IBM Watson\n\n\n## GitHub\n\n\n<https://github.com/Reality-Virtually-Hackathon/Drama-Club>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/export-revit-elements-to-view-in-a-frame",
            "title": "Export Revit elements to view in A-Frame",
            "blurb": "Easily get Revit elements into A-Frame, for a Web VR experience",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Revit model",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/534/datas/original.png"
                },
                {
                    "title": "Run the Dynamo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/537/datas/original.png"
                },
                {
                    "title": "HTML and OBJs are written by Dynamo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/540/datas/original.png"
                },
                {
                    "title": "Upload to a web server",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/535/datas/original.png"
                },
                {
                    "title": "View in browser",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/536/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Eric Boehlke",
                    "about": "Team of one",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/151/582/datas/profile.jpg"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>How to go from Revit to Web VR?</p>\n</div>",
            "content_md": "\nHow to go from Revit to Web VR?\n\n\n"
        },
        {
            "source": "https://devpost.com/software/words-world",
            "title": "Words World",
            "blurb": "Words world is a platform for people to understand the unspoken artistic world of people with autism.  ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Jiabao Li",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/11444183?height=180&v=3&width=180"
                },
                {
                    "name": "Lucas Cassiano",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/000a5395cf271408746d8dda9c7deb3d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Qi Xiong",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xKXrfbBlluQtc_2Fv3OOk9jlcRVtcP8FJNtroPolNm_1tiSR13tlC9jl-f0tB80Rn3plCv-A4erPNctwri32FPxjger-NcdcPi312tErnWLYY3-w0tBgu6k3jjoGjcY6V85pwhJu3b7?height=180&width=180"
                }
            ],
            "built_with": [
                "opencv",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Autism</p>\n<h2>What it does</h2>\n<h2>How I built it</h2>\n<h2>Challenges I ran into</h2>\n<h2>Accomplishments that I'm proud of</h2>\n<h2>What I learned</h2>\n<h2>What's next for Words World</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAutism\n\n\n## What it does\n\n\n## How I built it\n\n\n## Challenges I ran into\n\n\n## Accomplishments that I'm proud of\n\n\n## What I learned\n\n\n## What's next for Words World\n\n\n"
        },
        {
            "source": "https://devpost.com/software/med-for-medar-medical-application",
            "title": "Visual Med",
            "blurb": "easiness for doctors = health for everyone",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/_uEPx1_SOjA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Skull DICOM Viewer ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/681/datas/original.jpg"
                },
                {
                    "title": "Organ Reassemble Game",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/898/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Bolin Zhu",
                    "about": "Write up code for controller\nInteraction with Hololens\nSome modification with existing code",
                    "photo": "https://avatars3.githubusercontent.com/u/6730682?height=180&v=4&width=180"
                },
                {
                    "name": "Willy Ci",
                    "about": "build UI, controller, testing, build 3D model, ",
                    "photo": "https://media.licdn.com/mpr/mprx/0__RFqJaE9i1cqGayl60PVqpH9i1jeTOyl60lspUy9QtydTaGpi00sAVw9Tz-qGa1gi00VtpknENTWbVmlWeGHAVdsfNTHbVQg7eGcVRpBXvNkS2g3iJNMs7-Q73U4aVAu6IQ4gL9HUWl?height=180&width=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>For medical cases, people need to have a group review over everyone's work everyday in the morning. That usually has more than 100 cases via a stack of medical images a day. It is time consuming and everyone has to be on-site. So, we come up an idea to show images in 3d.\nFurthermore, we want to create something fun for students while learning. \nProof of concept for medicine.</p>\n<h2>What it does</h2>\n<p>It loads the medical images and turns them into 3d-alike look in Hololens. With interactions, they are able to manipulate the 'model' to right angle or slice for better look.\nWe build a little game for medical education purpose where students are asked to manipulate specific parts of a 3d model to re-assemble it.</p>\n<h2>How we built it</h2>\n<p>We built it using Unity and Hololens: add interaction(gestures) to control the image/model in Hololens</p>\n<h2>Challenges we ran into</h2>\n<ol>\n<li>Hololens API integration with some interactions we want</li>\n<li>Precision of gestures</li>\n<li>Imaging</li>\n<li>Performance\n## Accomplishments that we're proud of</li>\n<li>we actually pull that off</li>\n<li>results are good enough for two days work\n## What we learned</li>\n<li>More possible ways to develop for Hololens AR application</li>\n<li>Imaging</li>\n<li>Solve performance issue\n## What's next for Med for Med: AR Medical Application</li>\n<li>add remote session</li>\n<li>Dynamical loading from external drive\n## Credits\nAll the medical images/models are from UMass Medical\nHololens API\nSome control codes along with medical assets</li>\n</ol>\n</div>",
            "content_md": "\n## Inspiration\n\n\nFor medical cases, people need to have a group review over everyone's work everyday in the morning. That usually has more than 100 cases via a stack of medical images a day. It is time consuming and everyone has to be on-site. So, we come up an idea to show images in 3d.\nFurthermore, we want to create something fun for students while learning. \nProof of concept for medicine.\n\n\n## What it does\n\n\nIt loads the medical images and turns them into 3d-alike look in Hololens. With interactions, they are able to manipulate the 'model' to right angle or slice for better look.\nWe build a little game for medical education purpose where students are asked to manipulate specific parts of a 3d model to re-assemble it.\n\n\n## How we built it\n\n\nWe built it using Unity and Hololens: add interaction(gestures) to control the image/model in Hololens\n\n\n## Challenges we ran into\n\n\n1. Hololens API integration with some interactions we want\n2. Precision of gestures\n3. Imaging\n4. Performance\n## Accomplishments that we're proud of\n5. we actually pull that off\n6. results are good enough for two days work\n## What we learned\n7. More possible ways to develop for Hololens AR application\n8. Imaging\n9. Solve performance issue\n## What's next for Med for Med: AR Medical Application\n10. add remote session\n11. Dynamical loading from external drive\n## Credits\nAll the medical images/models are from UMass Medical\nHololens API\nSome control codes along with medical assets\n\n\n"
        },
        {
            "source": "https://devpost.com/software/serendipity-jp4b9q",
            "title": "Serendipity made at MIT's Reality, Virtually Hackathon",
            "blurb": "'x-ray' vision to see through the environment, wherein relevant events at that time are highlighted in space.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Muso Fun",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1475966827/yoz0um09vjx0fkahcgxy.jpg?height=180&width=180"
                },
                {
                    "name": "Hanbin Cho",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/511/datas/profile.jpg"
                },
                {
                    "name": "Warren Partridge",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/652/256/datas/profile.jpg"
                },
                {
                    "name": "Marisa Lu",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_fvQ1tga1N6MM1P-7hvY1pOf19_nMPlxmhvrgBHU1cXr49T2f5vy1VU_1r_kXzPOG8XyjsWuPVboZvvBP86_fsWSxxboJvvemL6_rleZtNQ0vx_P-WBzYApn_0-rdpv0xu5F099TwLN-?height=180&width=180"
                },
                {
                    "name": "Hadi Zayer",
                    "about": "",
                    "photo": "https://graph.facebook.com/1510171769000167/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "arkit",
                "mapbox",
                "unity"
            ],
            "content_html": "<div>\n<p>Team lead: Big Bin\nTeam lead's phone number 571-499-0280 \nRoom: The warmer roomer with yellow lighting Table 7\nCategory: Other: Closest umbrella we could think of was 'social vr' but ours is probably more like 'mixing' reality for lifestyle/productivity purposes</p>\n<h2>Inspiration</h2>\n<p><strong>Problem Space:</strong>\nThe digital age makes accessing information and doing more \u2018easier\u2019, but that doesn\u2019t mean life is any easier.  Stressing out from the information overload is easy! </p>\n<p>Being in the know about what\u2019s going on and acting on all the information from diverse sources, takes effort because the information given usually isn\u2019t applicable in the moment. So you organize, and you schedule, or just try to remember. </p>\n<p>Can we understand what's going on around us in a delightful, intuitive?</p>\n<p><strong>Goal:</strong>\nWe are inspired to present relevant information when and where it would be actionable for people in a delightful, intuitive way. Let's bring a little more serendipity to daily life and make 'busy' less stressful.</p>\n<p><strong>Interaction Goal:</strong>\nUser goes about their life, and the application lets them know of relevant events according to their profile\u2019s interests and various social medias, when and where the information is immediately actionable (time frame and location range can be adjusted to preferences). The events are shown embedded within a virtual 3D replica of the real environment simulated from your point of view. It is as if your phone was an xray for the world. </p>\n<p>For a more active interaction, if the user happens to realize they have free time, they can just as easily bring up the app and toggle time, space, and interest preferences for discovering events/interesting places. It\u2019d make being a tourist/traveler less stressful.</p>\n<p>(we believed 3D representation of the map would b e a more intuitive way for the user to understand the environment and spatial relationships. At any rate, it should be easier for most people to understand at a glance where something is if they understand the spatial context, rather than glancing at a flat 2D map in a bird's perspective)</p>\n<p><strong>Meta Goal:</strong> \nVR environments generated based off the real world, and informed by the user's live position in time and space, used to expand their understanding of what\u2019s going on around them physically and temporally in order to ultimately augment the social, qualitative reality of their life.</p>\n<h2>What we made</h2>\n<p>A working proof of concept that demonstrates how VR environments might be generated based off real world data and oriented to reflect the users location and orientation. </p>\n<h2>How it was built</h2>\n<p>Our proof of concept uses mapbox\u2019s Unity sdk to generate a virtual model of the real world around the user\u2019s live or inputted location. (Mapbox SDK provided map info like topography, floorplans, building elevations) The generation of the map model was tweaked to allow selection of individual buildings. The unity build for ios and android got the user's location information from the phone system and used compass data to orient the virtual environment (to script the angle and location of the camera in Unity) to match what we believed would be the user's point of view of the real world. We ran out of time to hook up the event spaces highlighted in the map to other API's, so the 'events' you see in the virtual world, aren't actually real.</p>\n<h2>UI / Interaction prototyping</h2>\n<p>Early prototypes to help with ideation and understanding what sort of interaction we want were done with 360 photos (Ricoh Theta Camera), photoshopped, and thrown into Aframe to view on mobile. UI prototype was quickly shaped in Sketch and plopped into Origami.</p>\n<h2>Challenges we ran into</h2>\n<p>It's always the little things that take time to solve. On a high level, the plan for how to execute wasn't particularly convoluted (mapbox, unity, ios/android and possible plumbing with social media API's/other sites to port event info in to a database) Getting started with Mapbox, and crash coursing Unity took a while and then the little bugs when building out for mobile kept us occupied.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I'm pretty excited for the concept, and am proud about the ideating process. Otherwise, I'm very proud of the team;\nwe all sloughed through new material and it was a learning experience all around.</p>\n<h2>What I learned</h2>\n<p>I should make design assets early/before the hackathon so that I can get more hands on coding experience while there are experienced developers around. (or not, it's hard to say; I had fun talking to people before deciding on running with an idea)</p>\n<h2>What's next for Serendipity</h2>\n<p>Make it work! I want this for real to use! Polish the interaction, the Unity shaders, the UX, and the visual language. If we get far enough with it, then start testing for edge cases, and porting real data in, etc, etc.\nOther than that....well the code is open source, so I want to see what other ideas people will have! </p>\n</div>",
            "content_md": "\nTeam lead: Big Bin\nTeam lead's phone number 571-499-0280 \nRoom: The warmer roomer with yellow lighting Table 7\nCategory: Other: Closest umbrella we could think of was 'social vr' but ours is probably more like 'mixing' reality for lifestyle/productivity purposes\n\n\n## Inspiration\n\n\n**Problem Space:**\nThe digital age makes accessing information and doing more \u2018easier\u2019, but that doesn\u2019t mean life is any easier. Stressing out from the information overload is easy! \n\n\nBeing in the know about what\u2019s going on and acting on all the information from diverse sources, takes effort because the information given usually isn\u2019t applicable in the moment. So you organize, and you schedule, or just try to remember. \n\n\nCan we understand what's going on around us in a delightful, intuitive?\n\n\n**Goal:**\nWe are inspired to present relevant information when and where it would be actionable for people in a delightful, intuitive way. Let's bring a little more serendipity to daily life and make 'busy' less stressful.\n\n\n**Interaction Goal:**\nUser goes about their life, and the application lets them know of relevant events according to their profile\u2019s interests and various social medias, when and where the information is immediately actionable (time frame and location range can be adjusted to preferences). The events are shown embedded within a virtual 3D replica of the real environment simulated from your point of view. It is as if your phone was an xray for the world. \n\n\nFor a more active interaction, if the user happens to realize they have free time, they can just as easily bring up the app and toggle time, space, and interest preferences for discovering events/interesting places. It\u2019d make being a tourist/traveler less stressful.\n\n\n(we believed 3D representation of the map would b e a more intuitive way for the user to understand the environment and spatial relationships. At any rate, it should be easier for most people to understand at a glance where something is if they understand the spatial context, rather than glancing at a flat 2D map in a bird's perspective)\n\n\n**Meta Goal:** \nVR environments generated based off the real world, and informed by the user's live position in time and space, used to expand their understanding of what\u2019s going on around them physically and temporally in order to ultimately augment the social, qualitative reality of their life.\n\n\n## What we made\n\n\nA working proof of concept that demonstrates how VR environments might be generated based off real world data and oriented to reflect the users location and orientation. \n\n\n## How it was built\n\n\nOur proof of concept uses mapbox\u2019s Unity sdk to generate a virtual model of the real world around the user\u2019s live or inputted location. (Mapbox SDK provided map info like topography, floorplans, building elevations) The generation of the map model was tweaked to allow selection of individual buildings. The unity build for ios and android got the user's location information from the phone system and used compass data to orient the virtual environment (to script the angle and location of the camera in Unity) to match what we believed would be the user's point of view of the real world. We ran out of time to hook up the event spaces highlighted in the map to other API's, so the 'events' you see in the virtual world, aren't actually real.\n\n\n## UI / Interaction prototyping\n\n\nEarly prototypes to help with ideation and understanding what sort of interaction we want were done with 360 photos (Ricoh Theta Camera), photoshopped, and thrown into Aframe to view on mobile. UI prototype was quickly shaped in Sketch and plopped into Origami.\n\n\n## Challenges we ran into\n\n\nIt's always the little things that take time to solve. On a high level, the plan for how to execute wasn't particularly convoluted (mapbox, unity, ios/android and possible plumbing with social media API's/other sites to port event info in to a database) Getting started with Mapbox, and crash coursing Unity took a while and then the little bugs when building out for mobile kept us occupied.\n\n\n## Accomplishments that I'm proud of\n\n\nI'm pretty excited for the concept, and am proud about the ideating process. Otherwise, I'm very proud of the team;\nwe all sloughed through new material and it was a learning experience all around.\n\n\n## What I learned\n\n\nI should make design assets early/before the hackathon so that I can get more hands on coding experience while there are experienced developers around. (or not, it's hard to say; I had fun talking to people before deciding on running with an idea)\n\n\n## What's next for Serendipity\n\n\nMake it work! I want this for real to use! Polish the interaction, the Unity shaders, the UX, and the visual language. If we get far enough with it, then start testing for edge cases, and porting real data in, etc, etc.\nOther than that....well the code is open source, so I want to see what other ideas people will have! \n\n\n"
        },
        {
            "source": "https://devpost.com/software/univrs",
            "title": "UniVRs",
            "blurb": "A WEbVR-based game where you collect items & connect with other people around the world",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "M. Adlan Ramly",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/538/657/datas/profile.png"
                },
                {
                    "name": "Lily Z.",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/10191088?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "javascript",
                "magicavoxel",
                "three.js",
                "vive"
            ],
            "content_html": "<div>\n<p>The world of virtual reality is a great platform to meet new people around the world with different backgrounds. There are many exclusive items &amp; decorations to collect based on geospatial location.</p>\n<p>Each player will have their own decorable home. Players can grow their distinct plants in their farm. They can sell it in the store or trade it to other players from different countries.</p>\n<p>The meeting space is a place where you can visit other player's home and where you can receive visitors of your home.</p>\n<p>Currently we are just 2 people working on this project. We won't have all the desired features at the end of the hackathon and we will try our best for the hackathon. We will continue the project after the hackathon ends.</p>\n</div>",
            "content_md": "\nThe world of virtual reality is a great platform to meet new people around the world with different backgrounds. There are many exclusive items & decorations to collect based on geospatial location.\n\n\nEach player will have their own decorable home. Players can grow their distinct plants in their farm. They can sell it in the store or trade it to other players from different countries.\n\n\nThe meeting space is a place where you can visit other player's home and where you can receive visitors of your home.\n\n\nCurrently we are just 2 people working on this project. We won't have all the desired features at the end of the hackathon and we will try our best for the hackathon. We will continue the project after the hackathon ends.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-farming-on-mars",
            "title": "VR Farming on Mars",
            "blurb": "Farming simulators aren't immersive enough - or extraterrestrial enough.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "1/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/651/datas/original.jpg"
                },
                {
                    "title": "2/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/650/datas/original.jpg"
                },
                {
                    "title": "3/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/652/datas/original.jpg"
                },
                {
                    "title": "4/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/653/datas/original.jpg"
                },
                {
                    "title": "5/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/654/datas/original.jpg"
                },
                {
                    "title": "6/6",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/655/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Nicholas (Nick) Rance",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/516/782/datas/profile.jpg"
                }
            ],
            "built_with": [
                "aframe",
                "meteor.js",
                "webvr"
            ],
            "content_html": "<div>\n<p><strong>Table #: 72</strong></p>\n<p><strong>Phone: 4074215139</strong></p>\n<p><strong>3rd Floor on the couches opposite the tangible media lab</strong></p>\n<h2>Try it here: <a href=\"http://farmingonmars.fun\" rel=\"nofollow\">www.farmingonmars.fun</a></h2>\n<h2>Plot</h2>\n<p>Elwin Musk has asked Snoop Dogg to be the lead astronaut on SpaceZ's first manned mission to Mars. The goal of this mission is create the first farm on Mars to start growing food to support a future human colony. Of course the adventurous Dogg agrees, help Snoop manage his plots, grow his farm and become the first martian.</p>\n<h2>Technical Description</h2>\n<p>My teammate and I both had backgrounds in web development so we decided that A-Frame was probably a cool framework to get our feet wet with VR. The code is available on <a href=\"https://github.com/Reality-Virtually-Hackathon/farmingonmars\" rel=\"nofollow\">this github repo</a> .</p>\n<p>The logic is fairly simple we made a grid of 's with each unit of the grid representing a farm plot. Everytime you click on a plot you either harvest it or plant a seed. When you plant a seed after sometime passes this seed grows into a Snoop Tree. When you harvest your crops you earn coins and with coins the size of your farmhouse increases.</p>\n</div>",
            "content_md": "\n**Table #: 72**\n\n\n**Phone: 4074215139**\n\n\n**3rd Floor on the couches opposite the tangible media lab**\n\n\n## Try it here: [www.farmingonmars.fun](http://farmingonmars.fun)\n\n\n## Plot\n\n\nElwin Musk has asked Snoop Dogg to be the lead astronaut on SpaceZ's first manned mission to Mars. The goal of this mission is create the first farm on Mars to start growing food to support a future human colony. Of course the adventurous Dogg agrees, help Snoop manage his plots, grow his farm and become the first martian.\n\n\n## Technical Description\n\n\nMy teammate and I both had backgrounds in web development so we decided that A-Frame was probably a cool framework to get our feet wet with VR. The code is available on [this github repo](https://github.com/Reality-Virtually-Hackathon/farmingonmars) .\n\n\nThe logic is fairly simple we made a grid of 's with each unit of the grid representing a farm plot. Everytime you click on a plot you either harvest it or plant a seed. When you plant a seed after sometime passes this seed grows into a Snoop Tree. When you harvest your crops you earn coins and with coins the size of your farmhouse increases.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/food-karma",
            "title": "Food Karma ",
            "blurb": "Use AR to create awareness about the ethics of your food purchases.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Arjun Madgavkar",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/12454070?height=180&v=4&width=180"
                },
                {
                    "name": "Anip Mehta",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/12862333?height=180&v=4&width=180"
                },
                {
                    "name": "Tiffany Hui",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0__7piWaBd0j_cobE97O6CCVVdREhXW649QwXifVGHYsG5296oLwEDXpVdMuuJEXUc7O6iGSzWPRGL7GZoWpJ0TVq59RGk7GCV7pJhER3ejVcH5qMQiS7mo7fMnm4N6GzI6xgfDLolmt4?height=180&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>Inspirtation:\nFactory farming is one of the biggest problems that faces the world today. Here are some key statistics though there are too many to list:</p>\n<ul>\n<li>99% of the nearly 10 billion farm animals in the USA are factory farmed. That's almost 9.9 billion animals in the US alone.</li>\n<li>The beaks of chickens, turkeys, and ducks are often removed in factory farms to reduce the excessive feather pecking and cannibalism seen among stressed, overcrowded birds.</li>\n<li>Due to genetic manipulation, 90% of broiler chickens have trouble\nwalking.</li>\n<li>Dairy cows typically live to their third lactation before being culled. Naturally, a cow can live for 20 years.</li>\n<li>Hog, chicken and cattle waste has polluted 35,000 miles of rivers in 22 states and contaminated groundwater in 17 states.</li>\n<li>Egg-laying hens are sometimes starved for up to 14 days, exposed to changing light patterns and given no water in order to shock their bodies into molting. </li>\n<li>Over 80% of US pigs have pneumonia upon slaughter.</li>\n</ul>\n<p>What It Does:</p>\n<ul>\n<li>Scan any grocery store item and AR content is displayed over it. </li>\n<li>The AR content shows (1) ethical rating based on web sources (2) other people's reviews of the product</li>\n</ul>\n<p>How We Built It:</p>\n<ul>\n<li>Image recognition using Microsoft API</li>\n<li>Swift iOS application</li>\n<li>Apple ARKit</li>\n</ul>\n<p>Challenges We Ran Into:</p>\n<ul>\n<li>Importing the models into ARKit</li>\n<li>Image recognition for specific brands</li>\n</ul>\n<p>Accomplishments:</p>\n<ul>\n<li>Creating something that will completely change the way people think about their food consumption</li>\n<li>Putting an impressive prototype together in a short amount of time</li>\n</ul>\n<p>What We Learned:</p>\n<ul>\n<li>Planning is extremely important</li>\n<li>Working as a unit is more important than any individual's skills. It's all about teamwork.</li>\n<li>Hackathons create an awesome environment for condensed learning. And they're pretty fun.</li>\n</ul>\n<p>What's Next for Food Karma:</p>\n<ul>\n<li>Improve image recognition accuracy</li>\n<li>Add other features to create a more rounded experience</li>\n</ul>\n</div>",
            "content_md": "\nInspirtation:\nFactory farming is one of the biggest problems that faces the world today. Here are some key statistics though there are too many to list:\n\n\n* 99% of the nearly 10 billion farm animals in the USA are factory farmed. That's almost 9.9 billion animals in the US alone.\n* The beaks of chickens, turkeys, and ducks are often removed in factory farms to reduce the excessive feather pecking and cannibalism seen among stressed, overcrowded birds.\n* Due to genetic manipulation, 90% of broiler chickens have trouble\nwalking.\n* Dairy cows typically live to their third lactation before being culled. Naturally, a cow can live for 20 years.\n* Hog, chicken and cattle waste has polluted 35,000 miles of rivers in 22 states and contaminated groundwater in 17 states.\n* Egg-laying hens are sometimes starved for up to 14 days, exposed to changing light patterns and given no water in order to shock their bodies into molting.\n* Over 80% of US pigs have pneumonia upon slaughter.\n\n\nWhat It Does:\n\n\n* Scan any grocery store item and AR content is displayed over it.\n* The AR content shows (1) ethical rating based on web sources (2) other people's reviews of the product\n\n\nHow We Built It:\n\n\n* Image recognition using Microsoft API\n* Swift iOS application\n* Apple ARKit\n\n\nChallenges We Ran Into:\n\n\n* Importing the models into ARKit\n* Image recognition for specific brands\n\n\nAccomplishments:\n\n\n* Creating something that will completely change the way people think about their food consumption\n* Putting an impressive prototype together in a short amount of time\n\n\nWhat We Learned:\n\n\n* Planning is extremely important\n* Working as a unit is more important than any individual's skills. It's all about teamwork.\n* Hackathons create an awesome environment for condensed learning. And they're pretty fun.\n\n\nWhat's Next for Food Karma:\n\n\n* Improve image recognition accuracy\n* Add other features to create a more rounded experience\n\n\n"
        },
        {
            "source": "https://devpost.com/software/readyplayer2-hldc62",
            "title": "ReadyPlayerTWO",
            "blurb": "Reenact your favorite movie scene in MR replacing one or more characters with you and your friends.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/b8KBFZzTKWA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Pablo Criado-Perez",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C4D03AQERiB2VHBNSjg/profile-displayphoto-shrink_800_800/0?e=1596067200&height=180&t=mcm8qOVnSG9pPsz0wMl4pzKi-V12Da11uCawqqm3hQ4&v=beta&width=180"
                },
                {
                    "name": "Manuel Leon Madrigal",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/f56094c28d484710e2efaf75ed18165a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "3ds-max",
                "maya",
                "microsoft-hololens",
                "unity",
                "windows-speech"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Most of us here today in this VR/AR/MR event were inspired by the capabilities and ideas brought by sci-fi writers and futurists. One of our team member was inspired by a book called Ready Player 1. Which led our team to come up with interactive way to reenact favorite movies or historical events. With the objective to re-live the moment that we love and cherish.</p>\n<h2>What it does</h2>\n<p>It's the Karaoke for Movies in Mixed Reality. Where multiple players can join a scene of their favorite movie and read-through the script. Throughout this experience the Virtual world plays a good part in the interaction of both players.\nThe juxtaposition of both the Virtual components (3D CGI elements) &amp; Real World(you, your friends, everything around) creates a unique experience like no other. This combination is allowed based on the way Hololens work</p>\n<h2>How we built it</h2>\n<p>Using Unity, Hololens, HoloToolKit, Windows Speech Recognition, 3DS Max, Maya, SketchUp and lots of patience </p>\n<h2>Challenges we ran into</h2>\n<p>1.Developing without a second pair of Hololens, to create the multi share experience.\n2.Generating Animated Scenes, since most of the 3D Models were not rigged\n3.Dealing with the Network Connections - There is an issue with the Hololens starting the dictation recognition while holding a shared session as a server.\n4.Making Shared Networks in the MIT WiFi</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Coming up with a cool idea, and putting all the pieces together\nBeing able to visualize a MVP after the two days of hard work.</p>\n<h2>What we learned</h2>\n<p>Maya, Speech recognition, Unity.</p>\n<h2>What's next for ReadyPlayer2</h2>\n<p>Add more experiences (scenes)\nImprove Graphics of the current 3D models\nImprove the Spatial Sound\nImproving the speech recognition\nUX/UI improvement, etc.\nAllow more than two players\nCreating Historical events for educational purposes</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMost of us here today in this VR/AR/MR event were inspired by the capabilities and ideas brought by sci-fi writers and futurists. One of our team member was inspired by a book called Ready Player 1. Which led our team to come up with interactive way to reenact favorite movies or historical events. With the objective to re-live the moment that we love and cherish.\n\n\n## What it does\n\n\nIt's the Karaoke for Movies in Mixed Reality. Where multiple players can join a scene of their favorite movie and read-through the script. Throughout this experience the Virtual world plays a good part in the interaction of both players.\nThe juxtaposition of both the Virtual components (3D CGI elements) & Real World(you, your friends, everything around) creates a unique experience like no other. This combination is allowed based on the way Hololens work\n\n\n## How we built it\n\n\nUsing Unity, Hololens, HoloToolKit, Windows Speech Recognition, 3DS Max, Maya, SketchUp and lots of patience \n\n\n## Challenges we ran into\n\n\n1.Developing without a second pair of Hololens, to create the multi share experience.\n2.Generating Animated Scenes, since most of the 3D Models were not rigged\n3.Dealing with the Network Connections - There is an issue with the Hololens starting the dictation recognition while holding a shared session as a server.\n4.Making Shared Networks in the MIT WiFi\n\n\n## Accomplishments that we're proud of\n\n\nComing up with a cool idea, and putting all the pieces together\nBeing able to visualize a MVP after the two days of hard work.\n\n\n## What we learned\n\n\nMaya, Speech recognition, Unity.\n\n\n## What's next for ReadyPlayer2\n\n\nAdd more experiences (scenes)\nImprove Graphics of the current 3D models\nImprove the Spatial Sound\nImproving the speech recognition\nUX/UI improvement, etc.\nAllow more than two players\nCreating Historical events for educational purposes\n\n\n"
        },
        {
            "source": "https://devpost.com/software/science-augmented-hfisjl",
            "title": "Science Augmented",
            "blurb": "Enables high school teachers to create engaging, collaborative  and guided learning experiences using AR.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/C-UFGlbhWNY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Instructor Home",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/842/datas/original.jpg"
                },
                {
                    "title": "Instructor Classroom ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/820/datas/original.jpg"
                },
                {
                    "title": "Instructor Activity",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/830/datas/original.jpg"
                },
                {
                    "title": "Instructor Mobile",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/843/datas/original.jpg"
                },
                {
                    "title": "Student Mobile",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/817/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Xindeling Pan",
                    "about": "I was the product designer on the project. I designed user experience and interface of the application. Also I brought Design Thinking into the project development to help us identify user personas and user stories. This ensured our product was designed to solve a real-world problem based on user needs.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/971/datas/profile.jpg"
                },
                {
                    "name": "Meredith Thompson",
                    "about": "I worked on conceptualizing and building the science content for the product. I shared my experience developing engaging and collaborative STEM activities and working with teachers to create a product that can be effective in today's classrooms. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/543/415/datas/profile.png"
                },
                {
                    "name": "Krystian Babilinski",
                    "about": "I was the developer for the teacher side of the application, including networking. This was my first time working with Unity Networking. It was also my first time working with a designer to deploy an app with responsive UI for standalone and Mobile.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/267/721/datas/profile.jpg"
                },
                {
                    "name": "Adrian Babilinski",
                    "about": "I worked on the Unity development of the AR app for mobile devices. While doing so, I also contributed to adding the animations and GUI into the project.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/691/datas/profile.jpg"
                },
                {
                    "name": "Venkatesh Kashyap",
                    "about": "I worked on the strategy of the product. Which included working on developing the informational material in the AR experience, strategizing the right material that makes use of the value proposition of AR and providing feedback to other member when necessary. ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Y7yIKAMd2AWr2oN3ZwHIjN1HHFWrWIFSlOH5vkXHeLLlWRJgOwk5ZixdIFkuEW4hZMkLRipW38oA7fphNpMcZLl5k8oK7fgDNpMHPXke7T0p5JQl4SfQ1KEMXzr86fopjxjFq9HEqD-?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe-creative-suite",
                "sketch",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>What is Science Augmented?</h2>\n<p>Science Augmented is an application that enables high school teachers to create engaging, collaborative and guided learning experiences using AR. </p>\n<h2>Why are we building this?</h2>\n<p>Science teachers want to create engaging activities to help their students learn. Increasingly, teachers have to compete for their students\u2019 attention. New technologies such as smartwatches, smartphones, VR/AR/MR devices, and gaming consoles seamlessly anchor the student\u2019s attention in a way that is not reflected in classroom practice.  Science teachers fight a losing battle when they rely on outdated technology to capture the attention of today\u2019s students.</p>\n<p>The Next Generation Science Standards (NGSS, 2013) require teacher to move away from describing science with a glossary of words and cookbook, to class activities that model the nature of science. Teachers would love to find new in-class activities to keep the students engaged throughout the learning process. Hence, Science Augmented will meet this real-world need to create a new generation of classroom experiences. </p>\n<p>Science classes like Biology and Chemistry are inherently 3 dimensional subjects. Current class materials like PowerPoint slides and word processing are insufficient for teachers to demonstrate and hard for students to understand. </p>\n<h2>How are we building this?</h2>\n<p>We started tackling this problem with identifying our user personas, high school teacher Mr.Hope, and high school girl Faith. Through empathy mapping exercise, we understood the real needs from our users. Rapid wireframing and prototyping on papers help us create intuitive experience.</p>\n<p>The application was developed using Unity. We used the Unity Networking service to connect the Standalone build with mobile devices. Unity\u2019s native Vuforia integration was used for the AR rendering. The design of the application was created using Adobe Creative suite and Sketch. This was imported into Unity and rendered using Unity UI and Text Mesh Pro. We obtained some of our molecular models from the RCSB Protein Data Bank and Google\u2019s Sketchup. </p>\n<h2>What are our features?</h2>\n<p>In the collaborative AR classroom, all perspectives are encouraged, and students are able to collaborate, and prototype concepts in class, under the guidance of the teacher. Our platform is a combination of informational aspect, or a generic lab exercise that shows the change in color of the chemicals. We deliver specific information with the adequate level of engagement. </p>\n<p>The value we are looking to create:</p>\n<p><strong>Visualizing orientation of molecules and bonds</strong>\nMolecular reactions and bonds are not just straight lines and summations on paper, in reality the orientation of the molecules in 3D and location of the bonds play an important role in the reaction. Science Augmented helps teachers deliver this value, while supplementing with interactive learning activities.  </p>\n<p><strong>A Novel, engaging learning environment</strong>\nEach student with our AR application on their mobile phones, log in to the classroom, and here the teacher assigns molecules to students, creating collaboration in students, while guiding them through the process. This creates immersion in the classroom by shifting the focus in the lesson from the \u2018what happens to molecules?\u2019 to  \u2018What should we do with the molecules?\u2019</p>\n<p><strong>A flexible and scalable learning platform</strong>\nThis will meet the need for teachers to move away from describing science with a glossary of words, with resources . This would replace a whole \u2018book shelf\u2019 of with a software update. A library of textual information is just another update away. Vuforia platform 98% of android, and 100% iOS devices.</p>\n<h2>Why Science Augmented?</h2>\n<p>Comparing with other AR/VR apps in the market, we are a departure from informational content with cool graphics. We are cool graphics and something similar to \u2018LEGO\u2019 blocks for chemistry, where students collaborate, and look to bind molecules in the right orientations and binding sites to complete the reaction. </p>\n<h2>The Impact</h2>\n<p>Science Augmented is a multidimensional resource. It allow teachers creating their our classroom, add their students, and leverage many collaborative in-class activities that specifically use the affordances of AR. We believe our application will increase engagement between teachers and students by bringing  an immersive guided learning experience to current K-12 education.</p>\n<h2>What have we learned?</h2>\n<p>We\u2019re very proud of our product and learned a lot throughout the Hackathon.  We have refreshed our memory of high school chemistry and biology, applied the analytical thinking we further developed in college and working with perfect strangers to have a lot of fun. &amp; AR is super COOL.</p>\n</div>",
            "content_md": "\n## What is Science Augmented?\n\n\nScience Augmented is an application that enables high school teachers to create engaging, collaborative and guided learning experiences using AR. \n\n\n## Why are we building this?\n\n\nScience teachers want to create engaging activities to help their students learn. Increasingly, teachers have to compete for their students\u2019 attention. New technologies such as smartwatches, smartphones, VR/AR/MR devices, and gaming consoles seamlessly anchor the student\u2019s attention in a way that is not reflected in classroom practice. Science teachers fight a losing battle when they rely on outdated technology to capture the attention of today\u2019s students.\n\n\nThe Next Generation Science Standards (NGSS, 2013) require teacher to move away from describing science with a glossary of words and cookbook, to class activities that model the nature of science. Teachers would love to find new in-class activities to keep the students engaged throughout the learning process. Hence, Science Augmented will meet this real-world need to create a new generation of classroom experiences. \n\n\nScience classes like Biology and Chemistry are inherently 3 dimensional subjects. Current class materials like PowerPoint slides and word processing are insufficient for teachers to demonstrate and hard for students to understand. \n\n\n## How are we building this?\n\n\nWe started tackling this problem with identifying our user personas, high school teacher Mr.Hope, and high school girl Faith. Through empathy mapping exercise, we understood the real needs from our users. Rapid wireframing and prototyping on papers help us create intuitive experience.\n\n\nThe application was developed using Unity. We used the Unity Networking service to connect the Standalone build with mobile devices. Unity\u2019s native Vuforia integration was used for the AR rendering. The design of the application was created using Adobe Creative suite and Sketch. This was imported into Unity and rendered using Unity UI and Text Mesh Pro. We obtained some of our molecular models from the RCSB Protein Data Bank and Google\u2019s Sketchup. \n\n\n## What are our features?\n\n\nIn the collaborative AR classroom, all perspectives are encouraged, and students are able to collaborate, and prototype concepts in class, under the guidance of the teacher. Our platform is a combination of informational aspect, or a generic lab exercise that shows the change in color of the chemicals. We deliver specific information with the adequate level of engagement. \n\n\nThe value we are looking to create:\n\n\n**Visualizing orientation of molecules and bonds**\nMolecular reactions and bonds are not just straight lines and summations on paper, in reality the orientation of the molecules in 3D and location of the bonds play an important role in the reaction. Science Augmented helps teachers deliver this value, while supplementing with interactive learning activities. \n\n\n**A Novel, engaging learning environment**\nEach student with our AR application on their mobile phones, log in to the classroom, and here the teacher assigns molecules to students, creating collaboration in students, while guiding them through the process. This creates immersion in the classroom by shifting the focus in the lesson from the \u2018what happens to molecules?\u2019 to \u2018What should we do with the molecules?\u2019\n\n\n**A flexible and scalable learning platform**\nThis will meet the need for teachers to move away from describing science with a glossary of words, with resources . This would replace a whole \u2018book shelf\u2019 of with a software update. A library of textual information is just another update away. Vuforia platform 98% of android, and 100% iOS devices.\n\n\n## Why Science Augmented?\n\n\nComparing with other AR/VR apps in the market, we are a departure from informational content with cool graphics. We are cool graphics and something similar to \u2018LEGO\u2019 blocks for chemistry, where students collaborate, and look to bind molecules in the right orientations and binding sites to complete the reaction. \n\n\n## The Impact\n\n\nScience Augmented is a multidimensional resource. It allow teachers creating their our classroom, add their students, and leverage many collaborative in-class activities that specifically use the affordances of AR. We believe our application will increase engagement between teachers and students by bringing an immersive guided learning experience to current K-12 education.\n\n\n## What have we learned?\n\n\nWe\u2019re very proud of our product and learned a lot throughout the Hackathon. We have refreshed our memory of high school chemistry and biology, applied the analytical thinking we further developed in college and working with perfect strangers to have a lot of fun. & AR is super COOL.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/proxemicsvr",
            "title": "ProxemicsVR",
            "blurb": "Using interpersonal distance as a game mechanic to teach users about the concept of proxemics.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/h25Fu5yTCFg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "We spent a lot of time discussing sociology, social VR behaviors, and cultural considerations.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/610/datas/original.jpg"
                },
                {
                    "title": "We didn&#39;t have a ruler, so we &quot;hacked&quot; one together using old paper.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/611/datas/original.jpg"
                },
                {
                    "title": "Our developer and designer worked well together; teaching each other new tricks along the way!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/612/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Quinn McCord",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/548/013/datas/profile.jpg"
                },
                {
                    "name": "ywang13",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/20326150?height=180&v=4&width=180"
                },
                {
                    "name": "Hirona  Hono",
                    "about": "",
                    "photo": "https://graph.facebook.com/10214753540620553/picture?height=180&width=180"
                },
                {
                    "name": "Eva Hoerth",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/383/034/datas/profile.jpg"
                }
            ],
            "built_with": [
                "empathy",
                "unity",
                "vrtk"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Social VR can be confusing and awkward. There are no social norms defined yet, making many interactions in VR discomforting to users of different backgrounds, cultures, and preferences. </p>\n<p><img data-canonical-url=\"https://cdn.uploadvr.com/wp-content/uploads/2015/12/AltspaceVR-new-avatars-m-and-f.gif\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--SpF0KbkW--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://cdn.uploadvr.com/wp-content/uploads/2015/12/AltspaceVR-new-avatars-m-and-f.gif\"/></p>\n<p>Proxemics is the concept and study of how individuals use space, how it makes us feel more or less comfortable, and to what extent it impacts the relationship between people. The original concept of proxemics was introduced by E.T. Hall, an anthropologist from the U.S. in 1963. </p>\n<p><a href=\"http://google.com.au/\" rel=\"nofollow\"><img data-canonical-url=\"http://cdn2.hubspot.net/hubfs/288364/Imported_Blog_Media/the-power-of-proxemics-creating-4-zones-for-connection-at-church.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--kmfjK_ra--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/http://cdn2.hubspot.net/hubfs/288364/Imported_Blog_Media/the-power-of-proxemics-creating-4-zones-for-connection-at-church.jpg\"/></a></p>\n<p>What if we could better communicate personal space preferences through the use of visuals, sound, and haptics in social VR?</p>\n<h2>Overview</h2>\n<p>ProxemicsVR visualizes the concept of \"personal space\" in order to create a better understanding of what's acceptable across different cultures. <br/> </p>\n<h2>TEAM INFO</h2>\n<h2>Team Name</h2>\n<p>ProxemicsVR <br/></p>\n<h2>Team Lead</h2>\n<p>Quinn McCord <br/></p>\n<h2>Team Members</h2>\n<p>1) Eva Hoerth <br/>\n2) Hirona Hono <br/>\n3) Quinn McCord - Team Lead <br/>\n4) Yong Wang <br/></p>\n<h2>Category</h2>\n<p>Education, AR &amp; VR for Good <br/></p>\n<h2>Location</h2>\n<p>Location: Floor 6 + Big open room near the deck <br/>\nTable Number: #52 <br/></p>\n<h2>Environment:</h2>\n<p>Platform: Oculus Rift <br/>\nDevelopment tools: Unity, Adobe Fuse, Microsoft Visual Studio <br/>\nSDKs : Oculus, VRTK <br/>\nAPIs: ?? <br/>\nAssets: Mixamo, Unity Asset Store <br/>\nLibraries: VRTK <br/>\nAny components not created at the hackathon: NA <br/></p>\n<h2>What will you experience?</h2>\n<p>You walk into a room of people, mostly strangers. You notice that each person has a bubble around them. </p>\n<p>If you accidentally come across their \"virtual personal space bubbles\" by accident, the sound will notify you. </p>\n<h2>What is Personal Space?</h2>\n<p>Personal space is an invisible area around your body that represents to what extent you feel comfortable around others. Avatars in ProxemicsVR have two bubbles around their body: the outer bubble represents \"social space\" and the inner bubble represents \"personal space.\"</p>\n<p>In the field of cross-cultural psychology and behavioral science, \"personal space\" consists of the following four ares;<br/></p>\n<ol>\n<li>Public distance: For public speaking <br/> </li>\n<li>Social distance: For interactions among acquaintances <br/> </li>\n<li>Personal distance: For interactions among good friends or family <br/> </li>\n<li>Intimate distance: For embracing, touching or whispering <br/> </li>\n</ol>\n<h2>How we built it</h2>\n<p><strong>STEP1_Brainstorm ideas</strong> <br/> \nWe brainstormed together how we can visualize people's personal space. Since it is a concept that is not visible to people in reality, we tried to come up with a new way to represent it, which gives people an opportunity to think about what personal space is, what it feels like, and how it could potentially influence their interaction with others. <br/> </p>\n<p><strong>STEP2_Design a project scope</strong> <br/> \nThere are many variables to determine one's understanding and deployment of one's personal space. Among the above-mentioned four categories of personal space (Public, Social, Personal, and Intimate), for instance, it is considered that the size of your personal space bubble changes over the course of time, who you are interacting with, in what context and environment. For the sake of the scope of this hackathon, we decided to keep the variables as simple as possible and decided to focus on \"Social\" and \"Personal.\" <br/> </p>\n<p><strong>STEP3_Interview participants</strong> <br/> \nOur hypothesis was that participants from different socio-cultural backgrounds have different sizes of bubbles for both social and personal space. In order to investigate that, we picked up 6 participants randomly, but with intention not to choose all of them from seemingly similar socio-cultural backgrounds (please see volunteer interviewees' photo below). First, one of the team members stood across an interviewee and ask the interviewee to walk and approach toward the team member. Then, the interviewee is asked to stop at the location she/he feels comfortable standing considering the distance her/him and our team member. We did this exercise for two rounds (to measure social distance and personal distance) and collected data. <br/></p>\n<p><strong>STEP5_Develop scenes and avatars</strong> <br/> \nWhile two members were collecting personal space data from volunteering interviewees, two other members of our team developed the Unity scene, game mechanism and avatars. </p>\n<p><strong>STEP4_Integrate proximity data to Unity scenes</strong> <br/> \nWe reflected the collected data of personal space into the size of the bubbles that each avatar carries around their body in Unity scene. At the end of the scene development, the movement of avatars, sounds of bubbles when collided with one another, logic of avatar movements were added so that users can see the visualized personal space bubbles when playing this exercise. <br/> </p>\n<h2>Challenges we ran into</h2>\n<p>Sociology is a complex subject; and trying to communicate theories in a virtual world is not an easy task.</p>\n<p>Our early challenges were:</p>\n<ol>\n<li>Figuring out how we could communicate personal space in social VR </li>\n<li>Defining what kind of value this would bring </li>\n<li>Avoiding bringing our own biases into the design of the simulation</li>\n<li>Identifying how we would make this experience more engaging and less educational</li>\n</ol>\n<h2>Accomplishments!</h2>\n<p>Although we only finished a small portion of what we wanted to, we dove deeper into areas that each team member wanted to learn more about, like:</p>\n<ol>\n<li>Sociology </li>\n<li>Unity scripting</li>\n<li>3D modelling </li>\n</ol>\n<h2>What's next for ProxemicsVR</h2>\n<p>We have members from all over the country, and one in Japan! We want to continue gathering data around social interactions and behaviors across cultures, and bring this information to VR creators so that they can design more inclusive social spaces that anyone of any background would enjoy using. Inclusivity is key to successful experiences!</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSocial VR can be confusing and awkward. There are no social norms defined yet, making many interactions in VR discomforting to users of different backgrounds, cultures, and preferences. \n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--SpF0KbkW--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://cdn.uploadvr.com/wp-content/uploads/2015/12/AltspaceVR-new-avatars-m-and-f.gif)\n\n\nProxemics is the concept and study of how individuals use space, how it makes us feel more or less comfortable, and to what extent it impacts the relationship between people. The original concept of proxemics was introduced by E.T. Hall, an anthropologist from the U.S. in 1963. \n\n\n[![](https://res.cloudinary.com/devpost/image/fetch/s--kmfjK_ra--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/http://cdn2.hubspot.net/hubfs/288364/Imported_Blog_Media/the-power-of-proxemics-creating-4-zones-for-connection-at-church.jpg)](http://google.com.au/)\n\n\nWhat if we could better communicate personal space preferences through the use of visuals, sound, and haptics in social VR?\n\n\n## Overview\n\n\nProxemicsVR visualizes the concept of \"personal space\" in order to create a better understanding of what's acceptable across different cultures.   \n \n\n\n## TEAM INFO\n\n\n## Team Name\n\n\nProxemicsVR   \n\n\n\n## Team Lead\n\n\nQuinn McCord   \n\n\n\n## Team Members\n\n\n1) Eva Hoerth   \n\n2) Hirona Hono   \n\n3) Quinn McCord - Team Lead   \n\n4) Yong Wang   \n\n\n\n## Category\n\n\nEducation, AR & VR for Good   \n\n\n\n## Location\n\n\nLocation: Floor 6 + Big open room near the deck   \n\nTable Number: #52   \n\n\n\n## Environment:\n\n\nPlatform: Oculus Rift   \n\nDevelopment tools: Unity, Adobe Fuse, Microsoft Visual Studio   \n\nSDKs : Oculus, VRTK   \n\nAPIs: ??   \n\nAssets: Mixamo, Unity Asset Store   \n\nLibraries: VRTK   \n\nAny components not created at the hackathon: NA   \n\n\n\n## What will you experience?\n\n\nYou walk into a room of people, mostly strangers. You notice that each person has a bubble around them. \n\n\nIf you accidentally come across their \"virtual personal space bubbles\" by accident, the sound will notify you. \n\n\n## What is Personal Space?\n\n\nPersonal space is an invisible area around your body that represents to what extent you feel comfortable around others. Avatars in ProxemicsVR have two bubbles around their body: the outer bubble represents \"social space\" and the inner bubble represents \"personal space.\"\n\n\nIn the field of cross-cultural psychology and behavioral science, \"personal space\" consists of the following four ares;  \n\n\n\n1. Public distance: For public speaking\n2. Social distance: For interactions among acquaintances\n3. Personal distance: For interactions among good friends or family\n4. Intimate distance: For embracing, touching or whispering\n\n\n## How we built it\n\n\n**STEP1\\_Brainstorm ideas**   \n \nWe brainstormed together how we can visualize people's personal space. Since it is a concept that is not visible to people in reality, we tried to come up with a new way to represent it, which gives people an opportunity to think about what personal space is, what it feels like, and how it could potentially influence their interaction with others.   \n \n\n\n**STEP2\\_Design a project scope**   \n \nThere are many variables to determine one's understanding and deployment of one's personal space. Among the above-mentioned four categories of personal space (Public, Social, Personal, and Intimate), for instance, it is considered that the size of your personal space bubble changes over the course of time, who you are interacting with, in what context and environment. For the sake of the scope of this hackathon, we decided to keep the variables as simple as possible and decided to focus on \"Social\" and \"Personal.\"   \n \n\n\n**STEP3\\_Interview participants**   \n \nOur hypothesis was that participants from different socio-cultural backgrounds have different sizes of bubbles for both social and personal space. In order to investigate that, we picked up 6 participants randomly, but with intention not to choose all of them from seemingly similar socio-cultural backgrounds (please see volunteer interviewees' photo below). First, one of the team members stood across an interviewee and ask the interviewee to walk and approach toward the team member. Then, the interviewee is asked to stop at the location she/he feels comfortable standing considering the distance her/him and our team member. We did this exercise for two rounds (to measure social distance and personal distance) and collected data.   \n\n\n\n**STEP5\\_Develop scenes and avatars**   \n \nWhile two members were collecting personal space data from volunteering interviewees, two other members of our team developed the Unity scene, game mechanism and avatars. \n\n\n**STEP4\\_Integrate proximity data to Unity scenes**   \n \nWe reflected the collected data of personal space into the size of the bubbles that each avatar carries around their body in Unity scene. At the end of the scene development, the movement of avatars, sounds of bubbles when collided with one another, logic of avatar movements were added so that users can see the visualized personal space bubbles when playing this exercise.   \n \n\n\n## Challenges we ran into\n\n\nSociology is a complex subject; and trying to communicate theories in a virtual world is not an easy task.\n\n\nOur early challenges were:\n\n\n1. Figuring out how we could communicate personal space in social VR\n2. Defining what kind of value this would bring\n3. Avoiding bringing our own biases into the design of the simulation\n4. Identifying how we would make this experience more engaging and less educational\n\n\n## Accomplishments!\n\n\nAlthough we only finished a small portion of what we wanted to, we dove deeper into areas that each team member wanted to learn more about, like:\n\n\n1. Sociology\n2. Unity scripting\n3. 3D modelling\n\n\n## What's next for ProxemicsVR\n\n\nWe have members from all over the country, and one in Japan! We want to continue gathering data around social interactions and behaviors across cultures, and bring this information to VR creators so that they can design more inclusive social spaces that anyone of any background would enjoy using. Inclusivity is key to successful experiences!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/strangers-with-us",
            "title": "Strangers Within Us",
            "blurb": "A ride through an unknown neighborhood reveals our misconceptions of those around us.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/pvEWU6p1ICE?enablejsapi=1&hl=en_US&rel=0&start=22&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Fabian Patino",
                    "about": "Contributed with my experience with 360 video on setting up the shots and with equipment.   Troubleshooting technical difficulties as well. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/703/datas/profile.jpg"
                },
                {
                    "name": "Haider Ali",
                    "about": "I worked as a VR Director of Photography ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_tDCUdnnQbDj8ImYFgy9VXTxbFRT8IO1BcdsU2CsbXZY3Im-UUDqvFFZbTJDjDx-VRyVVCkBFIYyTDV1VYYbb3CnImYyhDsncRYbZo__65xWuFuSwBfA9EcScaIQrks36tjiBSg_lAkm?height=180&width=180"
                },
                {
                    "name": "Ezequiellenard",
                    "about": "I worked on the script and as a creative producer",
                    "photo": "https://avatars3.githubusercontent.com/u/32619536?height=180&v=4&width=180"
                },
                {
                    "name": "Musharaf",
                    "about": "",
                    "photo": "https://graph.facebook.com/1932658956760887/picture?height=180&width=180"
                },
                {
                    "name": "Hemalt",
                    "about": "",
                    "photo": "https://graph.facebook.com/10156020312048322/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "photoshop",
                "ricoh",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>To explore the misconceptions we may have about unknown places and people.</p>\n<h2>What it does</h2>\n<p>Build empathy through a 360 video experience</p>\n<h2>How I built it</h2>\n<p>We used the Ricoh Theta V to film  and final cut to edit.  </p>\n<h2>Challenges I ran into</h2>\n<p>Framing the story into the correct perspective without falling into stereotypes.  Quickly planning out and filming in locations unknown to any of the devs.  Engaging locals to participate in our video experience.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Being able to include a community in this case which was the Boys and Girls Club of Boston.  We were able to connect and have a great time with a large range of demographics and the process itself become collaborative and inclusive. </p>\n<h2>What I learned</h2>\n<p>Not to judge a book by its cover.  Technology has a way to connect us and immerse us into situations we would normally want to avoid due to our misunderstandings of communities we don't have regular contact with.</p>\n<h2>What's next for Strangers With Us</h2>\n<p>Explore more interactive manners to further explore this seem and create greater empathy and understanding.  We want to create a better sense of community.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTo explore the misconceptions we may have about unknown places and people.\n\n\n## What it does\n\n\nBuild empathy through a 360 video experience\n\n\n## How I built it\n\n\nWe used the Ricoh Theta V to film and final cut to edit. \n\n\n## Challenges I ran into\n\n\nFraming the story into the correct perspective without falling into stereotypes. Quickly planning out and filming in locations unknown to any of the devs. Engaging locals to participate in our video experience.\n\n\n## Accomplishments that I'm proud of\n\n\nBeing able to include a community in this case which was the Boys and Girls Club of Boston. We were able to connect and have a great time with a large range of demographics and the process itself become collaborative and inclusive. \n\n\n## What I learned\n\n\nNot to judge a book by its cover. Technology has a way to connect us and immerse us into situations we would normally want to avoid due to our misunderstandings of communities we don't have regular contact with.\n\n\n## What's next for Strangers With Us\n\n\nExplore more interactive manners to further explore this seem and create greater empathy and understanding. We want to create a better sense of community.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/fast-foodie",
            "title": "Fast Foodie / TechCooks",
            "blurb": "AR Cooking Assistant",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/237345856?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Michael Wissner",
                    "about": "I worked on concept, storyboarding, UX, project management, source code control",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/969/datas/profile.jpg"
                },
                {
                    "name": "Geyao Zhang",
                    "about": "I worked on the Concept and Unity programming part. ",
                    "photo": "https://www.gravatar.com/avatar/c73bd33b9cec76faf43312863418dda3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&height=180&s=180&width=180"
                },
                {
                    "name": "Real Talkies",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/10420097?height=180&v=4&width=180"
                },
                {
                    "name": "yue lin",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/13626227?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "mac",
                "microsoft-hololens",
                "unity",
                "windows-10"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Cooking can be challenging because it involves multiple skills that involve prepping ingredients, managing time and temperature, and remembering sequences of steps.  It can be especially tricky to implement a recipe you have never cooked before. It would be helpful to have an assistant that could help with those tasks, to relieve the mental burden of the cook, and make the process less stressful and more relaxed.</p>\n<h2>What it does</h2>\n<p>This is an augmented reality app that helps a cook make a new recipe. It breaks down a recipe into its essential sequence of steps, and provides guidance to the cook, with simplified, animated, visual imagery overlaid on a real life view of the cooking environment.  A helpful narrator provides subtle cooking hints. It also assists with tasks that are challenging for humans, such as judging timing with floating timers and alarms, and with the addition of wireless thermometers, monitoring temperature.</p>\n<h2>How we built it</h2>\n<p>It was built by our team using both Mac and Windows, and pencil and paper.  Some of the technologies we used included Unity, Microsoft Hololens and its SDKs, Maya, Adobe Photoshop, Github.</p>\n<h2>Challenges we ran into</h2>\n<p>There were both application domain challenges, and technology challenges.</p>\n<ul>\n<li><p>Figuring how to represent a recipe in a visual way, that was both simple and expressive.</p></li>\n<li><p>Reducing a wordy recipe to its essential components.</p></li>\n<li><p>Reducing the scope of the project to something achievable in a short time with limited resources.</p></li>\n<li><p>Balancing what would make a reasonable present-time project, with futuristic aspirations.</p></li>\n<li><p>Working in a mixed Mac and Windows environment. Some aspects of the system could only be implemented on Windows, such as the Hololens functions, but several team members had Macs. </p></li>\n<li><p>Installing compatible versions of software on different platforms.</p></li>\n<li><p>Learning to use the tools and technologies.</p></li>\n<li><p>Coordinating a small team with various skills on an intensive, short term project.</p></li>\n</ul>\n<h2>What's next for Fast Foodie</h2>\n<p>There is a lot of potential for future enhancement, starting with parts of the original vision that were eliminated due to limitations on time and resources, and extending to future features that were inspired by working with the concept.</p>\n<ul>\n<li>We would like to add more animation to the visual graphics, and tie them to the physical objects that they are associated with. </li>\n<li>We would like to add real-time indicators such as count-down timers, and temperature read-outs (using wireless sensors), that would give a cook skills that humans normally are not good at. </li>\n<li>We would like the add flexibility to customize recipes, by omitting or adding optional ingredients or scaling the recipe for number of servings.</li>\n<li>We would like to add a process for choosing a recipe that allows the computer to recommend recipes, and the user to browse the suggestions and choose one. </li>\n<li>We would like to automate the process of turning a recipe into an AR experience, by using natural language analysis to translate the recipe into known concepts, and then mapping those concepts onto visual modules representing ingredients and actions, which can automatically be assembled into an AR experience.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nCooking can be challenging because it involves multiple skills that involve prepping ingredients, managing time and temperature, and remembering sequences of steps. It can be especially tricky to implement a recipe you have never cooked before. It would be helpful to have an assistant that could help with those tasks, to relieve the mental burden of the cook, and make the process less stressful and more relaxed.\n\n\n## What it does\n\n\nThis is an augmented reality app that helps a cook make a new recipe. It breaks down a recipe into its essential sequence of steps, and provides guidance to the cook, with simplified, animated, visual imagery overlaid on a real life view of the cooking environment. A helpful narrator provides subtle cooking hints. It also assists with tasks that are challenging for humans, such as judging timing with floating timers and alarms, and with the addition of wireless thermometers, monitoring temperature.\n\n\n## How we built it\n\n\nIt was built by our team using both Mac and Windows, and pencil and paper. Some of the technologies we used included Unity, Microsoft Hololens and its SDKs, Maya, Adobe Photoshop, Github.\n\n\n## Challenges we ran into\n\n\nThere were both application domain challenges, and technology challenges.\n\n\n* Figuring how to represent a recipe in a visual way, that was both simple and expressive.\n* Reducing a wordy recipe to its essential components.\n* Reducing the scope of the project to something achievable in a short time with limited resources.\n* Balancing what would make a reasonable present-time project, with futuristic aspirations.\n* Working in a mixed Mac and Windows environment. Some aspects of the system could only be implemented on Windows, such as the Hololens functions, but several team members had Macs.\n* Installing compatible versions of software on different platforms.\n* Learning to use the tools and technologies.\n* Coordinating a small team with various skills on an intensive, short term project.\n\n\n## What's next for Fast Foodie\n\n\nThere is a lot of potential for future enhancement, starting with parts of the original vision that were eliminated due to limitations on time and resources, and extending to future features that were inspired by working with the concept.\n\n\n* We would like to add more animation to the visual graphics, and tie them to the physical objects that they are associated with.\n* We would like to add real-time indicators such as count-down timers, and temperature read-outs (using wireless sensors), that would give a cook skills that humans normally are not good at.\n* We would like the add flexibility to customize recipes, by omitting or adding optional ingredients or scaling the recipe for number of servings.\n* We would like to add a process for choosing a recipe that allows the computer to recommend recipes, and the user to browse the suggestions and choose one.\n* We would like to automate the process of turning a recipe into an AR experience, by using natural language analysis to translate the recipe into known concepts, and then mapping those concepts onto visual modules representing ingredients and actions, which can automatically be assembled into an AR experience.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bobross-ar",
            "title": "BobRossAR",
            "blurb": "An AR experience that helps beginner artists learn to draw and paint by overlaying step-by-step instructors on canvas",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/UUyzZPqvf-o?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Anthony Liu",
                    "about": "I worked on the marker tracking system that localizes the painting canvas in the camera feed.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/402/822/datas/profile.jpg"
                },
                {
                    "name": "Tomiwa Oladele",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/30136471?height=180&v=4&width=180"
                },
                {
                    "name": "arapstar",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32597419?height=180&v=4&width=180"
                },
                {
                    "name": "Heath Palmer",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/15958006?height=180&v=4&width=180"
                },
                {
                    "name": "Seyitan Oke",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/412/928/datas/profile.jpg"
                }
            ],
            "built_with": [
                "html",
                "javascript"
            ],
            "content_html": "<div>\n<h1>Hackathon Submission Requirements</h1>\n<p><strong>Team Lead:</strong> Anthony Liu, 650-922-4476</p>\n<p><strong>Team Members:</strong> Anthony Liu, Heath Palmer, Ara Parikh, Tommy Oladele, Seyitan Oke </p>\n<p><strong>Category:</strong> Education, AR &amp; VR For Good</p>\n<p><strong>Brief Description:</strong> BobRossAR is a mobile augmented reality experience that helps students learn how to draw and paint by overlaying step-by-step instructions on the paper and canvases in front of them. </p>\n<p><strong>Location:</strong> 5th floor, to the right of the elevator on high top tables, Table 34</p>\n<p><strong>Environment:</strong> \nPlatform: HTML/Web; Assets: Minion.png image; Libraries: three.js, js-aruco for marker tracking </p>\n<h1>Why we built it</h1>\n<p>Arts funding in U.S. public schools have decreased--just 5% of elementary school students in Boston take a weekly art class as part of their education. They\u2019re missing education that is foundational to their development. Third grade art curriculum includes teaching core geometry concepts such as spatial understanding, depth perception, and positive and negative space. We need our kids to know that if AR/VR is the future!</p>\n<p>Our project, BobRossAR, wants to help increase access to basic art principles students just can\u2019t get in schools. Using just their phone, a sheet of paper, and a drawing tool, we teach them how to draw, just as easily as Bob Ross taught millions how to draw through TV. But, here we use mobile AR to break down the steps and overlay instructions onto your canvas to build a more immersive learning experience.  AR is the only way we can create an immersive learning experience that is similar to your teacher right next to you modelling drawing strokes. Mobile AR is cheap and easy to access-- a critical goal for our project. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>-We creatively solved tracking issues with other tracking libraries </p>\n</div>",
            "content_md": "\n# Hackathon Submission Requirements\n\n\n**Team Lead:** Anthony Liu, 650-922-4476\n\n\n**Team Members:** Anthony Liu, Heath Palmer, Ara Parikh, Tommy Oladele, Seyitan Oke \n\n\n**Category:** Education, AR & VR For Good\n\n\n**Brief Description:** BobRossAR is a mobile augmented reality experience that helps students learn how to draw and paint by overlaying step-by-step instructions on the paper and canvases in front of them. \n\n\n**Location:** 5th floor, to the right of the elevator on high top tables, Table 34\n\n\n**Environment:** \nPlatform: HTML/Web; Assets: Minion.png image; Libraries: three.js, js-aruco for marker tracking \n\n\n# Why we built it\n\n\nArts funding in U.S. public schools have decreased--just 5% of elementary school students in Boston take a weekly art class as part of their education. They\u2019re missing education that is foundational to their development. Third grade art curriculum includes teaching core geometry concepts such as spatial understanding, depth perception, and positive and negative space. We need our kids to know that if AR/VR is the future!\n\n\nOur project, BobRossAR, wants to help increase access to basic art principles students just can\u2019t get in schools. Using just their phone, a sheet of paper, and a drawing tool, we teach them how to draw, just as easily as Bob Ross taught millions how to draw through TV. But, here we use mobile AR to break down the steps and overlay instructions onto your canvas to build a more immersive learning experience. AR is the only way we can create an immersive learning experience that is similar to your teacher right next to you modelling drawing strokes. Mobile AR is cheap and easy to access-- a critical goal for our project. \n\n\n## Accomplishments that we're proud of\n\n\n-We creatively solved tracking issues with other tracking libraries \n\n\n"
        },
        {
            "source": "https://devpost.com/software/sparky-srh7k9",
            "title": "Sparky",
            "blurb": "Use AR to bring toys to life to comfort, entertain, and reassure children facing long-term hospitalization.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Sophie Salomon",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/16977639?height=180&v=4&width=180"
                },
                {
                    "name": "Harrison Krug",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e7730f6a737edab56c00266d5b3e9c4b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Abhigyan Kaustubh",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/192/813/datas/profile.jpg"
                }
            ],
            "built_with": [
                "arcore",
                "arkit",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Children facing long-term hospitalization or outpatient recovery often struggle with loneliness, boredom, and depression from the isolation caused by their restrictive environment. Particularly children whose parents work full-time, and who may be vulnerable to illness with exposure to other children or family members, face many lonely hours in a sterile, depersonalized, and often overwhelming environment, that may include medical machines and intimidating interactions with medical professionals. With this in mind, our goal is to improve the patient experience for these children by bringing an element of warmth and friendship to this traumatic and limiting environment. Project Sparky aims to assuage the loneliness, anxiety, and confusion that these young children might face in the hospital environment using accessible Augmented Reality through smartphone and tablet devices. </p>\n<h2>What it does</h2>\n<p>Project Sparky provides young children facing medical processes and hospitalization lasting longer than one week with a stuffed animal paired with an AR avatar of that plush that they can keep, in order to provide them a comfort object that doubles as a virtual friend. This encourages healthy processing of their situation as opposed to outright escapism, which could cause a jarring return to reality. By giving them a soft toy to hold when anxious as well as a friendly entity to interact with, this project helps bring a comforting element to the unpleasant, depersonalized hospital environment. It stimulates the children's imaginations and encourages social interaction, as well as discussion of their emotions and the positive, optimistic elements of their lives, as opposed to a passive, solo activity such as watching television. Version 0.1 includes a proof of concept of an example animated character with a semi-scripted set of interactions that can be seen through a phone with or without a headset on a surface based on a pairing with a physical stuffed animal. </p>\n<h2>How we built it</h2>\n<p>[...] Unity, ARCore, ARKit [...]</p>\n<h2>Challenges we ran into</h2>\n<p>[...]</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>[...]</p>\n<h2>What we learned</h2>\n<p>[...]</p>\n<h2>What's next for Sparky</h2>\n<p>[...]</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nChildren facing long-term hospitalization or outpatient recovery often struggle with loneliness, boredom, and depression from the isolation caused by their restrictive environment. Particularly children whose parents work full-time, and who may be vulnerable to illness with exposure to other children or family members, face many lonely hours in a sterile, depersonalized, and often overwhelming environment, that may include medical machines and intimidating interactions with medical professionals. With this in mind, our goal is to improve the patient experience for these children by bringing an element of warmth and friendship to this traumatic and limiting environment. Project Sparky aims to assuage the loneliness, anxiety, and confusion that these young children might face in the hospital environment using accessible Augmented Reality through smartphone and tablet devices. \n\n\n## What it does\n\n\nProject Sparky provides young children facing medical processes and hospitalization lasting longer than one week with a stuffed animal paired with an AR avatar of that plush that they can keep, in order to provide them a comfort object that doubles as a virtual friend. This encourages healthy processing of their situation as opposed to outright escapism, which could cause a jarring return to reality. By giving them a soft toy to hold when anxious as well as a friendly entity to interact with, this project helps bring a comforting element to the unpleasant, depersonalized hospital environment. It stimulates the children's imaginations and encourages social interaction, as well as discussion of their emotions and the positive, optimistic elements of their lives, as opposed to a passive, solo activity such as watching television. Version 0.1 includes a proof of concept of an example animated character with a semi-scripted set of interactions that can be seen through a phone with or without a headset on a surface based on a pairing with a physical stuffed animal. \n\n\n## How we built it\n\n\n[...] Unity, ARCore, ARKit [...]\n\n\n## Challenges we ran into\n\n\n[...]\n\n\n## Accomplishments that we're proud of\n\n\n[...]\n\n\n## What we learned\n\n\n[...]\n\n\n## What's next for Sparky\n\n\n[...]\n\n\n"
        },
        {
            "source": "https://devpost.com/software/symbio",
            "title": "Symbio",
            "blurb": "Collective and educational game across platforms (VR/AR to browsers/screens anywhere). Option of biofeedback ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/U7nnQhaa2u8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Screenshot of VR experience ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/545/313/datas/original.png"
                },
                {
                    "title": "Intro for browser player. Similar in VR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/749/datas/original.png"
                },
                {
                    "title": "Game description for browser player. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/747/datas/original.png"
                },
                {
                    "title": "Game description for browser player. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/752/datas/original.png"
                },
                {
                    "title": "Game description for browser player. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/754/datas/original.png"
                },
                {
                    "title": "Visualization of a Bacteria constitute a large part of microorganisms.B&#39;s have a number of shapes, ranging from spheres to rods and spirals.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/731/datas/original.png"
                },
                {
                    "title": "Visualization of lymphocytes as on browser screen. L&#39;s are a type of white blood cell respond to foreign invaders to help immune system.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/675/datas/original.png"
                },
                {
                    "title": "Visualization of dead food particle in blood flow.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/676/datas/original.png"
                },
                {
                    "title": "Visualization of a red blood cell.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/695/datas/original.png"
                },
                {
                    "title": "Visualization of a Neutrophil: most abundant type of white blood cells in mammals. They form an essential part of the innate immune system.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/727/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "rompo010",
                    "about": "Researched science behind game topic, designed interface  , characters, environment, and menus in 2d and created skybox and 3d models in VR. contributed to gameplay and concept ideation",
                    "photo": "https://avatars1.githubusercontent.com/u/32602808?height=180&v=4&width=180"
                },
                {
                    "name": "Hannes Bend",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/463/085/datas/profile.jpeg"
                },
                {
                    "name": "Florian Carls",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/543/281/datas/profile.jpg"
                },
                {
                    "name": "Lewis Gardner",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/578/datas/profile.png"
                },
                {
                    "name": "Roldan Melcon",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/543/307/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "html5",
                "javascript",
                "node.js",
                "oculus",
                "phaser.js",
                "socket.io",
                "unity"
            ],
            "content_html": "<div>\n<p>A collaborative experience of one VR user and screen users anywhere. The VR is connected to screens, phones or any browser device. The educational game offers collaborative learning about various topics.</p>\n<p>Today's first \u2018game\u2019 combines biology, synchronized breathing and interfaces, and interplay. The VR user plays a white blood cell inside a blood vessel to fight off \u2018bad\u2019 bacteria. The screen users can team up as blood cells, breathe to energize the VR user\u2019s entity \u2013 or become bacterial agents to disrupt the smooth blood flow.</p>\n<p>Gamified and embodied educational experiences and breathing techniques are scientifically suggested to improve health and productivity. Gamification (Johnson et al., 2016) can have a positive impact in health and wellbeing. Collaborative learning (Laal and Ghodsi, 2011) results in greater productivity, more caring and committed relationships, greater psychological health, social competence, and self esteem. Simple breathing techniques have shown to fight injection of e.coli bacteria within minutes (Kox et al., 2014).</p>\n<p>Virtual reality can be a solitary experience and its classroom or educational value limited. Excessive screen-based usage has been linked to less attention span. Studies conclude poor academic performance can be predicted by higher levels of smartphone use (Beland and Murphy, 2014) and electronic media usage (Junco and Cotten, 2012). Regular engagement with digital media technologies can lead to shorter attention spans (e.g., Egan, 2016)</p>\n<p>Symbio offers a gamified, collaborative and educational experience beyond the current limitations of the applied mediums. The program dynamics are easily expandable to new mediums such as AR, explore a variety of educational topics and integrate collective biofeedback for future biotechnology.</p>\n<p>Technology: VR experience built in Unity or Unreal Socket.io to exchange packets with Node server. Node JS server hosted on AWS and distributed via bit.ly shortened link. Express.js and Phaser.js for 2d Interface Socket.io backend</p>\n<p>Team Lead: Roldan Melcon \nTeam: Roldan Melcon, Nathan Romportl, Lewis Gardner, hannes bend, Florian Carls </p>\n<p>Team: Symbio \nTeam lead\u2019s mobile number: + 1 720 217 5305</p>\n<p>Education/VR&amp;AR for Good </p>\n<p>Collective gamified and embodied educational experience of one VR user and screen users anywhere. The VR is connected to screens, phones or any browser device. The educational game offers collaborative learning about various topics. \nToday's first \u2018game\u2019 combines biology, synchronized breathing and interfaces, and interplay. The VR user plays a white blood cell inside a blood vessel to fight off \u2018bad\u2019 bacteria. The screen users can team up as blood cells, breathe to energize the VR user\u2019s entity \u2013 or become bacterial agents to disrupt the smooth blood flow. Optional biofeedback (team experience of past VR projects) is integrated as suggestion and build in structure of future iterations. \nSymbio offers a gamified, collaborative and educational experience beyond the current limitations of the applied mediums. The program dynamics are easily expandable to new mediums such as AR, explore a variety of educational topics and integrate collective biofeedback for future biotechnology.</p>\n<p>Location: Floor: 6 + Room:  + Table Number: 31</p>\n<p>Environment:\nPlatform: VR cross-platform browser/mobile\nDevelopment tools: c#, Unity, Javascript, AdobeSuite, node.js, phaser.js,  express.js, socket.io, HTML5, Oculus, CSS 3, AWS\nSDKs: Oculus SDK\nAPIs:  None\nAssets: Oculus Utilities for Unity\nLibraries: None\nAny components not created at the hackathon: None</p>\n<p><a href=\"https://github.com/Reality-Virtually-Hackathon/biofield/\" rel=\"nofollow\">https://github.com/Reality-Virtually-Hackathon/biofield/</a></p>\n</div>",
            "content_md": "\nA collaborative experience of one VR user and screen users anywhere. The VR is connected to screens, phones or any browser device. The educational game offers collaborative learning about various topics.\n\n\nToday's first \u2018game\u2019 combines biology, synchronized breathing and interfaces, and interplay. The VR user plays a white blood cell inside a blood vessel to fight off \u2018bad\u2019 bacteria. The screen users can team up as blood cells, breathe to energize the VR user\u2019s entity \u2013 or become bacterial agents to disrupt the smooth blood flow.\n\n\nGamified and embodied educational experiences and breathing techniques are scientifically suggested to improve health and productivity. Gamification (Johnson et al., 2016) can have a positive impact in health and wellbeing. Collaborative learning (Laal and Ghodsi, 2011) results in greater productivity, more caring and committed relationships, greater psychological health, social competence, and self esteem. Simple breathing techniques have shown to fight injection of e.coli bacteria within minutes (Kox et al., 2014).\n\n\nVirtual reality can be a solitary experience and its classroom or educational value limited. Excessive screen-based usage has been linked to less attention span. Studies conclude poor academic performance can be predicted by higher levels of smartphone use (Beland and Murphy, 2014) and electronic media usage (Junco and Cotten, 2012). Regular engagement with digital media technologies can lead to shorter attention spans (e.g., Egan, 2016)\n\n\nSymbio offers a gamified, collaborative and educational experience beyond the current limitations of the applied mediums. The program dynamics are easily expandable to new mediums such as AR, explore a variety of educational topics and integrate collective biofeedback for future biotechnology.\n\n\nTechnology: VR experience built in Unity or Unreal Socket.io to exchange packets with Node server. Node JS server hosted on AWS and distributed via bit.ly shortened link. Express.js and Phaser.js for 2d Interface Socket.io backend\n\n\nTeam Lead: Roldan Melcon \nTeam: Roldan Melcon, Nathan Romportl, Lewis Gardner, hannes bend, Florian Carls \n\n\nTeam: Symbio \nTeam lead\u2019s mobile number: + 1 720 217 5305\n\n\nEducation/VR&AR for Good \n\n\nCollective gamified and embodied educational experience of one VR user and screen users anywhere. The VR is connected to screens, phones or any browser device. The educational game offers collaborative learning about various topics. \nToday's first \u2018game\u2019 combines biology, synchronized breathing and interfaces, and interplay. The VR user plays a white blood cell inside a blood vessel to fight off \u2018bad\u2019 bacteria. The screen users can team up as blood cells, breathe to energize the VR user\u2019s entity \u2013 or become bacterial agents to disrupt the smooth blood flow. Optional biofeedback (team experience of past VR projects) is integrated as suggestion and build in structure of future iterations. \nSymbio offers a gamified, collaborative and educational experience beyond the current limitations of the applied mediums. The program dynamics are easily expandable to new mediums such as AR, explore a variety of educational topics and integrate collective biofeedback for future biotechnology.\n\n\nLocation: Floor: 6 + Room: + Table Number: 31\n\n\nEnvironment:\nPlatform: VR cross-platform browser/mobile\nDevelopment tools: c#, Unity, Javascript, AdobeSuite, node.js, phaser.js, express.js, socket.io, HTML5, Oculus, CSS 3, AWS\nSDKs: Oculus SDK\nAPIs: None\nAssets: Oculus Utilities for Unity\nLibraries: None\nAny components not created at the hackathon: None\n\n\n<https://github.com/Reality-Virtually-Hackathon/biofield/>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dr-holo",
            "title": "Dr. Holo",
            "blurb": "Hololens AR App that focuses on optimizing patient care and reducing medical errors.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Z6ied3XJNgk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "JulietDiBenedetto",
                    "about": "worked on concept and UI design, in Unity and C#. Worked with team to learn to design for Hololens and for the first time learned to use Mixed Media Toolkit. We are still coding behaviors for the project as I respond to this. Go Hackers!",
                    "photo": "https://avatars0.githubusercontent.com/u/32585184?height=180&v=4&width=180"
                },
                {
                    "name": "Samantha Garcia",
                    "about": "Worked on initial UI design. Implemented searching algorithm to retrieve patient information based on MRN. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/537/870/datas/profile.jpg"
                },
                {
                    "name": "Alec Hoffmann",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/460/892/datas/profile.jpg"
                },
                {
                    "name": "Abel Paguio",
                    "about": "",
                    "photo": "https://graph.facebook.com/1683672631651822/picture?height=180&width=180"
                },
                {
                    "name": "DennysPelegrin",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/17902296?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<p>Application:</p>\n<p>-Purpose: Create a Hololens AR App that focuses on optimizing patient care and reduce medical errors. App functions within hospital premises to provide Dr.\u2019s and rotating medical staff \u201chands free\u201d access to interact with patient information on inter-hospital secure medical database. With potential to view Medical Imaging (MRI, Xray, Ultrasound) data in 3D to assist with treatment plan.</p>\n<p>-Design: Hololens- Unity</p>\n<p>-Required Components: UI Database- access and view appropriate patient data- digital patient records- confine app and device (hololens) to secure hospital database- patient information only accessible inter-hospital. Image capture and barcode scan component Input of information- voice to text conversion, scan digital characters on medical devices to text conversion</p>\n<p>-Benefit: Reduce risk of medical errors, nosocomial infections and malpractice Faster and \u201chands free\u201d interface for immediate access to patient information and treatment plan with capability to add new patient information, or update/order treatment plan/schedule/medications, without having to physically handle or flip through paper charts where information can get lost or misplaced. Ability to confirm with the hospital\u2019s secure internal database that the patient and treatment plan are the correct one Ability to monitor patient care inter-hospital and communicate instantly regarding patient care with hospital staff, real time updates</p>\n<p>*<em>We will be leaving on Monday at around 12:30 - 1:00 p.m. *</em></p>\n<p>\u2022 Team Leader: Samantha Garcia</p>\n<p>\u2022 Team Name: Dr. Holo</p>\n<p>\u2022 Team Leader\u2019s Mobile Number: 1 (305) 965-1895</p>\n<p>\u2022 List of all team members:\no   Dennys Arturo Pelegrin Santos\no   Samantha Garcia\no   Alec Hoffmann\no   Juliet DiBenedetto\no   Abel Rakuhana</p>\n<p>\u2022 Category: Healthcare</p>\n<p>\u2022 Brief Description: A Hololens application that focuses on optimizing patient care and reducing medical errors in an emergency room setting. </p>\n<p>\u2022 Location: third floor, E15-315, 53</p>\n<p>\u2022 Environment:\no   Platform: Hololens\no   Development tools: Unity, Hololens Emulator\no   SDKs: Microsoft\u2019s \u201cMixedRealityToolkit-Unity\u201d. <a href=\"https://github.com/Microsoft/MixedRealityToolkit-Unity\" rel=\"nofollow\">https://github.com/Microsoft/MixedRealityToolkit-Unity</a>\no   APIs: none\no   Assets: Personal medical imagery from high field MRI\no   Libraries: none\no   Components not created at hackathon: none</p>\n</div>",
            "content_md": "\nApplication:\n\n\n-Purpose: Create a Hololens AR App that focuses on optimizing patient care and reduce medical errors. App functions within hospital premises to provide Dr.\u2019s and rotating medical staff \u201chands free\u201d access to interact with patient information on inter-hospital secure medical database. With potential to view Medical Imaging (MRI, Xray, Ultrasound) data in 3D to assist with treatment plan.\n\n\n-Design: Hololens- Unity\n\n\n-Required Components: UI Database- access and view appropriate patient data- digital patient records- confine app and device (hololens) to secure hospital database- patient information only accessible inter-hospital. Image capture and barcode scan component Input of information- voice to text conversion, scan digital characters on medical devices to text conversion\n\n\n-Benefit: Reduce risk of medical errors, nosocomial infections and malpractice Faster and \u201chands free\u201d interface for immediate access to patient information and treatment plan with capability to add new patient information, or update/order treatment plan/schedule/medications, without having to physically handle or flip through paper charts where information can get lost or misplaced. Ability to confirm with the hospital\u2019s secure internal database that the patient and treatment plan are the correct one Ability to monitor patient care inter-hospital and communicate instantly regarding patient care with hospital staff, real time updates\n\n\n**We will be leaving on Monday at around 12:30 - 1:00 p.m. **\n\n\n\u2022 Team Leader: Samantha Garcia\n\n\n\u2022 Team Name: Dr. Holo\n\n\n\u2022 Team Leader\u2019s Mobile Number: 1 (305) 965-1895\n\n\n\u2022 List of all team members:\no Dennys Arturo Pelegrin Santos\no Samantha Garcia\no Alec Hoffmann\no Juliet DiBenedetto\no Abel Rakuhana\n\n\n\u2022 Category: Healthcare\n\n\n\u2022 Brief Description: A Hololens application that focuses on optimizing patient care and reducing medical errors in an emergency room setting. \n\n\n\u2022 Location: third floor, E15-315, 53\n\n\n\u2022 Environment:\no Platform: Hololens\no Development tools: Unity, Hololens Emulator\no SDKs: Microsoft\u2019s \u201cMixedRealityToolkit-Unity\u201d. <https://github.com/Microsoft/MixedRealityToolkit-Unity>\no APIs: none\no Assets: Personal medical imagery from high field MRI\no Libraries: none\no Components not created at hackathon: none\n\n\n"
        },
        {
            "source": "https://devpost.com/software/chillar",
            "title": "ChillAR",
            "blurb": "An AR meditation experience that transports you to a world of surreal bubbly creatures.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "ChillAR ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/829/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Shan Liu",
                    "about": "Design strategy, 3D modeling",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/517/592/datas/profile.png"
                },
                {
                    "name": "Jessica Ho",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_tBVQ93C0a8sA9IcutPmQjFk0ENxuvERSOBuHKNy0a1EDrsQr46meBLH0H1sxPVQ8Y6CeBLwxGn5S9mwORX0sOzEPbn529mGuYX05zqxYuBK8pZJr-vWIv8l7FixpxmvgcbUEt686KuM?height=180&width=180"
                },
                {
                    "name": "Megan Valanidas",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/24512730?height=180&v=4&width=180"
                },
                {
                    "name": "jsiegelmann",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/31635151?height=180&v=4&width=180"
                },
                {
                    "name": "zhaodiwang",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/32549370?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "maya",
                "merge-vr",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Your brain is fuzzy; you are tired but not ready to sleep; you want to escape to a private space in a public place -- ChillAR is here to help! Put on our AR headset, and you will experience surreal soundscapes and morphing geometries juxtaposed with your reality -- it is essentially your mind's screensaver. </p>\n<p>We used James Turrell's Skyspace as our inspiration, and explored mood colors and mood soundscapes to bring calming effects. </p>\n<h2>What it does</h2>\n<p>ChillAR is an augmented reality based application that overlays your reality with calming visuals and soundscapes. ChillAR uses the MergeVR cube as visual tracker and trigger for your calming AR experience. Start the app by rolling the MergeVR cube to dive into one of six augmented reality scenarios, each configured to sooth your mind through a different effect. Ranging from little orbs of light naturally spawning and dancing through your field of vision to a white sphere slowly growing in front of you in tandem to your heartbeat, ChillAR is supposed to work off of your natural body feedback to make each experience a personal one. Within one roll of the dice, the visuals will increase in intensity to create a natural crescendo, culminating in a white screen for a couple of seconds, until the opacity decreases and reality slowly sets in again.</p>\n<h2>How we built it</h2>\n<p>Our team consists of 4 designers and 1 software engineer. With a majority of us being visual thinkers, we started the project by brainstorming on whiteboards. We listed all the calming elements that we thought of, and narrowed down to three that were realistic for us to build. Three of the designers (Megan, Julian, Shan) were tasked with creating one visual effect each, architect Zhaodi was tasked with sound effects, and software engineer Jessica took on learning Vuforia and Unity integration. </p>\n<p>Technologies that we used were: Unity as game engine; Vuforia as AR platform; Merge cubes as visual trackers; Maya as asset builder. </p>\n<h2>Challenges we ran into</h2>\n<p>Because none of us had much Unity experience before this hackathon, our biggest challenge was to translate our vision into prototypes. We spent the first day to learn Unity and do tutorials, and the second day figuring out how to integrate our separate files together into one holistic experience. We had to eliminate down to one visual effect to demo, from the original three, because of a lack of technical support (e.g. the Maya animation Shan made could not be exported into Unity) </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We had a fantastic brainstorm session. With systematic design thinking, we successfully narrowed down from a vague idea \"relaxation in AR\", to actionable work plans for each teammate. </p>\n<p>Also, without little to no knowledge in Unity, we managed to individually create the visual effect that we envisioned, even though we could not bring them together. </p>\n<h2>What we learned</h2>\n<p>It's important to have at least one team member to implement the vision. </p>\n<h2>What's next for ChillAR</h2>\n<p>We want to have a variety of visual effects to accommodate different personal preferences. </p>\n<p>ChillAR can also become a share experience among multiple players - we are looking forward to expand this into an AR icebreaker. </p>\n<p>See our work in progress on GitHub: \n<a href=\"https://github.com/Reality-Virtually-Hackathon/ChillAR\" rel=\"nofollow\">link</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nYour brain is fuzzy; you are tired but not ready to sleep; you want to escape to a private space in a public place -- ChillAR is here to help! Put on our AR headset, and you will experience surreal soundscapes and morphing geometries juxtaposed with your reality -- it is essentially your mind's screensaver. \n\n\nWe used James Turrell's Skyspace as our inspiration, and explored mood colors and mood soundscapes to bring calming effects. \n\n\n## What it does\n\n\nChillAR is an augmented reality based application that overlays your reality with calming visuals and soundscapes. ChillAR uses the MergeVR cube as visual tracker and trigger for your calming AR experience. Start the app by rolling the MergeVR cube to dive into one of six augmented reality scenarios, each configured to sooth your mind through a different effect. Ranging from little orbs of light naturally spawning and dancing through your field of vision to a white sphere slowly growing in front of you in tandem to your heartbeat, ChillAR is supposed to work off of your natural body feedback to make each experience a personal one. Within one roll of the dice, the visuals will increase in intensity to create a natural crescendo, culminating in a white screen for a couple of seconds, until the opacity decreases and reality slowly sets in again.\n\n\n## How we built it\n\n\nOur team consists of 4 designers and 1 software engineer. With a majority of us being visual thinkers, we started the project by brainstorming on whiteboards. We listed all the calming elements that we thought of, and narrowed down to three that were realistic for us to build. Three of the designers (Megan, Julian, Shan) were tasked with creating one visual effect each, architect Zhaodi was tasked with sound effects, and software engineer Jessica took on learning Vuforia and Unity integration. \n\n\nTechnologies that we used were: Unity as game engine; Vuforia as AR platform; Merge cubes as visual trackers; Maya as asset builder. \n\n\n## Challenges we ran into\n\n\nBecause none of us had much Unity experience before this hackathon, our biggest challenge was to translate our vision into prototypes. We spent the first day to learn Unity and do tutorials, and the second day figuring out how to integrate our separate files together into one holistic experience. We had to eliminate down to one visual effect to demo, from the original three, because of a lack of technical support (e.g. the Maya animation Shan made could not be exported into Unity) \n\n\n## Accomplishments that we're proud of\n\n\nWe had a fantastic brainstorm session. With systematic design thinking, we successfully narrowed down from a vague idea \"relaxation in AR\", to actionable work plans for each teammate. \n\n\nAlso, without little to no knowledge in Unity, we managed to individually create the visual effect that we envisioned, even though we could not bring them together. \n\n\n## What we learned\n\n\nIt's important to have at least one team member to implement the vision. \n\n\n## What's next for ChillAR\n\n\nWe want to have a variety of visual effects to accommodate different personal preferences. \n\n\nChillAR can also become a share experience among multiple players - we are looking forward to expand this into an AR icebreaker. \n\n\nSee our work in progress on GitHub: \n[link](https://github.com/Reality-Virtually-Hackathon/ChillAR)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/chefar",
            "title": "ChefAR",
            "blurb": "Meet your private chef who teaches you how to create global flavors and curate healthy recipes, powered by AR. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/8nDT57yjxoI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Hacking AR with Hololens Day 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/665/datas/original.jpeg"
                },
                {
                    "title": "Hacking AR with HoloLens Day 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/546/666/datas/original.jpeg"
                }
            ],
            "team": [
                {
                    "name": "Lei Xia",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_m2A0tePeP9EcdFjf7abYMsgWvXen2NymC2dlBjpWN_pc2Xp-XgIAVyOWrBIcdbS_G2WAMygdVTBB7reYFxqisyrLxTB97rZGCxqplgLHN8Cs56_YaaCtAdIJ0qS66rf1IpK19_df4ea?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "microsoft-hololens",
                "sketch",
                "unity"
            ],
            "content_html": "<div>\n<h2>ChefAR in 1min</h2>\n<p>We created a learn-to-cook experience in AR that immerses a home chef fully in the cooking process. No more switching between ingredients and websites or videos (and touching your phone or tablet with dirty hands). The cooking instructions are overlaid on real food as you cook, making learning a much more hands-on and engaging experience. Our product completes the entire cooking cycle from learning what\u2019s available in the fridge, recommending recipes, to demoing step-by-step animated instructions with voice. Because our AI-enabled product handles the logistic hassles in the background, the home chef can focus on her relationship with food and fully enjoy the learning experience.</p>\n<h2>Problem Statement</h2>\n<p>Ever aspired to become a celebrity chef featured on Chef\u2019s Table? Well, we did - but let\u2019s face it, cooking is REALLY HARD. From picking ingredients, creating a recipe, adjusting flavors, to final plating, each step requires knowledge, focus, patience and even talent. And NO, tutorials on Youtube or BlueApron don\u2019t help much - they do not care about your grocery, schedule, or cooking pace. You may have to keep hitting the replay button with fingers covered in both grease and keyboard-harbored bacteria while trying to rescue yourself from a potential house fire... </p>\n<h2>Solution</h2>\n<p>Our application leverages AR and AI to solve all that, and more. It provides step-by-step assessment, recommendation and instructions to guide you through the entire cooking experience through Hololens. \nIt starts by recognizing the ingredients in your fridge, and based on that recommends multiple recipes that are both health-conscious and customizable. \nAfter a recipe is selected, it teaches you how to cook the dish, step-by-step, with verbal instructions and interactive animations at your own pace. \nIt values flavors as much as presentation. Towards the end, it will provide interactive plating examples to make sure that your dish looks restaurant-ready. \nPlus, you will have the chance to take a picture of your completed dish and share it with your friends, family, and fellow chefs via the social network.</p>\n<p><strong>Team Info</strong>\nTeam Lead: Lei Xia\nTeam Name: Team StayReal\nTeam Location: Outside of E14-433. 4th Floor Balcony</p>\n<p><strong>Platform</strong> Microsoft Holo Lens\n<strong>Development Tools</strong> Unity\n<strong>SDKs</strong>Hololens SDK\n<strong>APIs</strong>N/A\n<strong>Asset</strong>\n<a href=\"https://www.turbosquid.com/3d-models/free-pizza-juice-3d-model/831970\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/free-pizza-juice-3d-model/831970</a>\n<a href=\"https://www.turbosquid.com/3d-models/sushi-plate-chopsticks-obj-free/520054\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/sushi-plate-chopsticks-obj-free/520054</a>\n<a href=\"https://free3d.com/3d-model/3d-fish-model-low-poly-63627.html\" rel=\"nofollow\">https://free3d.com/3d-model/3d-fish-model-low-poly-63627.html</a>\n<strong>Libraries</strong> N/A</p>\n</div>",
            "content_md": "\n## ChefAR in 1min\n\n\nWe created a learn-to-cook experience in AR that immerses a home chef fully in the cooking process. No more switching between ingredients and websites or videos (and touching your phone or tablet with dirty hands). The cooking instructions are overlaid on real food as you cook, making learning a much more hands-on and engaging experience. Our product completes the entire cooking cycle from learning what\u2019s available in the fridge, recommending recipes, to demoing step-by-step animated instructions with voice. Because our AI-enabled product handles the logistic hassles in the background, the home chef can focus on her relationship with food and fully enjoy the learning experience.\n\n\n## Problem Statement\n\n\nEver aspired to become a celebrity chef featured on Chef\u2019s Table? Well, we did - but let\u2019s face it, cooking is REALLY HARD. From picking ingredients, creating a recipe, adjusting flavors, to final plating, each step requires knowledge, focus, patience and even talent. And NO, tutorials on Youtube or BlueApron don\u2019t help much - they do not care about your grocery, schedule, or cooking pace. You may have to keep hitting the replay button with fingers covered in both grease and keyboard-harbored bacteria while trying to rescue yourself from a potential house fire... \n\n\n## Solution\n\n\nOur application leverages AR and AI to solve all that, and more. It provides step-by-step assessment, recommendation and instructions to guide you through the entire cooking experience through Hololens. \nIt starts by recognizing the ingredients in your fridge, and based on that recommends multiple recipes that are both health-conscious and customizable. \nAfter a recipe is selected, it teaches you how to cook the dish, step-by-step, with verbal instructions and interactive animations at your own pace. \nIt values flavors as much as presentation. Towards the end, it will provide interactive plating examples to make sure that your dish looks restaurant-ready. \nPlus, you will have the chance to take a picture of your completed dish and share it with your friends, family, and fellow chefs via the social network.\n\n\n**Team Info**\nTeam Lead: Lei Xia\nTeam Name: Team StayReal\nTeam Location: Outside of E14-433. 4th Floor Balcony\n\n\n**Platform** Microsoft Holo Lens\n**Development Tools** Unity\n**SDKs**Hololens SDK\n**APIs**N/A\n**Asset**\n<https://www.turbosquid.com/3d-models/free-pizza-juice-3d-model/831970>\n<https://www.turbosquid.com/3d-models/sushi-plate-chopsticks-obj-free/520054>\n<https://free3d.com/3d-model/3d-fish-model-low-poly-63627.html>\n**Libraries** N/A\n\n\n"
        },
        {
            "source": "https://devpost.com/software/archengine",
            "title": "ArchEngine",
            "blurb": "Changing the paradigm: construction documents in augmented reality",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/dAAp4ehYJIg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "tvollaro",
                    "about": "I worked on initial interaction flow, UI design, 3D modeling and annotations in Unity. I also shot the final video.",
                    "photo": "https://avatars0.githubusercontent.com/u/1156830?height=180&v=4&width=180"
                },
                {
                    "name": "Adam Chernick",
                    "about": "I was the team Lead. I brought the Idea of Augmented Construction Documents to the Hackathon and was lucky to find a great team!  I did 3D modeling, design, and over saw app functionality and team moral.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/500/datas/profile.jpg"
                },
                {
                    "name": "Cale Geffre",
                    "about": "I worked on the main interfacing between our architectural models and animation systems with the  ARKit plugin for Unity. This allowed us to view the structural, mechanical, and furnished design sublayers on in augmented reality on an iPad",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/915/datas/profile.jpeg"
                },
                {
                    "name": "Liam Tuohy",
                    "about": "I did the basic zooming, layering, and transparency. I did the call out functionality for sectioning off more detailed area. There was some blow up animations that I couldn't get to implementing, but will do for version 2.",
                    "photo": "https://media.licdn.com/dms/image/C5603AQG-pO0dYpPokA/profile-displayphoto-shrink_800_800/0?e=1577923200&height=180&t=YgX6qpDavOcq-ifrSROIfAQPfXFsugRZMBjOe8qa6h0&v=beta&width=180"
                },
                {
                    "name": "Peter Procek",
                    "about": "I worked on an initial full-scale implementation of ArchEngine on hololens to project 2D construction documents In physical 3D space. While that endeavor was scrapped at the end, I helped in general team and design guidance, refining design and project goals as well as being the source control tzar, ensuring our team managed proper software source control. ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_Pk1lNqf2cGzaRFxfJzb0KrgDtGDaYcPiJkwl-heDvqw7YFGmvtb01P6DArkPjbCY18blzX5SRqUfjPG0Vc9SyhkTYqUmj1QGMc9tB3luvzmGcLgYzQ8pcvxgOTTYN1A1rNtgr1gL-AF?height=180&width=180"
                }
            ],
            "built_with": [
                "autodesk-revit",
                "microsoft-hololens",
                "unity",
                "xcode"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>For hundreds of years, buildings have been built with 2D construction documents. Even now, with fully coordinated 3D building information models - we STILL export to 2D documents. This leads to miscommunication, delays and errors.</p>\n<h2>What it does</h2>\n<p>ArchEngine reimagines how architects will convey design intent to builders. It uses Revit, the industry standard design application, to create interactive augmented reality views that can be used to illustrate how architectural details should be built. This allows for multiple people to view a AR view of the scaled model on the iPad. It also allows for builders to view a full scale model in-place for renovation retrofits.</p>\n<h2>How we built it</h2>\n<p>We built a 3D model of the 6th floor of the Media Lab using Autodesk Revit and Maya. \nWe then exported views to FBX into Unity and then compiled for iPad Pro and HoloLens.\nUnity SDKs:  ARKit and Hololens.</p>\n<h2>What's next for ArchEngine</h2>\n<p>Choosing which parts of the model to share and loading geometry on demand from a cloud service.</p>\n<p>Team Lead: Adam Chernick  mobile: 206-330-1295\nTeam Location: 6th floor (by coffee station)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nFor hundreds of years, buildings have been built with 2D construction documents. Even now, with fully coordinated 3D building information models - we STILL export to 2D documents. This leads to miscommunication, delays and errors.\n\n\n## What it does\n\n\nArchEngine reimagines how architects will convey design intent to builders. It uses Revit, the industry standard design application, to create interactive augmented reality views that can be used to illustrate how architectural details should be built. This allows for multiple people to view a AR view of the scaled model on the iPad. It also allows for builders to view a full scale model in-place for renovation retrofits.\n\n\n## How we built it\n\n\nWe built a 3D model of the 6th floor of the Media Lab using Autodesk Revit and Maya. \nWe then exported views to FBX into Unity and then compiled for iPad Pro and HoloLens.\nUnity SDKs: ARKit and Hololens.\n\n\n## What's next for ArchEngine\n\n\nChoosing which parts of the model to share and loading geometry on demand from a cloud service.\n\n\nTeam Lead: Adam Chernick mobile: 206-330-1295\nTeam Location: 6th floor (by coffee station)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/arneighborhood",
            "title": "ARNeighborhood",
            "blurb": "ARNeighborhood is an Augmented Reality educational tool designed to encourage people to learn about their community.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Krystal Cooper",
                    "about": "Krystal Cooper - Concept/Project Lead",
                    "photo": "https://avatars.githubusercontent.com/u/11076396?height=180&v=3&width=180"
                },
                {
                    "name": "Janeen Anderson",
                    "about": "I worked on AR Kit code development.  I integrated text to speech using Watson APIs.",
                    "photo": "https://media.licdn.com/dms/image/C4D03AQGhCwpegySbJg/profile-displayphoto-shrink_100_100/0?e=1545264000&height=180&t=eVI4bCoAUYYgaMKnBDYSUav2VqkYLc1tBMPS0wRr_8c&v=beta&width=180"
                },
                {
                    "name": "Brian Contreras",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_ORAOgKfo1vryHz1-sJ5rvCaozX4jHNx0R05OZksonXhySXYPU05xrC3oK5GjSvf7lEdxvCaEs_fpwr9_qeqGvkfQ0_fgwrV0BeqjY5VwqCVAT63_RJCPOAqsx9zSGrSaOIK-JcTvsOs?height=180&width=180"
                },
                {
                    "name": "Yifan Xing",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/15064985?height=180&v=3&width=180"
                },
                {
                    "name": "Fran Gardino",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/249fba9baa873e25aa63513e8aa11f0b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "arkit",
                "ios",
                "mapbox",
                "unity"
            ],
            "content_html": "<div>\n<h2>Hackathon Requirements</h2>\n<h2>Location: E14 building, 5th floor.  Room 514B</h2>\n<h2>The development tools used to build the project</h2>\n<p>This application utilizes the iOS platform, xCode, Apple ARkit, Mapbox, IBM Watson Text Synthesis API and Unity 3D.</p>\n<h2>Any assets used in the project</h2>\n<p>Information courtesy of the Architectural Heritage Foundation and the Boston Landmarks Commission. </p>\n<p>Icons: <a href=\"https://icons8.com/license\" rel=\"nofollow\">https://icons8.com/license</a></p>\n<h2>Any components not created at the hackathon</h2>\n<p>Photographs courtesy of Fran Gardino.</p>\n<h2>What it does</h2>\n<p>ARNeighborhood (pronounced Our Neigborhood) is an Augmented Reality educational\ntool designed to encourage people to learn about the history of their communities.\nOur goal is to create a tool that augments the user's experience by giving them\nan interactive way to learn the unique stories waiting to be discovered\nright around the corner. </p>\n</div>",
            "content_md": "\n## Hackathon Requirements\n\n\n## Location: E14 building, 5th floor. Room 514B\n\n\n## The development tools used to build the project\n\n\nThis application utilizes the iOS platform, xCode, Apple ARkit, Mapbox, IBM Watson Text Synthesis API and Unity 3D.\n\n\n## Any assets used in the project\n\n\nInformation courtesy of the Architectural Heritage Foundation and the Boston Landmarks Commission. \n\n\nIcons: <https://icons8.com/license>\n\n\n## Any components not created at the hackathon\n\n\nPhotographs courtesy of Fran Gardino.\n\n\n## What it does\n\n\nARNeighborhood (pronounced Our Neigborhood) is an Augmented Reality educational\ntool designed to encourage people to learn about the history of their communities.\nOur goal is to create a tool that augments the user's experience by giving them\nan interactive way to learn the unique stories waiting to be discovered\nright around the corner. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/toys-that-make-noise",
            "title": "Toys That Make Noise",
            "blurb": "An audiovisual experience into the digital stories of physical things",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Gabriel Brasil Goncalves",
                    "about": "Brainstormed the UI and created illustrated assets. Coordinated storytelling and presentations. Best practices for immersive and augmented reality applications. Brought food and danced!",
                    "photo": "https://www.gravatar.com/avatar/dbdb4f24c89c9a62755780bee039e090?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Barak Chamo",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/2883345?height=180&v=4&width=180"
                },
                {
                    "name": "maizsakat",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/31875356?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "ableton",
                "microsoft-hololens",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We're inspired by what the new medium of Mixed Reality and what that can mean for content creation as well as the intersection between digital content and physical objects / environments. </p>\n<h2>What it does</h2>\n<p>Toys That Make Noise is a mixed reality audiovisual experience that sonifies objects in the real world and transforms them into reactive musical instruments and soundscapes.</p>\n<h2>How we built it</h2>\n<p>We used Vuforia with the Hololens and tried to get AR working with Mixed Reality. We brought in audiovisual content to activate experiences. </p>\n<h2>Challenges we ran into</h2>\n<p>It was tricky to set up our Unity project, since we're using Hololens and Vuforia, which each recommend running on different Unity versions. Furthermore, once we were set up with our project, there were kinks to works out to get the workflow going smoothly between the two programs. We also ran into a few issues with scanning 3D objects with Vuforia. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We knew that using the Hololens with Vuforia might cause some issues, as the two haven't been widely used in conjunction, but we're proud of pushing beyond the technical challenges to create media that is truly innovative. </p>\n<h2>What we learned</h2>\n<p>Technically we learned how to use Mixed Reality with physical objects and integrate multiple Unity platforms and APIs. We learned how to create meaningful Mixed Reality experiences that infuse physical surfaces with digital content.  </p>\n<h2>What's next for Toys That Make Noise</h2>\n<p>We want to continue exploring a Mixed Reality platform for a new age of digital content management. </p>\n<h2>Third Party ASSETS AND LIBRARIES</h2>\n<ul>\n<li>MS Mixed Reality ToolKit</li>\n<li>Vuforia Unity SDK</li>\n<li>Giggly Bubble Free </li>\n<li>Full Tilt Boogie Particle Effects</li>\n</ul>\n<h2>Development Tools</h2>\n<ul>\n<li>Unity 3D</li>\n<li>Cinema 4D</li>\n<li>Vuforia Target Manager</li>\n<li>Reaper</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe're inspired by what the new medium of Mixed Reality and what that can mean for content creation as well as the intersection between digital content and physical objects / environments. \n\n\n## What it does\n\n\nToys That Make Noise is a mixed reality audiovisual experience that sonifies objects in the real world and transforms them into reactive musical instruments and soundscapes.\n\n\n## How we built it\n\n\nWe used Vuforia with the Hololens and tried to get AR working with Mixed Reality. We brought in audiovisual content to activate experiences. \n\n\n## Challenges we ran into\n\n\nIt was tricky to set up our Unity project, since we're using Hololens and Vuforia, which each recommend running on different Unity versions. Furthermore, once we were set up with our project, there were kinks to works out to get the workflow going smoothly between the two programs. We also ran into a few issues with scanning 3D objects with Vuforia. \n\n\n## Accomplishments that we're proud of\n\n\nWe knew that using the Hololens with Vuforia might cause some issues, as the two haven't been widely used in conjunction, but we're proud of pushing beyond the technical challenges to create media that is truly innovative. \n\n\n## What we learned\n\n\nTechnically we learned how to use Mixed Reality with physical objects and integrate multiple Unity platforms and APIs. We learned how to create meaningful Mixed Reality experiences that infuse physical surfaces with digital content. \n\n\n## What's next for Toys That Make Noise\n\n\nWe want to continue exploring a Mixed Reality platform for a new age of digital content management. \n\n\n## Third Party ASSETS AND LIBRARIES\n\n\n* MS Mixed Reality ToolKit\n* Vuforia Unity SDK\n* Giggly Bubble Free\n* Full Tilt Boogie Particle Effects\n\n\n## Development Tools\n\n\n* Unity 3D\n* Cinema 4D\n* Vuforia Target Manager\n* Reaper\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vrso-virtual-reality-symphony-orchestra",
            "title": "VRSO (Virtual Reality Symphony Orchestra)",
            "blurb": "An Educative VR Experience for Conducting a Symphonic Orchestra and Understanding Instrumental Roles in a Symphony",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/jak_2PMaoY0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Alexandria Heston",
                    "about": "3D Modeler and Interaction Designer",
                    "photo": "https://media.licdn.com/mpr/mprx/0_PotKeB6dBpRPEkPH9aXY85kHtgxmdq3H0UeKS_oHNgSfdkhZva6ruTFHKwxPEzlMMUeKLlkeUUp7fKN5xMBu8_5kOUpafKkNMMBPwCKWzsdCLQxHzH3jI9YZYa6OQK15rV1y2rx7ZGo?height=180&width=180"
                },
                {
                    "name": "Kiran Hegde",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4781482?height=180&v=4&width=180"
                },
                {
                    "name": "Patrick Ryan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0e725f1b1a03f708821588fc4bf82858?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "ShirlySpikes",
                    "about": "",
                    "photo": "https://graph.facebook.com/10212867947467710/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "blender",
                "itween",
                "kondak",
                "leap-motion",
                "qbase",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>An Educative VR Experience for Conducting a Classical Orchestra and Understanding Instrumental Roles in a Symphony</p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<p>Development is primarily in Unity, Model creation is done in Blender, Audio composition is done in Qbase/Kondak.\nVR Equipments are Hive &amp; Leapmotion</p>\n<h2>Challenges we ran into</h2>\n<p>Leapmotion \nBiggest snag was trying to do large hand gestures, since Leapmotion is more optimized for more hand-gestures</p>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<p>A hell lot of different things</p>\n<h2>What's next for VRSO (Virtual Reality Symphony Orchestra)</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAn Educative VR Experience for Conducting a Classical Orchestra and Understanding Instrumental Roles in a Symphony\n\n\n## What it does\n\n\n## How we built it\n\n\nDevelopment is primarily in Unity, Model creation is done in Blender, Audio composition is done in Qbase/Kondak.\nVR Equipments are Hive & Leapmotion\n\n\n## Challenges we ran into\n\n\nLeapmotion \nBiggest snag was trying to do large hand gestures, since Leapmotion is more optimized for more hand-gestures\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\nA hell lot of different things\n\n\n## What's next for VRSO (Virtual Reality Symphony Orchestra)\n\n\n"
        }
    ],
    "mit-reality-hack-2020": [
        {
            "source": "https://devpost.com/software/draft360",
            "title": "draft360",
            "blurb": "A WebXR Storyboarding Design Tool",
            "awards": [
                "Best of AR",
                "Best in Tools"
            ],
            "videos": [
                "https://www.youtube.com/embed/__nJBwGFMaA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "draft360 - A 360 WebVR storyboarding tool",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/578/datas/original.jpg"
                },
                {
                    "title": "Draw in 360",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/648/datas/original.jpg"
                },
                {
                    "title": "Add 2D Assets",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/647/datas/original.jpg"
                },
                {
                    "title": "Link Scenes with Portals",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/650/datas/original.jpg"
                },
                {
                    "title": "Comment to give feedback",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/652/datas/original.jpg"
                },
                {
                    "title": "Seamlessly share across devices",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/667/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Rui Jie Wang",
                    "about": "I brought forth the initial concept and worked on the product/UI/UX design, front-end design/development, PM and graphic design assets.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/930/943/datas/profile.png"
                },
                {
                    "name": "Tejas Shroff",
                    "about": "I worked on the backend nReal Unity implementation. I ended up using the phone that is attached to the nReal as a way to take pictures and as a microphone in order to rapidly annotate 360 scenes viewed through the nReal display",
                    "photo": "https://avatars1.githubusercontent.com/u/17838397?height=180&v=4&width=180"
                },
                {
                    "name": "Stepan Shabalin",
                    "about": "I worked on the WebXR frontend (wrote most of it) and made the backend.",
                    "photo": "https://avatars2.githubusercontent.com/u/46641404?height=180&v=4&width=180"
                },
                {
                    "name": "James Seo",
                    "about": "I helped with designing the project's UI/UX. I prototyped and implemented features of the WebXR part of the project. I worked on the project homepage.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/914/691/datas/profile.jpg"
                },
                {
                    "name": "Nicholas Woodward",
                    "about": "I worked on some back end web design, as well as created our product video and helped with the conceptual design through the eyes of an animator!",
                    "photo": "https://www.gravatar.com/avatar/2b8198daf3c0df4aa79e0886844f1db8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "aframe",
                "google-cardboard",
                "material-design-lite",
                "node.js",
                "nreal",
                "three.js",
                "unity",
                "webvr",
                "webxr"
            ],
            "content_html": "<div>\n<h1>Dev Post Summary</h1>\n<h2>Inspiration</h2>\n<p>As a designer working in the XR space I found many challenges in conceptualizing XR experiences from traditional 2D UI design &amp; prototyping tools. I would either have to sketch on paper or prototype in a chaotic workflow of VR/AR, Photoshop, Blender, Unity, etc. Even with these complex techniques, it was a hassle to port over and setup projects to actually share what the experience was like with with clients and developers before committing development resources.</p>\n<p>This is why I wanted to create a 360VR Design tool that was built with webXR and accessibility in mind to help XR creators (VR filmmakers or AR environment designers, for example) draft up and communicate their vision quickly &amp; intuitively.</p>\n<h2>What it does:</h2>\n<ul>\n<li>Create your key scenes however you want: this can be a 360 template grid, 360 CGI render, 360 photo, 360 snapshot from your favourite VR creation tool - you can import 360 content into the browser!</li>\n<li>Annotate and sketch directly on the 360 photosphere &amp; add image components;</li>\n<li>Portals link to other scenes to create interactive prototypes; </li>\n<li>Share URL to phones &amp; VR headsets to preview;</li>\n<li>Iterate earlier and create something awesome!</li>\n</ul>\n<h2>How we built it:</h2>\n<p>Designed initially in Figma + Chats + Coffee. We used the glitch.io framework to quickly prototype a web app using the A-frame library &amp; Javascript. 360 VR Scenes were created in Blender3D.</p>\n<h2>Challenges we ran into:</h2>\n<ul>\n<li>Drawing and projecting to a 360 photosphere - the equirectangular projection can sometimes be confusing, and it was very hard to get the canvas to update. I had to access raw three.js internals and create a sphere geometry and a texture;</li>\n<li>Data transfer and state management;</li>\n<li>Deciding between converting a three.js code base and platform accessibility with A-frame;</li>\n<li>Adapting input controls/paradigms from desktop, touchscreen tablet, mobile phone to AR or VR headsets;</li>\n<li>Nreal at this time in writing does not support WebXR.</li>\n</ul>\n<h2>Accomplishments that we're proud of:</h2>\n<ul>\n<li>The focus of the use case and clarity of solution in problem solving;</li>\n<li>It works on Desktop, Mobile, and some VR browsers;</li>\n<li>The breadth of interactions and solutions we prototyped in trying to solve this problem of traditional 2D conceptualization workflows with new and emerging XR pipelines;</li>\n<li>We created a companion Nreal prototyping app in Unity to showcase what the next generation of WebXR design/prototype;</li>\n<li>Desktop version works really well with Surface Studio;</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Nreal does not support WebXR;</li>\n<li>Glitch has interesting security proxy;</li>\n<li>Glitch has interesting weblink based asset management;</li>\n<li>Glitch makes collaboration very simple;</li>\n<li>A-Frame's ECS model is hard to understand but usable;</li>\n<li>How A-Frame works internally;</li>\n<li>There are so many interaction paradigms for desktop, mobile, 3DOF/6DOF VR, MR, AR;</li>\n<li>Sometimes code quality can be sacrificed for quick prototyping.</li>\n</ul>\n<h2>What's next for Draft360</h2>\n<p>The Nreal prototype shows a new and exciting workflow of taking pictures of real-world sketches. It also incorporates a voice recorder for sharing thoughts and ideas. We would love to extend webXR functionality when available.</p>\n</div>",
            "content_md": "\n# Dev Post Summary\n\n\n## Inspiration\n\n\nAs a designer working in the XR space I found many challenges in conceptualizing XR experiences from traditional 2D UI design & prototyping tools. I would either have to sketch on paper or prototype in a chaotic workflow of VR/AR, Photoshop, Blender, Unity, etc. Even with these complex techniques, it was a hassle to port over and setup projects to actually share what the experience was like with with clients and developers before committing development resources.\n\n\nThis is why I wanted to create a 360VR Design tool that was built with webXR and accessibility in mind to help XR creators (VR filmmakers or AR environment designers, for example) draft up and communicate their vision quickly & intuitively.\n\n\n## What it does:\n\n\n* Create your key scenes however you want: this can be a 360 template grid, 360 CGI render, 360 photo, 360 snapshot from your favourite VR creation tool - you can import 360 content into the browser!\n* Annotate and sketch directly on the 360 photosphere & add image components;\n* Portals link to other scenes to create interactive prototypes;\n* Share URL to phones & VR headsets to preview;\n* Iterate earlier and create something awesome!\n\n\n## How we built it:\n\n\nDesigned initially in Figma + Chats + Coffee. We used the glitch.io framework to quickly prototype a web app using the A-frame library & Javascript. 360 VR Scenes were created in Blender3D.\n\n\n## Challenges we ran into:\n\n\n* Drawing and projecting to a 360 photosphere - the equirectangular projection can sometimes be confusing, and it was very hard to get the canvas to update. I had to access raw three.js internals and create a sphere geometry and a texture;\n* Data transfer and state management;\n* Deciding between converting a three.js code base and platform accessibility with A-frame;\n* Adapting input controls/paradigms from desktop, touchscreen tablet, mobile phone to AR or VR headsets;\n* Nreal at this time in writing does not support WebXR.\n\n\n## Accomplishments that we're proud of:\n\n\n* The focus of the use case and clarity of solution in problem solving;\n* It works on Desktop, Mobile, and some VR browsers;\n* The breadth of interactions and solutions we prototyped in trying to solve this problem of traditional 2D conceptualization workflows with new and emerging XR pipelines;\n* We created a companion Nreal prototyping app in Unity to showcase what the next generation of WebXR design/prototype;\n* Desktop version works really well with Surface Studio;\n\n\n## What we learned\n\n\n* Nreal does not support WebXR;\n* Glitch has interesting security proxy;\n* Glitch has interesting weblink based asset management;\n* Glitch makes collaboration very simple;\n* A-Frame's ECS model is hard to understand but usable;\n* How A-Frame works internally;\n* There are so many interaction paradigms for desktop, mobile, 3DOF/6DOF VR, MR, AR;\n* Sometimes code quality can be sacrificed for quick prototyping.\n\n\n## What's next for Draft360\n\n\nThe Nreal prototype shows a new and exciting workflow of taking pictures of real-world sketches. It also incorporates a voice recorder for sharing thoughts and ideas. We would love to extend webXR functionality when available.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/accessibility-toolkit-for-unity",
            "title": "Accessibility Toolkit For Unity",
            "blurb": "An open source package for adding accessibility features to XR games, interactive experiences, education and more.",
            "awards": [
                "Best of VR",
                "Best application to Accessibility [Sponsored by Microsoft]"
            ],
            "videos": [
                "https://www.youtube.com/embed/NWkXf7ObV5s?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Subtitle screenshot",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/230/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Mo Kakwan",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/315474?height=180&v=4&width=180"
                },
                {
                    "name": "Scott Doxey",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/6753?height=180&v=4&width=180"
                },
                {
                    "name": "luiginicastro",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/58125435?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We noticed that many AR/VR apps didn't make experiences for people who were hard of hearing or hearing impaired. AR/VR is the future - and should be accessible for all. Given that the theme of the MIT Reality Hack was \"Building a Utopia\" we wanted to follow the old adage \"the world is what you make of it\" and create a world where anyone could easily create accessible and enriched AR/VR applications.</p>\n<h2>What it does</h2>\n<p>The Accessibility Toolkit will create context aware subtitles for VR/AR projects. This will encourage developers to include subtitles for those with hearing impairments since we know that the process can take a long time. By making the subtitle creation process more efficient we can have developers make their projects more inclusive for all users. </p>\n<h2>How we built it</h2>\n<p>We built it using the Unity package manager architecture. We made two repos, one that was the public facing package that people would install and another that was a demo app that used the package.</p>\n<h2>Challenges we ran into</h2>\n<p>Spatial context aware subtitles are very difficult to position in 3d space relative to the user as it requires a lot of spatial math. This is something we plan to address after the hackathon.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>When we conducted user interviews we found that a lot of the teams here at the hack would be very interested in using our toolkit. The interviews expanded further to where we found how our work would affect Hard of Hearing and Hearing Impaired people. But really, in the end, the hard of hearing folks we spoke with taught us that Accessibility isn't about making your app more accessible but making your AR/VR experiences better.</p>\n<h2>What we learned</h2>\n<p>Subtitling isn't just for Hard of Hearing and Hearing Disabled folks! It can be used by able hearing individuals and is very useful for internationalization and experiences where you're going to a country which you're not fluent in. Subtitles are just a useful feature in general.</p>\n<h2>What's next for Accessibility Toolkit For Unity</h2>\n<p>We have a large backlog of features we would like to implement like SRT file parsing, localization, dynamic context based positioning, better support for fonts, script event blocking and cloud based text to speech.</p>\n<p>We also have plans to add additional features not specific to hearing impairment like a magnifying glass for seeing objects at a distance, and haptic feedback for tooltips on smaller objects.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe noticed that many AR/VR apps didn't make experiences for people who were hard of hearing or hearing impaired. AR/VR is the future - and should be accessible for all. Given that the theme of the MIT Reality Hack was \"Building a Utopia\" we wanted to follow the old adage \"the world is what you make of it\" and create a world where anyone could easily create accessible and enriched AR/VR applications.\n\n\n## What it does\n\n\nThe Accessibility Toolkit will create context aware subtitles for VR/AR projects. This will encourage developers to include subtitles for those with hearing impairments since we know that the process can take a long time. By making the subtitle creation process more efficient we can have developers make their projects more inclusive for all users. \n\n\n## How we built it\n\n\nWe built it using the Unity package manager architecture. We made two repos, one that was the public facing package that people would install and another that was a demo app that used the package.\n\n\n## Challenges we ran into\n\n\nSpatial context aware subtitles are very difficult to position in 3d space relative to the user as it requires a lot of spatial math. This is something we plan to address after the hackathon.\n\n\n## Accomplishments that we're proud of\n\n\nWhen we conducted user interviews we found that a lot of the teams here at the hack would be very interested in using our toolkit. The interviews expanded further to where we found how our work would affect Hard of Hearing and Hearing Impaired people. But really, in the end, the hard of hearing folks we spoke with taught us that Accessibility isn't about making your app more accessible but making your AR/VR experiences better.\n\n\n## What we learned\n\n\nSubtitling isn't just for Hard of Hearing and Hearing Disabled folks! It can be used by able hearing individuals and is very useful for internationalization and experiences where you're going to a country which you're not fluent in. Subtitles are just a useful feature in general.\n\n\n## What's next for Accessibility Toolkit For Unity\n\n\nWe have a large backlog of features we would like to implement like SRT file parsing, localization, dynamic context based positioning, better support for fonts, script event blocking and cloud based text to speech.\n\n\nWe also have plans to add additional features not specific to hearing impairment like a magnifying glass for seeing objects at a distance, and haptic feedback for tooltips on smaller objects.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/project-x0bvzw8s654m",
            "title": "#\u00b3",
            "blurb": "#\u00b3 is a tool to create and curate spatial content. Easily leave notes and media in Augmented Reality using the phone\u2019s native keyboard without taking off your wearable. ",
            "awards": [
                "Best use of Nreal Light"
            ],
            "videos": [
                "https://www.youtube.com/embed/D8KYSyvnFCU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "William Liu",
                    "about": "I worked on the Nreal Android client in Unity and developed the backend.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/342/276/datas/profile.jpg"
                },
                {
                    "name": "Colin Keenan",
                    "about": "I built sounds, wrote story, tinkered with Unity, and went on a very trying trip to the Cambridge Dunkin Donuts.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/458/datas/profile.jpg"
                },
                {
                    "name": "Kathy Wang",
                    "about": "I worked on UI design for the android app, 3D modeling and rendering, all the visual elements, hashtag contents, video editing and animations.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/027/datas/profile.jpeg"
                },
                {
                    "name": "Leon Wu",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/13173037?height=180&v=3&width=180"
                },
                {
                    "name": "JaredMonkey",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/356520?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "android",
                "azure",
                "c#",
                "figma",
                "mobile",
                "mongodb",
                "nreal",
                "python",
                "unity",
                "vectary"
            ],
            "content_html": "<div>\n<p>On Day 1 of Reality Hack, our team members found one another through our shared interest in how spatial realities relate to one another, how the display of location-persistent text and models should be prioritized relative to one another, and how to facilitate user-centered curation of the spatial media relevant to their needs.</p>\n<p>For Leon, coming to Cambridge for the hackathon brought him back to last summer and the many happy moments he shared with dancers on the MIT ballroom team. Yuting and Colin were lost between workshops sessions, wishing they could call up relevant wayfinding but anticipating that such an interface could be just as cluttered as the cork boards in the university hallways. Jared and William came to the project specifically to answer, \u201cWhat is a hashtag for spatial reality?\u201d</p>\n<p>Our team\u2019s design philosophy examines the precedent social media to clarify the utility of hashtags and user-assigned metadata channels within existing virtual spaces and forums. While user-submitted tags differ from platform to platform, they share a curation function- connecting users with channels of communication relevant to their wants and needs. From Internet Relay Chat to Twitter and beyond, hashtags have an established lineage as a stand-in for a user saying, \u201cI think this content is relevant to topic X.\u201d We immediately saw a need for equivalent functionality in virtual spaces and across digital realities. Once users are empowered to post content affixed to the underlying \u2018default reality,\u2019 the volume of content will quickly exceed the physical space\u2019s ability to comfortably accommodate this content. </p>\n<p>AR is still an emerging technology, and many of the challenges we faced came when we pushed the technology right up to its current limitations. Things we take for granted, even using a smartphone keyboard or posting a video, are radically different because we\u2019re working with tech that\u2019s still rough around the edges.</p>\n<p>We learned about how the world of atoms and the world of bits interact. You wouldn\u2019t think that a lightweight pair of glasses could be uncomfortable, but try wearing it as its embedded computer starts warming up your eyebrows. Or try wearing your regular pair of glasses behind the Nreal, pushing it closer to the tip of your nose.</p>\n<p>We learned that there\u2019s so much room to grow in this space, and we\u2019re unbelievably excited to see what the future holds!</p>\n<h2>What languages, APIs, hardware, hosts, libraries, UI Kits or frameworks are you using?</h2>\n<p>The client is built with Unity (in C#) for Nreal using the beta NRSDK. We used many of the unique functionalities of Nreal including accurate image tracking and using the phone as a controller. We also experimented with the offline mapping system currently in beta, but found that changing ambient conditions at our workspace significantly affected our anchoring system. As such, we currently base our anchoring on tracked images, but plan to switch to the SDK\u2019s offline mapping system once it becomes more robust. </p>\n<p>Nreal\u2019s choice of using an Android phone as the controller provided us with many unique opportunities to use the phone\u2019s services. As such, we were able to implement a user-friendly UI on the phone screen. In input fields, the builtin Android keyboard gives a much more fluid experience than typing experiences on other AR or VR devices, and responsive button presses on the phone screen also contribute to the experience. We also poll the phone\u2019s builtin location and compass data in order to tag objects with location data. Finally, the Android phone\u2019s ability to use mobile data fits perfectly with our vision of being able to see and place media outdoors where the user may not have wifi.</p>\n<p>The backend is written in Python (with pymongo) and is deployed on Azure as a Functions App. The backend functions as a bridge between our client and our Azure Cosmos database by providing handlers for various HTTP requests. In order to provide scalability, we created our web API following the REST guidelines. We wrote a custom RESTful client in Unity via the UnityWebRequest class to interact with the database from the client.</p>\n<p>In order to experiment with how our program could scale to VR devices, we used Aardvark, an open source framework to develop AR apps in VR. We used Figma, Rhino and Vectary to prototype and design UI elements and user experience. The video was created with Premiere Pro and After Effects.</p>\n<h2>What is your team number?</h2>\n<p>28</p>\n<h2>What is your table number?</h2>\n<p>6B-07</p>\n<h2>Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)</h2>\n<p>Best of AR</p>\n<h2>Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)</h2>\n<p>Best of VR</p>\n</div>",
            "content_md": "\nOn Day 1 of Reality Hack, our team members found one another through our shared interest in how spatial realities relate to one another, how the display of location-persistent text and models should be prioritized relative to one another, and how to facilitate user-centered curation of the spatial media relevant to their needs.\n\n\nFor Leon, coming to Cambridge for the hackathon brought him back to last summer and the many happy moments he shared with dancers on the MIT ballroom team. Yuting and Colin were lost between workshops sessions, wishing they could call up relevant wayfinding but anticipating that such an interface could be just as cluttered as the cork boards in the university hallways. Jared and William came to the project specifically to answer, \u201cWhat is a hashtag for spatial reality?\u201d\n\n\nOur team\u2019s design philosophy examines the precedent social media to clarify the utility of hashtags and user-assigned metadata channels within existing virtual spaces and forums. While user-submitted tags differ from platform to platform, they share a curation function- connecting users with channels of communication relevant to their wants and needs. From Internet Relay Chat to Twitter and beyond, hashtags have an established lineage as a stand-in for a user saying, \u201cI think this content is relevant to topic X.\u201d We immediately saw a need for equivalent functionality in virtual spaces and across digital realities. Once users are empowered to post content affixed to the underlying \u2018default reality,\u2019 the volume of content will quickly exceed the physical space\u2019s ability to comfortably accommodate this content. \n\n\nAR is still an emerging technology, and many of the challenges we faced came when we pushed the technology right up to its current limitations. Things we take for granted, even using a smartphone keyboard or posting a video, are radically different because we\u2019re working with tech that\u2019s still rough around the edges.\n\n\nWe learned about how the world of atoms and the world of bits interact. You wouldn\u2019t think that a lightweight pair of glasses could be uncomfortable, but try wearing it as its embedded computer starts warming up your eyebrows. Or try wearing your regular pair of glasses behind the Nreal, pushing it closer to the tip of your nose.\n\n\nWe learned that there\u2019s so much room to grow in this space, and we\u2019re unbelievably excited to see what the future holds!\n\n\n## What languages, APIs, hardware, hosts, libraries, UI Kits or frameworks are you using?\n\n\nThe client is built with Unity (in C#) for Nreal using the beta NRSDK. We used many of the unique functionalities of Nreal including accurate image tracking and using the phone as a controller. We also experimented with the offline mapping system currently in beta, but found that changing ambient conditions at our workspace significantly affected our anchoring system. As such, we currently base our anchoring on tracked images, but plan to switch to the SDK\u2019s offline mapping system once it becomes more robust. \n\n\nNreal\u2019s choice of using an Android phone as the controller provided us with many unique opportunities to use the phone\u2019s services. As such, we were able to implement a user-friendly UI on the phone screen. In input fields, the builtin Android keyboard gives a much more fluid experience than typing experiences on other AR or VR devices, and responsive button presses on the phone screen also contribute to the experience. We also poll the phone\u2019s builtin location and compass data in order to tag objects with location data. Finally, the Android phone\u2019s ability to use mobile data fits perfectly with our vision of being able to see and place media outdoors where the user may not have wifi.\n\n\nThe backend is written in Python (with pymongo) and is deployed on Azure as a Functions App. The backend functions as a bridge between our client and our Azure Cosmos database by providing handlers for various HTTP requests. In order to provide scalability, we created our web API following the REST guidelines. We wrote a custom RESTful client in Unity via the UnityWebRequest class to interact with the database from the client.\n\n\nIn order to experiment with how our program could scale to VR devices, we used Aardvark, an open source framework to develop AR apps in VR. We used Figma, Rhino and Vectary to prototype and design UI elements and user experience. The video was created with Premiere Pro and After Effects.\n\n\n## What is your team number?\n\n\n28\n\n\n## What is your table number?\n\n\n6B-07\n\n\n## Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)\n\n\nBest of AR\n\n\n## Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)\n\n\nBest of VR\n\n\n"
        },
        {
            "source": "https://devpost.com/software/nutrimonster",
            "title": "NutriMonster",
            "blurb": "THE FUTURE OF PACKAGING",
            "awards": [
                "Best of Vuforia [Sponsored by PTC]"
            ],
            "videos": [
                "https://player.vimeo.com/video/385862807?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "NutriMonster",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/398/datas/original.png"
                },
                {
                    "title": "Nutri-Score (view in far distance via smartphone)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/408/datas/original.png"
                },
                {
                    "title": "Nutri-Moster Data (View in close distance via smartphone)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/404/datas/original.png"
                },
                {
                    "title": "Nutri-Moster Data (View in close distance via MagicLeap/AR Glasses)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/399/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Kshitij Gajapure",
                    "about": "I helped with implementing the core vuforia functionality of detecting the markers and getting the metadata (barcode) associated with it from the cloud. Implemented the distance detection feature in Unity.",
                    "photo": "https://avatars1.githubusercontent.com/u/36204389?height=180&v=4&width=180"
                },
                {
                    "name": "Max Bredow",
                    "about": "UX research: what information are users most interested in? How and when to display it to not overload them with information?",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/900/862/datas/profile.jpeg"
                },
                {
                    "name": "Zhuyuan He",
                    "about": "I help with the storyboard and user journey, 3D modeling, graphics, interaction design ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/428/datas/profile.jpg"
                },
                {
                    "name": "Weiqi Zhao",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/514/396/datas/profile.png"
                },
                {
                    "name": "Francis Szakacs",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/329/datas/profile.png"
                }
            ],
            "built_with": [
                "andorid",
                "apis",
                "blender",
                "hardware",
                "machine-learning",
                "magicleap",
                "mobile",
                "nutriscore",
                "openfoodfacts",
                "sketch",
                "unity",
                "visual-studio",
                "vuforia"
            ],
            "content_html": "<div>\n<h1>Problem:</h1>\n<p>Physical products can only display a limited amount of information, and companies chose to display what sells - not what you need to know.</p>\n<h1>Solution:</h1>\n<p>NutriMonsters overlays physical products with an AR-filter to only display the information you really need to know, stripping away any manipulative influence of brands or claims.</p>\n<h1>Displayed information</h1>\n<p>We used the logos of the NutriScore, a food labelling system already used throughout Europe (France, Spain, UK, Belgium, Switzerland, Germany). The NutriScore is a summary of the nutritional quality of foods. Its disadvantage is: consumers don't understand <em>why</em> a product gets a good or a bad rating. By choosing how much information to display at which distance, NutriMonsters helps consumers to quickly browse but also to see more detailed information when needed.</p>\n<h1>Tech Stack:</h1>\n<p>The MVP is an app for Android and MagicLeap: the app detects images through Vuforia and pulls nutritional information through a Rest API from OpenFoodFacts, the largest open-source food database with over 1 Million food products.</p>\n<h1>Why AR?</h1>\n<p>We see image recognition as an addition to barcode reading, not a replacement; The advantage of image recognition is when comparing two products at the same time; it is more intuitive and allows to recognize a product from further away than barcodes. We built this prototype with upcoming wearable glasses in our mind.</p>\n<h1>Verticals:</h1>\n<p>This technology is also applicable for diabetics (to show glycemic index/load), people with intolerances or people concerned about unethical or unsustainable products, especially in cosmetics.</p>\n<h1>Vision:</h1>\n<p>We believe that primary packaging may be redundant in a world of deliveries. This AR shopping experience is an omnichannel experience that combines the advantages of the online world (variety) with the offline world (discovery) and sets us up for ghost supermarkets, where products wouldn't be in the store anymore but are displayed on displays.</p>\n</div>",
            "content_md": "\n# Problem:\n\n\nPhysical products can only display a limited amount of information, and companies chose to display what sells - not what you need to know.\n\n\n# Solution:\n\n\nNutriMonsters overlays physical products with an AR-filter to only display the information you really need to know, stripping away any manipulative influence of brands or claims.\n\n\n# Displayed information\n\n\nWe used the logos of the NutriScore, a food labelling system already used throughout Europe (France, Spain, UK, Belgium, Switzerland, Germany). The NutriScore is a summary of the nutritional quality of foods. Its disadvantage is: consumers don't understand *why* a product gets a good or a bad rating. By choosing how much information to display at which distance, NutriMonsters helps consumers to quickly browse but also to see more detailed information when needed.\n\n\n# Tech Stack:\n\n\nThe MVP is an app for Android and MagicLeap: the app detects images through Vuforia and pulls nutritional information through a Rest API from OpenFoodFacts, the largest open-source food database with over 1 Million food products.\n\n\n# Why AR?\n\n\nWe see image recognition as an addition to barcode reading, not a replacement; The advantage of image recognition is when comparing two products at the same time; it is more intuitive and allows to recognize a product from further away than barcodes. We built this prototype with upcoming wearable glasses in our mind.\n\n\n# Verticals:\n\n\nThis technology is also applicable for diabetics (to show glycemic index/load), people with intolerances or people concerned about unethical or unsustainable products, especially in cosmetics.\n\n\n# Vision:\n\n\nWe believe that primary packaging may be redundant in a world of deliveries. This AR shopping experience is an omnichannel experience that combines the advantages of the online world (variety) with the offline world (discovery) and sets us up for ghost supermarkets, where products wouldn't be in the store anymore but are displayed on displays.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/voice-of-one",
            "title": "Voice of One",
            "blurb": "Your voice has the power to change the world!",
            "awards": [
                "Film Festival/Storytelling[Sponsored by VRTO]",
                "Social Good [Sponsored by MIT Center for Advanced Virtuality]"
            ],
            "videos": [
                "https://www.youtube.com/embed/k_4ieKh0op0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Initial sketch for the enviroment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/280/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Shirley McPhaul",
                    "about": "Concept ideation, Narrative design, interaction ideation and documentation. ",
                    "photo": "https://graph.facebook.com/10161345079000654/picture?height=180&width=180"
                },
                {
                    "name": "samluckart",
                    "about": "Hi! My name is Sam Luck and I was the VR Artist and Animator of this project! All of the assets and environments were drawn/made during the hackathon (2 days) using Quill (Oculus).  I truly believe the efficiency and simplicity of Quill is unmatched for events like the XR Hackathon and Global Game Jam. I really enjoyed creating the world and characters for our concept also a big thank you to my awesome team!",
                    "photo": "https://graph.facebook.com/10158132278498699/picture?height=180&width=180"
                },
                {
                    "name": "Jesse Paterson",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGFZh12LSwPnQ/profile-displayphoto-shrink_100_100/0?e=1554336000&height=180&t=D8PF7oihaq0gmcDEvnCJqfUkpeBoLGyEzRmTE-pOA9g&v=beta&width=180"
                },
                {
                    "name": "Cory Robertson",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQHObewRWx4MiQ/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=gWPgR_sSBbZp0yYkP3KC74Z8fEeTP0hagHbMcNA5LW4&v=beta&width=180"
                },
                {
                    "name": "morgansmith",
                    "about": "",
                    "photo": "https://graph.facebook.com/2926896767342627/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "azure",
                "c#",
                "quill",
                "reverb",
                "speechapi",
                "steamvr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>From the beginning, we wanted to create something that would have an impact and resonate with a general audience while really taking advantage of the medium. We arrived at the idea of using our voice as a main mechanic because we wanted to do just that, give them a voice and show them they can use it. We know that standing up to injustice is a difficult thing to do, and VR is a great way to practice putting yourself in difficult situations, channeling emotions, and learning new soft skills.</p>\n<h2>What it does</h2>\n<p>As the player moves around an eerie grey landscape, they are able to use the sound of their voice to bring color back into the world and engage with the story. The narrative takes the player through several scenarios where they have to take action in order to intercede for others while a dark figure spectates . At the end of the experience the user is left to confront the shadow only to realize it is the representation of their own silence.</p>\n<h2>How I built it</h2>\n<p>The experience was built in Unity and the art was made almost entirely in VR using Quill.</p>\n<p>We knew that we wanted to incorporate elements of fantasy, interactivity, scripted poetic language, and an immersive environment. On that first night, we nailed down a basic story structure, themes we wanted to return to, and the visual aesthetics we wanted to implement throughout the experience. When we began hacking on Friday we were able to jump right into asset creation in Quill, establishing the underlying framework for the interactive speech elements, and script-writing </p>\n<h2>Challenges I ran into</h2>\n<p>The shader that made possible turning the grayscale environments into color was a true challenge to create, as was the fact that we didn't get the chance to play test before the judging period.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>The environment is a true accomplishment (all thanks to our artist Sam Luck). The fact that we were able to get the shader working and all four scenes animated in such a short amount of time is a true accomplishment.</p>\n<h2>What I learned</h2>\n<p>We learned so much from this experience! Not just about the software we used, but also about interactivity and storytelling in VR.</p>\n<h2>What's next for Voice of One</h2>\n<p>For the final version of the experience, we want to polish the interactions and do some UX testing so that we make sure our message comes through. We also want to include subtitles and an additional shader for the color blind, alongside translations in several languages (ideally sign language as well!). We'd also like to have speech recognition, so that positive words make the environment change, while negative ones will take the colors away.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nFrom the beginning, we wanted to create something that would have an impact and resonate with a general audience while really taking advantage of the medium. We arrived at the idea of using our voice as a main mechanic because we wanted to do just that, give them a voice and show them they can use it. We know that standing up to injustice is a difficult thing to do, and VR is a great way to practice putting yourself in difficult situations, channeling emotions, and learning new soft skills.\n\n\n## What it does\n\n\nAs the player moves around an eerie grey landscape, they are able to use the sound of their voice to bring color back into the world and engage with the story. The narrative takes the player through several scenarios where they have to take action in order to intercede for others while a dark figure spectates . At the end of the experience the user is left to confront the shadow only to realize it is the representation of their own silence.\n\n\n## How I built it\n\n\nThe experience was built in Unity and the art was made almost entirely in VR using Quill.\n\n\nWe knew that we wanted to incorporate elements of fantasy, interactivity, scripted poetic language, and an immersive environment. On that first night, we nailed down a basic story structure, themes we wanted to return to, and the visual aesthetics we wanted to implement throughout the experience. When we began hacking on Friday we were able to jump right into asset creation in Quill, establishing the underlying framework for the interactive speech elements, and script-writing \n\n\n## Challenges I ran into\n\n\nThe shader that made possible turning the grayscale environments into color was a true challenge to create, as was the fact that we didn't get the chance to play test before the judging period.\n\n\n## Accomplishments that I'm proud of\n\n\nThe environment is a true accomplishment (all thanks to our artist Sam Luck). The fact that we were able to get the shader working and all four scenes animated in such a short amount of time is a true accomplishment.\n\n\n## What I learned\n\n\nWe learned so much from this experience! Not just about the software we used, but also about interactivity and storytelling in VR.\n\n\n## What's next for Voice of One\n\n\nFor the final version of the experience, we want to polish the interactions and do some UX testing so that we make sure our message comes through. We also want to include subtitles and an additional shader for the color blind, alongside translations in several languages (ideally sign language as well!). We'd also like to have speech recognition, so that positive words make the environment change, while negative ones will take the colors away.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rising-tide",
            "title": "Rising Tide",
            "blurb": "Experience the future effects of climate change as Boston streets flood around you in a hurricane super-storm. ",
            "awards": [
                "The Wild West"
            ],
            "videos": [
                "https://www.youtube.com/embed/PxnAjtimAXQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Replicate MIT Building 10 Hall",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/431/datas/original.png"
                },
                {
                    "title": "Mechanical Tendon Resistance",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/432/datas/original.png"
                },
                {
                    "title": "Flood the hall and feel the water rising",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/433/datas/original.png"
                },
                {
                    "title": "Underwater Haptics with Contact CI Gloves",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/434/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Liam Broza",
                    "about": "I worked on the world scale XR meshing, code integration, and hardware/software optimization.",
                    "photo": "https://avatars3.githubusercontent.com/u/5104160?height=180&v=3&width=180"
                },
                {
                    "name": "Tom Buchanan",
                    "about": "Contact CI haptic glove integration for realistic water interactions. In doing so and bringing the sense of touch to the future of media consumption, higher levels of empathy and understanding can be achieved.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/116/datas/profile.JPG"
                },
                {
                    "name": "Grace Ma",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/54190353?height=180&v=4&width=180"
                },
                {
                    "name": "Claire Chuff",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5603AQEYQBuRlopT3Q/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=JAjQFr31Vli9R8W1toofdxgR5wEKbBPewfx2xfhfCjQ&v=beta&width=180"
                }
            ],
            "built_with": [
                "contact-ci-gloves",
                "mapbox",
                "unity",
                "varjo-xr1"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>What is the future of impact journalism? How do you activate the audience of the future?</p>\n<p>XR offers a whole new way to connect with visceral experience. </p>\n<p>Experience the future effects of climate change as Boston streets flood around you in a hurricane super-storm. </p>\n<h2>What it does</h2>\n<p>Rising Tide is an way to experience a futuristic means of consuming news and media. Our focus for the media and experience is to educate viewers on the future effects of climate change. The viewer is alerted to a news update and can view the article, inside the article they are able to experience a flooding scenario in both AR and VR to put real life context to the large scale problems threatening our planet.</p>\n<h2>How we built it</h2>\n<p>We centered the experience around the extremely rare, unique, and powerful Varjo XR1 to make a unique mixed reality experience. </p>\n<p>We added ContactCI haptic Gloves and a Tesla Suit jacket to add sensory immersion to the experience.</p>\n<p>PLEASE NOTE: WE HAD TO MAKE 2 VIDEOS BECAUSE OF EDIT ISSUES.</p>\n<p>2ND VIDEO: <a href=\"https://youtu.be/Vl69IQYsD1M?t=98\" rel=\"nofollow\">https://youtu.be/Vl69IQYsD1M?t=98</a></p>\n<p>We sought to create a massive scale mixed reality scene all the way to the horizon. To do this we used geodata from Mapbox to create a world scale storm out of the window of the MIT media lab conference center.</p>\n<h2>Challenges we ran into</h2>\n<p>Very few computers run the XR1, we had to try a dozen laptops and workstations over a day and a half to find a computer that could fully run the headset's displays, cameras, and sensors. This set us back for 1/3 of the hack.</p>\n<p>The nature of Unity's IDE makes it very hard to collaborate on engineering and design. We had to do significant rework to get all of our work into prefabs that could be integrated on and combined into a single code base and experience. </p>\n<p>The Varjo XR1 is extremely hard to optimize because it takes a very long time to load up and run. Debugger is even worse. XR1 dev speed is much slower than normal XR headsets.</p>\n<h2>Accomplishments that we are proud of</h2>\n<p>We completed all of the hardest parts of the concept we wanted build despite all of the extreme setbacks. </p>\n<p>We are going to complete the project after the time frame of the hack because the XR1 is such a unique device and we have the opportunity to create something truly novel.</p>\n<h2>What we learned</h2>\n<p>The immense challenge forced us to learn new Unity, design, and teamwork skills.</p>\n<p>We all learned the hard way how to work with alpha hardware and what our expectations should be.</p>\n<p>We all had long hours to meet our schedule and were forced to learn how to be productive without sleep.</p>\n<h2>What's next for Rising Tide</h2>\n<p>We wish to rebuild the experience work anywhere in the world using cloud data.</p>\n<p>We also want to bring Rising Tide to all major XR platforms.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWhat is the future of impact journalism? How do you activate the audience of the future?\n\n\nXR offers a whole new way to connect with visceral experience. \n\n\nExperience the future effects of climate change as Boston streets flood around you in a hurricane super-storm. \n\n\n## What it does\n\n\nRising Tide is an way to experience a futuristic means of consuming news and media. Our focus for the media and experience is to educate viewers on the future effects of climate change. The viewer is alerted to a news update and can view the article, inside the article they are able to experience a flooding scenario in both AR and VR to put real life context to the large scale problems threatening our planet.\n\n\n## How we built it\n\n\nWe centered the experience around the extremely rare, unique, and powerful Varjo XR1 to make a unique mixed reality experience. \n\n\nWe added ContactCI haptic Gloves and a Tesla Suit jacket to add sensory immersion to the experience.\n\n\nPLEASE NOTE: WE HAD TO MAKE 2 VIDEOS BECAUSE OF EDIT ISSUES.\n\n\n2ND VIDEO: <https://youtu.be/Vl69IQYsD1M?t=98>\n\n\nWe sought to create a massive scale mixed reality scene all the way to the horizon. To do this we used geodata from Mapbox to create a world scale storm out of the window of the MIT media lab conference center.\n\n\n## Challenges we ran into\n\n\nVery few computers run the XR1, we had to try a dozen laptops and workstations over a day and a half to find a computer that could fully run the headset's displays, cameras, and sensors. This set us back for 1/3 of the hack.\n\n\nThe nature of Unity's IDE makes it very hard to collaborate on engineering and design. We had to do significant rework to get all of our work into prefabs that could be integrated on and combined into a single code base and experience. \n\n\nThe Varjo XR1 is extremely hard to optimize because it takes a very long time to load up and run. Debugger is even worse. XR1 dev speed is much slower than normal XR headsets.\n\n\n## Accomplishments that we are proud of\n\n\nWe completed all of the hardest parts of the concept we wanted build despite all of the extreme setbacks. \n\n\nWe are going to complete the project after the time frame of the hack because the XR1 is such a unique device and we have the opportunity to create something truly novel.\n\n\n## What we learned\n\n\nThe immense challenge forced us to learn new Unity, design, and teamwork skills.\n\n\nWe all learned the hard way how to work with alpha hardware and what our expectations should be.\n\n\nWe all had long hours to meet our schedule and were forced to learn how to be productive without sleep.\n\n\n## What's next for Rising Tide\n\n\nWe wish to rebuild the experience work anywhere in the world using cloud data.\n\n\nWe also want to bring Rising Tide to all major XR platforms.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/talkai",
            "title": "TalkAI (Team 035)",
            "blurb": "AI-driven, AR-enabled tool that helps people overcome the fear of public speaking and develop the ability to be heard.",
            "awards": [
                "Best in Tools"
            ],
            "videos": [
                "https://www.youtube.com/embed/9g7O3tgTsTU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "sxing2015 Xing",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/17692110?height=180&v=4&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>We are very passionate about giving everyone the ability to have their voices heard! Public speaking is a fear facing many of us, and because of this fear and the general lack of resources, many talented people do not have the chance to receive credit for their work or to make their opinions known.</p>\n<p>We began by brainstorming the key painpoint facing speakers today, and arrived at the key issue of the lack of opportunities to present in front of real audience while feeling like a safe environment. Therefore, we are working to create a virtual environment, where virtual audience can react and give feedback to the speaker, so the speaker can practice speaking and improve while not feeling unsafe or worried. We created this environment on the NReal platform.</p>\n<p>In generating the reaction in real time, we use natural language processing to extract sentiment from the user's spoken content (using Valence Aware Dictionary and Sentiment Reasoner). We also used neural networks to train a model on the speech audio file (using 1600+ video clips from RAVDESS dataset). Through the two-dimensional analysis, we are able to create a model that listens to the user's spoken content and generate a reaction accordingly, which then feeds into the reaction of our virtual audience.</p>\n<p>Our team created audience reaction animations for common emotions such as engaged, happy, sad, surprised. We also added verbal feedback that correspond to each one of the emotions. So while the user wears the device and speaks, the user is able to see the virtual audience, who reacts real time.</p>\n<p>We have also done work around improvement-focused feedback areas, such as speaking speed, tonal variety, volume adjustment. We have not had enough time to engineer the front end of these features yet, but these are areas that we are looking to work on going forward!</p>\n</div>",
            "content_md": "\nWe are very passionate about giving everyone the ability to have their voices heard! Public speaking is a fear facing many of us, and because of this fear and the general lack of resources, many talented people do not have the chance to receive credit for their work or to make their opinions known.\n\n\nWe began by brainstorming the key painpoint facing speakers today, and arrived at the key issue of the lack of opportunities to present in front of real audience while feeling like a safe environment. Therefore, we are working to create a virtual environment, where virtual audience can react and give feedback to the speaker, so the speaker can practice speaking and improve while not feeling unsafe or worried. We created this environment on the NReal platform.\n\n\nIn generating the reaction in real time, we use natural language processing to extract sentiment from the user's spoken content (using Valence Aware Dictionary and Sentiment Reasoner). We also used neural networks to train a model on the speech audio file (using 1600+ video clips from RAVDESS dataset). Through the two-dimensional analysis, we are able to create a model that listens to the user's spoken content and generate a reaction accordingly, which then feeds into the reaction of our virtual audience.\n\n\nOur team created audience reaction animations for common emotions such as engaged, happy, sad, surprised. We also added verbal feedback that correspond to each one of the emotions. So while the user wears the device and speaks, the user is able to see the virtual audience, who reacts real time.\n\n\nWe have also done work around improvement-focused feedback areas, such as speaking speed, tonal variety, volume adjustment. We have not had enough time to engineer the front end of these features yet, but these are areas that we are looking to work on going forward!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vsibl",
            "title": "VSiBL",
            "blurb": "A social augmented reality (AR) platform for spatial communication.",
            "awards": [
                "Best Application of Data Visualization",
                "Everyday XR"
            ],
            "videos": [
                "https://www.youtube.com/embed/vroRSxnLMpw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Emily Shoemaker",
                    "about": "For this project I worked on experience design and branding as well as did a wide variety of auxiliary support tasks (write-ups, video, logo, market research, etc.).",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/343/datas/profile.PNG"
                },
                {
                    "name": "Yogi Rana",
                    "about": "I contributed to user interface & user experience design (via storyboards and user flow charts), planning & strategy of the development of product feature & functionality, and helping manage the delivery of the MVP. \nAdditionally I also helped create the messaging of major concepts of the project, for the video and for our final pitch to the judges.",
                    "photo": "https://www.gravatar.com/avatar/59da371f04ae6de50c248be54f9b65ea?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ryan Kopinsky",
                    "about": "For this project, I worked on the development of the multi-user spatial mesh network. Specifically, on establishing a shared reference frame among users and transmitting data through a decentralized, mesh network.",
                    "photo": "https://www.gravatar.com/avatar/09a7741b353b893d0f013a8588915a2d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Alex Casella",
                    "about": "For this project, I worked on defining our data model, development of our cloud realtime database, and connecting the database to the frontend AR. I also worked on adding the consent and incognito configuration to the solution. In addition, I worked on business strategy and design.  ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/105/729/datas/profile.png"
                }
            ],
            "built_with": [
                "arkit",
                "c#",
                "ios",
                "microsoft-hololens",
                "unity",
                "xcode"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The team formation process at this hackathon posed a challenge for us. We needed to form groups of individuals that were interested in the same topic/type of project while at the same time bringing an appropriate diversity of skills to the table. Our group found four members but got stuck when looking for a fifth to fill the \"designer/artist\" role. Amidst the chaos of the crowd, it was almost impossible to locate people with this set of skills so that we could pitch them our original idea (which was related to AR overlays for clothing). We pivoted on the spot and decided to work on this specific problem: the difficulty of live team formation at a hackathon. The more work we did to flesh out our concept, however, the more we realized the wide variety of use cases that could be addressed with similar technology. Thus, the VSiBL platform project was born.</p>\n<h2>What it does</h2>\n<p><strong>Overview</strong></p>\n<p>VSiBL makes it possible for people in the same physical space to discover and connect with one another by using AR to reveal contextual information that is anchored to the users\u2019 personal devices. Using a multi-user spatial mesh network to establish a shared reference frame, VSiBL users join an AR session overlaying their current environment. In the session, participants can visually identify each other\u2019s location as well as whatever information they have chosen to express about themselves.</p>\n<p><strong>Use Cases</strong></p>\n<p>This tool is extremely useful in situations where traditional methods of communication (such as speaking) are not viable. There are many such occasions: in the hackathon team formation example, it is difficult to be heard when yelling over a crowded room. In another scenario, a hearing-impaired individual might be attending a social mixer and wish to visually identify who else in the room can speak sign language and where those people are standing. Other people at the same social mixer may want to visually identify who is single and ready to mingle. Finally, teachers may wish to scan their classroom before beginning a lecture and identify where students with certain learning disorders are sitting so that they can adjust their pedagogy accordingly during the lesson. This is just a small sample of the possible use cases for VSiBL.</p>\n<p><strong>Consent</strong></p>\n<p>Consent was one of the primary topics of discussion for our team during this hackathon. VSiBL makes your personal information visible and localized to your position in an environment, so there may be scenarios in which users engage in predatory behavior. We would like to emphasize that VSiBL will only display information that users have knowingly and voluntarily chosen to share with others. Future versions of the platform will (1) provide extensive privacy controls and (2) have UI features that clearly indicate to users what information about them is visible to others at all times.</p>\n<h2>How we built it</h2>\n<p>We aimed to build a shared experience that would allow hackathon participants to view the roles of other participants via data anchored to them, signified by an AR marker and the title of the role (colors varied based on the user role).</p>\n<p>We leveraged the functionality of Apple ARKit to synchronize multiple devices to a single multi-user session by leveraging spatial mapping to create a point cloud within the environment and enabling a shared reference frame. </p>\n<p>Once this shared reference frame is established, new devices/participants would join a mesh network where users can use their device cameras to visualize information anchored above each others\u2019 physical location in that space (the information would be tied to a unique identifier associated with every unique user).</p>\n<p>Simultaneously, we created a real-time database hosted on a Firebase Cloud server. This database can add/remove new users and assign information, such as Name, Occupation, and other fields. </p>\n<p>We enabled each user to be able to input information that would be dynamically stored and retrieved from this real-time database, then display this information in AR. This would also enable users to perform actions like queries to filter the information displayed in their view, enabling only the participants with the information that met the criteria of the query to be visible (while \u201cturning off\u201d those that did not meet the criteria).</p>\n<h2>Challenges we ran into</h2>\n<p>We wanted to display data and 3D assets on top of the user\u2019s head. However, in the current prototype, we exclusively used mobile devices. This presented a challenge given that we only had the pose of the mobile device and not the user\u2019s head. To overcome this challenge, we created a head pose estimation to properly display data and content on top of the user. More work is needed to make this more robust for a diverse audience.</p>\n<p>Once a common reference frame is established, data is communicated over the decentralized mesh network. This is both a strength and a weakness. When a user gets kicked from an active session, information is lost. To maintain persistence across sessions, a cloud real-time database was used to keep data synced and backed up. The cloud database essentially served as our source of truth.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Most AR applications allow you to add 3D content anchored to a space or apply face filters. However, to our knowledge, anchoring content to a person that can move through a space is a novel concept. VSiBL gives information a spatial context. We are really proud of this and cannot wait to continue development of our platform to support XR creators to create the future of personal, spatial communication and expression.</p>\n<p>VSiBL will enable a new generation of social and spatial applications across a wide variety of use cases such as social, education, accessibility, entertainment and much more. We look forward to supporting XR creators to build the future with the VSiBL platform.</p>\n<h2>What we learned</h2>\n<p>We gained a fundamental understanding of the underlying technology that supports frameworks such as ARKit and RealityKit. We learned that each AR-enabled device builds a spatial map to understand the surrounding environment. To create a multi-user experience, devices should have a common understanding of the environment and a shared reference frame. Synchronizing devices in new environments can be challenging, so this was certainly a great learning experience. Furthermore, we also learned that device synchronization can be greatly improved through pre-mapping environments. This will be implemented in future iterations of the VSiBL platform. </p>\n<h2>What's next for VSiBL</h2>\n<p>We are so excited by all the possible use cases for VSiBL and plan to build a much more robust platform to support them.</p>\n<p><strong>Filtering</strong></p>\n<p>Future users of VSiBL will able to filter through the markers that appear in the AR overlay, making it easier to find the people they are looking for. In the hackathon team formation scenario, markers might be different colors and/or feature different icons to identify each person as a Developer, Designer, or Specialist. If I was trying to find a Designer, I could use a checklist on the key/legend for those markers to hide everyone that did not identify themselves as a Designer.</p>\n<p><strong>Privacy Features</strong></p>\n<p>Through a combination of UI information bars, public and private rooms, and functions such as \u201cincognito mode,\u201d users will always know exactly what others can see about them and may choose to stop sharing their information at any time. For example, while all information might be visible to everyone in a \"public\" VSiBL environment, users can also create \"private\" environments in which participants' data is only visible to certain people.</p>\n<p><strong>Proximity-Based Information</strong></p>\n<p>We imagine use cases such as team formation and job fairs where users are scanning the room and finding people they want to connect with, then approaching them to initiate a conversation. As they get closer to others, more information would become available in the AR layer.</p>\n<p><strong>Integrations</strong></p>\n<p>The future VSiBL platform will integrate with other platforms that have public APIs so that users could easily make more types of information about themselves available, should they be in a context where sharing it is relevant.</p>\n<p><strong>Headsets</strong></p>\n<p>In this hackathon, we built an app for iOS. Next, we will build for Hololens in anticipation of a future where more people are wearing headsets on a daily basis.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe team formation process at this hackathon posed a challenge for us. We needed to form groups of individuals that were interested in the same topic/type of project while at the same time bringing an appropriate diversity of skills to the table. Our group found four members but got stuck when looking for a fifth to fill the \"designer/artist\" role. Amidst the chaos of the crowd, it was almost impossible to locate people with this set of skills so that we could pitch them our original idea (which was related to AR overlays for clothing). We pivoted on the spot and decided to work on this specific problem: the difficulty of live team formation at a hackathon. The more work we did to flesh out our concept, however, the more we realized the wide variety of use cases that could be addressed with similar technology. Thus, the VSiBL platform project was born.\n\n\n## What it does\n\n\n**Overview**\n\n\nVSiBL makes it possible for people in the same physical space to discover and connect with one another by using AR to reveal contextual information that is anchored to the users\u2019 personal devices. Using a multi-user spatial mesh network to establish a shared reference frame, VSiBL users join an AR session overlaying their current environment. In the session, participants can visually identify each other\u2019s location as well as whatever information they have chosen to express about themselves.\n\n\n**Use Cases**\n\n\nThis tool is extremely useful in situations where traditional methods of communication (such as speaking) are not viable. There are many such occasions: in the hackathon team formation example, it is difficult to be heard when yelling over a crowded room. In another scenario, a hearing-impaired individual might be attending a social mixer and wish to visually identify who else in the room can speak sign language and where those people are standing. Other people at the same social mixer may want to visually identify who is single and ready to mingle. Finally, teachers may wish to scan their classroom before beginning a lecture and identify where students with certain learning disorders are sitting so that they can adjust their pedagogy accordingly during the lesson. This is just a small sample of the possible use cases for VSiBL.\n\n\n**Consent**\n\n\nConsent was one of the primary topics of discussion for our team during this hackathon. VSiBL makes your personal information visible and localized to your position in an environment, so there may be scenarios in which users engage in predatory behavior. We would like to emphasize that VSiBL will only display information that users have knowingly and voluntarily chosen to share with others. Future versions of the platform will (1) provide extensive privacy controls and (2) have UI features that clearly indicate to users what information about them is visible to others at all times.\n\n\n## How we built it\n\n\nWe aimed to build a shared experience that would allow hackathon participants to view the roles of other participants via data anchored to them, signified by an AR marker and the title of the role (colors varied based on the user role).\n\n\nWe leveraged the functionality of Apple ARKit to synchronize multiple devices to a single multi-user session by leveraging spatial mapping to create a point cloud within the environment and enabling a shared reference frame. \n\n\nOnce this shared reference frame is established, new devices/participants would join a mesh network where users can use their device cameras to visualize information anchored above each others\u2019 physical location in that space (the information would be tied to a unique identifier associated with every unique user).\n\n\nSimultaneously, we created a real-time database hosted on a Firebase Cloud server. This database can add/remove new users and assign information, such as Name, Occupation, and other fields. \n\n\nWe enabled each user to be able to input information that would be dynamically stored and retrieved from this real-time database, then display this information in AR. This would also enable users to perform actions like queries to filter the information displayed in their view, enabling only the participants with the information that met the criteria of the query to be visible (while \u201cturning off\u201d those that did not meet the criteria).\n\n\n## Challenges we ran into\n\n\nWe wanted to display data and 3D assets on top of the user\u2019s head. However, in the current prototype, we exclusively used mobile devices. This presented a challenge given that we only had the pose of the mobile device and not the user\u2019s head. To overcome this challenge, we created a head pose estimation to properly display data and content on top of the user. More work is needed to make this more robust for a diverse audience.\n\n\nOnce a common reference frame is established, data is communicated over the decentralized mesh network. This is both a strength and a weakness. When a user gets kicked from an active session, information is lost. To maintain persistence across sessions, a cloud real-time database was used to keep data synced and backed up. The cloud database essentially served as our source of truth.\n\n\n## Accomplishments that we're proud of\n\n\nMost AR applications allow you to add 3D content anchored to a space or apply face filters. However, to our knowledge, anchoring content to a person that can move through a space is a novel concept. VSiBL gives information a spatial context. We are really proud of this and cannot wait to continue development of our platform to support XR creators to create the future of personal, spatial communication and expression.\n\n\nVSiBL will enable a new generation of social and spatial applications across a wide variety of use cases such as social, education, accessibility, entertainment and much more. We look forward to supporting XR creators to build the future with the VSiBL platform.\n\n\n## What we learned\n\n\nWe gained a fundamental understanding of the underlying technology that supports frameworks such as ARKit and RealityKit. We learned that each AR-enabled device builds a spatial map to understand the surrounding environment. To create a multi-user experience, devices should have a common understanding of the environment and a shared reference frame. Synchronizing devices in new environments can be challenging, so this was certainly a great learning experience. Furthermore, we also learned that device synchronization can be greatly improved through pre-mapping environments. This will be implemented in future iterations of the VSiBL platform. \n\n\n## What's next for VSiBL\n\n\nWe are so excited by all the possible use cases for VSiBL and plan to build a much more robust platform to support them.\n\n\n**Filtering**\n\n\nFuture users of VSiBL will able to filter through the markers that appear in the AR overlay, making it easier to find the people they are looking for. In the hackathon team formation scenario, markers might be different colors and/or feature different icons to identify each person as a Developer, Designer, or Specialist. If I was trying to find a Designer, I could use a checklist on the key/legend for those markers to hide everyone that did not identify themselves as a Designer.\n\n\n**Privacy Features**\n\n\nThrough a combination of UI information bars, public and private rooms, and functions such as \u201cincognito mode,\u201d users will always know exactly what others can see about them and may choose to stop sharing their information at any time. For example, while all information might be visible to everyone in a \"public\" VSiBL environment, users can also create \"private\" environments in which participants' data is only visible to certain people.\n\n\n**Proximity-Based Information**\n\n\nWe imagine use cases such as team formation and job fairs where users are scanning the room and finding people they want to connect with, then approaching them to initiate a conversation. As they get closer to others, more information would become available in the AR layer.\n\n\n**Integrations**\n\n\nThe future VSiBL platform will integrate with other platforms that have public APIs so that users could easily make more types of information about themselves available, should they be in a context where sharing it is relevant.\n\n\n**Headsets**\n\n\nIn this hackathon, we built an app for iOS. Next, we will build for Hololens in anticipation of a future where more people are wearing headsets on a daily basis.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/spell-bound",
            "title": "Spell Bound",
            "blurb": "Spell Bound is a VR learning game to help children with dyslexia and dysgraphia learn letter formation and word recognition.",
            "awards": [
                "Learning, Education, and Research [Sponsored by HP]",
                "Health and Wellness + Medical"
            ],
            "videos": [
                "https://www.youtube.com/embed/elxW5wplnX0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Welcome to Spell Bound!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/331/datas/original.png"
                },
                {
                    "title": "The wizard guides the child through the game with audio prompts",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/330/datas/original.png"
                },
                {
                    "title": "At first it&#39;s easy to write the letters.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/332/datas/original.png"
                },
                {
                    "title": "The game system detects the accuracy of the writing",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/334/datas/original.png"
                },
                {
                    "title": "When the letter is written correctly, the game gets smaller to make the game harder.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/335/datas/original.png"
                },
                {
                    "title": "After the child writes the letter at the hardest difficulty, they read the word out loud to cast the spell.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/333/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Taylor Gilbert",
                    "about": "Designed the game structure and interactions, designed the UI for the dashboard, and wrote the narration script.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/529/datas/profile.jpg"
                },
                {
                    "name": "Mike McCready",
                    "about": "I helped in project planning, built the scene layout, implemented the spatial audio and core VR interactions.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/440/datas/profile.jpg"
                },
                {
                    "name": "Kai Curtis",
                    "about": "I handled database setup, implemented data upload from the app, and developed the web-based dashboard. I'm looking forward to remembering how to sleep.",
                    "photo": "https://avatars3.githubusercontent.com/u/21962388?height=180&v=4&width=180"
                },
                {
                    "name": "April Speight",
                    "about": "Project manager, sourcing game assets and creating the script for speech recognition.",
                    "photo": "https://avatars1.githubusercontent.com/u/34195866?height=180&v=4&width=180"
                },
                {
                    "name": "Logan Smith",
                    "about": "Gameplay logic, engineering, and integration.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/465/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "firebase",
                "oculus",
                "react",
                "recharts",
                "steam-sdk",
                "unity",
                "vr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The team formed with the goal of breaking down communication barriers between people through a tool is uses the strengths of VR. Children with dyslexia and dysgraphia often have frustrating experiences learning fundamental writing and reading skills, so we used the immersive nature of VR to engage their attention with an entertaining game while they learn. </p>\n<h2>What it does</h2>\n<p>Spell Bound is a VR learning game to help children with dyslexia and dysgraphia learn letter formation and word recognition. An animated wizard guides them through the process of writing letters with a wand to complete magic words and then reading the word out loud to cast the spell.</p>\n<p>The app is intended to be used only under the supervision of a qualified professional such as an occupational therapist or special-ed teacher. These professionals use play activities for instruction and therapy, so we extended that methodology into an immersive, measurable learning tool.</p>\n<p>The immersive nature of VR and the magical story elements keeps the child\u2019s attention engaged while completing tasks that would otherwise frustrate them. Letter formation is most often taught in conjunction with handwriting, which is especially difficult for children with dyslexia/dysgraphia because of the fine motor skills handwriting requires. VR controllers rely primarily on arm movement for the interactions we designed, which allows the child to focus on letter formation separately from fine motor skills.</p>\n<p>Performance is collected from gameplay into a dashboard which provides an assessment of the child\u2019s development in improving their reading and writing skills. This information is used by the child\u2019s instructor or reading/speech therapist to identify areas of improvement and determine the next course of action for instruction.</p>\n<h2>How I built it</h2>\n<p>We imported a wizard library into Unity for the environment and included an animated speaking mage who provides instruction to help the child get started with the game. At the start of gameplay, we provide a word with a missing letter. The mage communicates through an audio asset component which plays various .mp3 audio files depending on where the child is in their gameplay.</p>\n<p>We needed to be able to detect the accuracy of the writing as well as provide instantaneous visual feedback, which is important for improving learning. When the want interacts with the letter outline spheres fade in to give a visual indicator. The score is measured by the distance from the center of the line.</p>\n<p>We implemented hints throughout game plan depending on whether the child drew the requested letter correctly. We established a threshold to account for if the letter created with the wand is within a specified boundary. If the child draws a letter outside the boundary provided for the letter, they\u2019re both given a hint (i.e. a brief look at the full letter or a trace outline) and asked to try again.</p>\n<p>As the child correctly draws the letter, the size of the boundary decreases and the process of drawing the respective letter loops until the child draws the letter correctly. For the final level of gameplay, we used a method to invoke keywordRecognizer which listens for a keyword (i.e the full word that the child spells) and in return plays an audio asset which indicates whether the verbal pronunciation is correct or incorrect.</p>\n<p>At the end of each session of gameplay, the results are sent to a dashboard for review by the therapist or teacher. The dashboard displays data and visualizations for the user\u2019s proficiency per letter, average proficiency per session, and average proficiency over time. This information helps the evaluator to identify areas of struggle and track improvement.</p>\n<h2>Challenges I ran into</h2>\n<p>The biggest challenge was integrating the various methods we used such as collision detection to confirm the letters are drawn correct, speech recognition to check if the word is read correctly, audio narration to explain the game to the child, and many effects to keep the game engaging.</p>\n<p>A surprising challenge with the writing was the wide variety of potential states such as the size of the letters, hints, and the various ways for the user pass and fail each stage. There were a lot of ways the game could react to the different states, so there were many unexpected outcomes along the way.</p>\n<p>For the speech component, we realized that KeywordRecognizer makes it difficult to use conditional statements. Initially, we wanted to provide logic such as: if the user doesn\u2019t say the keyword, then play \u201cx\u201d audio file. To overcome this, we used WaitForSeconds(5) to wait 5 seconds to check whether the user says the keyword. If the user didn\u2019t say the keyword, then we played the \u2018incorrect response\u2019 audio asset. However, if the user said the correct keyword, then StopCoroutine(coroutine); stops the KeywordRecognizer to ensure that only the \u2018correct response\u2019 audio asset plays.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We dedicated a significant amount of time in the beginning towards planning which made a huge difference once we opened our laptops and began building! Although we had a plan in place, we were pretty agile in our process and were able to descope on the fly if it was determined that implementing a feature would be a drain on our resources (i.e. each other) and our time.</p>\n<h2>What I learned</h2>\n<p>There are various ways to get to the same solution - it just sometimes requires a bit of creative thinking! We iterated through 4 different methods of implementing speech recognition before settling on the solution we chose for this project. Also, since we are trying to do something we normally do in 2D software in a 3D environment. It\u2019s not something people are often familiar with, so you can\u2019t assume existing knowledge of interaction methods.</p>\n<h2>What's next for Spell Bound</h2>\n<p>Implementing additional challenges within the gameplay would be a great addition to Spell Bound. There are other dyslexia activities that could be translated well into a VR experience and it would be awesome to build those activities into the game.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe team formed with the goal of breaking down communication barriers between people through a tool is uses the strengths of VR. Children with dyslexia and dysgraphia often have frustrating experiences learning fundamental writing and reading skills, so we used the immersive nature of VR to engage their attention with an entertaining game while they learn. \n\n\n## What it does\n\n\nSpell Bound is a VR learning game to help children with dyslexia and dysgraphia learn letter formation and word recognition. An animated wizard guides them through the process of writing letters with a wand to complete magic words and then reading the word out loud to cast the spell.\n\n\nThe app is intended to be used only under the supervision of a qualified professional such as an occupational therapist or special-ed teacher. These professionals use play activities for instruction and therapy, so we extended that methodology into an immersive, measurable learning tool.\n\n\nThe immersive nature of VR and the magical story elements keeps the child\u2019s attention engaged while completing tasks that would otherwise frustrate them. Letter formation is most often taught in conjunction with handwriting, which is especially difficult for children with dyslexia/dysgraphia because of the fine motor skills handwriting requires. VR controllers rely primarily on arm movement for the interactions we designed, which allows the child to focus on letter formation separately from fine motor skills.\n\n\nPerformance is collected from gameplay into a dashboard which provides an assessment of the child\u2019s development in improving their reading and writing skills. This information is used by the child\u2019s instructor or reading/speech therapist to identify areas of improvement and determine the next course of action for instruction.\n\n\n## How I built it\n\n\nWe imported a wizard library into Unity for the environment and included an animated speaking mage who provides instruction to help the child get started with the game. At the start of gameplay, we provide a word with a missing letter. The mage communicates through an audio asset component which plays various .mp3 audio files depending on where the child is in their gameplay.\n\n\nWe needed to be able to detect the accuracy of the writing as well as provide instantaneous visual feedback, which is important for improving learning. When the want interacts with the letter outline spheres fade in to give a visual indicator. The score is measured by the distance from the center of the line.\n\n\nWe implemented hints throughout game plan depending on whether the child drew the requested letter correctly. We established a threshold to account for if the letter created with the wand is within a specified boundary. If the child draws a letter outside the boundary provided for the letter, they\u2019re both given a hint (i.e. a brief look at the full letter or a trace outline) and asked to try again.\n\n\nAs the child correctly draws the letter, the size of the boundary decreases and the process of drawing the respective letter loops until the child draws the letter correctly. For the final level of gameplay, we used a method to invoke keywordRecognizer which listens for a keyword (i.e the full word that the child spells) and in return plays an audio asset which indicates whether the verbal pronunciation is correct or incorrect.\n\n\nAt the end of each session of gameplay, the results are sent to a dashboard for review by the therapist or teacher. The dashboard displays data and visualizations for the user\u2019s proficiency per letter, average proficiency per session, and average proficiency over time. This information helps the evaluator to identify areas of struggle and track improvement.\n\n\n## Challenges I ran into\n\n\nThe biggest challenge was integrating the various methods we used such as collision detection to confirm the letters are drawn correct, speech recognition to check if the word is read correctly, audio narration to explain the game to the child, and many effects to keep the game engaging.\n\n\nA surprising challenge with the writing was the wide variety of potential states such as the size of the letters, hints, and the various ways for the user pass and fail each stage. There were a lot of ways the game could react to the different states, so there were many unexpected outcomes along the way.\n\n\nFor the speech component, we realized that KeywordRecognizer makes it difficult to use conditional statements. Initially, we wanted to provide logic such as: if the user doesn\u2019t say the keyword, then play \u201cx\u201d audio file. To overcome this, we used WaitForSeconds(5) to wait 5 seconds to check whether the user says the keyword. If the user didn\u2019t say the keyword, then we played the \u2018incorrect response\u2019 audio asset. However, if the user said the correct keyword, then StopCoroutine(coroutine); stops the KeywordRecognizer to ensure that only the \u2018correct response\u2019 audio asset plays.\n\n\n## Accomplishments that I'm proud of\n\n\nWe dedicated a significant amount of time in the beginning towards planning which made a huge difference once we opened our laptops and began building! Although we had a plan in place, we were pretty agile in our process and were able to descope on the fly if it was determined that implementing a feature would be a drain on our resources (i.e. each other) and our time.\n\n\n## What I learned\n\n\nThere are various ways to get to the same solution - it just sometimes requires a bit of creative thinking! We iterated through 4 different methods of implementing speech recognition before settling on the solution we chose for this project. Also, since we are trying to do something we normally do in 2D software in a 3D environment. It\u2019s not something people are often familiar with, so you can\u2019t assume existing knowledge of interaction methods.\n\n\n## What's next for Spell Bound\n\n\nImplementing additional challenges within the gameplay would be a great addition to Spell Bound. There are other dyslexia activities that could be translated well into a VR experience and it would be awesome to build those activities into the game.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ghosbustxr",
            "title": "GhostBustXR",
            "blurb": "We aim to break the barriers in between the VR and the physical world. Social gatherings won't be the same anymore in this cross platform multiplayer game in ghosts vs. human.  ",
            "awards": [
                "Most Fun!"
            ],
            "videos": [
                "https://www.youtube.com/embed/aUwOWMqDkcQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Description of GhostBustXR!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/918/274/datas/original.png"
                },
                {
                    "title": "We&#39;re compatible with a variety of XR devices to allow everyone to be able to play together.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/918/272/datas/original.png"
                },
                {
                    "title": "A wider view of what you see from the previous graphic",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/910/datas/original.png"
                },
                {
                    "title": "What you see in AR from a mobile phone. You have a little ghost avatar and can control it.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/414/datas/original.png"
                },
                {
                    "title": "What you see in AR when they is another AR/mobile player in the game",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/413/datas/original.jpg"
                },
                {
                    "title": "A view of you and your VR friend in AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/918/275/datas/original.jpg"
                },
                {
                    "title": "A view of your AR friend in VR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/918/273/datas/original.jpg"
                },
                {
                    "title": "Setting an anchor in AR at the beginning of the game to set your playspace",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/415/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Sam De Lara",
                    "about": "3D Artist",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/919/240/datas/profile.png"
                },
                {
                    "name": "F O",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/980/054/datas/profile.jpg"
                },
                {
                    "name": "Chris Papenfu\u00df",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/517/947/datas/profile.png"
                },
                {
                    "name": "Jason Evans",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/417/132/datas/profile.png"
                },
                {
                    "name": "Josh Widdicombe",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9230b3fb059967ce65a8b1a0ffb50164?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "android",
                "azure",
                "blender",
                "htc",
                "ios",
                "microsoft-hololens",
                "mrtk",
                "oculus-quest",
                "photon",
                "substance",
                "unity",
                "valve"
            ],
            "content_html": "<div>\n<h2>The Videos</h2>\n<ul>\n<li>Intro: https://youtu.be/aUwOWMqDkcQ</li>\n<li>Demo: https://youtu.be/4L08MydQf98</li>\n<li>Interview with the developer: https://youtu.be/1mH2JYmEAC0</li>\n<li>Interview with the designer: https://youtu.be/4obnM-JDYF4</li>\n</ul>\n<h2>Intro</h2>\n<p>GhostBustXR is an asymmetrical multiplayer social XR game that creates friendly competition between one human and 3 ghosts. The trick is they are all controlled in real time by people either on their phones or on their headset. It\u2019s a fun spooky game for everyone to play!</p>\n<h2>Inspiration</h2>\n<p>Most of the people using VR these days just have one device so when they invite friends to play they have to play by turns and wait. The result is that VR/MR players are often isolated in their play experience. If anything, there can be a screen that other spectators can view what the player sees. Now, they can be active participants and even be a core mechanic of game play.</p>\n<h2>What it does</h2>\n<p>GhostBustXR is a multiplayer game where end-users can be players, spectators, or ghosts depending on the type of device (XR headsets or mobile phones) they use to connect to the game.\nIt is multi-platform compatible. You are able to have different experiences depending on if you are accessing the game from a mobile device or XR headset. This increases replay ability among friends. </p>\n<h2>Gameplay</h2>\n<p>As a mobile phone general AR player, you will be able to control a ghost avatar within your AR scene. As a VR player, you will be in the scene armed with a flashlight. As a ghost, you and your other ghost friends need to run into the VR player(s) 3 times to ensure a victory. If you are a VR player, you need to make sure you survive the onslaught of ghosts for 1 minute, repelling any ghosts you see with your flashlight.  </p>\n<h2>How we built it</h2>\n<p>Team work, brainstorming, creativity and a lot of good vibes. (Also through Unity, Photon, and spatial anchors.)</p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Spatial anchors seems to not work well with Open VR in one Project</li>\n<li>Mixing a lot of frameworks</li>\n<li>Transferring positions with different scales</li>\n<li>Rigging problems with 3D assets</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<p>Multiplayer gaming- we were able to combine both game experiences and make a very engaging game.</p>\n<h2>What we learned</h2>\n<ul>\n<li>How to handle spatial anchors</li>\n<li>Working with multiple target devices</li>\n<li>Better rigging in Blender</li>\n<li>Create multiplayer applications with Photon</li>\n</ul>\n<h2>What's next for GhostBustXR</h2>\n<p>We have ideas for creating different levels of the same game and for extending the game using different set ups like island and monkeys, boats and sharks, etc. We also have several power-up ideas for the current level too, such as a vacuum cleaner item for the human, a combined ghost superpower, etc. </p>\n</div>",
            "content_md": "\n## The Videos\n\n\n* Intro: https://youtu.be/aUwOWMqDkcQ\n* Demo: https://youtu.be/4L08MydQf98\n* Interview with the developer: https://youtu.be/1mH2JYmEAC0\n* Interview with the designer: https://youtu.be/4obnM-JDYF4\n\n\n## Intro\n\n\nGhostBustXR is an asymmetrical multiplayer social XR game that creates friendly competition between one human and 3 ghosts. The trick is they are all controlled in real time by people either on their phones or on their headset. It\u2019s a fun spooky game for everyone to play!\n\n\n## Inspiration\n\n\nMost of the people using VR these days just have one device so when they invite friends to play they have to play by turns and wait. The result is that VR/MR players are often isolated in their play experience. If anything, there can be a screen that other spectators can view what the player sees. Now, they can be active participants and even be a core mechanic of game play.\n\n\n## What it does\n\n\nGhostBustXR is a multiplayer game where end-users can be players, spectators, or ghosts depending on the type of device (XR headsets or mobile phones) they use to connect to the game.\nIt is multi-platform compatible. You are able to have different experiences depending on if you are accessing the game from a mobile device or XR headset. This increases replay ability among friends. \n\n\n## Gameplay\n\n\nAs a mobile phone general AR player, you will be able to control a ghost avatar within your AR scene. As a VR player, you will be in the scene armed with a flashlight. As a ghost, you and your other ghost friends need to run into the VR player(s) 3 times to ensure a victory. If you are a VR player, you need to make sure you survive the onslaught of ghosts for 1 minute, repelling any ghosts you see with your flashlight. \n\n\n## How we built it\n\n\nTeam work, brainstorming, creativity and a lot of good vibes. (Also through Unity, Photon, and spatial anchors.)\n\n\n## Challenges we ran into\n\n\n* Spatial anchors seems to not work well with Open VR in one Project\n* Mixing a lot of frameworks\n* Transferring positions with different scales\n* Rigging problems with 3D assets\n\n\n## Accomplishments that we're proud of\n\n\nMultiplayer gaming- we were able to combine both game experiences and make a very engaging game.\n\n\n## What we learned\n\n\n* How to handle spatial anchors\n* Working with multiple target devices\n* Better rigging in Blender\n* Create multiplayer applications with Photon\n\n\n## What's next for GhostBustXR\n\n\nWe have ideas for creating different levels of the same game and for extending the game using different set ups like island and monkeys, boats and sharks, etc. We also have several power-up ideas for the current level too, such as a vacuum cleaner item for the human, a combined ghost superpower, etc. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/spacear",
            "title": "SpaceAR",
            "blurb": "An augmented reality space planning tool for Hololens 2. Users arrange spaces with real-time feedback on key adjacency metrics, area sizes, number of occupant, and the design history.",
            "awards": [
                "Logistics, Training, Productivity [Sponsored by PTC]"
            ],
            "videos": [
                "https://www.youtube.com/embed/H9bt-yOnYKk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "8 key analysis categories based on adjacencies",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/436/datas/original.png"
                },
                {
                    "title": "Input spatial matrix uses the model geometry and turns it into live analytics",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/437/datas/original.png"
                },
                {
                    "title": "Team overview",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/438/datas/original.png"
                },
                {
                    "title": "The problem with current analogue spatial layout methods",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/439/datas/original.png"
                },
                {
                    "title": "Team photo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/447/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "JS",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/35227625?height=180&v=4&width=180"
                },
                {
                    "name": "Luke Gehron",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4754292?height=180&v=4&width=180"
                },
                {
                    "name": "Jeff Matarrese",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/2220906?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "hololens2",
                "microsoft-hololens",
                "mrtk",
                "unity"
            ],
            "content_html": "<div>\n<h1>SpaceAR</h1>\n<p>An XR application developed during MIT Reality Hack 2020 with Unity for the Hololens 2.</p>\n<h2>Inspiration</h2>\n<p>Building space planning is typically an analogue process involving sketching, and moving physical model representations of space around on a table. Often multiple design team members and sometimes stakeholders are involved in this process. Our team saw an opportunity to utilize spatial computing to improve this process, taking advantage of the opportunity to collaborate, view more data, and change designs on the fly. </p>\n<h2>Overview</h2>\n<p>SpaceAR is an XR application for building space planning. The use case explored in the current version is creating a building floor plan specifically for hackathons. A predetermined set of spaces (e.g., working space, mentor space, hacker space, relaxation space, etc) are loaded into the scene and users can move the spaces and see floor area, and number of hackers for the working spaces.</p>\n<p>To understand the results better, the space plan is analyzed in real time based on 8 key parameters (e.g., proximity of hackers to mentors) and feedback is provided through a vertical plot that can be toggled on and off. The better proposed layout, the higher score bars.</p>\n<p>To compare all created layouts, users can get into the history mode, where with the slider can move through all states and see the various iterations. </p>\n<h2>How to</h2>\n<p>Start the SpaceAR app, then start to move spaces to fit the layout you like. The space information tool tips can be toggled with the space information button displaying area and possible number of hackers. To see the scoring bars, turn on the \"Turn Analytics button\". To preview the history, turn on the \"History\" button and move the slider to travel back to specific place in history.</p>\n<p>Additionally, more spaces can be added. Additional spaces are in the floating panel on the right. Just grab additional box and place in on the grid.</p>\n<h2>Team Members</h2>\n<p><strong>Jeff Matarrese</strong> // UX Strategist (and former architect)\n<strong>Luke Gehron</strong> // VR/AR Developer (and former architect)\n<strong>Justyna Szychowska</strong> // Computational Design Specialist (and former architect)</p>\n<h2>Challenges</h2>\n<p>Prior to this experience, no team member had worked with wearable AR/Mixed Reality devices. Getting the right software and extensions installed (including a fourth laptop for our Mac user), and then getting the Hololens to cooperate with our computers took over 8 hours of work.</p>\n<p>Once we were up and running we took a fail-fast approach, rapidly building and iterating on our design. We found a lot of issues along the way including coping with the narrow view angle of the Hololens, and attempting to customize the MRTK prefabs for our own uses.</p>\n<h2>Licence</h2>\n<p>This project is licensed under the MIT License.</p>\n</div>",
            "content_md": "\n# SpaceAR\n\n\nAn XR application developed during MIT Reality Hack 2020 with Unity for the Hololens 2.\n\n\n## Inspiration\n\n\nBuilding space planning is typically an analogue process involving sketching, and moving physical model representations of space around on a table. Often multiple design team members and sometimes stakeholders are involved in this process. Our team saw an opportunity to utilize spatial computing to improve this process, taking advantage of the opportunity to collaborate, view more data, and change designs on the fly. \n\n\n## Overview\n\n\nSpaceAR is an XR application for building space planning. The use case explored in the current version is creating a building floor plan specifically for hackathons. A predetermined set of spaces (e.g., working space, mentor space, hacker space, relaxation space, etc) are loaded into the scene and users can move the spaces and see floor area, and number of hackers for the working spaces.\n\n\nTo understand the results better, the space plan is analyzed in real time based on 8 key parameters (e.g., proximity of hackers to mentors) and feedback is provided through a vertical plot that can be toggled on and off. The better proposed layout, the higher score bars.\n\n\nTo compare all created layouts, users can get into the history mode, where with the slider can move through all states and see the various iterations. \n\n\n## How to\n\n\nStart the SpaceAR app, then start to move spaces to fit the layout you like. The space information tool tips can be toggled with the space information button displaying area and possible number of hackers. To see the scoring bars, turn on the \"Turn Analytics button\". To preview the history, turn on the \"History\" button and move the slider to travel back to specific place in history.\n\n\nAdditionally, more spaces can be added. Additional spaces are in the floating panel on the right. Just grab additional box and place in on the grid.\n\n\n## Team Members\n\n\n**Jeff Matarrese** // UX Strategist (and former architect)\n**Luke Gehron** // VR/AR Developer (and former architect)\n**Justyna Szychowska** // Computational Design Specialist (and former architect)\n\n\n## Challenges\n\n\nPrior to this experience, no team member had worked with wearable AR/Mixed Reality devices. Getting the right software and extensions installed (including a fourth laptop for our Mac user), and then getting the Hololens to cooperate with our computers took over 8 hours of work.\n\n\nOnce we were up and running we took a fail-fast approach, rapidly building and iterating on our design. We found a lot of issues along the way including coping with the narrow view angle of the Hololens, and attempting to customize the MRTK prefabs for our own uses.\n\n\n## Licence\n\n\nThis project is licensed under the MIT License.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/airspace",
            "title": "AirSpace",
            "blurb": "Augmented Reality for Air Traffic Control",
            "awards": [
                "Future Mobility: UX and new interaction methods [Sponsored by Volvo Cars]"
            ],
            "videos": [
                "https://www.youtube.com/embed/Q_xMBfK21oc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Terrell Ibanez",
                    "about": "Team Lead / Developer",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/979/datas/profile.jpg"
                },
                {
                    "name": "ManuelGarzon",
                    "about": "I am a industrial designer. I work in ideation, human centered design and UX",
                    "photo": "https://graph.facebook.com/10156456110535146/picture?height=180&width=180"
                },
                {
                    "name": "Adam Halawani",
                    "about": "VR/AR Unity developer ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/561/146/datas/profile.jpg"
                },
                {
                    "name": "Juan Gill",
                    "about": "Lead Designer - Unity Integration with 3D assets. Environment and Animation Design",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/924/153/datas/profile.jpeg"
                },
                {
                    "name": "Fan Marin",
                    "about": "UX/UI, branding, video editing. ",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GjR7rztQ3psFobnxN5RlWG5kD9zgnwVEUpEe9hQ?height=180&width=180"
                }
            ],
            "built_with": [
                "c#",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Air traffic controllers use flat radar displays to view air traffic around an airport. However, unlike cars, planes move in 3 dimensions as they have an altitude component, so it's difficult to visualize exactly how far away they are from each other. Two planes overlapping may be right on top of each other about to crash, or several nautical miles away, free and clear.</p>\n<h2>What it does</h2>\n<p>AirSpace air traffic controllers visualize airplanes in 3D world space as opposed to 2D. This prototype shows the Boston Logan International airport and planes with their airline / flight information.</p>\n<h2>How we built it</h2>\n<p>We used the Unity engine with the Magic Leap One augmented reality headset.</p>\n<h2>Challenges we ran into</h2>\n<p>Had difficulties with detecting flat surfaces using plane detection for airport placement in the real-world</p>\n<p>Had issues with animations not scaling properly in Unity</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Being able to place airports on flat surfaces</p>\n<p>Visualizing planes in 3D world space</p>\n<h2>What we learned</h2>\n<p>How to develop for the Magic Leap</p>\n<p>How to use Unity animations</p>\n<h2>What's next for AirSpace</h2>\n<p>Interactive Flight Management - Adding the ability for air traffic controllers to issue commands directly in the interface</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAir traffic controllers use flat radar displays to view air traffic around an airport. However, unlike cars, planes move in 3 dimensions as they have an altitude component, so it's difficult to visualize exactly how far away they are from each other. Two planes overlapping may be right on top of each other about to crash, or several nautical miles away, free and clear.\n\n\n## What it does\n\n\nAirSpace air traffic controllers visualize airplanes in 3D world space as opposed to 2D. This prototype shows the Boston Logan International airport and planes with their airline / flight information.\n\n\n## How we built it\n\n\nWe used the Unity engine with the Magic Leap One augmented reality headset.\n\n\n## Challenges we ran into\n\n\nHad difficulties with detecting flat surfaces using plane detection for airport placement in the real-world\n\n\nHad issues with animations not scaling properly in Unity\n\n\n## Accomplishments that we're proud of\n\n\nBeing able to place airports on flat surfaces\n\n\nVisualizing planes in 3D world space\n\n\n## What we learned\n\n\nHow to develop for the Magic Leap\n\n\nHow to use Unity animations\n\n\n## What's next for AirSpace\n\n\nInteractive Flight Management - Adding the ability for air traffic controllers to issue commands directly in the interface\n\n\n"
        },
        {
            "source": "https://devpost.com/software/brain-drain-the-academic-adventure-to-save-the-world",
            "title": "Brain Drain-The Academic Adventure to Save the World!",
            "blurb": "Are you ready to join the Revolution?",
            "awards": [
                "Best Use of Eye Tracking [Sponsored by HTC Vive]"
            ],
            "videos": [
                "https://player.vimeo.com/video/385864626?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Rebecca Gill Clarke",
                    "about": "I worked on the visual design of the game and led some initial UX exercises to get the project started.",
                    "photo": "https://avatars2.githubusercontent.com/u/39564891?height=180&v=4&width=180"
                },
                {
                    "name": "Joshua Marris",
                    "about": "I managed logistics, game logic and hardware troubleshooting.",
                    "photo": "https://avatars2.githubusercontent.com/u/6978629?height=180&v=4&width=180"
                },
                {
                    "name": "Shannon Putman",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/137/datas/profile.jpg"
                },
                {
                    "name": "Jennifer Swann",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQFUZFKnweHNEA/profile-displayphoto-shrink_100_100/0?e=1532563200&height=180&t=aV_KoxSBxOutIhS9Gl1Yf-HE03MMgh-bZLAhQL8shbo&v=beta&width=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration:</h2>\n<p>There were two related but distinct inspirations behind Brain Drain. I have spent 13 years working as an educator and observing firsthand the desperate need for a change. As a society we are failing our students by refusing to update our educational practices. We need to stop trying to change the way they learn and start changing the way we teach. </p>\n<p>As a special education teacher, I would watch my students fight and struggle to find a way to gain access to the curriculum equal to their general education peers. A new app would come out that \u201cimproved scores by so many percentage points\u201d but it required the user to listen to prompts. My students who are deaf could not play that. Another new app would debut, but that one would require students to manipulate small numbers on the screen or type numbers in under a certain time. That eliminated my students with underdeveloped fine motor skills. The next \u201cacademic game\u201d hits the market, and this one finally makes learning fun. In order to do that however, you have to visually sift through a massive array of information whilst multiple rounds of distracting noises are pummeling you. There goes the access for my students with Autism Spectrum Disorder.</p>\n<p>During the course of development, specific and intentional decisions were made so that this game will be played by students of all ability levels. Utilizing the latest technology including eye tracking, this game is engaging for all students. When we harness the power of VR we are only limited by our imaginations, and students who were left out of participating in the past due to wheelchairs, lack of fine motor skills, non-verbal communication, finally have an experience designed with them in mind. The best part about making those specific decisions is that it forced us to create new and inventive ways of responding to academic questions, and come to find out, those ways are fun for all kids.</p>\n<h2>What it does:</h2>\n<p>Challenges the player to answer a series of standards based subject specific questions in new and inventive ways. Worksheets are a thing of the past.</p>\n<h3>Brain Drain</h3>\n<p><a href=\"https://youtu.be/TeTAb5nN77c\" rel=\"nofollow\">Video Here</a></p>\n<p>During dark and troubling times, two doctors are hard at work in their wicked laboratory.  It is in this dark and evil place that Dr. Apprentice and Dr. Novice create a potential world changing virus.  They have created the \u201cPhantom Virus\u201d which takes kids from hard working, intelligent students, to \u201cphantoms\u201d who walk around repeating different academic questions until they receive an answer.</p>\n<p>Dr. Apprentice and Dr. Novice finally had their virus, but they did not know how to release its wrath upon the world. Finally they thought of a brilliant plan\u2026they put the virus in unhealthy foods! Sodas, candy, any food that they knew kids liked but that was not good for them got dosed with the Phantom Virus.  Within days the virus started to spread world-wide. They thought they had won; they celebrated because they did not think there was anyone who could stop them!  But\u2026mankind\u2019s last hope rested with a brother and sister duo. Backed by their two beagles, they were ready to save the world!</p>\n<p>Watson and Spiller were playing outside when they started to notice all the adults were gone. All the sudden a turtle, raccoon and dog start talking to them! They explain about the virus and inform Spiller and Watson that they are the world\u2019s only hope! Spiller and Watson knew they had to act fast if they were going to save the world! The animals did not want them to go, but they knew they were the up for the challenge. They loaded them up with plenty of fresh fruit, vegetables, toothbrushes, toothpaste, deodorant, and water to help fight off the Phantom kids if they needed to.\n \u201cYou will have help and protection out there\u201d The animals looked at their pet beagles, Wendi and Jessi as they spoke.  </p>\n<p>Wendi and Jessi barked in agreement, letting them know that they would be there to protect them no matter what!\nThis is where you come in!</p>\n<p>You can play as either Spiller or Watson, and if you have a friend you can team up and play together!\nYou will travel around the world and as you come upon a Phantom, you have three choices.  You can answer their question correctly; throw fruits/vegetables/water/hygiene products at them to buy yourself some time while you think of the correct answer. Lastly, if you cannot think of an answer you can ask Wendi and Jessi for help, but be careful because they only can give you one answer each per level (unless you find some hidden dog treats throughout the level, and build up a few bonus answers).</p>\n<p>If you answer correctly you will cure the Phantom and turn him/her back into a child who is ready to join your army! You will need a giant army with you when you finally make it to the Doctor\u2019s laboratory, where you will fight to take them out once and for all!</p>\n<p>Be prepared, each world offers different subjects you will have to conquer. </p>\n<p>The ultimate challenge of Brain Drain is there is no mouse or keyboard. YOU are the controller! If your character needs to jump, then you need to jump, if your character has to run, put your running shoes on because you will be moving! You will be out of your desk, moving, jumping, running and learning all at the same time! </p>\n<p>As you move around the world, you can collect other prizes to help you along the way. You will find scooters, bicycles, and roller skates to help you move faster. As you make your way from world to world, the questions will get harder, so be ready!  When you have to stop the game, you will return to \u201chome base.\u201d Here your mother will make you a home cooked meal, and you can rest up for the next day\u2019s adventure. While at home, you will log onto your computer and see how many Phantoms you saved, what questions you were asked, and what % you answered correctly. This will allow you to understand what subjects you did well on, and what subjects you need to practice for the next round!\nDo you have the skills and willingness to study hard so that you can save the world from certain doom!? If so, get your mind ready and begin your adventure through Brain Drain! </p>\n<h2>How I built it:</h2>\n<p>Brain Drain is a virtual reality experience, therefore we built the educational game with Unity using the HTC VIVE Toolkit. We coded the game in C# and built assets with Blender. We also utilized Adobe Mixamo to help rig some of our custom created assets.</p>\n<h2>Challenges I ran into:</h2>\n<p>As we were all very new to development, one of the biggest hurdles was learning to collaborate with the tools at our disposal.  Coding and scene changes will have be easier for each of us to track once all members of our team have experience with git, and even though the Collab utility in Unity helped us bring our assets and scenes together in the end, it was enough to make development harder from the outset.</p>\n<p>Another challenge was in understanding how the interfaces of the SDKs, the Unity environment, and our own code needed to be made to be used together.  The mentors did all they could to show us how to implement a lot of our high-level ideas, but simple concepts such as how to create a script that can be triggered by an onclick() event within the 3D environment, or how to specify the parameters that are exposed in the very visual Unity window's UI, are an almost necessary aspect of learning Unity development from a scripting perspective.</p>\n<p>Overall, we had to overcome our newness to the technology, to working on a large software development project in a group, and working on such a tight deadline.  It's easy to appreciate that even the most experienced groups at this event had similar challenges, at their own level, and it's worth noting that we are no longer naive 'noobs' in the arena of VR/AR/MR/XR hacking!</p>\n<h2>Accomplishments that I'm proud of:</h2>\n<p>During the course of development, specific and intentional decisions were made so that this game will be played by students of all ability levels. Utilizing the latest technology including eye tracking, this game is engaging for all students. When we harness the power of VR we are only limited by our imaginations, and students who were left out of participating in the past due to wheelchairs, lack of fine motor skills, non-verbal communication, finally have an experience designed with them in mind.</p>\n<h2>What I learned:</h2>\n<p>That teamwork makes the dream work is not just a cute quip. This project was the living definition of that statement. This game idea has been a dream of mine for six years but I never had the right people to complete the dream. This team brought skills of unparalleled levels, ideas so ingenious I never could have imagined, and lifted this game with a pure sense of elegance.</p>\n<h2>What's next for test:</h2>\n<ul>\n<li>Multiple modes of interaction to accommodate students of various skill and ability</li>\n<li>The ability for teachers to take the basic framework and customize the environment to build in not only testing, but engaging lessons, exercises and multimodal, multimedia learning experiences</li>\n<li>Further development and a successful educational revolution! </li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration:\n\n\nThere were two related but distinct inspirations behind Brain Drain. I have spent 13 years working as an educator and observing firsthand the desperate need for a change. As a society we are failing our students by refusing to update our educational practices. We need to stop trying to change the way they learn and start changing the way we teach. \n\n\nAs a special education teacher, I would watch my students fight and struggle to find a way to gain access to the curriculum equal to their general education peers. A new app would come out that \u201cimproved scores by so many percentage points\u201d but it required the user to listen to prompts. My students who are deaf could not play that. Another new app would debut, but that one would require students to manipulate small numbers on the screen or type numbers in under a certain time. That eliminated my students with underdeveloped fine motor skills. The next \u201cacademic game\u201d hits the market, and this one finally makes learning fun. In order to do that however, you have to visually sift through a massive array of information whilst multiple rounds of distracting noises are pummeling you. There goes the access for my students with Autism Spectrum Disorder.\n\n\nDuring the course of development, specific and intentional decisions were made so that this game will be played by students of all ability levels. Utilizing the latest technology including eye tracking, this game is engaging for all students. When we harness the power of VR we are only limited by our imaginations, and students who were left out of participating in the past due to wheelchairs, lack of fine motor skills, non-verbal communication, finally have an experience designed with them in mind. The best part about making those specific decisions is that it forced us to create new and inventive ways of responding to academic questions, and come to find out, those ways are fun for all kids.\n\n\n## What it does:\n\n\nChallenges the player to answer a series of standards based subject specific questions in new and inventive ways. Worksheets are a thing of the past.\n\n\n### Brain Drain\n\n\n[Video Here](https://youtu.be/TeTAb5nN77c)\n\n\nDuring dark and troubling times, two doctors are hard at work in their wicked laboratory. It is in this dark and evil place that Dr. Apprentice and Dr. Novice create a potential world changing virus. They have created the \u201cPhantom Virus\u201d which takes kids from hard working, intelligent students, to \u201cphantoms\u201d who walk around repeating different academic questions until they receive an answer.\n\n\nDr. Apprentice and Dr. Novice finally had their virus, but they did not know how to release its wrath upon the world. Finally they thought of a brilliant plan\u2026they put the virus in unhealthy foods! Sodas, candy, any food that they knew kids liked but that was not good for them got dosed with the Phantom Virus. Within days the virus started to spread world-wide. They thought they had won; they celebrated because they did not think there was anyone who could stop them! But\u2026mankind\u2019s last hope rested with a brother and sister duo. Backed by their two beagles, they were ready to save the world!\n\n\nWatson and Spiller were playing outside when they started to notice all the adults were gone. All the sudden a turtle, raccoon and dog start talking to them! They explain about the virus and inform Spiller and Watson that they are the world\u2019s only hope! Spiller and Watson knew they had to act fast if they were going to save the world! The animals did not want them to go, but they knew they were the up for the challenge. They loaded them up with plenty of fresh fruit, vegetables, toothbrushes, toothpaste, deodorant, and water to help fight off the Phantom kids if they needed to.\n \u201cYou will have help and protection out there\u201d The animals looked at their pet beagles, Wendi and Jessi as they spoke. \n\n\nWendi and Jessi barked in agreement, letting them know that they would be there to protect them no matter what!\nThis is where you come in!\n\n\nYou can play as either Spiller or Watson, and if you have a friend you can team up and play together!\nYou will travel around the world and as you come upon a Phantom, you have three choices. You can answer their question correctly; throw fruits/vegetables/water/hygiene products at them to buy yourself some time while you think of the correct answer. Lastly, if you cannot think of an answer you can ask Wendi and Jessi for help, but be careful because they only can give you one answer each per level (unless you find some hidden dog treats throughout the level, and build up a few bonus answers).\n\n\nIf you answer correctly you will cure the Phantom and turn him/her back into a child who is ready to join your army! You will need a giant army with you when you finally make it to the Doctor\u2019s laboratory, where you will fight to take them out once and for all!\n\n\nBe prepared, each world offers different subjects you will have to conquer. \n\n\nThe ultimate challenge of Brain Drain is there is no mouse or keyboard. YOU are the controller! If your character needs to jump, then you need to jump, if your character has to run, put your running shoes on because you will be moving! You will be out of your desk, moving, jumping, running and learning all at the same time! \n\n\nAs you move around the world, you can collect other prizes to help you along the way. You will find scooters, bicycles, and roller skates to help you move faster. As you make your way from world to world, the questions will get harder, so be ready! When you have to stop the game, you will return to \u201chome base.\u201d Here your mother will make you a home cooked meal, and you can rest up for the next day\u2019s adventure. While at home, you will log onto your computer and see how many Phantoms you saved, what questions you were asked, and what % you answered correctly. This will allow you to understand what subjects you did well on, and what subjects you need to practice for the next round!\nDo you have the skills and willingness to study hard so that you can save the world from certain doom!? If so, get your mind ready and begin your adventure through Brain Drain! \n\n\n## How I built it:\n\n\nBrain Drain is a virtual reality experience, therefore we built the educational game with Unity using the HTC VIVE Toolkit. We coded the game in C# and built assets with Blender. We also utilized Adobe Mixamo to help rig some of our custom created assets.\n\n\n## Challenges I ran into:\n\n\nAs we were all very new to development, one of the biggest hurdles was learning to collaborate with the tools at our disposal. Coding and scene changes will have be easier for each of us to track once all members of our team have experience with git, and even though the Collab utility in Unity helped us bring our assets and scenes together in the end, it was enough to make development harder from the outset.\n\n\nAnother challenge was in understanding how the interfaces of the SDKs, the Unity environment, and our own code needed to be made to be used together. The mentors did all they could to show us how to implement a lot of our high-level ideas, but simple concepts such as how to create a script that can be triggered by an onclick() event within the 3D environment, or how to specify the parameters that are exposed in the very visual Unity window's UI, are an almost necessary aspect of learning Unity development from a scripting perspective.\n\n\nOverall, we had to overcome our newness to the technology, to working on a large software development project in a group, and working on such a tight deadline. It's easy to appreciate that even the most experienced groups at this event had similar challenges, at their own level, and it's worth noting that we are no longer naive 'noobs' in the arena of VR/AR/MR/XR hacking!\n\n\n## Accomplishments that I'm proud of:\n\n\nDuring the course of development, specific and intentional decisions were made so that this game will be played by students of all ability levels. Utilizing the latest technology including eye tracking, this game is engaging for all students. When we harness the power of VR we are only limited by our imaginations, and students who were left out of participating in the past due to wheelchairs, lack of fine motor skills, non-verbal communication, finally have an experience designed with them in mind.\n\n\n## What I learned:\n\n\nThat teamwork makes the dream work is not just a cute quip. This project was the living definition of that statement. This game idea has been a dream of mine for six years but I never had the right people to complete the dream. This team brought skills of unparalleled levels, ideas so ingenious I never could have imagined, and lifted this game with a pure sense of elegance.\n\n\n## What's next for test:\n\n\n* Multiple modes of interaction to accommodate students of various skill and ability\n* The ability for teachers to take the basic framework and customize the environment to build in not only testing, but engaging lessons, exercises and multimodal, multimedia learning experiences\n* Further development and a successful educational revolution!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-mind-s-eye",
            "title": "The Mind's Eye",
            "blurb": "A detective game that teaches the player about confirmation bias and group think.",
            "awards": [
                "Best Use of Eye Tracking [Sponsored by HTC Vive]"
            ],
            "videos": [
                "https://www.youtube.com/embed/kPyQaeu1ZEM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Weiyu Feng",
                    "about": "I mainly worked on the unity programming. I also helped gameplay design.",
                    "photo": "https://avatars1.githubusercontent.com/u/47650296?height=180&v=4&width=180"
                },
                {
                    "name": "Muhammadrustamov",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/60074493?height=180&v=4&width=180"
                },
                {
                    "name": "Anna Shabayev",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/353/377/datas/profile.jpg"
                },
                {
                    "name": "Nathanael  McMahon",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/914/601/datas/profile.JPG"
                },
                {
                    "name": "Mavis Yu",
                    "about": "",
                    "photo": "https://graph.facebook.com/770420403485327/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "c#",
                "htcvive",
                "sranipal",
                "steamvr",
                "unity",
                "vive",
                "viveproi"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Eye tracking technology. Psychology courses. And data collection.</p>\n<p>Big social media giants like Facebook are in some ways exacerbating confirmation bias by measuring click rates/eye ball times and showing users information they would be more likely to linger on based on these measurements, to maximize eyeball time to sell to advertisers. This contributes to political polarization and has even contributed to the rise of right wing extremism in U.S., Europe, and around the globe. We'd like to educate people about how their choices may be influenced.</p>\n<p>Use cases: For general public, in classrooms, orientation for corporate employees, detectives, policeman, judges etc. </p>\n<h2>What it does</h2>\n<p>Our game \u201cThe Mind\u2019s Eye\u201d is a murder mystery game. You are given a question about the crime scene and you need to look around to try to find clues. In future iterations, looking at characters or objects will prompt an audio description.</p>\n<h2>How I built it</h2>\n<p>Used model assets from the Unity asset store and from Sketchfab.</p>\n<h2>Challenges I ran into</h2>\n<p>First time working with eye tracking with very little documentation.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>First time using or developing for eye tracking technology.</p>\n<h2>What we learned</h2>\n<p>Design and implementation of eye tracking. As well as different biases that</p>\n<h2>What's next for The Mind's Eye</h2>\n<p>Use pupil dilation to gather more information about a user's interest.</p>\n<h2>What is your team number?</h2>\n<p>067</p>\n<h2>What is your table number?</h2>\n<p>6B-10</p>\n<h2>Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)</h2>\n<p>Educational</p>\n<h2>Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)</h2>\n<p>Social Good</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEye tracking technology. Psychology courses. And data collection.\n\n\nBig social media giants like Facebook are in some ways exacerbating confirmation bias by measuring click rates/eye ball times and showing users information they would be more likely to linger on based on these measurements, to maximize eyeball time to sell to advertisers. This contributes to political polarization and has even contributed to the rise of right wing extremism in U.S., Europe, and around the globe. We'd like to educate people about how their choices may be influenced.\n\n\nUse cases: For general public, in classrooms, orientation for corporate employees, detectives, policeman, judges etc. \n\n\n## What it does\n\n\nOur game \u201cThe Mind\u2019s Eye\u201d is a murder mystery game. You are given a question about the crime scene and you need to look around to try to find clues. In future iterations, looking at characters or objects will prompt an audio description.\n\n\n## How I built it\n\n\nUsed model assets from the Unity asset store and from Sketchfab.\n\n\n## Challenges I ran into\n\n\nFirst time working with eye tracking with very little documentation.\n\n\n## Accomplishments that I'm proud of\n\n\nFirst time using or developing for eye tracking technology.\n\n\n## What we learned\n\n\nDesign and implementation of eye tracking. As well as different biases that\n\n\n## What's next for The Mind's Eye\n\n\nUse pupil dilation to gather more information about a user's interest.\n\n\n## What is your team number?\n\n\n067\n\n\n## What is your table number?\n\n\n6B-10\n\n\n## Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)\n\n\nEducational\n\n\n## Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)\n\n\nSocial Good\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ar-community-garden-2sgw61",
            "title": "AR Community Garden",
            "blurb": "Plant AR Seeds to propagate AR Urban Edible & Medicinal Plants. Discover the Dandelion Patch, created by ~100 participants of the MIT Reality Hackathon and Public Expo. Pollinate plants with AR Bee!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/zYaMRR4Hwxs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "AR Community Garden Mission",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/229/datas/original.JPG"
                },
                {
                    "title": "AR Seeds for Edible &amp; Medicinal Plants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/228/datas/original.JPG"
                },
                {
                    "title": "AR Community Dandelion Patch Participants ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/231/datas/original.JPG"
                },
                {
                    "title": "AR Community Dandelion Seed Target. Try it out with ARize App https://arize.page.link/eY9mKkHSQZqiGjH76",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/226/datas/original.JPG"
                },
                {
                    "title": "AR Seed Targets. Try them all with ARize! ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/230/datas/original.jpg"
                },
                {
                    "title": "Environmental Sketches of MIT Media Lab. Oliver Dansinger",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/221/datas/original.JPG"
                },
                {
                    "title": "Non-Edible Plants, Tilt Brush. Oliver Dansinger",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/235/datas/original.JPG"
                },
                {
                    "title": "Social XR Drawing Lesson ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/219/datas/original.jpg"
                },
                {
                    "title": "Community Dandelion Patch ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/224/datas/original.JPG"
                },
                {
                    "title": "Pollinating Bee",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/218/datas/original.JPG"
                },
                {
                    "title": "Dandelion for XR Art Show. Try it out: https://arize.page.link/PzMFongpiUaoFydX7",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/935/232/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Paige Dansinger",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/166/datas/profile.JPG"
                }
            ],
            "built_with": [
                "autocad",
                "blender",
                "oculusquest",
                "poly",
                "procreate",
                "tiltbrush",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our inspiration was to engage members of the community at MIT Reality Hackathon and Public Expo in creative ways to engage and identify edible and medicinal plants to build individual, group, and community resilience through drawing together.</p>\n<h2>What it does</h2>\n<p>This art installation invites participants to plant AR Seeds, watch AR Plants grow, and pollinate the plants with an AR Bee. People propagating and pollinate plants using AR creates deeper connection to the plants in inhabited environments. Discovering edible and medicinal plants empowers the public. Creating and recognizing a dandelion, or other flower one created in a VR Dandelion Patch makes people feel awesome! They see their work with every one else's and know they are in an inclusive and more equitable shared shared space; this is Utopian.  </p>\n<h2>How I built it</h2>\n<p>We used Unity, Vuforia, Tilt Brush, Oculus Quest, and Procreate App for the Hackathon. I also added all AR images to the ARize App. AR Dandelion Patch created by participants of MIT Reality Hackathon. Try it out: <a href=\"https://arize.page.link/Y4ek2H4sZaGozsBn9\" rel=\"nofollow\">https://arize.page.link/Y4ek2H4sZaGozsBn9</a></p>\n<p>We built by practicing listening to our partner's skills, voices, and experiences while supporting new growing opportunities. We also built it by having a strong mission, perspective, and ability to edit unnecessary elements. Our team each had a purpose and was able to combine our work together. As XR Artist, I personally loved drawing in Tilt Brush with over 50 people at the Hackathon. I wanted to include diverse people from all over the world. For some it was their first-time drawing in Tilt Brush. I would instruct them their right hand was Hand of Infinite Power, that by pressing the button they can create anything in the world. And their left or non-dominated hand was their Hand of Infinite Choices, with tools, nibs, and brushes. Each participant felt super-empowered to create something that expressed their joy and individuality. By doing this I created new friends, connected people together in a group activity that created deeper connections and took opportunity to add XR public art practice &amp; XR Social Ed community resilience-building in simple pleasant ways. </p>\n<h2>What's next for AR Community Garden</h2>\n<p>A single dandelion <a href=\"https://arize.page.link/o2E1rVeqZyWo27uz9\" rel=\"nofollow\">https://arize.page.link/o2E1rVeqZyWo27uz9</a> will be included in the XR Art Show in San Diego. Additionally, being exposed to VIVE Eye and hand tracking inspired me to learn how to use them for future projects, and to import my work into Nreal through friends at FXG. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur inspiration was to engage members of the community at MIT Reality Hackathon and Public Expo in creative ways to engage and identify edible and medicinal plants to build individual, group, and community resilience through drawing together.\n\n\n## What it does\n\n\nThis art installation invites participants to plant AR Seeds, watch AR Plants grow, and pollinate the plants with an AR Bee. People propagating and pollinate plants using AR creates deeper connection to the plants in inhabited environments. Discovering edible and medicinal plants empowers the public. Creating and recognizing a dandelion, or other flower one created in a VR Dandelion Patch makes people feel awesome! They see their work with every one else's and know they are in an inclusive and more equitable shared shared space; this is Utopian. \n\n\n## How I built it\n\n\nWe used Unity, Vuforia, Tilt Brush, Oculus Quest, and Procreate App for the Hackathon. I also added all AR images to the ARize App. AR Dandelion Patch created by participants of MIT Reality Hackathon. Try it out: <https://arize.page.link/Y4ek2H4sZaGozsBn9>\n\n\nWe built by practicing listening to our partner's skills, voices, and experiences while supporting new growing opportunities. We also built it by having a strong mission, perspective, and ability to edit unnecessary elements. Our team each had a purpose and was able to combine our work together. As XR Artist, I personally loved drawing in Tilt Brush with over 50 people at the Hackathon. I wanted to include diverse people from all over the world. For some it was their first-time drawing in Tilt Brush. I would instruct them their right hand was Hand of Infinite Power, that by pressing the button they can create anything in the world. And their left or non-dominated hand was their Hand of Infinite Choices, with tools, nibs, and brushes. Each participant felt super-empowered to create something that expressed their joy and individuality. By doing this I created new friends, connected people together in a group activity that created deeper connections and took opportunity to add XR public art practice & XR Social Ed community resilience-building in simple pleasant ways. \n\n\n## What's next for AR Community Garden\n\n\nA single dandelion <https://arize.page.link/o2E1rVeqZyWo27uz9> will be included in the XR Art Show in San Diego. Additionally, being exposed to VIVE Eye and hand tracking inspired me to learn how to use them for future projects, and to import my work into Nreal through friends at FXG. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/arpet-3j4kbv",
            "title": "ARpet",
            "blurb": "Everyone loves pets, right? We propose intelligent augmented reality (AR) pets that guide us to places, help us meet our fitness goals, and much more..",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/389402436?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Joe",
                    "about": "I did most of the development in regards to connecting to the Hololens and instantiating our points of interest.",
                    "photo": "https://avatars0.githubusercontent.com/u/15370969?height=180&v=4&width=180"
                },
                {
                    "name": "Samuel Jr. Frimpong",
                    "about": "I helped out with programming the logic of the cat movement and the interactions between the user, the cat, and the virtual buttons.",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5603AQGjN5tB1Rsf5w/profile-displayphoto-shrink_800_800/0?e=1586390400&height=180&t=NCaKm4KmPV5A9I325A8edqwQfRj6EUv0RimzfRTDRtI&v=beta&width=180"
                },
                {
                    "name": "Jatin Arora",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQH8HKXVxa4tmQ/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=xsz1v-MKLZrNowffO9t0XmfZknbL0bKcvvUcoHFCbzc&v=beta&width=180"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<p>Coming up soon.</p>\n</div>",
            "content_md": "\nComing up soon.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/virtuoso-xr-immersive-music-education",
            "title": "Virtuoso XR: Immersive Music Education\u2028",
            "blurb": "Virtuoso XR is an immersive, fun, and tactile platform that hopes to democratize music education - bringing musical concepts intuitively into the real world, and directly onto our instruments",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/NbI2fu8XRjM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Andrew Johnson",
                    "about": "I designed, modeled and coded the visible aspects of the UI and feedback FX. I worked with Mike to architect the overall flow and game loop and keep things in scope. I also did the audio design and sequencing for the rhythms. ",
                    "photo": "https://www.gravatar.com/avatar/bbdc8ecfb8f9af48b568386a1d9ebc78?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "varun girdhar",
                    "about": "My Contributions were as follows :",
                    "photo": "https://lh4.googleusercontent.com/-WPYgmZ9b5O0/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucn730vLLD6nExWfVtixPXoXH8ETWw/s96-c/c/photo.jpg?height=180&width=180"
                },
                {
                    "name": "Marlon Fuentes",
                    "about": "I brought subject matter expertise from the field of ethnomusicology to guide pedagogy of learning how to count rhythm using time unit box system. Part of the mission of this app is to bridge cultures by way of rhythm and so my academic background in musical systems of the world allowed me to put together a small three part samba demo for our first interactive lesson. I created midi maps for samba loops and helped design the overall value proposition for this app seeking to solve learner and instructor challenges, create gains, and satisfy job requirements. We identified mathematical learning standards that could be explored as well as social studies and historical lessons. ",
                    "photo": "https://graph.facebook.com/10155750215229904/picture?height=180&width=180"
                },
                {
                    "name": "Mike Mandel",
                    "about": "I created the core music systems in C# for the project (importing content from Midi, displaying music patterns, evaluating success, etc) along with the overall progression system that created the gameplay loop.   I also helped provide some of the broad creative vision around the project being a new kind of intuitive education platform.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/382/datas/profile.jpeg"
                },
                {
                    "name": "Nicholas Chapa",
                    "about": "",
                    "photo": "https://graph.facebook.com/10110178645853936/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "drywetmidi",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<p>VirtuosoXR is a music education platform that we hope could democratize music education. Performing music is one the greatest joys in the world, but many people do not have access to great musical education or encounter a frustrating learning process that leads them to eventually give up. By augmenting makeshift tabletop surfaces and real instruments (such as drums and guitars), we can create an intuitive and tactile platform for music education. Outside of the classroom, Virtuoso XR is a fun way to learn an instrument, but inside the classroom, we can offer teachers, parents, and students new ways of mastering the fundamentals of musical performance. It is our goal to not only teach musical, mathematical, and cultural concepts, but to increase the agency a student feels when approaching learning, so that after spending time on Virtuoso XR, they feel empowered to create their own music.</p>\n<p>We built a tiny slice of this vision for the hackathon, in the form of a series of lessons designed teach rhythmic concepts from cultures around the world. Our app creates virtual drum targets aligned with a real tabletop, and provides intuitive spatial cues for players to use to hit the targets in time with a series of samba rhythms. After learning about the instrument that produces the sound, they are tasked with performing the rhythm directly on the table. Our experience was inspired by the simplicity and accessibility of Time Unit Box System, a method used by Ethnomusicologists to teach rhythm to musicians without western musical training in reading notation. </p>\n<p>We eventually hope to expand our app to support a wide array of instruments, song content, and lesson packs. We also hope to use volumetric performances from master musicians to provide another layer authentic and intuitive learning to our platform. This is just the beginning!</p>\n</div>",
            "content_md": "\nVirtuosoXR is a music education platform that we hope could democratize music education. Performing music is one the greatest joys in the world, but many people do not have access to great musical education or encounter a frustrating learning process that leads them to eventually give up. By augmenting makeshift tabletop surfaces and real instruments (such as drums and guitars), we can create an intuitive and tactile platform for music education. Outside of the classroom, Virtuoso XR is a fun way to learn an instrument, but inside the classroom, we can offer teachers, parents, and students new ways of mastering the fundamentals of musical performance. It is our goal to not only teach musical, mathematical, and cultural concepts, but to increase the agency a student feels when approaching learning, so that after spending time on Virtuoso XR, they feel empowered to create their own music.\n\n\nWe built a tiny slice of this vision for the hackathon, in the form of a series of lessons designed teach rhythmic concepts from cultures around the world. Our app creates virtual drum targets aligned with a real tabletop, and provides intuitive spatial cues for players to use to hit the targets in time with a series of samba rhythms. After learning about the instrument that produces the sound, they are tasked with performing the rhythm directly on the table. Our experience was inspired by the simplicity and accessibility of Time Unit Box System, a method used by Ethnomusicologists to teach rhythm to musicians without western musical training in reading notation. \n\n\nWe eventually hope to expand our app to support a wide array of instruments, song content, and lesson packs. We also hope to use volumetric performances from master musicians to provide another layer authentic and intuitive learning to our platform. This is just the beginning!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-focus-field",
            "title": "The Focus Field",
            "blurb": "A Hololens 2 application for training sustained eye-contact.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/-AZYzc2a-Ik?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "user persona",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/155/datas/original.png"
                },
                {
                    "title": "people with ADHD and autism are generally uncomfortable with sustained eye contact",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/156/datas/original.png"
                },
                {
                    "title": "Augmented visual cues help cue users meet goals that they&#39;ve set.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/158/datas/original.png"
                },
                {
                    "title": "Radiating rings emanating from the point of focus (the other person&#39;s eyes) to help redirect their attention back to the focus target.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/157/datas/original.png"
                },
                {
                    "title": "Users set goals and monitor their progress using the mobile app.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/144/datas/original.png"
                },
                {
                    "title": "Live data from the Hololens is synced to the user&#39;s account, using Azure services.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/148/datas/original.png"
                },
                {
                    "title": "Our tools include Unity, Hololens 2, MRTK, Photoshop, Illustrator, Rhino, and Azure ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/146/datas/original.png"
                },
                {
                    "title": "We use Azure to sync information from the headset to a user&#39;s personal profile, allowing for more data analytics.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/147/datas/original.png"
                },
                {
                    "title": "The data from multiple sessions is aggregated and displayed for the users on their account.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/145/datas/original.png"
                },
                {
                    "title": "Our Team",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/372/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Calvin Fong",
                    "about": "I worked on a project (.NET Standard) that could be plugged in to unity to upload data to the cloud (Azure Blob Service). Specifically the data was recording what was in focus and for how long.",
                    "photo": "https://graph.facebook.com/10156502038957234/picture?height=180&width=180"
                },
                {
                    "name": "Tibi Leonov",
                    "about": "I worked with my team on designing and prototyping the product. I helped iterate on the UX design to create an accessible and user friendly flow. I used Unity to implement and iterate on the mechanics that take full advantage of the vision tracking capabilities of the HoloLens 2.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/682/893/datas/profile.png"
                },
                {
                    "name": "Harman Bhutani",
                    "about": "I mostly worked on architectural view of the product from the ideation of the idea to the development of the product. I worked on integrating various segments of our project such as Face detection , Object detection and using the data to analyse the performance of users . How better they are progressing in their work  . I made a neural network model which I trained with the data of my fellow team members and we had one person with ADHD , I took his data as positive data and other four team member data as negative one . Further I gathered data from different teams . For making my model not be biased with negative cases , I programatically made test cases so we are feeding the unbiased data to our model using many parameters as inputs from various eye movements , change in size of pupil , change in tensions of eye skin and many more .",
                    "photo": "https://avatars3.githubusercontent.com/u/11242265?height=180&v=4&width=180"
                },
                {
                    "name": "Ahn Mur",
                    "about": "Together with Anne-Elise, I focused on design direction and project management. We specified the user story and created the UX flow, including the interface assets. Testing various filter effects and colour schemes on the Hololens 2, I designed the logo and branding to suit both the AR tool and the companion phone app. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/178/datas/profile.png"
                },
                {
                    "name": "Anne-Elise Chung",
                    "about": "I tag-teamed working in Unity, writing code to work with eye tracking, integrating different libraries, and bug fixing, Additionally, I worked with team members to map and test user stories and flow, and create graphics to display on the Hololens 2 as well as our mobile prototype. Our group worked collectively to test and collect feedback from users.",
                    "photo": "https://avatars0.githubusercontent.com/u/30807117?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "augmented-reality",
                "augmentedreality",
                "azure",
                "eyetracking",
                "figma",
                "github",
                "microsoft-hololens",
                "mrtk",
                "photoshop",
                "premiere",
                "rhino",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our team was sitting in the audience at the Opening Ceremonies of the Reality Hack 2020, after a long and exciting day of meeting people and attending workshops. Though the keynotes were amazing, one of our group members found it hard to focus. Her attention would wander amidst the visual cacophony of phones, people making comments, and other bystanders. At this moment, she wondered if she could make something to help maintain her focus with augmented reality. The Focus Field was thus conceived.</p>\n<h2>What it does</h2>\n<p>The Focus Field encourages and rewards sustained attention by providing augmented visual cues and filters in response to a user's eye movements.</p>\n<h2>How we built it</h2>\n<p>We built this in pieces with each team member fleshing out individual features and testing them to see if they worked. We gradually merged our different portions together.</p>\n<h2>Challenges we ran into</h2>\n<p>None of us had ever developed for Hololens before and there was a lot of initial learning and head-scratching.\nWe gained many findings from our testing with users on autism and ADHD that challenged our original project direction.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of.. eye-tracking and creating a feature-complete vertical slice of our product. Creating graphics and high fidelity mobile prototype that completes the user's flow with our app. Getting Azure services to work so that the Hololens 2 device would send live data to the cloud. Working iteratively and testing as much as we could with mentors.</p>\n<h2>What we learned</h2>\n<p>How to work with MRTK and Hololens 2. Working with Azure Cloud. Working and designing for use of eye-tracking. Learning about ADHD and autism.</p>\n<h2>What's next for The Focus Field</h2>\n<p>Creating The Focus Field at this hackathon allowed us to have a proof of concept and testable demo to share with a larger and more diverse set of users.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur team was sitting in the audience at the Opening Ceremonies of the Reality Hack 2020, after a long and exciting day of meeting people and attending workshops. Though the keynotes were amazing, one of our group members found it hard to focus. Her attention would wander amidst the visual cacophony of phones, people making comments, and other bystanders. At this moment, she wondered if she could make something to help maintain her focus with augmented reality. The Focus Field was thus conceived.\n\n\n## What it does\n\n\nThe Focus Field encourages and rewards sustained attention by providing augmented visual cues and filters in response to a user's eye movements.\n\n\n## How we built it\n\n\nWe built this in pieces with each team member fleshing out individual features and testing them to see if they worked. We gradually merged our different portions together.\n\n\n## Challenges we ran into\n\n\nNone of us had ever developed for Hololens before and there was a lot of initial learning and head-scratching.\nWe gained many findings from our testing with users on autism and ADHD that challenged our original project direction.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of.. eye-tracking and creating a feature-complete vertical slice of our product. Creating graphics and high fidelity mobile prototype that completes the user's flow with our app. Getting Azure services to work so that the Hololens 2 device would send live data to the cloud. Working iteratively and testing as much as we could with mentors.\n\n\n## What we learned\n\n\nHow to work with MRTK and Hololens 2. Working with Azure Cloud. Working and designing for use of eye-tracking. Learning about ADHD and autism.\n\n\n## What's next for The Focus Field\n\n\nCreating The Focus Field at this hackathon allowed us to have a proof of concept and testable demo to share with a larger and more diverse set of users.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/aredge",
            "title": "AREdge",
            "blurb": "A front line worker solution for training and coordinating complex tasks.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/wNOU6VpHP88?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "New Step by Step Guides with HoloLens 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/391/datas/original.png"
                },
                {
                    "title": "Current Painful experience",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/387/datas/original.jpg"
                },
                {
                    "title": "Ticket management system integrated with HoloLens 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/427/datas/original.png"
                },
                {
                    "title": "Way Finding experience with HoloLens 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/425/datas/original.png"
                },
                {
                    "title": "Diagnose with HoloLens App ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/449/datas/original.png"
                },
                {
                    "title": "Extra Hands from Remote Expert with VR + AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/404/datas/original.png"
                },
                {
                    "title": "Analyse BI after AR Guides and Extra Assistant ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/438/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Chris Garza",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQFFjUMvDks_SA/profile-displayphoto-shrink_800_800/0?e=1584576000&height=180&t=xMrqpBfYW7q0eIZ6poVxiu4upRxdZVx0FeE1mpSSB1k&v=beta&width=180"
                },
                {
                    "name": "Robson Beaudry",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQFE798p9akbPA/profile-displayphoto-shrink_800_800/0?e=1584576000&height=180&t=Ya9SFfkKoX3Z0gW5y1Eid2AUb6gPWoS6_il5TYy_dcI&v=beta&width=180"
                },
                {
                    "name": "Mandy Xun",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/15975ded2ddb86d6749f2cc6b3562e01?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Laura Riley",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c66538b4f0c67695c7be0efedf6d5999?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                ".net-core",
                "azure-functions",
                "azure-kinect-dk-(boday-tracking)",
                "azure-pipelines",
                "dynamics-365-web-api",
                "github",
                "holographic-remoting-2.0.18-(nuget)",
                "hololens-2",
                "leap-motion-(hand-tracking)",
                "mrtk-v2.2.0",
                "office-365-graph-api",
                "unity-2019.2",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We\u2019ve all been helped at some point in our lives - whether from instructional material or personally from experts.</p>\n<p>We\u2019ve all had to coordinate with others, and have seen the friction that results.</p>\n<p>We each have a pretty good vantage point on how the future of the workplace will require more rapid training for coming challenges.</p>\n<h2>What it does</h2>\n<p>Streamlines remote worker coordination and training.</p>\n<h2>How we built it</h2>\n<ul>\n<li>First we identified the overarching problem (front line worker training and coordinating), and focused in on a data centre use case.</li>\n<li>We then moved on to ideating solutions, and building a storyboard once we settled on one.</li>\n<li>Identified what could constitute our MVP (and what our highest hopes would be)</li>\n<li>Set up infrastructure and software configuration</li>\n<li>Constructed our demo environment with Microsoft guides. Began integrating Azure and Power BI.</li>\n<li>Set up networking to communicate between a computer (with Leap Motion) and a Hololens.</li>\n<li>Using Leap Motion and Azure Kinect to share point cloud and hands</li>\n<li>Leveraged our mentors to overcome some specific technical issues.</li>\n<li>Consumed massive amounts of caffeine and sugar.</li>\n<li>Test, test, test and iterate</li>\n<li>High fived each other for a great accomplishment!</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Wifi Network Issues (dropped connections, latency and blocked ports on UDP)</li>\n<li>Integrating multiple apps into a single experience</li>\n<li>Identifying the MVP while still using AR capabilities</li>\n<li>Hololens 2 had issues with: Battery life, Connectivity, Crashes (device would lock up and just shut off, requiring a reboot), User management and switching users was a frustrating experience and required many air taps to login to multiple accounts (Windows 10, Office 365, etc.)</li>\n<li>Our team agrees that the Hololens issues we ran into caused delays and risked our ability to show a seamless solution (otherwise, the device is very capable and hand tracking in AR was exceptional!)</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<ol>\n<li>A comprehensive demonstration of multiple AR experiences which have only recently become a reality</li>\n<li>The team orchestrated the content, framework and an MVP within a short time - we built a startup in just a few days!</li>\n<li>Our solution includes new processes for training and remote collaboration featuring AR capabilities such as hand tracking, remote assistance and tracking progress within a complex business process</li>\n</ol>\n<h2>What we learned</h2>\n<ul>\n<li>Hololens 2 (and AR/VR in general) enables new experiences and possibilities which have never been done before!</li>\n<li>Integrating different data capabilities will be key to the future of enterprise XR</li>\n<li>Experiential on the job training is possible without having an expert physically present</li>\n<li>Working with new tools for shared experiences: Vuforia, Photon, Holographic Remoting</li>\n</ul>\n<h2>What's next for AREdge</h2>\n<ul>\n<li>We will be publishing a whitepaper with Microsoft to document our journey in finding and creating a Mixed Reality experience</li>\n<li>The team expects to continue its open-source contributions and solidify the concepts we created together</li>\n<li>The feedback we are receiving is phenomenal - our shared passion with the community proves we have a marketable idea, thanks to the tremendous potential of Mixed Reality</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe\u2019ve all been helped at some point in our lives - whether from instructional material or personally from experts.\n\n\nWe\u2019ve all had to coordinate with others, and have seen the friction that results.\n\n\nWe each have a pretty good vantage point on how the future of the workplace will require more rapid training for coming challenges.\n\n\n## What it does\n\n\nStreamlines remote worker coordination and training.\n\n\n## How we built it\n\n\n* First we identified the overarching problem (front line worker training and coordinating), and focused in on a data centre use case.\n* We then moved on to ideating solutions, and building a storyboard once we settled on one.\n* Identified what could constitute our MVP (and what our highest hopes would be)\n* Set up infrastructure and software configuration\n* Constructed our demo environment with Microsoft guides. Began integrating Azure and Power BI.\n* Set up networking to communicate between a computer (with Leap Motion) and a Hololens.\n* Using Leap Motion and Azure Kinect to share point cloud and hands\n* Leveraged our mentors to overcome some specific technical issues.\n* Consumed massive amounts of caffeine and sugar.\n* Test, test, test and iterate\n* High fived each other for a great accomplishment!\n\n\n## Challenges we ran into\n\n\n* Wifi Network Issues (dropped connections, latency and blocked ports on UDP)\n* Integrating multiple apps into a single experience\n* Identifying the MVP while still using AR capabilities\n* Hololens 2 had issues with: Battery life, Connectivity, Crashes (device would lock up and just shut off, requiring a reboot), User management and switching users was a frustrating experience and required many air taps to login to multiple accounts (Windows 10, Office 365, etc.)\n* Our team agrees that the Hololens issues we ran into caused delays and risked our ability to show a seamless solution (otherwise, the device is very capable and hand tracking in AR was exceptional!)\n\n\n## Accomplishments that we're proud of\n\n\n1. A comprehensive demonstration of multiple AR experiences which have only recently become a reality\n2. The team orchestrated the content, framework and an MVP within a short time - we built a startup in just a few days!\n3. Our solution includes new processes for training and remote collaboration featuring AR capabilities such as hand tracking, remote assistance and tracking progress within a complex business process\n\n\n## What we learned\n\n\n* Hololens 2 (and AR/VR in general) enables new experiences and possibilities which have never been done before!\n* Integrating different data capabilities will be key to the future of enterprise XR\n* Experiential on the job training is possible without having an expert physically present\n* Working with new tools for shared experiences: Vuforia, Photon, Holographic Remoting\n\n\n## What's next for AREdge\n\n\n* We will be publishing a whitepaper with Microsoft to document our journey in finding and creating a Mixed Reality experience\n* The team expects to continue its open-source contributions and solidify the concepts we created together\n* The feedback we are receiving is phenomenal - our shared passion with the community proves we have a marketable idea, thanks to the tremendous potential of Mixed Reality\n\n\n"
        },
        {
            "source": "https://devpost.com/software/educationalxr",
            "title": "EducationalXR",
            "blurb": "It's an Educational Multi User Intractable Tool developed in Unity for XR.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/M9A9u-lwjTs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "HARI VAMSI YADAVALLI",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/869/019/datas/profile.jpg"
                },
                {
                    "name": "nirmita123",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/23376276?height=180&v=4&width=180"
                },
                {
                    "name": "Scottie113 Murrell",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/35502207?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "augmentedreality",
                "c#",
                "kinect",
                "normal",
                "normcore",
                "nreal",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Since more and more learning resources are going online these days, like the MIT openLearning, Coursera, etc. They all provide a solid static content. which means the persons watching the videos live stream cannot interact with the class or the teacher\nwe want to bring back the interactive experience of this classrooms to that of the online teaching. So the major flaw in the online teaching is it misses the interactive experience between the student and the teacher.</p>\n<h2>What it does</h2>\n<p>It's an AR based education platform for educators and students with real time Multiplayer networking and interactive sessions between them. The teacher can record the lectures and the students can access them later. A new feature is that AR Live Streaming where the 3D holographic of the teacher can be seen from anywhere in the world by people connected to that server.</p>\n<h2>How we built it</h2>\n<p>We built it using the unity, normcore, and nreal's sdk. We had used the Microsoft's Kinect for the body tracking and recording. We had used the ipi soft to capture the kinect's image and to map it to an avatar.</p>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<p>we able to make the multiplayer thing workout, so by this multiple students can be instructed simultaneously by the teacher with her laptop, and the students can visualize the experience in the nreal headsets. We are able to extract the body movement, tracking feature for the Microsoft kinect and successfully be able to map it to an avatar, which can be given an audio to explain about the scenarios.\nWe had created three sample scenes namely, solar system, anatomy of brain, and art to illustrate the working of our project.</p>\n<h2>What we learned</h2>\n<p>We learnt about the NReal's sdk and how to integrate the functionalities we need. The major difficulty came when we were trying to network two of these headsets and make them operate in the same state as in sync. So these are going to be our students.</p>\n<h2>What's next for EducationalXR</h2>\n<p>Create the 3d models of the modules to be taught and try implementing this on a small scale and validate our idea with the real world.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSince more and more learning resources are going online these days, like the MIT openLearning, Coursera, etc. They all provide a solid static content. which means the persons watching the videos live stream cannot interact with the class or the teacher\nwe want to bring back the interactive experience of this classrooms to that of the online teaching. So the major flaw in the online teaching is it misses the interactive experience between the student and the teacher.\n\n\n## What it does\n\n\nIt's an AR based education platform for educators and students with real time Multiplayer networking and interactive sessions between them. The teacher can record the lectures and the students can access them later. A new feature is that AR Live Streaming where the 3D holographic of the teacher can be seen from anywhere in the world by people connected to that server.\n\n\n## How we built it\n\n\nWe built it using the unity, normcore, and nreal's sdk. We had used the Microsoft's Kinect for the body tracking and recording. We had used the ipi soft to capture the kinect's image and to map it to an avatar.\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\nwe able to make the multiplayer thing workout, so by this multiple students can be instructed simultaneously by the teacher with her laptop, and the students can visualize the experience in the nreal headsets. We are able to extract the body movement, tracking feature for the Microsoft kinect and successfully be able to map it to an avatar, which can be given an audio to explain about the scenarios.\nWe had created three sample scenes namely, solar system, anatomy of brain, and art to illustrate the working of our project.\n\n\n## What we learned\n\n\nWe learnt about the NReal's sdk and how to integrate the functionalities we need. The major difficulty came when we were trying to network two of these headsets and make them operate in the same state as in sync. So these are going to be our students.\n\n\n## What's next for EducationalXR\n\n\nCreate the 3d models of the modules to be taught and try implementing this on a small scale and validate our idea with the real world.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/clear-biopsy-guidance",
            "title": "CLEAR - AR Biopsy Guidance System",
            "blurb": "Augmented Reality Surgical Navigation Tool for Microsoft HoloLens 2",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/_cgnHJqVcSg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Augmented Reality Biopsy Guidance",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/929/921/datas/original.jpg"
                },
                {
                    "title": "Augmented Reality Biopsy Guidance",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/929/923/datas/original.jpg"
                },
                {
                    "title": "CLEAR - Augmented Reality Biopsy Guidance - The Team",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/499/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Clare Jessey",
                    "about": "I worked on the visual design + brand identity of our solution (UI/UX) and defined our solution touch points. I crafted the story that we were aiming to tell in our demo and overall played a crucial role in ideation and North Star visioning. I made sure throughout the event to team up to win with my fellow hackers. ",
                    "photo": "https://www.gravatar.com/avatar/76d082111fca2222789ae71cbceed541?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Alessandro Boaro",
                    "about": "I provided the overall clinical framework for the project and the clinical expertise to evaluate the system in its development and final testing phases.",
                    "photo": "https://www.gravatar.com/avatar/9c7a54fb5358e74d1d895dfe72dccbda?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Yash Dhake",
                    "about": "I worked on defining story and UX, re-meshing and tweaking the MRI 3d data and the audio for all the interactions in the experience.",
                    "photo": "https://www.gravatar.com/avatar/e4bb97c173fe2a2a3686104e1eb3ecba?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Matt Alkaitis",
                    "about": "I helped create and refine overall workflow. I also tried and failed to integrate object tracking into the system.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/929/717/datas/profile.JPG"
                },
                {
                    "name": "Luca Beisel",
                    "about": "I was the lead developer of our team, programming the interactions for the HoloLens 2 in the C# language using the Unity development framework.",
                    "photo": "https://avatars0.githubusercontent.com/u/12306696?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "hololens2",
                "maya",
                "microsoft-hololens",
                "sketch",
                "slicer",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are a team of clinicians, developers and designers dedicated to solving practical challenges in healthcare. The inspiration for the CLEAR Biopsy Guidance System was to use augmented reality to guide neurosurgical brain biopsy procedures. We envisioned a system that could improve patients\u2019 outcomes and safety as well as physicians\u2019 working experience. </p>\n<h2>What it does</h2>\n<p>Clear \u2013 Biopsy Guidance provides an augmented reality environment with the following components:</p>\n<ul>\n<li>Patient identification information, to ensure patient safety</li>\n<li>Patient-specific radiological imaging and procedure planning information, to review the surgical plan</li>\n<li>The planned trajectory for the brain biopsy directly projected on the patient\u2019s head</li>\n</ul>\n<h2>How we built it</h2>\n<p>Our system was based on the Hololens 2 augmented reality display, using Unity to build the experience. For the imaging components, we used 3d slicer software to convert the 2D data into 3D geometry and Autodesk Maya to re-mesh and delete the outer skull surfaces. The UI development was done in Sketch software. Our application required precise mapping of digital objects and physical instruments. The current state of spatial hand and object tracking was a key challenge that limited the precision of our application. We also initially attempted real-time volume rendering of MRI data, but found the computational requirements to be too substantial for the Hololens 2 system to handle. </p>\n<h2>What's next</h2>\n<p>We\u2019re primarily proud of how we were able to synthesize multiple fields of expertise into a strong use case-driven approach. Our team brought insight from neurosurgery, medical imaging, real-time rendering, unity and design. We truly enjoyed working together. The case for augmented reality applications in healthcare is strong. Our team is interested in solving the critical issues that will enable AR systems such as ours to reach clinical-grade performance.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are a team of clinicians, developers and designers dedicated to solving practical challenges in healthcare. The inspiration for the CLEAR Biopsy Guidance System was to use augmented reality to guide neurosurgical brain biopsy procedures. We envisioned a system that could improve patients\u2019 outcomes and safety as well as physicians\u2019 working experience. \n\n\n## What it does\n\n\nClear \u2013 Biopsy Guidance provides an augmented reality environment with the following components:\n\n\n* Patient identification information, to ensure patient safety\n* Patient-specific radiological imaging and procedure planning information, to review the surgical plan\n* The planned trajectory for the brain biopsy directly projected on the patient\u2019s head\n\n\n## How we built it\n\n\nOur system was based on the Hololens 2 augmented reality display, using Unity to build the experience. For the imaging components, we used 3d slicer software to convert the 2D data into 3D geometry and Autodesk Maya to re-mesh and delete the outer skull surfaces. The UI development was done in Sketch software. Our application required precise mapping of digital objects and physical instruments. The current state of spatial hand and object tracking was a key challenge that limited the precision of our application. We also initially attempted real-time volume rendering of MRI data, but found the computational requirements to be too substantial for the Hololens 2 system to handle. \n\n\n## What's next\n\n\nWe\u2019re primarily proud of how we were able to synthesize multiple fields of expertise into a strong use case-driven approach. Our team brought insight from neurosurgery, medical imaging, real-time rendering, unity and design. We truly enjoyed working together. The case for augmented reality applications in healthcare is strong. Our team is interested in solving the critical issues that will enable AR systems such as ours to reach clinical-grade performance.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/arehab",
            "title": "ARehab",
            "blurb": "Combinig the use of Hololens2 and Kinect for Azure for an immersive, engaging and fun rehabilitation",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385983447?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Defend Earth while rehabilitating!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/730/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Michael Potts",
                    "about": "I helped with 3d modeling, animation  and design ",
                    "photo": "https://media-exp1.licdn.com/dms/image/C4E03AQHg1d2Di9NIpQ/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=cCY1J8AP80F6WG2UwUnJdmJBCH9_nGD36hPDcgcG-0M&v=beta&width=180"
                },
                {
                    "name": "Paolo Meriggi",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/14932390?height=180&v=4&width=180"
                },
                {
                    "name": "Xinyun Chen",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQGxxJX0x6WMzA/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=UOhgHwEvSDTcgX8BdeMJ4MZRCT_Bp1hQ7OAiLUP4EF0&v=beta&width=180"
                },
                {
                    "name": "Luiz Evandro",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46703887?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "kinect",
                "microsoft-hololens",
                "mrtk",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Conventional Rehabilitation, is often based on the repetition of spcified movements. This can result in a low engagement and in a poor compliance by the patient. There are many techonogical solutions to make it more engaging and interesting, but they have still some limitations. Among the many, two technologies are currently largely studied: Kinect based rehab and VR/AR based rehabilitation. </p>\n<p>The first has great potentials, but it is not immersive and forces the patient to interact with an avatar in a virtual environment presented usually on a large screen. on the other hand, Immersive VR/AR (usually based on Head Mounted Displays (HMD), can be higly immersive, but they require other devices to keep track of the whole body segments or they are just limited to upper limbs or solely the tracking of the hands (when they are in the line of sight of the sensor). </p>\n<p>We thought to integrate Kinect and Hololens 2 to combine the best features of both technology, offering the patient an immersive (and engaging) experience, while tracking (and measuring) all the ody segments, included those not tracked by the Hololens2 </p>\n<h2>What it does</h2>\n<p>It offers a interesting and interactive rehabilitation experience, based on a simple nice story of a possible aline invasion that the patient should fight, by playin two different games, one stationary, more relate to upper limbs motor activities, and one more cogintive and full-body exercise</p>\n<h2>How we built it</h2>\n<p>We used Unity3d/MRTK and C# to develop two app, one which runs on the PC where the Kinect for Azure is connected, and one directly on the Hololens </p>\n<h2>Challenges we ran into</h2>\n<p>Make Kinect and Hololens talk to each other\nCAlculate the transformation from the Kinect reference system into the Hololens one</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We succesfully developed a nice and engaging environment for a funny and engagin AR Rehabilitation experiece, based on the idea of a small town where to fight an alien invasion. </p>\n<h2>What we learned</h2>\n<p>We learnt a lot about the different approaches of the 4 members of the team and how to deal with these brand new devices.</p>\n<h2>What's next for ARehab</h2>\n<p>Hopefully we'd like to make it an extended version more stable and robust.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nConventional Rehabilitation, is often based on the repetition of spcified movements. This can result in a low engagement and in a poor compliance by the patient. There are many techonogical solutions to make it more engaging and interesting, but they have still some limitations. Among the many, two technologies are currently largely studied: Kinect based rehab and VR/AR based rehabilitation. \n\n\nThe first has great potentials, but it is not immersive and forces the patient to interact with an avatar in a virtual environment presented usually on a large screen. on the other hand, Immersive VR/AR (usually based on Head Mounted Displays (HMD), can be higly immersive, but they require other devices to keep track of the whole body segments or they are just limited to upper limbs or solely the tracking of the hands (when they are in the line of sight of the sensor). \n\n\nWe thought to integrate Kinect and Hololens 2 to combine the best features of both technology, offering the patient an immersive (and engaging) experience, while tracking (and measuring) all the ody segments, included those not tracked by the Hololens2 \n\n\n## What it does\n\n\nIt offers a interesting and interactive rehabilitation experience, based on a simple nice story of a possible aline invasion that the patient should fight, by playin two different games, one stationary, more relate to upper limbs motor activities, and one more cogintive and full-body exercise\n\n\n## How we built it\n\n\nWe used Unity3d/MRTK and C# to develop two app, one which runs on the PC where the Kinect for Azure is connected, and one directly on the Hololens \n\n\n## Challenges we ran into\n\n\nMake Kinect and Hololens talk to each other\nCAlculate the transformation from the Kinect reference system into the Hololens one\n\n\n## Accomplishments that we're proud of\n\n\nWe succesfully developed a nice and engaging environment for a funny and engagin AR Rehabilitation experiece, based on the idea of a small town where to fight an alien invasion. \n\n\n## What we learned\n\n\nWe learnt a lot about the different approaches of the 4 members of the team and how to deal with these brand new devices.\n\n\n## What's next for ARehab\n\n\nHopefully we'd like to make it an extended version more stable and robust.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/magic-dots",
            "title": "Magic Dots",
            "blurb": "Engaging AR storybooks to spark children's imaginations for future education and storytelling",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/eK3sf53fOyE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Chris Low",
                    "about": "Integration of 3D assets with Nreal Headset.",
                    "photo": "https://avatars3.githubusercontent.com/u/21194612?height=180&v=4&width=180"
                },
                {
                    "name": "Han Zhu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/f6e017781f7a61ac5bec49e070ad8659?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Yohe Wang",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a06a79c5cede6aa6008be8a820cddf1a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Meichun Cai",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/547/134/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "c#",
                "nreal",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We make AR books with extraordinary effects and make stories come to life to provide a stimulating learning experience for children. \nWe re-imagine how the next generation can experience the illustrated stories in a playful, immersive and inspiring way.</p>\n<h2>What it does</h2>\n<p>In this demo, we've augmented a children book \"Yayoi Kusama: From Here to Infinity.\"  This colorful children book illustrates the life stories of the artist, Yayoi Kusama, who grew up with nature and was in love with art, suddenly found herself had a vision that everything in the world was covered by arrays of polka dots.\nWe've created 7 popup 3D scenes to encourage children to step into the infinity reality with Yayoi Kusama and get inspirations from her life story. </p>\n<h2>How I built it</h2>\n<p>We created AR contents popping up from different pages of the book by using Vuforia in Unity. \nAnd we used Nreal for the mixed reality experiences.</p>\n<h2>Challenges I ran into</h2>\n<p>Integrated AR contents and effects across different platforms</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We learned how to integrate AR contents with Nreal.\nWe've created 7 scenes inspired by the story of the children book.</p>\n<h2>What we learned</h2>\n<p>We learned how AR can be a powerful tool to educate children and facilitate a better learning experience.</p>\n<h2>What's next for Magic Dots</h2>\n<p>We would like to provide more AR interactions for users to learn, enjoy and play with the book and build an AR app to share these interactive stories with kids and their parents all over the world.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe make AR books with extraordinary effects and make stories come to life to provide a stimulating learning experience for children. \nWe re-imagine how the next generation can experience the illustrated stories in a playful, immersive and inspiring way.\n\n\n## What it does\n\n\nIn this demo, we've augmented a children book \"Yayoi Kusama: From Here to Infinity.\" This colorful children book illustrates the life stories of the artist, Yayoi Kusama, who grew up with nature and was in love with art, suddenly found herself had a vision that everything in the world was covered by arrays of polka dots.\nWe've created 7 popup 3D scenes to encourage children to step into the infinity reality with Yayoi Kusama and get inspirations from her life story. \n\n\n## How I built it\n\n\nWe created AR contents popping up from different pages of the book by using Vuforia in Unity. \nAnd we used Nreal for the mixed reality experiences.\n\n\n## Challenges I ran into\n\n\nIntegrated AR contents and effects across different platforms\n\n\n## Accomplishments that I'm proud of\n\n\nWe learned how to integrate AR contents with Nreal.\nWe've created 7 scenes inspired by the story of the children book.\n\n\n## What we learned\n\n\nWe learned how AR can be a powerful tool to educate children and facilitate a better learning experience.\n\n\n## What's next for Magic Dots\n\n\nWe would like to provide more AR interactions for users to learn, enjoy and play with the book and build an AR app to share these interactive stories with kids and their parents all over the world.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/sledar",
            "title": "SledAR",
            "blurb": "Like \"Line Rider\" but use real-life objects and AR tools to create a path for your sledding character! We would like to expand this game further to encourage conversations about climate change.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385918086?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "jewingo Wingo",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/7698025?height=180&v=4&width=180"
                },
                {
                    "name": "Yui Wei",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/8603121?height=180&v=3&width=180"
                }
            ],
            "built_with": [
                "lumin",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>A lot of our favorite VR experiences are short and capitalize on a fun central mechanic. We wanted to make an AR game that could capitalize on the \"room mesh\" generated by spacial computing platforms in the AR space to make fun interactions. \"Line Rider\" came to mind as a browser game in which manipulating space generated a fun, user-led interaction. The user would be able to spread snow over the scanned surfaces to create slopes for the character to sled down. The interaction could be extended to create conversations about climate change by having the \"pickups\" that the user tries to steer the character into as plastic recyclables and other items/actions associated with positive environmental impact (eg. planting trees).</p>\n<h2>What it does</h2>\n<p>You can use the Magic Leap controller to teleport a character on a sled in front of you; the character will sled down the slopes generated by the mesh</p>\n<h2>How I built</h2>\n<p>The character is physically simulated using a ball collider and rigidbody in Unity, and the sled and character are rotated using spherical linear interpolation to match the rotation fitting the normal vector to the ground they are sledding on and the velocity vector of the rigid body. The collider has a physics material with very low friction. We created a shader for the Lightweight/Universal Render Pipeline, but had to scrap it and restart it after learning that there is not currently support for the pipeline - unfortunately this set us back from accomplishing the narrative we aimed for. The result is a fun AR toy, with which users can try to extend the path of the sled by creating stacks and slopes out of real-life objects.</p>\n<h2>Challenges I ran into</h2>\n<p>Programming the snowfall shader and then attempting to reprogramthe snowfall shader outside of Shadergraph proved difficult for our team. We also may have benefited from more help from the Magic Leap team early on, as a misconception about the development pipeline prevented us from moving forward with the project for a lot of the hackathon. In the future we would like to leverage the availability of mentors more in learning skills from such projects.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>The character's movement is fun to interact with, as the spherical collider makes it easy to predict its movement and its tendency to rotate towards the direction of movement gives the character more personality than a rolling ball. </p>\n<h2>What I learned</h2>\n<p>We learned about creating vertex shaders, as well as some other types of shaders, and about the Magic Leap API and implementation.</p>\n<h2>What's next for SledAR</h2>\n<p>Finishing the snow piling mechanic is the main priority; then we would like to generate starting points and end points based on the room's collision mesh, as well as obstacles and pick-ups. These features could help create more of a narrative throughout gameplay rather than the more repetitive simple interactions we've achieved.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nA lot of our favorite VR experiences are short and capitalize on a fun central mechanic. We wanted to make an AR game that could capitalize on the \"room mesh\" generated by spacial computing platforms in the AR space to make fun interactions. \"Line Rider\" came to mind as a browser game in which manipulating space generated a fun, user-led interaction. The user would be able to spread snow over the scanned surfaces to create slopes for the character to sled down. The interaction could be extended to create conversations about climate change by having the \"pickups\" that the user tries to steer the character into as plastic recyclables and other items/actions associated with positive environmental impact (eg. planting trees).\n\n\n## What it does\n\n\nYou can use the Magic Leap controller to teleport a character on a sled in front of you; the character will sled down the slopes generated by the mesh\n\n\n## How I built\n\n\nThe character is physically simulated using a ball collider and rigidbody in Unity, and the sled and character are rotated using spherical linear interpolation to match the rotation fitting the normal vector to the ground they are sledding on and the velocity vector of the rigid body. The collider has a physics material with very low friction. We created a shader for the Lightweight/Universal Render Pipeline, but had to scrap it and restart it after learning that there is not currently support for the pipeline - unfortunately this set us back from accomplishing the narrative we aimed for. The result is a fun AR toy, with which users can try to extend the path of the sled by creating stacks and slopes out of real-life objects.\n\n\n## Challenges I ran into\n\n\nProgramming the snowfall shader and then attempting to reprogramthe snowfall shader outside of Shadergraph proved difficult for our team. We also may have benefited from more help from the Magic Leap team early on, as a misconception about the development pipeline prevented us from moving forward with the project for a lot of the hackathon. In the future we would like to leverage the availability of mentors more in learning skills from such projects.\n\n\n## Accomplishments that I'm proud of\n\n\nThe character's movement is fun to interact with, as the spherical collider makes it easy to predict its movement and its tendency to rotate towards the direction of movement gives the character more personality than a rolling ball. \n\n\n## What I learned\n\n\nWe learned about creating vertex shaders, as well as some other types of shaders, and about the Magic Leap API and implementation.\n\n\n## What's next for SledAR\n\n\nFinishing the snow piling mechanic is the main priority; then we would like to generate starting points and end points based on the room's collision mesh, as well as obstacles and pick-ups. These features could help create more of a narrative throughout gameplay rather than the more repetitive simple interactions we've achieved.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/mixr-e946oj",
            "title": "MiXR - Spatial Synthesizer",
            "blurb": "Merging the mundane and the abstract with spatial sound in VR.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/V_XULehu524?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "MiXR Studios logo.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/773/datas/original.png"
                },
                {
                    "title": "Another view.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/480/datas/original.png"
                },
                {
                    "title": "Objects in the space.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/479/datas/original.png"
                },
                {
                    "title": "Photo of the team who created this!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/502/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Melissa Schmitz",
                    "about": "I helped develop our initial concept, storyboards, and worked on the neon scene design in Unreal from scratch.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/656/015/datas/profile.jpeg"
                },
                {
                    "name": "Ryan Reede",
                    "about": "I helped w/ vizdev, art direction, realtime lighting and scene layout in UE4. I also modeled and textured some assets in Blender!",
                    "photo": "https://avatars0.githubusercontent.com/u/12736540?height=180&v=4&width=180"
                },
                {
                    "name": "Nachi Gargi",
                    "about": "I worked on the audio synthesis engines in Unreal Engine as well as VR interactions",
                    "photo": "https://www.gravatar.com/avatar/d3ca85b82feaf69742a77916d2ab101f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kevin King",
                    "about": "I worked on the Unreal Engine blueprints, VR input and audio interactions.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/527/265/datas/profile.jpg"
                },
                {
                    "name": "Danlu",
                    "about": "",
                    "photo": "https://graph.facebook.com/3391059957631884/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "blender",
                "steamvr",
                "unreal-engine"
            ],
            "content_html": "<div>\n<p>*** <strong>Put on headphones for the video! It is a spatial audio experience!</strong> ***</p>\n<h2>Abstract</h2>\n<p>With MiXR, we set out to use VR technology to create an acousmatic experience unlike anything the real world has to offer. Only through exploration and play do MiXR's underlying systems emerge. MiXR is the ultimate audio-visual synthesizer that maps the unique ways in which individuals move through space to a generative environment.</p>\n<p>With MiXR Studios, everyone is an artist.</p>\n<h2>Design Inspiration</h2>\n<p>We set out to allow people to experience music in an abstract way rather than using 3D representations of real instrument shapes (like floating disks to look like a drum pad, or a set of rectangular keys for a keyboard). Some of our favorite games, such as Beat Saber, allow you to enjoy music through movement. We wanted movement to be at the core of how you explore the sound.</p>\n<p>For the visuals, we wanted to create a world that visually represented the electronica sound of the synthesizer through a vaporwave 80s gamer theme. You start the game within a gamer space where you can interact with the objects in the room, creating sound with them in ways you wouldn't expect, effectively orchestrating a blend between the \"real\" and abstract worlds.</p>\n<h2>What it does</h2>\n<p>Users are placed in a neon-lit studio and are encouraged to interact with objects in the scene. Movement and interaction with each object creates a unique sound that is meant to invoke the feel of the object. </p>\n<h2>How We built it</h2>\n<p>We chose Unreal Engine for the graphics fidelity and the ease of using Blueprints to create logic. Blender was used to create some of the 3D models while some others were either default StarterContent assets from Epic Games or free from CGTrader. Lighting, room layout, and overall design of the space was original. Some audio assets were created in Ableton Live. The full scene composition, however, was built from scratch.</p>\n<p>MiXR builds on the new audio engine introduced in Unreal Engine 4.19 which supports modular and granular synthesis. With full body tracking, we are able to map the entire user's body to parameters of the synthesizer, as well as control shader effects to bring the audio to life.</p>\n<h2>Challenges We ran into</h2>\n<p>Most of us were using Unreal Engine for the first time ever! So we stumbled a bit at first getting things to work. But with the amazing guidance of mentors and a bit of persistence, we made it work!</p>\n<h2>Accomplishments that We're proud of</h2>\n<p>Learning Unreal, custom neon glowing art (without the performance issues!), real-time audio interaction, hand-designed feet, algorithmic chord composition, generative audio, and surviving Git merges.</p>\n<h2>What We learned</h2>\n<p>Most of us were just learning Unreal for the first time, but in particular Blueprints, scene rendering, prepping assets for import, creating emissive materials instead of adding more light to improve VR FPS, editing on sublevels, etc.</p>\n<h2>What's next for Mixr</h2>\n<p>Different environments to match new soundscapes (e.g., greenhouse for meditative, stage for rock, etc.), even more object interactions to modulate sound, and a multiplayer experience.</p>\n</div>",
            "content_md": "\n*** **Put on headphones for the video! It is a spatial audio experience!** ***\n\n\n## Abstract\n\n\nWith MiXR, we set out to use VR technology to create an acousmatic experience unlike anything the real world has to offer. Only through exploration and play do MiXR's underlying systems emerge. MiXR is the ultimate audio-visual synthesizer that maps the unique ways in which individuals move through space to a generative environment.\n\n\nWith MiXR Studios, everyone is an artist.\n\n\n## Design Inspiration\n\n\nWe set out to allow people to experience music in an abstract way rather than using 3D representations of real instrument shapes (like floating disks to look like a drum pad, or a set of rectangular keys for a keyboard). Some of our favorite games, such as Beat Saber, allow you to enjoy music through movement. We wanted movement to be at the core of how you explore the sound.\n\n\nFor the visuals, we wanted to create a world that visually represented the electronica sound of the synthesizer through a vaporwave 80s gamer theme. You start the game within a gamer space where you can interact with the objects in the room, creating sound with them in ways you wouldn't expect, effectively orchestrating a blend between the \"real\" and abstract worlds.\n\n\n## What it does\n\n\nUsers are placed in a neon-lit studio and are encouraged to interact with objects in the scene. Movement and interaction with each object creates a unique sound that is meant to invoke the feel of the object. \n\n\n## How We built it\n\n\nWe chose Unreal Engine for the graphics fidelity and the ease of using Blueprints to create logic. Blender was used to create some of the 3D models while some others were either default StarterContent assets from Epic Games or free from CGTrader. Lighting, room layout, and overall design of the space was original. Some audio assets were created in Ableton Live. The full scene composition, however, was built from scratch.\n\n\nMiXR builds on the new audio engine introduced in Unreal Engine 4.19 which supports modular and granular synthesis. With full body tracking, we are able to map the entire user's body to parameters of the synthesizer, as well as control shader effects to bring the audio to life.\n\n\n## Challenges We ran into\n\n\nMost of us were using Unreal Engine for the first time ever! So we stumbled a bit at first getting things to work. But with the amazing guidance of mentors and a bit of persistence, we made it work!\n\n\n## Accomplishments that We're proud of\n\n\nLearning Unreal, custom neon glowing art (without the performance issues!), real-time audio interaction, hand-designed feet, algorithmic chord composition, generative audio, and surviving Git merges.\n\n\n## What We learned\n\n\nMost of us were just learning Unreal for the first time, but in particular Blueprints, scene rendering, prepping assets for import, creating emissive materials instead of adding more light to improve VR FPS, editing on sublevels, etc.\n\n\n## What's next for Mixr\n\n\nDifferent environments to match new soundscapes (e.g., greenhouse for meditative, stage for rock, etc.), even more object interactions to modulate sound, and a multiplayer experience.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/blinkenlights",
            "title": "Blinkenlights",
            "blurb": "AR assistant to improve quality of life for those affected with Alzheimer's Disease. ",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Intro Screen",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/514/datas/original.png"
                },
                {
                    "title": "Voice Memo Good Morning Message",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/515/datas/original.png"
                },
                {
                    "title": "Temporal Reminder: Medication",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/513/datas/original.png"
                },
                {
                    "title": "Identifying Faces",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/520/datas/original.png"
                },
                {
                    "title": "Facial Recognition",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/516/datas/original.png"
                },
                {
                    "title": "Person Identified + Memory Recall",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/519/datas/original.png"
                },
                {
                    "title": "Open Reminder Platform for Family",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/512/datas/original.png"
                },
                {
                    "title": "Compilation of Image Assets",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/518/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Vignesh Ravichandran",
                    "about": "I worked on applying face recognition on board the magic leap but was only able to get face detection done by the end. ",
                    "photo": "https://www.gravatar.com/avatar/7619e7870e49eca3cac261be4774dc9e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Max Orozco",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/13499370?height=180&v=4&width=180"
                },
                {
                    "name": "Eliza Li",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGBr34k1UIkMw/profile-displayphoto-shrink_100_100/0?e=1548288000&height=180&t=FiQB0jyqIPJ5w1668ElEXcqReDrMJWHtG5WHNSdKDlk&v=beta&width=180"
                },
                {
                    "name": "Erin Mittmann",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C4E03AQFB1n753t8g_g/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=Yy8PGB1xtZyaDEEQHB9XCEHfS1he4fPlIKrRERAxLOI&v=beta&width=180"
                },
                {
                    "name": "Lewis Gardner",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/578/datas/profile.png"
                }
            ],
            "built_with": [
                "c#",
                "opencv",
                "unity"
            ],
            "content_html": "<div>\n<h1>Blinkenlights</h1>\n<p>Alzheimer's Disease (AD) is the leading cause of cognitive decline in the western world, affecting a third of Americans over age 85. The disease slowly degrades memory and cognitive skills, hindering the ability to carry out daily activities, inevitably leading to full-time care and death. </p>\n<p>Blinkenlights is a AR assistant that accompanies individuals with AD throughout their disease progression to have a better quality of life by promoting independence, assisting with memory recall and encouraging social interaction. </p>\n<h2>Features</h2>\n<h4>Temporal &amp; Spatial Reminder System</h4>\n<p>Provides users with temporal or location-tagged reminders to assist with day-to-day activities.</p>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72685301-d463f180-3ab6-11ea-865f-eccab43f8ec9.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--mTQ8I8dz--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685301-d463f180-3ab6-11ea-865f-eccab43f8ec9.png\"/></p>\n<h4>Facial Recognition &amp; Memory Recall (Reminiscence)</h4>\n<p>Uses facial recognition to help users identify stored people and pulls up shared videos and images for user to review and trigger memory recall of relationship. </p>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72685304-d463f180-3ab6-11ea-8765-c56315decf74.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--lRh8dQcJ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685304-d463f180-3ab6-11ea-8765-c56315decf74.png\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72685986-b483fc00-3abd-11ea-8a5c-ecd4dc0e3283.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--APqXXyPo--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685986-b483fc00-3abd-11ea-8a5c-ecd4dc0e3283.png\"/></p>\n<h4>Engagement platform for team of caretakers</h4>\n<p>Provide platform for remote family members and caretakers to engage in care for the individual by sending reminders, adding events to calendar and send voice memos, and monitor user's task progress.</p>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72685367-7aaff700-3ab7-11ea-88cb-274eb7abe5d7.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--hW2Ek6aJ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685367-7aaff700-3ab7-11ea-88cb-274eb7abe5d7.png\"/></p>\n<h4>40 Hz Therapy</h4>\n<p>State-of-art research in Alzheimer's disease has shown that emitting light and playing sound at 40 Hz frequency can drastically reduce beta-amyloid plaques in various areas of the brain in mice models of Alzheimer's Disease, thereby improving ability to perform memory-related tasks. Although findings have not yet shown proven that it is effective in humans, we incorporated this feature to the end-of-day routine as a way to explore the efficacy of this research finding. </p>\n<p>Read below for relevant information on research findings:\n<a href=\"http://news.mit.edu/2019/brain-wave-stimulation-improve-alzheimers-0314\" rel=\"nofollow\">http://news.mit.edu/2019/brain-wave-stimulation-improve-alzheimers-0314</a></p>\n<h2>Dependencies</h2>\n<p>The following dependencies are required.</p>\n<ul>\n<li><a href=\"https://www.magicleap.com/news/product-updates/lumin-os-0-97-and-sdk-0-22\" rel=\"nofollow\">Lumin SDK with Lumin OS 0.97</a></li>\n<li><a href=\"https://unity3d.com/unity/beta/2019.2.0b10\" rel=\"nofollow\">Unity 2019.2.0b10</a></li>\n<li><a href=\"https://assetstore.unity.com/packages/tools/integration/opencv-for-unity-21088?aid=1011l4ehR\" rel=\"nofollow\">OpenCV for Unity</a></li>\n<li><a href=\"https://assetstore.unity.com/packages/tools/integration/opencv-plus-unity-85928\" rel=\"nofollow\">OpenCV + Unity (Free asset for recognition)</a></li>\n</ul>\n<h3>Installation procedure for OpenCV for Unity with Magic Leap</h3>\n<ol>\n<li>Create a new project in Unity 2019.2.0b10 </li>\n<li>Import the v0.22 MagicLeap SDK </li>\n<li>Allow unsafe code to enable use of OpenCV assemblies </li>\n<li>Import the OpenCV for Unity package from asset store into your project</li>\n<li>Complete setup for OpenCV for Unity and Set Plugin Import Settings on the Tools menu under OpenCV for Unity</li>\n<li>Move \u201cOpenCVForUnity/StreamingAssets/\u201d folder to the Unity's program's \u201cAssets/\u201d folder</li>\n<li>Provide privileges for camera usage, microphone usage and Computer Vision usage on the LuminSDK publishing settings</li>\n<li>Ensure the front camera lights are lit up to ensure camera usage is functional</li>\n</ol>\n<h2>Research</h2>\n<p>We conducted secondary research and interviewed 3 family members of Alzheimer patients, along with two experts (neurologist &amp; PhD candidate in Gerontology and Dementia studies) to better understand the problem space and identify user needs.</p>\n<h3>Interview Questions</h3>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684826-f444e680-3ab1-11ea-8ace-01239664d153.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--pId5ef-p--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684826-f444e680-3ab1-11ea-8ace-01239664d153.jpg\"/></p>\n<h3>Key Findings</h3>\n<ol>\n<li>Memory loss interferes with daily tasks such as medication compliance, turning off burner, and self care \n&gt; \"She would...forget how many pills to take and so we had to monitor the pills\" - <em>S.O. on her mother</em>\n<br/></li>\n</ol>\n<blockquote>\n<p>\"Hard time with personal care/grooming\" - <em>M.T. on his father</em></p>\n</blockquote>\n<ol>\n<li>Social interaction slows down disease progression and helps the patients feel happier\n&gt; \"Loneliness is a risk factor for depression which can cause a quicker decline. They would not take as good care of themselves if they're also experiencing depression. There is total loss of motivation to do so.\" - _AS, Corporate Director of Dementia Programs at Tutera Senior Living and Health Care _</li>\n</ol>\n<p><br/></p>\n<blockquote>\n<p>\"More visits from family, people who knew him and loved him [would help him feel happier and independent]\" - <em>M.T. on his father</em></p>\n</blockquote>\n<ol>\n<li>Photos and sounds are helpful in reminiscence therapy to help patients trigger memories\n&gt; \"We really rely on their family photos. A lot of times, people will look at photos and go through them to make sure they still remember in early stages. Photos are good at sparking memory.\" - _A.S.</li>\n</ol>\n<h2>Process</h2>\n<h3>Brainstorming</h3>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684825-f3ac5000-3ab1-11ea-9bc1-72e2a7af8e56.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--VH_Kr02_--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684825-f3ac5000-3ab1-11ea-9bc1-72e2a7af8e56.jpg\"/></p>\n<h3>Problem Statement</h3>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684821-f3ac5000-3ab1-11ea-9910-42a5ff944c45.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--blaTVb6H--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684821-f3ac5000-3ab1-11ea-9910-42a5ff944c45.jpg\"/></p>\n<h3>Journey Mapping</h3>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684822-f3ac5000-3ab1-11ea-9068-0aecbc7b5c82.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--HJKaPky4--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684822-f3ac5000-3ab1-11ea-9068-0aecbc7b5c82.jpg\"/></p>\n<h3>User Personas + Flows</h3>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684824-f3ac5000-3ab1-11ea-916f-7400959087c4.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--Pmo2pRT_--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684824-f3ac5000-3ab1-11ea-916f-7400959087c4.jpg\"/>\n<img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72684823-f3ac5000-3ab1-11ea-982f-284a3a5b3238.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s---chVE5M4--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684823-f3ac5000-3ab1-11ea-982f-284a3a5b3238.jpg\"/></p>\n<h2>Design Elements</h2>\n<p><img alt=\"\" data-canonical-url=\"https://user-images.githubusercontent.com/17633019/72685471-60c2e400-3ab8-11ea-8d6b-d4ddb4be2d93.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--dsks7BgE--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685471-60c2e400-3ab8-11ea-8d6b-d4ddb4be2d93.png\"/></p>\n<h3>Disclosure about the use of the OpenCV for Unity paid asset:</h3>\n<p>The use of the paid asset was permitted by organizers and Magic Leap team to help us efficiently bootstrap the development of the facial detection/recognition module in our project at 10pm EST on 18th Jan 2020 due to significant difficulties the teams faced which trying to include OpenCV with magic leap. </p>\n</div>",
            "content_md": "\n# Blinkenlights\n\n\nAlzheimer's Disease (AD) is the leading cause of cognitive decline in the western world, affecting a third of Americans over age 85. The disease slowly degrades memory and cognitive skills, hindering the ability to carry out daily activities, inevitably leading to full-time care and death. \n\n\nBlinkenlights is a AR assistant that accompanies individuals with AD throughout their disease progression to have a better quality of life by promoting independence, assisting with memory recall and encouraging social interaction. \n\n\n## Features\n\n\n#### Temporal & Spatial Reminder System\n\n\nProvides users with temporal or location-tagged reminders to assist with day-to-day activities.\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--mTQ8I8dz--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685301-d463f180-3ab6-11ea-865f-eccab43f8ec9.png)\n\n\n#### Facial Recognition & Memory Recall (Reminiscence)\n\n\nUses facial recognition to help users identify stored people and pulls up shared videos and images for user to review and trigger memory recall of relationship. \n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--lRh8dQcJ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685304-d463f180-3ab6-11ea-8765-c56315decf74.png)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--APqXXyPo--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685986-b483fc00-3abd-11ea-8a5c-ecd4dc0e3283.png)\n\n\n#### Engagement platform for team of caretakers\n\n\nProvide platform for remote family members and caretakers to engage in care for the individual by sending reminders, adding events to calendar and send voice memos, and monitor user's task progress.\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--hW2Ek6aJ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685367-7aaff700-3ab7-11ea-88cb-274eb7abe5d7.png)\n\n\n#### 40 Hz Therapy\n\n\nState-of-art research in Alzheimer's disease has shown that emitting light and playing sound at 40 Hz frequency can drastically reduce beta-amyloid plaques in various areas of the brain in mice models of Alzheimer's Disease, thereby improving ability to perform memory-related tasks. Although findings have not yet shown proven that it is effective in humans, we incorporated this feature to the end-of-day routine as a way to explore the efficacy of this research finding. \n\n\nRead below for relevant information on research findings:\n<http://news.mit.edu/2019/brain-wave-stimulation-improve-alzheimers-0314>\n\n\n## Dependencies\n\n\nThe following dependencies are required.\n\n\n* [Lumin SDK with Lumin OS 0.97](https://www.magicleap.com/news/product-updates/lumin-os-0-97-and-sdk-0-22)\n* [Unity 2019.2.0b10](https://unity3d.com/unity/beta/2019.2.0b10)\n* [OpenCV for Unity](https://assetstore.unity.com/packages/tools/integration/opencv-for-unity-21088?aid=1011l4ehR)\n* [OpenCV + Unity (Free asset for recognition)](https://assetstore.unity.com/packages/tools/integration/opencv-plus-unity-85928)\n\n\n### Installation procedure for OpenCV for Unity with Magic Leap\n\n\n1. Create a new project in Unity 2019.2.0b10\n2. Import the v0.22 MagicLeap SDK\n3. Allow unsafe code to enable use of OpenCV assemblies\n4. Import the OpenCV for Unity package from asset store into your project\n5. Complete setup for OpenCV for Unity and Set Plugin Import Settings on the Tools menu under OpenCV for Unity\n6. Move \u201cOpenCVForUnity/StreamingAssets/\u201d folder to the Unity's program's \u201cAssets/\u201d folder\n7. Provide privileges for camera usage, microphone usage and Computer Vision usage on the LuminSDK publishing settings\n8. Ensure the front camera lights are lit up to ensure camera usage is functional\n\n\n## Research\n\n\nWe conducted secondary research and interviewed 3 family members of Alzheimer patients, along with two experts (neurologist & PhD candidate in Gerontology and Dementia studies) to better understand the problem space and identify user needs.\n\n\n### Interview Questions\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--pId5ef-p--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684826-f444e680-3ab1-11ea-8ace-01239664d153.jpg)\n\n\n### Key Findings\n\n\n1. Memory loss interferes with daily tasks such as medication compliance, turning off burner, and self care \n> \"She would...forget how many pills to take and so we had to monitor the pills\" - *S.O. on her mother*\n\n\n\n> \n> \"Hard time with personal care/grooming\" - *M.T. on his father*\n> \n> \n> \n\n\n1. Social interaction slows down disease progression and helps the patients feel happier\n> \"Loneliness is a risk factor for depression which can cause a quicker decline. They would not take as good care of themselves if they're also experiencing depression. There is total loss of motivation to do so.\" - \\_AS, Corporate Director of Dementia Programs at Tutera Senior Living and Health Care \\_\n\n\n  \n\n\n\n\n> \n> \"More visits from family, people who knew him and loved him [would help him feel happier and independent]\" - *M.T. on his father*\n> \n> \n> \n\n\n1. Photos and sounds are helpful in reminiscence therapy to help patients trigger memories\n> \"We really rely on their family photos. A lot of times, people will look at photos and go through them to make sure they still remember in early stages. Photos are good at sparking memory.\" - \\_A.S.\n\n\n## Process\n\n\n### Brainstorming\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--VH_Kr02_--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684825-f3ac5000-3ab1-11ea-9bc1-72e2a7af8e56.jpg)\n\n\n### Problem Statement\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--blaTVb6H--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684821-f3ac5000-3ab1-11ea-9910-42a5ff944c45.jpg)\n\n\n### Journey Mapping\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--HJKaPky4--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684822-f3ac5000-3ab1-11ea-9068-0aecbc7b5c82.jpg)\n\n\n### User Personas + Flows\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--Pmo2pRT_--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684824-f3ac5000-3ab1-11ea-916f-7400959087c4.jpg)\n![](https://res.cloudinary.com/devpost/image/fetch/s---chVE5M4--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72684823-f3ac5000-3ab1-11ea-982f-284a3a5b3238.jpg)\n\n\n## Design Elements\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--dsks7BgE--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://user-images.githubusercontent.com/17633019/72685471-60c2e400-3ab8-11ea-8d6b-d4ddb4be2d93.png)\n\n\n### Disclosure about the use of the OpenCV for Unity paid asset:\n\n\nThe use of the paid asset was permitted by organizers and Magic Leap team to help us efficiently bootstrap the development of the facial detection/recognition module in our project at 10pm EST on 18th Jan 2020 due to significant difficulties the teams faced which trying to include OpenCV with magic leap. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/progressivelyenhancedindoornavigationwebxr",
            "title": "ProgressivelyEnhancedIndoorNavigationWebXR",
            "blurb": "An accessible open-source indoor navigation solution that is platform independent based on universal design principles.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/60FAuQSH0tk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Keyboard accessible selection of buildings",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/422/datas/original.png"
                },
                {
                    "title": "Keyboard accessible selection looping through floors (children) of a building (parent)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/421/datas/original.png"
                },
                {
                    "title": "Single floor navigation grid with wayfinding examples",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/634/datas/original.png"
                },
                {
                    "title": "Single floor navigation grid WebXR comp birdseye view",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/635/datas/original.png"
                },
                {
                    "title": "Single floor navigation grid WebXR comp floor level view",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/636/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Roland Dubois",
                    "about": "WebXR/Aframe & Accessibility",
                    "photo": "https://avatars.githubusercontent.com/u/347570?height=180&v=3&width=180"
                },
                {
                    "name": "peter locharernkul",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/dc036e5a720307e1445cb9512f6df5d4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Hat Nguyen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e8cc9122dd96e41edfdf29fa1789149b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "aframe",
                "javascript",
                "unity",
                "webxr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Institutions that are government-funded like hospitals, research labs, educational facilities, and universities are currently unable to showcase their campuses/buildings in a three-dimensional and interactive way because there is no fallback for assistive technology. To make the navigation of 3D/XR accessible is a necessity by law due to strict ADA compliance guidelines.\nWe not only wanted to add richer context/user-relevant metadata to the ecosystem of a building but also made sure that we can navigate a 3-dimensional space with voiceover, 2D, 3D and XR.\nCurrently, the accessibility maps of institutions are mostly flat PDFs, with our universal design approach we are tagging buildings down to the single room level with user-relevant key data to make way-finding and navigation a user-centric experience.\nE.g. I am wheelchair bound so please only show me access to elevators and lifts when guiding me safely to my indoor destination. </p>\n<h2>What it does</h2>\n<p>A universal design approach of a building &gt; floor &gt; room selection and spatial navigation grid to guide users from \none indoor location to another safely and effectively.</p>\n<h2>How we built it</h2>\n<p>Using platform independence WebXR to build for 1D (voice or text), 2D, 3D, and VR devices\nPlanning in Whimsical and on a whiteboard. Software: Aframe/ThreeJS, Javascript, HMTL, Glitch, Unity/Vuforia SDK/Azure Anchor, C#, VoiceOver (Assistive Technology Screenreader). Hardware: Hololens 2, HTC Vive Focus, Cardboard VR/Mobile/Desktop, </p>\n<h2>Challenges we ran into</h2>\n<p>Time, hardware compatibility, and technical resources\nWebXR API implementation on device browsers is experimental and unstable.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Created an open-source platform and node-based (JSON) tree navigation system that could be a gateway to new layers of data to enrich the user experience for indoor navigation. Adding context relevant to individuals in relation to a living/changing 3-dimensional building ecosystem. Helping users to reach places safely and efficiently.  </p>\n<h2>What we learned</h2>\n<p>We managed our time well and got our big tasks completed. We learned how to use Azure anchors and setting up the Hololens dev environment.</p>\n<h2>What's next for ProgressivelyEnhancedIndoorNavigationWebXR</h2>\n<p>We will be writing a Medium post.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nInstitutions that are government-funded like hospitals, research labs, educational facilities, and universities are currently unable to showcase their campuses/buildings in a three-dimensional and interactive way because there is no fallback for assistive technology. To make the navigation of 3D/XR accessible is a necessity by law due to strict ADA compliance guidelines.\nWe not only wanted to add richer context/user-relevant metadata to the ecosystem of a building but also made sure that we can navigate a 3-dimensional space with voiceover, 2D, 3D and XR.\nCurrently, the accessibility maps of institutions are mostly flat PDFs, with our universal design approach we are tagging buildings down to the single room level with user-relevant key data to make way-finding and navigation a user-centric experience.\nE.g. I am wheelchair bound so please only show me access to elevators and lifts when guiding me safely to my indoor destination. \n\n\n## What it does\n\n\nA universal design approach of a building > floor > room selection and spatial navigation grid to guide users from \none indoor location to another safely and effectively.\n\n\n## How we built it\n\n\nUsing platform independence WebXR to build for 1D (voice or text), 2D, 3D, and VR devices\nPlanning in Whimsical and on a whiteboard. Software: Aframe/ThreeJS, Javascript, HMTL, Glitch, Unity/Vuforia SDK/Azure Anchor, C#, VoiceOver (Assistive Technology Screenreader). Hardware: Hololens 2, HTC Vive Focus, Cardboard VR/Mobile/Desktop, \n\n\n## Challenges we ran into\n\n\nTime, hardware compatibility, and technical resources\nWebXR API implementation on device browsers is experimental and unstable.\n\n\n## Accomplishments that we're proud of\n\n\nCreated an open-source platform and node-based (JSON) tree navigation system that could be a gateway to new layers of data to enrich the user experience for indoor navigation. Adding context relevant to individuals in relation to a living/changing 3-dimensional building ecosystem. Helping users to reach places safely and efficiently. \n\n\n## What we learned\n\n\nWe managed our time well and got our big tasks completed. We learned how to use Azure anchors and setting up the Hololens dev environment.\n\n\n## What's next for ProgressivelyEnhancedIndoorNavigationWebXR\n\n\nWe will be writing a Medium post.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/flockar-v26h7f",
            "title": "flockAR",
            "blurb": "Augmented design thinking ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/xvzfrLghMhk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Sabrina Naumovski",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/332/datas/profile.JPG"
                }
            ],
            "built_with": [
                "c#",
                "microsoft-hololens",
                "nreal",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>viral videos of cats</p>\n<h2>What it does</h2>\n<p>takes some plans, makes them 3d or something like that and shows you what a layout can be. </p>\n<h2>How I built it</h2>\n<p>Unity</p>\n<h2>Challenges I ran into</h2>\n<p>Hololens dev </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Failing and failing fast</p>\n<h2>What I learned</h2>\n<p>Baby steps </p>\n<h2>What's next for flockAR</h2>\n<p>Probably go to sleep, maybe some other dev shit on it later. tbd </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nviral videos of cats\n\n\n## What it does\n\n\ntakes some plans, makes them 3d or something like that and shows you what a layout can be. \n\n\n## How I built it\n\n\nUnity\n\n\n## Challenges I ran into\n\n\nHololens dev \n\n\n## Accomplishments that I'm proud of\n\n\nFailing and failing fast\n\n\n## What I learned\n\n\nBaby steps \n\n\n## What's next for flockAR\n\n\nProbably go to sleep, maybe some other dev shit on it later. tbd \n\n\n"
        },
        {
            "source": "https://devpost.com/software/pan-gu",
            "title": "Pan Gu",
            "blurb": "A fun experience allowing you to become a god who can manipulate the 5 elements: Metal, Water, Wood, Fire, Earth to create impressive reactions of the elements with massive GPU generated particles. ",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385868599?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Wei Dai",
                    "about": "I worked on all the interaction design (gestures, gesture-interactions, interactions between different particles), user experience design and the programming of instrumental control system.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/064/datas/profile.jpeg"
                },
                {
                    "name": "Dawn Nguyen",
                    "about": "I worked on the Storyline (initial Journey Map), brainstorming the interaction with Wei, created the LWRP Shaders for the 5 elements, designed the UI for the interaction tutorial, worked with Wei on the Sound Effects, and preparing content for our submission",
                    "photo": "https://avatars2.githubusercontent.com/u/15383891?height=180&v=4&width=180"
                },
                {
                    "name": "Rustin Wollin",
                    "about": "I implemented the fluid particle simulation, interactions between particles, and developed additional shaders",
                    "photo": "https://avatars2.githubusercontent.com/u/3959144?height=180&v=4&width=180"
                },
                {
                    "name": "julia biasi",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/95ef36f84ce5aa6e9ad43b457157d608?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "oculus-gear-vr",
                "photoshop",
                "tilt-brush",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The project is loosely based on the Chinese creation mythology, where the world is built by the giant, Pan Gu, with five traditional Chinese elements. We are also inspired by previous experiments with particle sandbox toys in the vein of the classic Falling Sand games.  We started out with a vision of creating a large scale, fun and impressive visual experience simulating physical particles or fluids. </p>\n<h2>What it does</h2>\n<p>It is a virtual playground where players play as Pan Gu, a progenitor god of ancient Chinese mythology, and use different gestures to manipulate massive amounts of GPU generated particles. There are 5 types of particles representing the 5 elements, different reactions occur when two different types of particles collide with each other. We also use players' hand-movement as an instrument to let them compose and play their own music while playing in VR. Using spheres as physical particles, the interaction of the user with the spheres causing their collision, and morphing from one element to another.</p>\n<h2>How we built it</h2>\n<p>We started developing our ideas by brainstorming and creating the experience journey map in Miro Board. The team also explored different interactions and sound effects to improve the immersive and engagement for the experience. With every interaction, we hope we can spark a positive emotion from our players. </p>\n<p>Wei focused on interaction design and experimenting different form of hand gestures and implemented them using Unity.</p>\n<p>Rustin converted a reference OpenGL implementation of SPH on the GPU into a version targeting Unity's Compute Shader API, then implemented a system for processing the gesture input data as part of the particle system update loop. Once these basic interactions were finished, more types of elements and interactions between elements were programmed to create the feeling of a dynamic environment where particles react to create interesting emergent interactions.</p>\n<p>Julia created 5 elements in Tilt Brush to populate in the environment. Dawn used Photoshop to design the Interaction Tutorial UI and used Shader Graph to create some materials under LWRP for the experience.</p>\n<h2>Challenges we ran into</h2>\n<p>Originally we were going to target Unity's Lightweight Render Pipeline, but ran into poor support for previewing scenes on Oculus Quest and had to abandon LWRP halfway through.\nMost of Saturday ended up being wasted on first a C# Job System implementation of SPH with improved spatial partitioning, and then a hybrid method where neighbor lists are calculated on the CPU and sent to the GPU every frame, but both methods actually ended up slower than a simple brute force implementation on the GPU!</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The interaction between different elements and the four gestures used to control these particles are insanely satisfying. We managed to have hall 5 types of particles and 4 ways of interaction working (Thanks to our programming powerhouse, Rustin). Our original design has really good outcomes. Another accomplishment we are proud of is that you can generate different beats and sound and compose your own music simply with your hand-movements. It really brings another layer of drama and emotion to the already awesome particle interactions.</p>\n<h2>What we learned</h2>\n<p>Implementing particles with limited lifetimes that can be removed from the GPU compute buffer at runtime, as well as particles that dynamically spawn other particles, originally seemed like it might be out of scope for a 2.5 day hackathon, but the functionality turned out to be fairly straightforward using an AppendStructuredBuffer.\nImplementing particle interactions once the basic physics simulation was working turned out to be deceptively simple and we saw some really interesting emergent behavior with relatively simple rules.</p>\n<h2>What's next for Pan Gu</h2>\n<p>Pan Gu will have the ability to create terrain, weather, ocean and animals out of the 5 elements.  Our player will also have the ability to teleport and move higher and far away to see their creation. Further than that, this will also be a multiplayer experience, where the other player will play as \"human\" and seeing how Pan Gu is manipulating the 5 elements in their world. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe project is loosely based on the Chinese creation mythology, where the world is built by the giant, Pan Gu, with five traditional Chinese elements. We are also inspired by previous experiments with particle sandbox toys in the vein of the classic Falling Sand games. We started out with a vision of creating a large scale, fun and impressive visual experience simulating physical particles or fluids. \n\n\n## What it does\n\n\nIt is a virtual playground where players play as Pan Gu, a progenitor god of ancient Chinese mythology, and use different gestures to manipulate massive amounts of GPU generated particles. There are 5 types of particles representing the 5 elements, different reactions occur when two different types of particles collide with each other. We also use players' hand-movement as an instrument to let them compose and play their own music while playing in VR. Using spheres as physical particles, the interaction of the user with the spheres causing their collision, and morphing from one element to another.\n\n\n## How we built it\n\n\nWe started developing our ideas by brainstorming and creating the experience journey map in Miro Board. The team also explored different interactions and sound effects to improve the immersive and engagement for the experience. With every interaction, we hope we can spark a positive emotion from our players. \n\n\nWei focused on interaction design and experimenting different form of hand gestures and implemented them using Unity.\n\n\nRustin converted a reference OpenGL implementation of SPH on the GPU into a version targeting Unity's Compute Shader API, then implemented a system for processing the gesture input data as part of the particle system update loop. Once these basic interactions were finished, more types of elements and interactions between elements were programmed to create the feeling of a dynamic environment where particles react to create interesting emergent interactions.\n\n\nJulia created 5 elements in Tilt Brush to populate in the environment. Dawn used Photoshop to design the Interaction Tutorial UI and used Shader Graph to create some materials under LWRP for the experience.\n\n\n## Challenges we ran into\n\n\nOriginally we were going to target Unity's Lightweight Render Pipeline, but ran into poor support for previewing scenes on Oculus Quest and had to abandon LWRP halfway through.\nMost of Saturday ended up being wasted on first a C# Job System implementation of SPH with improved spatial partitioning, and then a hybrid method where neighbor lists are calculated on the CPU and sent to the GPU every frame, but both methods actually ended up slower than a simple brute force implementation on the GPU!\n\n\n## Accomplishments that we're proud of\n\n\nThe interaction between different elements and the four gestures used to control these particles are insanely satisfying. We managed to have hall 5 types of particles and 4 ways of interaction working (Thanks to our programming powerhouse, Rustin). Our original design has really good outcomes. Another accomplishment we are proud of is that you can generate different beats and sound and compose your own music simply with your hand-movements. It really brings another layer of drama and emotion to the already awesome particle interactions.\n\n\n## What we learned\n\n\nImplementing particles with limited lifetimes that can be removed from the GPU compute buffer at runtime, as well as particles that dynamically spawn other particles, originally seemed like it might be out of scope for a 2.5 day hackathon, but the functionality turned out to be fairly straightforward using an AppendStructuredBuffer.\nImplementing particle interactions once the basic physics simulation was working turned out to be deceptively simple and we saw some really interesting emergent behavior with relatively simple rules.\n\n\n## What's next for Pan Gu\n\n\nPan Gu will have the ability to create terrain, weather, ocean and animals out of the 5 elements. Our player will also have the ability to teleport and move higher and far away to see their creation. Further than that, this will also be a multiplayer experience, where the other player will play as \"human\" and seeing how Pan Gu is manipulating the 5 elements in their world. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/catch-me-if-you-can-ygbl0c",
            "title": "Catch Me If You Can",
            "blurb": "You are abducted by Kelp of the Kelpies, a race that lives in a realm parallel to our universe. The only way to get back home is by catching him within a crowd of other Kelpies",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Z0dSNSujWdk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Salma Aldawood",
                    "about": "I worked on the design and implementation of the main character on the game including the random generator of its different variation. As well as helping on implementing the control pointer to interact with game objects through magic leap tools.It was my first time ever using unity and Magic leap but it was an exciting experience.",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GhSl2Ck32iJdAdTbzXo2GIH9C2OoZs6NsET3H-GCb8=s96-c?height=180&width=180"
                },
                {
                    "name": "ShahadThobaiti",
                    "about": "I worked on configuring unity and magic leap. Adding spatial functionality to the game and editing the intro functions. It was my first time dealing with AR systems and Unity. ",
                    "photo": "https://avatars0.githubusercontent.com/u/35901600?height=180&v=4&width=180"
                },
                {
                    "name": "Tariq4495",
                    "about": "I worked on creating different 3D Models for multiple cultures to be distributed in our AR plain. Helped on some of the graphics and visuals work.",
                    "photo": "https://avatars2.githubusercontent.com/u/60040917?height=180&v=4&width=180"
                },
                {
                    "name": "Zeyad Al Awwad",
                    "about": "I mainly focused on the NPC swarm behavior and game manager, plus some work on integrating Magic Leap features like spatial mapping and user controls.",
                    "photo": "https://www.gravatar.com/avatar/2a6e2712e9e56a6c73c7e1824a79750a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Arwa Alsaati",
                    "about": "I worked on the design and graphics integrated in the project. I helped with some of the basic coding in C# and Unity. I created the introduction and ending of the game.",
                    "photo": "https://avatars2.githubusercontent.com/u/36837528?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "magic-leap",
                "magicleap",
                "photoshop",
                "sketch",
                "sketchup",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The game starts out with the player being captured by Kelp of the Kelpies, a species that lives in parallel to our own universe (the virtual one). Even though you can't see them in our world, they share the same space as us. Kelp has abducted you into their world, and the only way to return back to your world is by capturing him! Hence, the name \"Catch Me if You Can\". The message we are trying to send is based on the idea that just as we and the Kelpies are diverse in races and ethnicities, we all live in the same space and environment. Whether virtual or physical, everyone's presence is welcomed.</p>\n<h2>What it does</h2>\n<p>The game is built around tracking down Kelp, a unique NPC, from a large swarm of Kelpies (between 100-1000) as they wander around a large area of the room (approximately 10mx10m in the final version). You can grab Kelpies by pointing at them and pressing the trigger. You win the game when you can successfully grab Kelp. </p>\n<h2>How I built it</h2>\n<p>We used a lot of Unity features to help with development, especially their built-in navigation and collision tools. We created our own stochastic algorithm to control the swarm behavior and randomize NPC appearance. To use Magic Leap's display and controller, we used a lot of the official tutorials and sample scripts.</p>\n<h2>Challenges I ran into</h2>\n<p>All of us were beginners in Unity, XR and C# so we faced a significant learning curve. Our first day was spent getting familiar with the software and we didn't start the actual implementation until Saturday. </p>\n<p>Most of our time was spent learning how to use Magic Leap and implementing interactive controls, so we didn't include most of the features we planned.</p>\n<p>We also ran into several technical issues with cryptic error messages, including one related to the debug scripts.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We learned a lot about Unity and implementing cool features like mixed reality and spatial controls. We are really excited to work on more XR software in the future and feel prepared to tackle more ambitious projects in the future. </p>\n<h2>What I learned</h2>\n<p>We learned how to use Unity and programming XR visuals/controls with Magic Leap. We've become much more familiar with the capabilities of these technologies and what works. </p>\n<h2>What's next for Catch Me If You Can</h2>\n<p>We have a long list of features we wanted to include! We were interested in incorporating more mixed reality features, allowing characters to navigate around real-world obstacles and placing virtual buildings throughout the room. We also wanted to include more sophisticated AI behavior and spatial audio to help track the target. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe game starts out with the player being captured by Kelp of the Kelpies, a species that lives in parallel to our own universe (the virtual one). Even though you can't see them in our world, they share the same space as us. Kelp has abducted you into their world, and the only way to return back to your world is by capturing him! Hence, the name \"Catch Me if You Can\". The message we are trying to send is based on the idea that just as we and the Kelpies are diverse in races and ethnicities, we all live in the same space and environment. Whether virtual or physical, everyone's presence is welcomed.\n\n\n## What it does\n\n\nThe game is built around tracking down Kelp, a unique NPC, from a large swarm of Kelpies (between 100-1000) as they wander around a large area of the room (approximately 10mx10m in the final version). You can grab Kelpies by pointing at them and pressing the trigger. You win the game when you can successfully grab Kelp. \n\n\n## How I built it\n\n\nWe used a lot of Unity features to help with development, especially their built-in navigation and collision tools. We created our own stochastic algorithm to control the swarm behavior and randomize NPC appearance. To use Magic Leap's display and controller, we used a lot of the official tutorials and sample scripts.\n\n\n## Challenges I ran into\n\n\nAll of us were beginners in Unity, XR and C# so we faced a significant learning curve. Our first day was spent getting familiar with the software and we didn't start the actual implementation until Saturday. \n\n\nMost of our time was spent learning how to use Magic Leap and implementing interactive controls, so we didn't include most of the features we planned.\n\n\nWe also ran into several technical issues with cryptic error messages, including one related to the debug scripts.\n\n\n## Accomplishments that I'm proud of\n\n\nWe learned a lot about Unity and implementing cool features like mixed reality and spatial controls. We are really excited to work on more XR software in the future and feel prepared to tackle more ambitious projects in the future. \n\n\n## What I learned\n\n\nWe learned how to use Unity and programming XR visuals/controls with Magic Leap. We've become much more familiar with the capabilities of these technologies and what works. \n\n\n## What's next for Catch Me If You Can\n\n\nWe have a long list of features we wanted to include! We were interested in incorporating more mixed reality features, allowing characters to navigate around real-world obstacles and placing virtual buildings throughout the room. We also wanted to include more sophisticated AI behavior and spatial audio to help track the target. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/climate-control",
            "title": "Climate Control",
            "blurb": "An immersive VR experience offers a story of hope for our shared future on Earth.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385867862?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Go solar!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/449/datas/original.png"
                },
                {
                    "title": "Inner peace",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/450/datas/original.png"
                },
                {
                    "title": "Permaculture fields",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/343/datas/original.png"
                },
                {
                    "title": "Terra and Auccu",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/373/datas/original.png"
                },
                {
                    "title": "Team Climate Control",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/384/datas/original.jpg"
                },
                {
                    "title": "Hacking af",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/445/datas/original.jpg"
                },
                {
                    "title": "In action",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/446/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "The Bad Lament",
                    "about": "I worked as art director, created assets using Tilt Brush, developing the story and also in Unity placing assets and helping debug. ",
                    "photo": "https://graph.facebook.com/10156556737886644/picture?height=180&width=180"
                },
                {
                    "name": "Marlon Romero",
                    "about": "I work in Autodesk Maya and Unity 3D modeling, rigging some assets and the Characters (Terra and Auccu)",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/505/255/datas/profile.jpg"
                },
                {
                    "name": "Chip Parrish",
                    "about": "I designed the user experience and environment, composed and produced original music, collaborated on script writing and recording, and generated assets in Tilt Brush.",
                    "photo": "https://avatars1.githubusercontent.com/u/3925595?height=180&v=4&width=180"
                },
                {
                    "name": "Daniela Aguilar",
                    "about": "I worked with some doing some designs using till brush and free sources like Sketchfab, storytelling, translations, and was used as a character.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/091/569/datas/profile.JPG"
                },
                {
                    "name": "Adam Sauer",
                    "about": "I worked in Unity, bringing the assets and objects together to build the overall scenes",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/544/762/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "eye",
                "glsl",
                "hlsl",
                "htc",
                "javascript",
                "maya",
                "mixamo",
                "pro",
                "shaderlab",
                "tiltbrush",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Upon the principle of doing more with less, Buckminster Fuller designed the geodesic dome, a system that maximizes the structural integrity of a contained, hemispherical volume with minimal material resources organized in triangular patterns. Under domes of glass and steel, artificial biospheres have been cultivated to shelter and sustain life in otherwise inhospitable environments. Our changing climate implores us to reflect on the consequences of our shared impact on the living Earth system and how our grandchildren might fare in the latter half of the 21st century.</p>\n<h2>What it does</h2>\n<p>Climate Control delivers a hopeful story about the resilience of life on Earth to survive, despite the effects of our anthropogenic pollution, or as Fuller framed it: \"nothing but the resources we are not harvesting.\" We must venture beyond only behavioral changes at the individual level and must become stewards of our planet on a societal level now to prevent potential futures of societal collapse from coming to fruition.</p>\n<h2>How we built it</h2>\n<p>Unity/C#, Tilt Brush, Google Poly, Maya, Visual Studio, HTC Vive Pro Eye, Bellus 3D, Adobe Mixamo, Android Voice Recorder, Mixcraft Studio 9, Akai Professional MPK Mini MKII, Freesound.</p>\n<p>Everything in the Biosphere is replicated to focus on real life solutions, used in places like Biosphere 2 in Arizona and how life can be sustainable with the right resources in a climate controlled environment.   </p>\n<h2>Challenges we ran into</h2>\n<p>We began development with Quest then pivoted half way through to Vive Pro Eye. We also recorded both English and Spanish dubs as well as translated text of our script to implement closed captioning for users who are deaf or hard of hearing. We imported a package developed by another group to display these closed captions in our experience and plan to implement this in a future build as ran out of time during the hackathon. We also ran out of time to implement the character animations and create a camera timeline for the piece, so a teleport option was implemented for judging. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Writing a plot that constructs a cohesive, compelling narrative for our story to build our world and tell our story. \nDesigning the environment and building an immersive world that offers a glimmer of hope.\nCapturing in 3D two of our team members faces to use as the characters for the narrative. </p>\n<h2>What we learned</h2>\n<p>By our powers combined, we can create a vision for a better world.</p>\n<h2>What's next for Climate Control</h2>\n<p>Our team plans to seek opportunities to further develop and demo this project at festivals around the world to share our story.</p>\n<p>GPL-3.0-or-later</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nUpon the principle of doing more with less, Buckminster Fuller designed the geodesic dome, a system that maximizes the structural integrity of a contained, hemispherical volume with minimal material resources organized in triangular patterns. Under domes of glass and steel, artificial biospheres have been cultivated to shelter and sustain life in otherwise inhospitable environments. Our changing climate implores us to reflect on the consequences of our shared impact on the living Earth system and how our grandchildren might fare in the latter half of the 21st century.\n\n\n## What it does\n\n\nClimate Control delivers a hopeful story about the resilience of life on Earth to survive, despite the effects of our anthropogenic pollution, or as Fuller framed it: \"nothing but the resources we are not harvesting.\" We must venture beyond only behavioral changes at the individual level and must become stewards of our planet on a societal level now to prevent potential futures of societal collapse from coming to fruition.\n\n\n## How we built it\n\n\nUnity/C#, Tilt Brush, Google Poly, Maya, Visual Studio, HTC Vive Pro Eye, Bellus 3D, Adobe Mixamo, Android Voice Recorder, Mixcraft Studio 9, Akai Professional MPK Mini MKII, Freesound.\n\n\nEverything in the Biosphere is replicated to focus on real life solutions, used in places like Biosphere 2 in Arizona and how life can be sustainable with the right resources in a climate controlled environment. \n\n\n## Challenges we ran into\n\n\nWe began development with Quest then pivoted half way through to Vive Pro Eye. We also recorded both English and Spanish dubs as well as translated text of our script to implement closed captioning for users who are deaf or hard of hearing. We imported a package developed by another group to display these closed captions in our experience and plan to implement this in a future build as ran out of time during the hackathon. We also ran out of time to implement the character animations and create a camera timeline for the piece, so a teleport option was implemented for judging. \n\n\n## Accomplishments that we're proud of\n\n\nWriting a plot that constructs a cohesive, compelling narrative for our story to build our world and tell our story. \nDesigning the environment and building an immersive world that offers a glimmer of hope.\nCapturing in 3D two of our team members faces to use as the characters for the narrative. \n\n\n## What we learned\n\n\nBy our powers combined, we can create a vision for a better world.\n\n\n## What's next for Climate Control\n\n\nOur team plans to seek opportunities to further develop and demo this project at festivals around the world to share our story.\n\n\nGPL-3.0-or-later\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ecstasis",
            "title": "Ecstasis",
            "blurb": "Through AR and VR, Ecstasis reinforces social connection between therapists and patients under palliative and hospice care.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/i3d5ENwf5cE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Peter Iordanov",
                    "about": "I setup and built the UI on the Project North Star headset, and wrote scripts to map animations to be controlled by Leap Motion UI elements",
                    "photo": "https://avatars.githubusercontent.com/u/5401480?height=180&v=3&width=180"
                },
                {
                    "name": "Ryan Weatherby",
                    "about": "Visual design and conceptualization - auxiliary motion graphics",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5603AQHoz5pxKuIHlw/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=Bqp8SMaNe7EskYqJonuDvl7sRE6sxveX1ZNipIQ2mUE&v=beta&width=180"
                },
                {
                    "name": "Mike Dopsa",
                    "about": "3D Modeling, Animation and sound design. \nAR and VR experience design, Programming for networking & Holographic Sensors.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/719/datas/profile.jpg"
                },
                {
                    "name": "masternader",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/41163985?height=180&v=4&width=180"
                },
                {
                    "name": "Simi Shenoy",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/272/datas/profile.jpg"
                }
            ],
            "built_with": [
                "google-poly",
                "leap-motion",
                "maya",
                "normcore",
                "oculus",
                "project-north-star",
                "real-sense",
                "rhinoceros",
                "steam",
                "tiltbrush",
                "unity"
            ],
            "content_html": "<div>\n<h2>Problem</h2>\n<p>According to WHO, each year an estimated 40 million people are in need of palliative and hospice care. The global need for this type of care will continue to grow as a result of the rising burden of noncommunicable diseases and ageing populations. Early palliative and hospice care reduces unnecessary hospital admissions and the use of health services. </p>\n<p>Palliative and hospice care refer to an interdisciplinary medical care-giving approach aimed at optimizing quality of life, mitigating suffering from the stress of terminal illness and providing comfort and care to patients and their families to acceptance and undergo a transformation of perception. </p>\n<p>Virtual Embodiment has been proven to be an effective clinical and therapeutic application of virtual reality as a therapy (Bourdin, P. 2017). But therein lies a problem: virtualization of patients by therapists using head-mounted virtual reality devices effectively diminishes the social connection between them. The strong bonds therapists spend months developing with their patients becomes threatened by a new care paradigm that deemphasizes human interaction in favor of \u201cdosed\u201d or \u201cautoplay\u201d therapy.</p>\n<p>More information on palliative care and its dynamic socio-economic challenges can be found here: <a href=\"https://www.who.int/news-room/fact-sheets/detail/palliative-care\" rel=\"nofollow\">https://www.who.int/news-room/fact-sheets/detail/palliative-care</a></p>\n<h2>Solution</h2>\n<p>Virtual reality (VR) and virtual embodiment (VE) have been demonstrated to be therapeutically effective tools for helping to cope with the reality of transitional palliative and hospice care (Bourdin, P. 2017). Current clinical therapy standards are confined to mainly provide relief by sedation, rarely indicating more access to support groups and professionals should the patient request it. Effective therapies for near end of life patients have been narrowing drastically. Virtual Reality and Virtual Embodiment provides a powerful vehicle for the transition into hospice care through immersion, the therapeutically viable engine that drives the heart of a convincing VR experience.</p>\n<p>Ecstasis (to be or stand outside oneself, a removal to elsewhere) is an immersive VR journey that reconnects the relationship between therapist and patient and reduces stress of patients in need of palliative care allowing them to cope with their illnesses with a more positive mindset.</p>\n<p>Based on a white paper that studies how \"fear of death was found to be lower in the experimental group that underwent virtual out-of-body experiences\" (OBE) (Bourdin,1), Ecstasis was designed as an OBE generator controlled by trusted therapists that would personalize the experience based on the patient\u2019s biofeedback.   </p>\n<p>Bourdin, P., Barberia, I., Oliva, R. and Slater, M. (2017). A Virtual Out-of-Body Experience Reduces Fear of Death.</p>\n<h2>What it does</h2>\n<p>Therapist takes a patient on a real-time immersive and responsive journey in VR. The clinician has access to a hand\u2019s free control system UI that allows them to set the environment, control movement and embody various digital assets to further enhance the relationship between therapist and patient. The intent of the experience is to take the patient on an immersive journey that results in an out of body (OBE) experience. It is through these immersive journeys, personally tailored by the therapist based on each patients care needs. \nThe therapist\u2019s digital avatar accompanies the patient in their immersive VR journey. At the very end of the experience, the patient is put in a virtual out of body experience.\nCareful consideration of biometric feedback will allow the therapist to disseminate the OBE with maximum effectiveness because it will be synchronized to the exact moment that patient proprioception has been primed for the OBE.\nEcstasis creates an increasingly immersive cascade of visuotactile techniques that are injected into the experience and controlled by the clinician in real-time based on analysis of biomarkers that indicate \u201creadiness\u201d, what we call the subjects suggestibility for the therapy. \nOnce a patient is qualified to be a suitable candidate for VR therapy, a licensed therapist from the hospice care team will work closely with the patient to enable a custom virtual out of body experience (VOBE)\nVR Therapy is effective, however, there tends to be a sense of isolation, especially in a therapeutic context. Ecstasis restores experience of guided therapy by allowing the therapist to read the patient while controlling the immersive VR experience in real time while encouraging the therapists engagement in real time. An OBE is created by a systematic manipulation of audiovisual cues to elicit \u201cavatarization\u201d the embodiment of patient into our digital model by giving them full control over this new avatar.</p>\n<h2>How we built it</h2>\n<p>We built this in two parts, a VR application with the Oculus Quest and an AR component using Project North Star headset.</p>\n<p>We setup a Project North Star headset with an Intel Real Sense for 6DOF tracking and a Leap Motion for hand tracking, and created a movable user interface to control the state of the VR experience. This interface would then translate into controlling the flow of the experience, such as triggering the out of body experience or slowing down the experience for the user.</p>\n<h2>Challenges we ran into</h2>\n<p>We first tried to set up the North Star headset using the newly released SteamVR drivers and use a Vive tracker to track head position. This proved to be very difficult, with problems arising from the tracker\u2019s transform being off as well as SteamVR failing to build on our computer. We eventually switched to the RealSense camera which took us only a half hour of setup and worked wonderfully.</p>\n<p>Triggering animations based on Leap Motion APIs proved difficult. We tried to use the Doozy UI library to help configure these actions, but we unable to install the Unity Package along with Leap Motion plugin. We resolved this by writing code to sync the animations with the Leap Motion\u2019s Slider state. We synced it between both the elements in the project and across the network using the NormalCore library. </p>\n<p>Another problem we faced was integrating both headsets into the same Unity project. We had gotten NormCore to create a full multiplayer environment but once we added the other headset package it fell apart. We ran into this too soon to the deadline and left us without being able to demonstrate the two parts working in tandem.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of having one of only two working North Star project headsets at the hackathon and took on the incredibly daunting task of integrating this already prototypical headset into a scene with a VR device. We also helped give input to both the newly released SteamVR plugin for North Star and the NormalAPI, which will help both of the projects mature in the future.</p>\n<h2>What we learned</h2>\n<p>We learned how to develop on the Project North Star as well as working with the SDKs of RealSense, Leap Motion, and SteamVR. We learned many teamwork skills to break down and prioritize our tasks and design what interactions we would create, both physically and virtually.</p>\n<h2>Citations</h2>\n<p>Nhpco.org. (2019). NHPCO Facts Figures. [online] Available at: <a href=\"https://www.nhpco.org/wp-content/uploads/2019/07/2018_NHPCO_Facts_Figures.pdf\" rel=\"nofollow\">https://www.nhpco.org/wp-content/uploads/2019/07/2018_NHPCO_Facts_Figures.pdf</a> [Accessed 19 Jan. 2020].</p>\n<p>Bourdin, P., Barberia, I., Oliva, R. and Slater, M. (2017). A Virtual Out-of-Body Experience Reduces Fear of Death.</p>\n</div>",
            "content_md": "\n## Problem\n\n\nAccording to WHO, each year an estimated 40 million people are in need of palliative and hospice care. The global need for this type of care will continue to grow as a result of the rising burden of noncommunicable diseases and ageing populations. Early palliative and hospice care reduces unnecessary hospital admissions and the use of health services. \n\n\nPalliative and hospice care refer to an interdisciplinary medical care-giving approach aimed at optimizing quality of life, mitigating suffering from the stress of terminal illness and providing comfort and care to patients and their families to acceptance and undergo a transformation of perception. \n\n\nVirtual Embodiment has been proven to be an effective clinical and therapeutic application of virtual reality as a therapy (Bourdin, P. 2017). But therein lies a problem: virtualization of patients by therapists using head-mounted virtual reality devices effectively diminishes the social connection between them. The strong bonds therapists spend months developing with their patients becomes threatened by a new care paradigm that deemphasizes human interaction in favor of \u201cdosed\u201d or \u201cautoplay\u201d therapy.\n\n\nMore information on palliative care and its dynamic socio-economic challenges can be found here: <https://www.who.int/news-room/fact-sheets/detail/palliative-care>\n\n\n## Solution\n\n\nVirtual reality (VR) and virtual embodiment (VE) have been demonstrated to be therapeutically effective tools for helping to cope with the reality of transitional palliative and hospice care (Bourdin, P. 2017). Current clinical therapy standards are confined to mainly provide relief by sedation, rarely indicating more access to support groups and professionals should the patient request it. Effective therapies for near end of life patients have been narrowing drastically. Virtual Reality and Virtual Embodiment provides a powerful vehicle for the transition into hospice care through immersion, the therapeutically viable engine that drives the heart of a convincing VR experience.\n\n\nEcstasis (to be or stand outside oneself, a removal to elsewhere) is an immersive VR journey that reconnects the relationship between therapist and patient and reduces stress of patients in need of palliative care allowing them to cope with their illnesses with a more positive mindset.\n\n\nBased on a white paper that studies how \"fear of death was found to be lower in the experimental group that underwent virtual out-of-body experiences\" (OBE) (Bourdin,1), Ecstasis was designed as an OBE generator controlled by trusted therapists that would personalize the experience based on the patient\u2019s biofeedback. \n\n\nBourdin, P., Barberia, I., Oliva, R. and Slater, M. (2017). A Virtual Out-of-Body Experience Reduces Fear of Death.\n\n\n## What it does\n\n\nTherapist takes a patient on a real-time immersive and responsive journey in VR. The clinician has access to a hand\u2019s free control system UI that allows them to set the environment, control movement and embody various digital assets to further enhance the relationship between therapist and patient. The intent of the experience is to take the patient on an immersive journey that results in an out of body (OBE) experience. It is through these immersive journeys, personally tailored by the therapist based on each patients care needs. \nThe therapist\u2019s digital avatar accompanies the patient in their immersive VR journey. At the very end of the experience, the patient is put in a virtual out of body experience.\nCareful consideration of biometric feedback will allow the therapist to disseminate the OBE with maximum effectiveness because it will be synchronized to the exact moment that patient proprioception has been primed for the OBE.\nEcstasis creates an increasingly immersive cascade of visuotactile techniques that are injected into the experience and controlled by the clinician in real-time based on analysis of biomarkers that indicate \u201creadiness\u201d, what we call the subjects suggestibility for the therapy. \nOnce a patient is qualified to be a suitable candidate for VR therapy, a licensed therapist from the hospice care team will work closely with the patient to enable a custom virtual out of body experience (VOBE)\nVR Therapy is effective, however, there tends to be a sense of isolation, especially in a therapeutic context. Ecstasis restores experience of guided therapy by allowing the therapist to read the patient while controlling the immersive VR experience in real time while encouraging the therapists engagement in real time. An OBE is created by a systematic manipulation of audiovisual cues to elicit \u201cavatarization\u201d the embodiment of patient into our digital model by giving them full control over this new avatar.\n\n\n## How we built it\n\n\nWe built this in two parts, a VR application with the Oculus Quest and an AR component using Project North Star headset.\n\n\nWe setup a Project North Star headset with an Intel Real Sense for 6DOF tracking and a Leap Motion for hand tracking, and created a movable user interface to control the state of the VR experience. This interface would then translate into controlling the flow of the experience, such as triggering the out of body experience or slowing down the experience for the user.\n\n\n## Challenges we ran into\n\n\nWe first tried to set up the North Star headset using the newly released SteamVR drivers and use a Vive tracker to track head position. This proved to be very difficult, with problems arising from the tracker\u2019s transform being off as well as SteamVR failing to build on our computer. We eventually switched to the RealSense camera which took us only a half hour of setup and worked wonderfully.\n\n\nTriggering animations based on Leap Motion APIs proved difficult. We tried to use the Doozy UI library to help configure these actions, but we unable to install the Unity Package along with Leap Motion plugin. We resolved this by writing code to sync the animations with the Leap Motion\u2019s Slider state. We synced it between both the elements in the project and across the network using the NormalCore library. \n\n\nAnother problem we faced was integrating both headsets into the same Unity project. We had gotten NormCore to create a full multiplayer environment but once we added the other headset package it fell apart. We ran into this too soon to the deadline and left us without being able to demonstrate the two parts working in tandem.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of having one of only two working North Star project headsets at the hackathon and took on the incredibly daunting task of integrating this already prototypical headset into a scene with a VR device. We also helped give input to both the newly released SteamVR plugin for North Star and the NormalAPI, which will help both of the projects mature in the future.\n\n\n## What we learned\n\n\nWe learned how to develop on the Project North Star as well as working with the SDKs of RealSense, Leap Motion, and SteamVR. We learned many teamwork skills to break down and prioritize our tasks and design what interactions we would create, both physically and virtually.\n\n\n## Citations\n\n\nNhpco.org. (2019). NHPCO Facts Figures. [online] Available at: <https://www.nhpco.org/wp-content/uploads/2019/07/2018_NHPCO_Facts_Figures.pdf> [Accessed 19 Jan. 2020].\n\n\nBourdin, P., Barberia, I., Oliva, R. and Slater, M. (2017). A Virtual Out-of-Body Experience Reduces Fear of Death.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/words-0u5pet",
            "title": "Words",
            "blurb": "It's mainly an vr-art installation about my first time in the US. I put words and situations together which I experienced during my time here.  ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "SilvanaNoise Vasquez-Keller",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/60072218?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "brush",
                "tilt",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>I was inspired by everything I encountered here every day. It was very intensive and different from what I was used from Europe. I was challenged by the simplest things to do like crossing the street by pushing an arrow button, taking a hot shower and finding my way...</p>\n<h2>What it does</h2>\n<p>Hopefully it shows how it feels like to be a stranger at a totally new place.</p>\n<h2>How I built it</h2>\n<p>tilt brush, unity</p>\n<h2>Challenges I ran into</h2>\n<p>to survive </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I am sure that I have found a completely new form language for an artistic implementation of my experience in vr. </p>\n<h2>What I learned</h2>\n<p>Food is really important to work productively.</p>\n<h2>What's next for Words</h2>\n<p>Showing it to an audience in vr at different exhibitions.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nI was inspired by everything I encountered here every day. It was very intensive and different from what I was used from Europe. I was challenged by the simplest things to do like crossing the street by pushing an arrow button, taking a hot shower and finding my way...\n\n\n## What it does\n\n\nHopefully it shows how it feels like to be a stranger at a totally new place.\n\n\n## How I built it\n\n\ntilt brush, unity\n\n\n## Challenges I ran into\n\n\nto survive \n\n\n## Accomplishments that I'm proud of\n\n\nI am sure that I have found a completely new form language for an artistic implementation of my experience in vr. \n\n\n## What I learned\n\n\nFood is really important to work productively.\n\n\n## What's next for Words\n\n\nShowing it to an audience in vr at different exhibitions.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/suis",
            "title": "Spatial Universal Interaction System",
            "blurb": "Platform agnostic middleware for accessible AR/VR interactions",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/g4IGNlxbwGI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Technical architecture",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/008/datas/original.jpg"
                },
                {
                    "title": "Debugging",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/020/datas/original.jpg"
                },
                {
                    "title": "Having fun!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/065/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Val Mack",
                    "about": "Helped to define hackathon scope and demo interactions. Also aided in implementation through pair programming!",
                    "photo": "https://avatars3.githubusercontent.com/u/8445610?height=180&v=4&width=180"
                },
                {
                    "name": "Nova King",
                    "about": "I worked on the scripting to link together events.",
                    "photo": "https://avatars0.githubusercontent.com/u/4541968?height=180&v=4&width=180"
                },
                {
                    "name": "Nhan Tran",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/616/datas/profile.JPG"
                }
            ],
            "built_with": [
                "ar",
                "c#",
                "microsoft-hololens",
                "northstar",
                "steam",
                "unity",
                "vr"
            ],
            "content_html": "<div>\n<h1>Spatial Universal Interaction System (for Unity)</h1>\n<p>Platform agnostic middleware for accessible XR interactions.</p>\n<h2>About</h2>\n<p>SUIS is an interaction system (an open-source package tool for Unity) that is 1) platform-agnostic, and 2) offers accessibility options baked-in.</p>\n<p>If someone has limited mobility or accessibility needs (e.g. amputees, people with arthritis), developers can easily implement accessibility features that would allow the person to still engage in the immersive experience without limitation.</p>\n<p>In addition, instead of implementing custom code for a combination of input devices, developers and creators can use our control middleware to make <strong>one set of actions</strong> work with <strong>any inputs</strong> (e.g. head pointer, hands, eyes, controller, gamepad) on <strong>any XR platform</strong>.</p>\n<h2>Motivations</h2>\n<ul>\n<li>As XR developers and designers, we want to make our games and applications accessible, but doing that is not easy.</li>\n<li>It takes deliberate practice to make accessibility a priority. And it takes a lot of work to build accessible features into our systems.</li>\n<li>This shouldn't be so hard--imagine a world in which all people, no matter what their physical ability, are able to have the engaging experiences of any XR game or application.</li>\n<li>Turns out, it's not so hard!</li>\n</ul>\n<h2>Build</h2>\n<p>So far we have created:</p>\n<ul>\n<li>A \"flick\" action script that allows the headset wearer to naturally move an object with only head motion</li>\n<li>A set of SDF (signed distance field) scripts that detect pointer distance, and negative distance from game objects</li>\n<li>Exposure system that uses the SDFs to select and activate objects (and capture wearer intent of object selection)</li>\n<li>System architecture design for generic interfaces that developers can use to build accessibility features</li>\n<li>Project North Star calibration and input mapping scripts for Unity</li>\n<li>Vive wand calibration and input mapping scripts for Unity</li>\n</ul>\n<h2>Usage</h2>\n<p>Download the SUIS package, apply the Exposure script to any game object, define an Action that should be triggered on the game object.</p>\n<h2>Future Work</h2>\n<ul>\n<li>Finish the InputManager to disable and enable actions within a single frame</li>\n<li>Create fallback classes</li>\n<li>Finish mapping the 4 input categories to the action interface</li>\n</ul>\n<h2>Challenges</h2>\n<ul>\n<li>We spent Friday until Saturday at 7pm just getting our computer to run the headset in Unity.</li>\n<li>We had disagreements in our team at first and worked with mentors to come to a compromise that we all felt really happy about in the end. (Thank you mentors!)</li>\n<li>We only had 1 computer for our team</li>\n</ul>\n<h2>Learnings</h2>\n<p>We learned how to distill a large system into a small demonstrable example which was our hackathon demo.\nWe also learned from mentors about the inner workings of MRTK and XRTK, and the unique benefits of our system.\nWe learned how to use Project North Star with Vive wands.</p>\n</div>",
            "content_md": "\n# Spatial Universal Interaction System (for Unity)\n\n\nPlatform agnostic middleware for accessible XR interactions.\n\n\n## About\n\n\nSUIS is an interaction system (an open-source package tool for Unity) that is 1) platform-agnostic, and 2) offers accessibility options baked-in.\n\n\nIf someone has limited mobility or accessibility needs (e.g. amputees, people with arthritis), developers can easily implement accessibility features that would allow the person to still engage in the immersive experience without limitation.\n\n\nIn addition, instead of implementing custom code for a combination of input devices, developers and creators can use our control middleware to make **one set of actions** work with **any inputs** (e.g. head pointer, hands, eyes, controller, gamepad) on **any XR platform**.\n\n\n## Motivations\n\n\n* As XR developers and designers, we want to make our games and applications accessible, but doing that is not easy.\n* It takes deliberate practice to make accessibility a priority. And it takes a lot of work to build accessible features into our systems.\n* This shouldn't be so hard--imagine a world in which all people, no matter what their physical ability, are able to have the engaging experiences of any XR game or application.\n* Turns out, it's not so hard!\n\n\n## Build\n\n\nSo far we have created:\n\n\n* A \"flick\" action script that allows the headset wearer to naturally move an object with only head motion\n* A set of SDF (signed distance field) scripts that detect pointer distance, and negative distance from game objects\n* Exposure system that uses the SDFs to select and activate objects (and capture wearer intent of object selection)\n* System architecture design for generic interfaces that developers can use to build accessibility features\n* Project North Star calibration and input mapping scripts for Unity\n* Vive wand calibration and input mapping scripts for Unity\n\n\n## Usage\n\n\nDownload the SUIS package, apply the Exposure script to any game object, define an Action that should be triggered on the game object.\n\n\n## Future Work\n\n\n* Finish the InputManager to disable and enable actions within a single frame\n* Create fallback classes\n* Finish mapping the 4 input categories to the action interface\n\n\n## Challenges\n\n\n* We spent Friday until Saturday at 7pm just getting our computer to run the headset in Unity.\n* We had disagreements in our team at first and worked with mentors to come to a compromise that we all felt really happy about in the end. (Thank you mentors!)\n* We only had 1 computer for our team\n\n\n## Learnings\n\n\nWe learned how to distill a large system into a small demonstrable example which was our hackathon demo.\nWe also learned from mentors about the inner workings of MRTK and XRTK, and the unique benefits of our system.\nWe learned how to use Project North Star with Vive wands.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/down-payment-on-the-dream",
            "title": "Down Payment on the Dream: The True Story of Frank Legree",
            "blurb": "Down Payment on the Dream is the story of 7-year-old Frankie Legree whose African American family integrated an all-white Miami neighborhood in 1957.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/FY55NZpP_Vs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Xavier Apostol",
                    "about": "I was one of two Unity Developers and the Sound Designer for this project.",
                    "photo": "https://avatars2.githubusercontent.com/u/21956869?height=180&v=4&width=180"
                },
                {
                    "name": "Cristina Rose",
                    "about": "I was the writer/storyteller on this project.",
                    "photo": "https://www.gravatar.com/avatar/4dfbb1b12752dafe6088ed1f1121dbfa?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lafiya Watson Ramirez",
                    "about": "I was one of the Unity developers and visual artists. It was my first time using Nreal, and it was a great learning experience.",
                    "photo": "https://www.gravatar.com/avatar/a0f84fb0f2ee43b6b42d8fc8f46f4db9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ann Bennett",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/174/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "github",
                "looks-back-at-what-it-was-like-to-be-part-of-the-first-african-american-family-in-a-(formerly)-all-white-neighborhood.-by-merely-existing-in-their-new-house",
                "nreal",
                "unity",
                "who-was-just-a-kid-in-1957-miami"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>In 2017, we (Katja Esson - Director/Producer and Ann Bennett - Producer) began filming our feature-length documentary, RAZING LIBERTY SQUARE, that tells the story of Florida climate gentrification by following the redevelopment of a historic African American public housing project in Miami and its impact on long-time residents.  As we collected personal narratives of  Liberty Square residents whose neighborhood is about to be razed to the ground, we discovered a rich deposit of stories of informative and inspirational stories.  Our mixed reality experience aims to bring the forgotten voices and event of this historic neighborhood to light, providing historical and cultural context for a public which often remains unaware of the historic importance of Liberty City in the crafting of Miami as a modern, cultural crossroads. </p>\n<h2>What it does</h2>\n<p>Documents the the life of Frank Legree during the 1950s.</p>\n<h2>How we built it</h2>\n<p>Research / design, iterative prototyping, and continuous check-ins. Unity Collaborate really helped us bring the experience together.</p>\n<h2>Challenges we ran into</h2>\n<p>Learning how to finesse the hardware, bringing together visual, sound FX, voice-over narration, triggers, animations, etc to honor a story that deserved to be told.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Finishing the experience and establishing a solid team relationship.</p>\n<h2>What we learned</h2>\n<p>A lot about AR / nreal, teamwork, collaboration, and the life of Frank Legree (and how people respond to it).</p>\n<h2>What's next for Down Payment on the Dream</h2>\n<p>To expand it using more image (i.e. - videos, music, animations, etc.). We'd like to turn it into a fully interactive, educational, and historical storytelling experience.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nIn 2017, we (Katja Esson - Director/Producer and Ann Bennett - Producer) began filming our feature-length documentary, RAZING LIBERTY SQUARE, that tells the story of Florida climate gentrification by following the redevelopment of a historic African American public housing project in Miami and its impact on long-time residents. As we collected personal narratives of Liberty Square residents whose neighborhood is about to be razed to the ground, we discovered a rich deposit of stories of informative and inspirational stories. Our mixed reality experience aims to bring the forgotten voices and event of this historic neighborhood to light, providing historical and cultural context for a public which often remains unaware of the historic importance of Liberty City in the crafting of Miami as a modern, cultural crossroads. \n\n\n## What it does\n\n\nDocuments the the life of Frank Legree during the 1950s.\n\n\n## How we built it\n\n\nResearch / design, iterative prototyping, and continuous check-ins. Unity Collaborate really helped us bring the experience together.\n\n\n## Challenges we ran into\n\n\nLearning how to finesse the hardware, bringing together visual, sound FX, voice-over narration, triggers, animations, etc to honor a story that deserved to be told.\n\n\n## Accomplishments that we're proud of\n\n\nFinishing the experience and establishing a solid team relationship.\n\n\n## What we learned\n\n\nA lot about AR / nreal, teamwork, collaboration, and the life of Frank Legree (and how people respond to it).\n\n\n## What's next for Down Payment on the Dream\n\n\nTo expand it using more image (i.e. - videos, music, animations, etc.). We'd like to turn it into a fully interactive, educational, and historical storytelling experience.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/detective-trainar-qnyka6",
            "title": "Detective Trainar",
            "blurb": "A Snapchat Crime Solving Game using Snapchat Lenses ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/V424zUPef0c?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Mitch Chaiet",
                    "about": "Idea, Lens Studio Development",
                    "photo": "https://graph.facebook.com/10213294310487138/picture?height=180&width=180"
                },
                {
                    "name": "David Blair Adams",
                    "about": "",
                    "photo": "https://graph.facebook.com/10162466759235500/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "lensstudio",
                "nreal",
                "snapchat"
            ],
            "content_html": "<div>\n<h1>DetectiveTrainar</h1>\n<p>A Snapchat Crime Solving Game using Snapchat Lenses built for MIT Reality Hack</p>\n<h2>Inspiration</h2>\n<p>For the 2020 MIT Reality Hack, our team aspired to create an interactive, augmented reality storytelling experience that uses Snapchat in a first-of-its-kind way. In addition to Snapchat, our team wanted to take advantage of the latest pieces of hardware available to MIT Reality Hack attendees.</p>\n<h2>What it Does</h2>\n<p>DetectiveTrainar is an augmented reality investigative simulation made for Snapchat or the Nreal headset. </p>\n<h2>How we built it</h2>\n<p>The experiences were built in Unity, with the Nreal SDK, and on Snapchat Lens Studio. </p>\n<h2>Challenges</h2>\n<p>For Snapchat - Our team originally aspired to make an interactive murder mystery told through Snapchat. Snapchat Lenses need to be less than 4mb. Given the file size limitations, our team created each crime scene by using a mix of low poly 3D models and PNG. </p>\n<p>For NReal - This was our first time developing for the NReal headset. It took a bit of time for our team to learn how to use the NReal. We struggled with occasional glitches and with the integration between the headset and mobile controller. Given the limited time of the hackathon, we could not completely fix this bug. As a quick solution, we found experiences load best if the user is an empty room and if they calibrate pointing their phone at the floor.</p>\n<h2>Accomplishments</h2>\n<p>We successfully built an interactive experience told through Snapchat, a medium which has yet to be explored for storytelling purposes. Our team member Blair successfully built an investigative story with NReal, a headset he had not touched prior to the start of this hackathon.</p>\n<h2>What we learned</h2>\n<p>The story should dictate the medium. When initially ideating, our team faced a few constraints as we attempted to pair traditional storytelling aspects with Augmented Reality. Augmented Reality is its own experiential medium and experiences ought to be had in nontraditional, nonlinear ways. Project management is essential for keeping all team members on task.</p>\n<h2>Whats\u2019 Next</h2>\n<p>Our team would like to send this experience to the Snap development team, in hopes that interactive between filters could be added to a future Snapchat update. </p>\n<p><img alt=\"RealityHackLogo\" data-canonical-url=\"https://assets.website-files.com/5daf899dfcdc19f2a01e9c5f/5dbc3269df87c098cd487b86_LogoPNG%20copy.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--vnjkMY6X--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://assets.website-files.com/5daf899dfcdc19f2a01e9c5f/5dbc3269df87c098cd487b86_LogoPNG%2520copy.png\"/></p>\n</div>",
            "content_md": "\n# DetectiveTrainar\n\n\nA Snapchat Crime Solving Game using Snapchat Lenses built for MIT Reality Hack\n\n\n## Inspiration\n\n\nFor the 2020 MIT Reality Hack, our team aspired to create an interactive, augmented reality storytelling experience that uses Snapchat in a first-of-its-kind way. In addition to Snapchat, our team wanted to take advantage of the latest pieces of hardware available to MIT Reality Hack attendees.\n\n\n## What it Does\n\n\nDetectiveTrainar is an augmented reality investigative simulation made for Snapchat or the Nreal headset. \n\n\n## How we built it\n\n\nThe experiences were built in Unity, with the Nreal SDK, and on Snapchat Lens Studio. \n\n\n## Challenges\n\n\nFor Snapchat - Our team originally aspired to make an interactive murder mystery told through Snapchat. Snapchat Lenses need to be less than 4mb. Given the file size limitations, our team created each crime scene by using a mix of low poly 3D models and PNG. \n\n\nFor NReal - This was our first time developing for the NReal headset. It took a bit of time for our team to learn how to use the NReal. We struggled with occasional glitches and with the integration between the headset and mobile controller. Given the limited time of the hackathon, we could not completely fix this bug. As a quick solution, we found experiences load best if the user is an empty room and if they calibrate pointing their phone at the floor.\n\n\n## Accomplishments\n\n\nWe successfully built an interactive experience told through Snapchat, a medium which has yet to be explored for storytelling purposes. Our team member Blair successfully built an investigative story with NReal, a headset he had not touched prior to the start of this hackathon.\n\n\n## What we learned\n\n\nThe story should dictate the medium. When initially ideating, our team faced a few constraints as we attempted to pair traditional storytelling aspects with Augmented Reality. Augmented Reality is its own experiential medium and experiences ought to be had in nontraditional, nonlinear ways. Project management is essential for keeping all team members on task.\n\n\n## Whats\u2019 Next\n\n\nOur team would like to send this experience to the Snap development team, in hopes that interactive between filters could be added to a future Snapchat update. \n\n\n![RealityHackLogo](https://res.cloudinary.com/devpost/image/fetch/s--vnjkMY6X--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://assets.website-files.com/5daf899dfcdc19f2a01e9c5f/5dbc3269df87c098cd487b86_LogoPNG%2520copy.png)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/curiouser",
            "title": "Curiouser",
            "blurb": "Alice in Wonderland narrative themed puzzle game using scaling to locomote and as a game mechanic.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/nzGr-g7sZIA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Hallway",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/314/datas/original.png"
                },
                {
                    "title": "Garden View",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/315/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Matt Rossman",
                    "about": "VR development, Git, and video editing.",
                    "photo": "https://avatars2.githubusercontent.com/u/22670878?height=180&v=4&width=180"
                },
                {
                    "name": "Marcel .",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/959/177/datas/profile.png"
                },
                {
                    "name": "Dawn Norton",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d589adfed2894e1f1b2029341f0c302e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Vikram Gupta",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/58710897?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "3ds-max",
                "ableton",
                "adobe-illustrator",
                "blender",
                "c#",
                "hpvr",
                "oculus",
                "oculusintegration",
                "photoshop",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Years ago at another XR MIT hackathon, another participant pitched the idea of using scaling locomotion as a solution to the room-scale vr problem.   Marcel thought that this was an awesome concept but didn't know what to do with it.</p>\n<p>A few months ago while listening to an audiobook of Alice's Adventures in Wonderland he realized that the scene where she eats various growing and shrinking foods to escape the hall and house was a beautiful little premade puzzle using that same scaling mechanic.  She needs to pick up and eat different foods to change sizes to reach certain keys and change size again to fit through doors.</p>\n<h2>What it does</h2>\n<p>On an Oculus Rift Alice(you) finds herself in a long hallway. She must pickup a key to open a tiny door.  The tiny door opens revealing a cake in a cave and a beautiful garden beyond.  Eating the cake will make Alice big which allows her to pick up a bottle from a shelf behind her. She is now too far from the door though.  Using the cake to grow big and take big steps and the cake to grow small Alice finally makes her way through the small door and towards the garden.  What happens next? Only Alice knows... </p>\n<h2>How I built it</h2>\n<p>We used Unity and the Oculus integration plugin to build our project.  All models were made from scratch in 3ds Max and Blender.  The music and some sounds were composed in Ableton and other sounds were downloaded from freesound.org.</p>\n<h2>Challenges I ran into</h2>\n<p>The play space would scale around a single point instead of at the player.  Some curiouser maths were needed to solve that problem. </p>\n<p>A deleted object in runtime was not ending a released trigger.</p>\n<p>Clipping through walls revealed the big finish.  A fading black panel that blocked the players vision was used to fix that.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>WE HAD A FINISHED POLISHED-ISH EXPERIENCE.  Its pretty cool.</p>\n<h2>What I learned</h2>\n<p>Alot about Unity and VR. Not to trust Resonance. I experimented with haptics though we didn't get to implement it.  You can do math in unity inspector fields.  M&amp;Ms and Kool-aid Jammers fuel you. Teamwork. Practice. Discord (the app).</p>\n<h2>What's next for Curiouser</h2>\n<p>Sleep.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nYears ago at another XR MIT hackathon, another participant pitched the idea of using scaling locomotion as a solution to the room-scale vr problem. Marcel thought that this was an awesome concept but didn't know what to do with it.\n\n\nA few months ago while listening to an audiobook of Alice's Adventures in Wonderland he realized that the scene where she eats various growing and shrinking foods to escape the hall and house was a beautiful little premade puzzle using that same scaling mechanic. She needs to pick up and eat different foods to change sizes to reach certain keys and change size again to fit through doors.\n\n\n## What it does\n\n\nOn an Oculus Rift Alice(you) finds herself in a long hallway. She must pickup a key to open a tiny door. The tiny door opens revealing a cake in a cave and a beautiful garden beyond. Eating the cake will make Alice big which allows her to pick up a bottle from a shelf behind her. She is now too far from the door though. Using the cake to grow big and take big steps and the cake to grow small Alice finally makes her way through the small door and towards the garden. What happens next? Only Alice knows... \n\n\n## How I built it\n\n\nWe used Unity and the Oculus integration plugin to build our project. All models were made from scratch in 3ds Max and Blender. The music and some sounds were composed in Ableton and other sounds were downloaded from freesound.org.\n\n\n## Challenges I ran into\n\n\nThe play space would scale around a single point instead of at the player. Some curiouser maths were needed to solve that problem. \n\n\nA deleted object in runtime was not ending a released trigger.\n\n\nClipping through walls revealed the big finish. A fading black panel that blocked the players vision was used to fix that.\n\n\n## Accomplishments that I'm proud of\n\n\nWE HAD A FINISHED POLISHED-ISH EXPERIENCE. Its pretty cool.\n\n\n## What I learned\n\n\nAlot about Unity and VR. Not to trust Resonance. I experimented with haptics though we didn't get to implement it. You can do math in unity inspector fields. M&Ms and Kool-aid Jammers fuel you. Teamwork. Practice. Discord (the app).\n\n\n## What's next for Curiouser\n\n\nSleep.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dungeon-of-death",
            "title": "Dungeon of Elements: Perilous Journey of the Avatar",
            "blurb": "Fire. Air. Water. Earth. THE POWER OF ELEMENTS IS IN YOUR HANDS! Can you breakout of these VR dungeons and escape to freedom?",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/pp77vCz3uxo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Dungeon of Elements, Match-It Scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/255/datas/original.png"
                },
                {
                    "title": "Control Instructions",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/172/datas/original.jpg"
                },
                {
                    "title": "Control Instructions",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/171/datas/original.jpg"
                },
                {
                    "title": "Sample of a Dungeon Room",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/036/datas/original.png"
                },
                {
                    "title": "Sample Of a Puzzle Called Match It, where you must summon the elements in the order they flash",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/037/datas/original.png"
                },
                {
                    "title": "Obstacles Within The Puzzle, that must be pushed away with air or fire",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/038/datas/original.png"
                },
                {
                    "title": "Time Based Puzzle, where the user must summon fire for a length of time",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/039/datas/original.png"
                },
                {
                    "title": "A music based puzzle, where the pipes must be hit with air to recreate the sound",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/077/datas/original.png"
                },
                {
                    "title": "This puzzle requires you to grow specific trees based off of a riddle",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/141/datas/original.png"
                },
                {
                    "title": "This puzzle requires you to imitate the movements of the swirl to open the door",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/142/datas/original.png"
                },
                {
                    "title": "This is an example with instructions for the controls",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/143/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Lauren Zhang",
                    "about": "Created the particle effects, built the environment assets, coded interaction of conjured element with environment objects.",
                    "photo": "https://avatars3.githubusercontent.com/u/49242010?height=180&v=4&width=180"
                },
                {
                    "name": "Evelyn Liu",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/106/252/datas/profile.jpg"
                },
                {
                    "name": "Jared Yellen",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C4E03AQHtgttx40DIHg/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=QNfo8oSjx5SFpbjO2stGWG1EL8amnQ9WHVMLOyM4K5A&v=beta&width=180"
                },
                {
                    "name": "Dan Donato",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/10944648?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration:</h2>\n<p>Avatar the Last Airbender is an American anime that aired on Nickelodeon for three seasons. Avatar is set in an Asiatic-like world in which some people can manipulate the classical elements with psychokinetic variants of the Chinese martial arts known as \"bending\". The Avatar must bring peace and harmony to the world by ending the Fire Nation's war with the rest of the Four Nations.</p>\n<h2>What it does</h2>\n<p><strong>Dungeon of Elements: Perilous Journey of the Avatar</strong> Is a puzzle game where the player must control the 4 elements in VR to solve puzzles. The game is a instance based puzzler, where each time you load into the game it chooses a number of random puzzles to be what you have to play through. The catch is that each time you fail a puzzle you lose a life, and if you lose all of them you have to completely restart with a new set of random puzzles.</p>\n<h2>How we built it</h2>\n<p>The project was created in Unity, and we divided our team into different roles to divide and conquer the project. One of our members focused on creating the Element Summoning and physics, one of us focused on creating the visuals and emitters, one of us focused on creating and building the puzzles into scenes, and one of us focused on building and designing the game elements. Once each group had managed all the pieces, we began to combine each piece together and turn it into a finished product.</p>\n<h2>Challenges we ran into</h2>\n<p>One of the biggest issues was determining hardware, since our computers only had an HDMI. This resulted in running it on the Oculus Quest, which adds a lot of time due to building for every iteration.</p>\n<h2>What we learned</h2>\n<p>It's not a faith in technology, it's a faith in people. Even the most difficult of challenges can be fun when working with the right people. We struggled with setting up the Oculus devices for development, and therefore were extremely behind after day one. Yet, with the support of our teammates, we were able to carry-through with our vision with a ton of laughter.</p>\n<h2>What's next for Dungeon of Death</h2>\n<p>Ideally we would like to add levels of difficulty, more physics interactions for the elements, and of course, many more puzzles. The ideal version would be to somehow have procedurally generated puzzles, since one of our main focuses is replayability. </p>\n<h2>Assets</h2>\n<ul>\n<li>3D objects from CGTrader</li>\n<li>Audio from bensound.com</li>\n</ul>\n<h2>Image Gallery</h2>\n<p>Please refer to the gallery above for detailed escape strategy for each of the 15 scenes.</p>\n<h2>Grand Challenge Choice #1: Most Fun</h2>\n</div>",
            "content_md": "\n## Inspiration:\n\n\nAvatar the Last Airbender is an American anime that aired on Nickelodeon for three seasons. Avatar is set in an Asiatic-like world in which some people can manipulate the classical elements with psychokinetic variants of the Chinese martial arts known as \"bending\". The Avatar must bring peace and harmony to the world by ending the Fire Nation's war with the rest of the Four Nations.\n\n\n## What it does\n\n\n**Dungeon of Elements: Perilous Journey of the Avatar** Is a puzzle game where the player must control the 4 elements in VR to solve puzzles. The game is a instance based puzzler, where each time you load into the game it chooses a number of random puzzles to be what you have to play through. The catch is that each time you fail a puzzle you lose a life, and if you lose all of them you have to completely restart with a new set of random puzzles.\n\n\n## How we built it\n\n\nThe project was created in Unity, and we divided our team into different roles to divide and conquer the project. One of our members focused on creating the Element Summoning and physics, one of us focused on creating the visuals and emitters, one of us focused on creating and building the puzzles into scenes, and one of us focused on building and designing the game elements. Once each group had managed all the pieces, we began to combine each piece together and turn it into a finished product.\n\n\n## Challenges we ran into\n\n\nOne of the biggest issues was determining hardware, since our computers only had an HDMI. This resulted in running it on the Oculus Quest, which adds a lot of time due to building for every iteration.\n\n\n## What we learned\n\n\nIt's not a faith in technology, it's a faith in people. Even the most difficult of challenges can be fun when working with the right people. We struggled with setting up the Oculus devices for development, and therefore were extremely behind after day one. Yet, with the support of our teammates, we were able to carry-through with our vision with a ton of laughter.\n\n\n## What's next for Dungeon of Death\n\n\nIdeally we would like to add levels of difficulty, more physics interactions for the elements, and of course, many more puzzles. The ideal version would be to somehow have procedurally generated puzzles, since one of our main focuses is replayability. \n\n\n## Assets\n\n\n* 3D objects from CGTrader\n* Audio from bensound.com\n\n\n## Image Gallery\n\n\nPlease refer to the gallery above for detailed escape strategy for each of the 15 scenes.\n\n\n## Grand Challenge Choice #1: Most Fun\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-fourth-illusion",
            "title": "When I Was Little",
            "blurb": "A short narrative that uses AR and VR to illustrate differences in perspective and encourage people to communicate.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385865311?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Devon Kennedy",
                    "about": "michael_bass@brown.edu\nkathleenbryan24@gmail.com",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/738/786/datas/profile.jpg"
                },
                {
                    "name": "Alex Coulombe",
                    "about": "I came into the hackathon knowing I wanted to do something with a multiuser AR/VR experience where the AR viewer sees a god's eye view that includes the VR player. I also thought it would be delightful to get to play with Depthkit. All of this happened-- thanks team!",
                    "photo": "https://media.licdn.com/mpr/mprx/0_0fkZG6VBGakfbpDM0IyZu5xB6wnP3YYM9YrZF-gBbp0-3axdrfYndqxBTSN-TSTkvf-ZW6pcESztCVcBzyTko-lRfSz1CVRLKyTU31k9Xuhxe2hMpDczTbEF74fiIVuqJ0LNQDU8OCH?height=180&width=180"
                },
                {
                    "name": "michael bass",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/45740001?height=180&v=4&width=180"
                },
                {
                    "name": "Kathleen Bryan",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGPz4uey-z3YQ/profile-displayphoto-shrink_100_100/0?e=1544054400&height=180&t=IK4GcnZk9oaYnEMkWoKYI_Hpkd4vgcBvgWo8OZff6lE&v=beta&width=180"
                },
                {
                    "name": "Kalila Shapiro",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGsERTrHvlP2g/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=XIxbd0tldOs9kz7ur8bGalFmjUpeC5P7hk_mgAOYDtM&v=beta&width=180"
                }
            ],
            "built_with": [
                "adobe-audition",
                "adobe-illustrator",
                "adobe-suite",
                "ar",
                "augmented-reality",
                "azure",
                "azure-kinect",
                "c#",
                "depthkit",
                "magic-leap",
                "normcore",
                "oculus-quest",
                "unity",
                "virtual-reality",
                "volumetric",
                "volumetric-capture",
                "vr"
            ],
            "content_html": "<div>\n<p>When I Was Little is a shared memory from two different perspectives. Our narrator recalls an emotionally charged childhood memory \u2014 as they recount the story, two players experience it simultaneously. One player views the memory unfold in first-person virtual reality. The other player watches the story transpire in third-person, birds-eye-view augmented reality.</p>\n<p>The multiplayer experience mirrors the difference between the field and observer perspectives in an autobiographical memory. Sometimes, we remember episodes as seen through our own eyes \u2014 other times, it's as if we're watching the events from afar.</p>\n<p>It is a double screen with VR/AR graphics.</p>\n</div>",
            "content_md": "\nWhen I Was Little is a shared memory from two different perspectives. Our narrator recalls an emotionally charged childhood memory \u2014 as they recount the story, two players experience it simultaneously. One player views the memory unfold in first-person virtual reality. The other player watches the story transpire in third-person, birds-eye-view augmented reality.\n\n\nThe multiplayer experience mirrors the difference between the field and observer perspectives in an autobiographical memory. Sometimes, we remember episodes as seen through our own eyes \u2014 other times, it's as if we're watching the events from afar.\n\n\nIt is a double screen with VR/AR graphics.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/youtopian",
            "title": "YOUtopia",
            "blurb": "The future powered by your mind",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/AkBptwuxxwE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "User testing",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/233/datas/original.png"
                },
                {
                    "title": "outside town",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/231/datas/original.png"
                },
                {
                    "title": "inner world",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/232/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Wanyue Wang",
                    "about": "I worked on collecting EEG data from headset and processing EEG data by Python and unity. Co-designed the concept and experience flow.",
                    "photo": "https://www.gravatar.com/avatar/e61dfa076bdf072fda17349132e12051?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Chuyi Wu",
                    "about": "I mainly programmed Unity Script for the data-generative visual effects, and the trigger system, and merged the pieces in Unity project.\nI also co-designed and defined the concept and narrative experience flow.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/914/610/datas/profile.jpg"
                },
                {
                    "name": "Tina Chen",
                    "about": "I worked on creative thinking, idea generating and help with 3D modeling in Unity. I came up with the user journey map and do the graphic design of our project logo and wrap up our idea in the intro video.",
                    "photo": "https://www.gravatar.com/avatar/a8e1f8aff6f78383716bad1151f7fd40?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Yangli Liu",
                    "about": "I mainly worked on visual design, building 3D assets and unity set up (including lighting, sound, animation and teleportation). I also managed the project timeline and kept an eye on any post for our team.",
                    "photo": "https://www.gravatar.com/avatar/10a10f8eaa149decf69e9dab979880a7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "emotiv",
                "oculus",
                "python",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our team is interested in people's emotions and how these emotions can impact our world. Therefore, we decided to let users see their inner energy. We transform EEG detecting data to the variation which would change the color, light, shape even the type of the city that users can see in VR. Utopia should be the world that can be customized by every unique individual, with their mind power. So, we say that future should be YOUtopia.</p>\n<h2>What it does</h2>\n<p>Experience the world created by your mind and interact with it.</p>\n<p>First, a short intro animation takes you into the experience and you will see a fantasy world based on reality. You can explore the environment on your own and find an entry to your utopia, during which your EEG data will be recorded. When you find the entry, you can explore a unique utopian world generated by your EEG that reflects your mindset. </p>\n<h2>How we built it</h2>\n<p>Our team started with a brainstorming about how the future world would look like, and how everyone's emotions or mindset can impact the future. Based on our idea, we created a simple user journey and a narrative brief to guide our later development.</p>\n<ul>\n<li>Unity (VR development)</li>\n<li>Emotiv Epoc+ EEG headset (EEG data collection)</li>\n<li>Python (Read EEG data from headset and process data)</li>\n<li>Oculus Rift</li>\n<li>Tilt Brush (3D modeling)</li>\n<li>Google Block (3D modeling)</li>\n<li>Free sound resource (background music)</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<p>Everyone has a totally different opinion about the world, especially the future world. We tried to design an experience that is personalized for everyone. So the biggest challenge is how to create a generative world that represents the YOUtopian. We spent lots of time researching on several different visual styles and implement methods. Due to time limitation, we chose a mixed cyberpunk and authentic style to represent the \"current world\" and the \"future world\". We modeled most of assets by Tilt Brush. (There are still many things to improve.)</p>\n<p>Making use of EEG data is also challenging, because the it takes in data with various time gap (1-10s). EEG was also new to us, we spent time understanding the data format that EEG headset provides and how to integrate it into Unity development. We also tried to understand the meaning of those data, and develop a story with visual effects. Due to the lack of experienced Unity developer, we had a long scripting learning curve to creative generative art.</p>\n<p>Working with different VR SDKs is frustrated for us, we encountered many unexpected bugs due to unknown reasons.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We successfully parse the data and wrote our own script to generative smooth animation. We also built a visually pleasing and exciting VR environment for audience to explore around with teleportation. The whole journey and narrative experience is simple but poetic and procedural. Although we worked on different assets individually, the end result is very aesthetically consistent.</p>\n<h2>What we learned</h2>\n<ul>\n<li>Work with Unity and changing data.</li>\n<li>Understand a bit more about EEG data.</li>\n<li>Team cooperation</li>\n</ul>\n<h2>What's next for YOUtopian</h2>\n<p>We the experience can be more unique for different people. It would be better if we can make more generative shapes for interactions with a more accurate EEG sensor.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur team is interested in people's emotions and how these emotions can impact our world. Therefore, we decided to let users see their inner energy. We transform EEG detecting data to the variation which would change the color, light, shape even the type of the city that users can see in VR. Utopia should be the world that can be customized by every unique individual, with their mind power. So, we say that future should be YOUtopia.\n\n\n## What it does\n\n\nExperience the world created by your mind and interact with it.\n\n\nFirst, a short intro animation takes you into the experience and you will see a fantasy world based on reality. You can explore the environment on your own and find an entry to your utopia, during which your EEG data will be recorded. When you find the entry, you can explore a unique utopian world generated by your EEG that reflects your mindset. \n\n\n## How we built it\n\n\nOur team started with a brainstorming about how the future world would look like, and how everyone's emotions or mindset can impact the future. Based on our idea, we created a simple user journey and a narrative brief to guide our later development.\n\n\n* Unity (VR development)\n* Emotiv Epoc+ EEG headset (EEG data collection)\n* Python (Read EEG data from headset and process data)\n* Oculus Rift\n* Tilt Brush (3D modeling)\n* Google Block (3D modeling)\n* Free sound resource (background music)\n\n\n## Challenges we ran into\n\n\nEveryone has a totally different opinion about the world, especially the future world. We tried to design an experience that is personalized for everyone. So the biggest challenge is how to create a generative world that represents the YOUtopian. We spent lots of time researching on several different visual styles and implement methods. Due to time limitation, we chose a mixed cyberpunk and authentic style to represent the \"current world\" and the \"future world\". We modeled most of assets by Tilt Brush. (There are still many things to improve.)\n\n\nMaking use of EEG data is also challenging, because the it takes in data with various time gap (1-10s). EEG was also new to us, we spent time understanding the data format that EEG headset provides and how to integrate it into Unity development. We also tried to understand the meaning of those data, and develop a story with visual effects. Due to the lack of experienced Unity developer, we had a long scripting learning curve to creative generative art.\n\n\nWorking with different VR SDKs is frustrated for us, we encountered many unexpected bugs due to unknown reasons.\n\n\n## Accomplishments that we're proud of\n\n\nWe successfully parse the data and wrote our own script to generative smooth animation. We also built a visually pleasing and exciting VR environment for audience to explore around with teleportation. The whole journey and narrative experience is simple but poetic and procedural. Although we worked on different assets individually, the end result is very aesthetically consistent.\n\n\n## What we learned\n\n\n* Work with Unity and changing data.\n* Understand a bit more about EEG data.\n* Team cooperation\n\n\n## What's next for YOUtopian\n\n\nWe the experience can be more unique for different people. It would be better if we can make more generative shapes for interactions with a more accurate EEG sensor.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/mixalise",
            "title": "Mixalise",
            "blurb": "Youtube video content creation in AR. Record in a virtual environment. Add new lighting, camera angles, or even change sets on the fly.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/K-qH84BM1rA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "The scene in Unity",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/227/datas/original.png"
                },
                {
                    "title": "You can see an entire 3D enviromenet we created, or use your own",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/255/datas/original.PNG"
                },
                {
                    "title": "Start recording by doing an ok/pinch gesture",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/234/datas/original.jpeg"
                },
                {
                    "title": "User in an AR headset puppeteering an avatar in a virtual set",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/207/datas/original.png"
                },
                {
                    "title": "Moving the camera and view finder in experience to modify the camera angle which is output. The camera has a cute face so you look at it",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/208/datas/original.png"
                },
                {
                    "title": "Smashing that subscribe button to celebrate with an airhorn and blast some confetti in the room",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/209/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Mikko Haapoja",
                    "about": "- I setup an Inverse Kinematics rig on our character\n- Wrote logic to solve for Magic Leap hand tracking\n- Implemented the ability to grab objects\n- Random scripts to fire interactions such as airhorns and hide tutorials",
                    "photo": "https://avatars2.githubusercontent.com/u/496903?height=180&v=4&width=180"
                },
                {
                    "name": "Ben Clarke",
                    "about": "I wrote software to record position data for a user, and then replay it.  I also wrote an audio capture system, a ViewFinder camera system, a camera switcher, and several other pieces of functionality for the project.",
                    "photo": "https://avatars1.githubusercontent.com/u/29555850?height=180&v=4&width=180"
                },
                {
                    "name": "Yinjia Wen",
                    "about": "Graphic and UI Assets,\nAssist with Experience Design,\nFilming and Video Editing",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/936/250/datas/profile.jpg"
                },
                {
                    "name": "Brian C McDonald",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/60007142?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "augmented-reality",
                "blender",
                "c#",
                "magicleap",
                "solidworks",
                "unity"
            ],
            "content_html": "<div>\n<h1>Inspiration</h1>\n<p>Mikko\u2019s brother is a Youtuber by profession. He likes to evangelize everyone telling them to start publishing content on Youtube since it\u2019s changed his life. This is hard since the cost to create quality videos is high. To create quality videos you require a DSLR Camera, Lighting, and Locations to film in. We hope that AR and VR could possibly democratize video creation by giving people the option to create videos entirely virtually solving the need to have a camera, lighting, or location. People who live in small rooms in expensive cities can now have enough virtual space to make professional videos. We also hope that this new medium which can have a high level of anonymity can give people a voice to communicate. For instance product reviews created with our application could have a higher level of truth compared to the often rosey reviews posted to Youtube. Also by giving users the ability to choose their virtual avatar they can embody a character which is more representative of who they identify with.</p>\n<h1>How we built it</h1>\n<p>We built and designed all the 3D assets in our scene, working from Solidworks to Blender to Unity. We iterated using GitHub and Unity, getting the basic interactions right and then working on details like set design. We implemented custom code to record and playback scenes. In our application we had to create logic to solve for hand orientation for the Magic Leap.</p>\n<h1>What we learned</h1>\n<ul>\n<li>Since we\u2019re not bound by physical limitations we can use a human like face as a camera which is more approachable than the typical form factor of a camera</li>\n<li>Hand tracking can be a bit limiting for the actor since the hands need to be in view of the AR headset</li>\n<li>Many learnings about the Magic Leap Tool Chain and how Magic Leap works</li>\n<li>How to create assets for VR and to optimize these assets to run in lightweight headsets</li>\n</ul>\n<h1>Challenges we faced</h1>\n<ul>\n<li>Beyond tracking key points for hands Magic Leap does not return rotational information for hand tracking. A large portion of time was spent implementing this functionality</li>\n<li>Many members were using software that was not their specialty, and spent time learning Unity, Blender, and Git</li>\n</ul>\n</div>",
            "content_md": "\n# Inspiration\n\n\nMikko\u2019s brother is a Youtuber by profession. He likes to evangelize everyone telling them to start publishing content on Youtube since it\u2019s changed his life. This is hard since the cost to create quality videos is high. To create quality videos you require a DSLR Camera, Lighting, and Locations to film in. We hope that AR and VR could possibly democratize video creation by giving people the option to create videos entirely virtually solving the need to have a camera, lighting, or location. People who live in small rooms in expensive cities can now have enough virtual space to make professional videos. We also hope that this new medium which can have a high level of anonymity can give people a voice to communicate. For instance product reviews created with our application could have a higher level of truth compared to the often rosey reviews posted to Youtube. Also by giving users the ability to choose their virtual avatar they can embody a character which is more representative of who they identify with.\n\n\n# How we built it\n\n\nWe built and designed all the 3D assets in our scene, working from Solidworks to Blender to Unity. We iterated using GitHub and Unity, getting the basic interactions right and then working on details like set design. We implemented custom code to record and playback scenes. In our application we had to create logic to solve for hand orientation for the Magic Leap.\n\n\n# What we learned\n\n\n* Since we\u2019re not bound by physical limitations we can use a human like face as a camera which is more approachable than the typical form factor of a camera\n* Hand tracking can be a bit limiting for the actor since the hands need to be in view of the AR headset\n* Many learnings about the Magic Leap Tool Chain and how Magic Leap works\n* How to create assets for VR and to optimize these assets to run in lightweight headsets\n\n\n# Challenges we faced\n\n\n* Beyond tracking key points for hands Magic Leap does not return rotational information for hand tracking. A large portion of time was spent implementing this functionality\n* Many members were using software that was not their specialty, and spent time learning Unity, Blender, and Git\n\n\n"
        },
        {
            "source": "https://devpost.com/software/life-cmlf20",
            "title": "Life",
            "blurb": "Life is a Virtual reality experience transforming real-time datastream into an immersive surreal environment. This whole project is about only one everchanging number - the world population.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Uf0n0wXQ6K4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Flower representing each person&#39;s unique birth ID",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/243/datas/original.png"
                },
                {
                    "title": "Teleport",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/244/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "miaguo19 Guo",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/54534621?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<ul>\n<li>The theory of id, ego, and superego</li>\n<li>Landscape painting</li>\n<li>The idea and conception of life and death</li>\n<li>Visualization</li>\n</ul>\n<h2>What it does</h2>\n<ul>\n<li>A whole 3d experience about a single 2d number.</li>\n<li>The VR application visualizes live population data from worldometer.org by using the nature and landscape as the canvas. </li>\n<li>It shows how the world population changes at every moment. </li>\n<li>The user can pick up random flowers on the ground that represents the new life being born at each moment. The users will be able to see the unique id of the flower.</li>\n<li>The user will be able to interact with the fish and whales at the end. It represents the dead since life comes from water. </li>\n<li>It offers a holistic and different experience of data visualization.</li>\n</ul>\n<h2>How I built it</h2>\n<ul>\n<li>We used Unity and C#</li>\n</ul>\n<h2>Challenges I ran into</h2>\n<ul>\n<li>Wiring the data into the visualization</li>\n</ul>\n<h2>Accomplishments that I'm proud of</h2>\n<p>This is the first VR project for some of the members. </p>\n<h2>What I learned</h2>\n<ul>\n<li>Unity development. </li>\n</ul>\n<h2>What's next for Life</h2>\n<p>We imagined 2 more scenes, where the player will enter the ocean and submerge into the dead world. We will link the live death data and use similar visualizations. \nFirst, add two other scenes. Due to the time limitation, we could only build the first</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\n* The theory of id, ego, and superego\n* Landscape painting\n* The idea and conception of life and death\n* Visualization\n\n\n## What it does\n\n\n* A whole 3d experience about a single 2d number.\n* The VR application visualizes live population data from worldometer.org by using the nature and landscape as the canvas.\n* It shows how the world population changes at every moment.\n* The user can pick up random flowers on the ground that represents the new life being born at each moment. The users will be able to see the unique id of the flower.\n* The user will be able to interact with the fish and whales at the end. It represents the dead since life comes from water.\n* It offers a holistic and different experience of data visualization.\n\n\n## How I built it\n\n\n* We used Unity and C#\n\n\n## Challenges I ran into\n\n\n* Wiring the data into the visualization\n\n\n## Accomplishments that I'm proud of\n\n\nThis is the first VR project for some of the members. \n\n\n## What I learned\n\n\n* Unity development.\n\n\n## What's next for Life\n\n\nWe imagined 2 more scenes, where the player will enter the ocean and submerge into the dead world. We will link the live death data and use similar visualizations. \nFirst, add two other scenes. Due to the time limitation, we could only build the first\n\n\n"
        },
        {
            "source": "https://devpost.com/software/to-gather-0z24pf",
            "title": "To-Gather",
            "blurb": "Making new friends and building a community is difficult and we understand that, hence we bring to you To-Gather to bring your community together! ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/j3iyORDj1mc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ikshita Gupta",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a52e60b023803d8c13aaea0db52526ab?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kristen Dong",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQGu1GHQLVZTZQ/profile-displayphoto-shrink_100_100/0?e=1542844800&height=180&t=JWT-7v3zyRDaawRyVDk_kRevuvn7VnULYQhSnDWk9e0&v=beta&width=180"
                },
                {
                    "name": "shanilp7",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/23419126?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-creative-suite",
                "aframe",
                "arjs",
                "blender",
                "brackets",
                "github",
                "rhinoceros",
                "sketch"
            ],
            "content_html": "<div>\n<h2>Inspiration- In today's world, social media is dominating our interactions, people don't know anymore what it feels like to be in communities and get to interact with new individuals in person. With that in mind, we created a collaborative, narrative building application that utilizes mobile AR. To help us design the user experience, we imagined this would be implemented in a large apartment complex where people tend to not know their neighbors.</h2>\n<h2>What it does- Users search for AR markers that contain part of a narrative. To complete the narrative, they need to meet up in person and scan all the markers together. To further encourage genuine conversations, the narratives focus on things you usually don\u2019t discuss with strangers, such as vulnerabilities and loneliness.</h2>\n<h2>How I built it - We used Aframe and Arjs platforms to primarily built our framework. We used creative tools live Adobe, Blender, sketch to make our model more interactive and visually appealing</h2>\n<h2>Accomplishments that I'm proud of - All of us, in our team had no prior experience with XR, and so this was a learning curve for all of us; realizing we could make something fully functional when we just started to learn about it and also accomplish our collective goal to fight Social Isolation.We ran into difficulty to work with multiple marker proximity and for the multiple marker to load a single model. Generally teams works in cluster of collective roles like all developers work together, the designers work together but with our project since we all came from different domain it was challenging to bring on board everyone's viewpoint and come to a solution.</h2>\n<h2>What I learned- Working with different roles when creating a technical product, learning about their needs - business and technical, product management and working with Aframe software.</h2>\n<h2>Future additions to this project- Although we have a working prototype of our model, we still would like to add features to enhance user experience like having option to choose from a genre of stories to have animated AR experience and to provide login functionality as well as the ability to be able to keep our application in community moderation.</h2>\n<p>Grand Challenge Choice #1 - Social Good\nGrand Challenge Choice #2 - AR</p>\n</div>",
            "content_md": "\n## Inspiration- In today's world, social media is dominating our interactions, people don't know anymore what it feels like to be in communities and get to interact with new individuals in person. With that in mind, we created a collaborative, narrative building application that utilizes mobile AR. To help us design the user experience, we imagined this would be implemented in a large apartment complex where people tend to not know their neighbors.\n\n\n## What it does- Users search for AR markers that contain part of a narrative. To complete the narrative, they need to meet up in person and scan all the markers together. To further encourage genuine conversations, the narratives focus on things you usually don\u2019t discuss with strangers, such as vulnerabilities and loneliness.\n\n\n## How I built it - We used Aframe and Arjs platforms to primarily built our framework. We used creative tools live Adobe, Blender, sketch to make our model more interactive and visually appealing\n\n\n## Accomplishments that I'm proud of - All of us, in our team had no prior experience with XR, and so this was a learning curve for all of us; realizing we could make something fully functional when we just started to learn about it and also accomplish our collective goal to fight Social Isolation.We ran into difficulty to work with multiple marker proximity and for the multiple marker to load a single model. Generally teams works in cluster of collective roles like all developers work together, the designers work together but with our project since we all came from different domain it was challenging to bring on board everyone's viewpoint and come to a solution.\n\n\n## What I learned- Working with different roles when creating a technical product, learning about their needs - business and technical, product management and working with Aframe software.\n\n\n## Future additions to this project- Although we have a working prototype of our model, we still would like to add features to enhance user experience like having option to choose from a genre of stories to have animated AR experience and to provide login functionality as well as the ability to be able to keep our application in community moderation.\n\n\nGrand Challenge Choice #1 - Social Good\nGrand Challenge Choice #2 - AR\n\n\n"
        },
        {
            "source": "https://devpost.com/software/icarus-s3eqbw",
            "title": "Icarus",
            "blurb": "A Simulation of the Human Anthropocene",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385861344?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Primary Navigation Interface - Photo 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/121/datas/original.jpg"
                },
                {
                    "title": "Primary Navigation Interface - Photo 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/122/datas/original.jpg"
                },
                {
                    "title": "Primary Navigation Interface - Photo 3",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/123/datas/original.jpg"
                },
                {
                    "title": "Luscious Forest - Photo 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/131/datas/original.jpg"
                },
                {
                    "title": "Luscious Forest - Photo 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/132/datas/original.jpg"
                },
                {
                    "title": "Luscious Forest - Photo 3",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/125/datas/original.jpg"
                },
                {
                    "title": "Incinerated Forest",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/124/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "deleted deleted",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/6bd927cbe9b5c30834e5f44cb03c3541?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "aftereffects",
                "c#",
                "cinema4d",
                "houdini",
                "leap-motion",
                "mediaencoder",
                "octanerender",
                "photoshop",
                "substancedesigner",
                "substancepainter",
                "topazlabs",
                "unfold3d",
                "unity",
                "varjo"
            ],
            "content_html": "<div>\n<h1>Icarus - Earth Has Fallen.</h1>\n<p>Welcome to Icarus. Inspired by recent wildfires and natural disasters, Icarus is an ultra-high resolution virtual reality experience. The experience begins in a man-made room, giving the user the choice to one of three natural environments.</p>\n<p>Upon entering one of the environments, users are presented with the ability to impact their world through direct object manipulation. As users steadily tarnish the world, a social parallel to what humanity has done to Earth, the conditions degrade rapidly.</p>\n<p>The experience ends in a eerie scene of the world bereft of life, on fire. This is a commentary on the recent wildfires and natural disasters that have been ravaging the planet. We'd like Icarus to raise awareness for these kinds of events, and thanks to the ultra-high resolution of the Varjo headset, we can depict these kinds of changes in unparalleled quality.</p>\n</div>",
            "content_md": "\n# Icarus - Earth Has Fallen.\n\n\nWelcome to Icarus. Inspired by recent wildfires and natural disasters, Icarus is an ultra-high resolution virtual reality experience. The experience begins in a man-made room, giving the user the choice to one of three natural environments.\n\n\nUpon entering one of the environments, users are presented with the ability to impact their world through direct object manipulation. As users steadily tarnish the world, a social parallel to what humanity has done to Earth, the conditions degrade rapidly.\n\n\nThe experience ends in a eerie scene of the world bereft of life, on fire. This is a commentary on the recent wildfires and natural disasters that have been ravaging the planet. We'd like Icarus to raise awareness for these kinds of events, and thanks to the ultra-high resolution of the Varjo headset, we can depict these kinds of changes in unparalleled quality.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/seniar",
            "title": "SeniAR",
            "blurb": "Build shareable affective memories for elders ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/l5sdauz7VPk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Logo Design ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/063/datas/original.png"
                },
                {
                    "title": "Mobile 3d environment builder ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/774/datas/original.png"
                },
                {
                    "title": "Yinggui&#39;s AR message to his grandmother",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/541/datas/original.png"
                },
                {
                    "title": "Social impact for elders",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/066/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Brandon Chen",
                    "about": "HoloLens 2 Developer",
                    "photo": "https://www.gravatar.com/avatar/02c8583d222f72c3e55e5bd48aec79d9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Hyejun Youn",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/864/885/datas/profile.png"
                },
                {
                    "name": "Yinggui Yang",
                    "about": "",
                    "photo": "https://graph.facebook.com/168314427783054/picture?height=180&width=180"
                },
                {
                    "name": "Camille Jeanjean",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5603AQGvAc_1wYw4Hg/profile-displayphoto-shrink_800_800/0?e=1584576000&height=180&t=wGRpj4yo9CpZXa7jk95PVxuVLKLnWIEfr4qfcjIC9I4&v=beta&width=180"
                }
            ],
            "built_with": [
                "aftereffect",
                "antoinejeanjeanpiano",
                "c#",
                "maya",
                "microsoft-hololens",
                "miscrosoftmixedreality",
                "mixedrealitytoolkit",
                "mrtk",
                "photogrammetrysdk",
                "photoshop",
                "proto.io",
                "sketch",
                "sketchfab",
                "soundjay",
                "unity",
                "youtube(audio)"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Last Christmas, one of our team members saw her grandmother who now lives in a nursing home. She is now bedridden with a very limited mobility, but she is still so alive inside. Our teammate could see the liveliness in her eyes. She talked about her <strong>hopes to move, travel and share memories with us</strong>. Her vision is the inspiration of this project. Moreover, everyone has grandparents, and this application will let you <strong>build shareable memories with elders in augmented spaces, and eventually create a new virtual heritage between you and them</strong>.</p>\n<h2>Problems</h2>\n<p>Elders suffer from <strong>social isolation</strong> by staying in a limited space for a long time, which eventually causes many complications such as <strong>loss of muscle strength and mental illness</strong>. We decided to build AR Intimate messenger that connects younger generations with elders through technology. <strong>How can we create an affective environment to reconnect generations, and fulfill their hopes to share memories with us?</strong></p>\n<h2>What it does</h2>\n<p>SeniAR is an AR application for family members to create an AR space using <strong>photogrammetry technique</strong> and 3D templates, and send the new space to elders using <strong>AR Cloud</strong>. Elders receive the augmented reality messages along with text messages from their family. They can immerse into the environment, be relaxed, exercise, listen to music and sound and share memories with their family, creating intimate feelings. We believed that interaction in AR allows elders to be more engaged to situations, and exercise their bodies in a more entertaining way. For restricted and disabled people, eye-tracking on Hololens2 seems the resolution of this isolation.</p>\n<p>You will craft a shareable memorial environment with landscape and registered audio message to send to your grandmother laying on her bed and using an HOLOLENS2  in her nursing home. </p>\n<p><strong>Here are some steps:</strong></p>\n<p>Mobile users\n</p><li>A user choose from two options - Capture and Share. </li>\n<br/>Capture - 3D scan objects, plants, portraits and more using photogrammetry software.\n<br/>Share - a user can create an AR environment from choosing templates on the application, then add 3d models such as plants and trees or your 3d scanned models.\n<li>Register an intimate audio message for your family members through a memory you have in common. </li>\n<li>Send the message to your family or friends using AR Cloud! </li><p></p>\n<p>Hololens users</p>\n<p></p><li>An alarm pops up saying that you have a new message! </li>\n<li>Open the message, and AR environment will appear along with the voice message. She can read it, if she can hear well. </li>\n<li>Immerse into the AR environment by interacting with the objects. </li><p></p>\n<p><strong>Why AR Cloud?</strong></p>\n<p>We believe that using AR cloud, we could create different layers of AR templates environments that can be easily shared and be shared by friends and family, and users to be immersed into the environments.  Private shareable and open-source environments are provided. </p>\n<h2>How we built it</h2>\n<p>We built the project in Unity. The 3D assets were obtained for free on Sketchfab, free Unity assets, and video resources from Youtube. We experimented with some other ways to share the file with Aframe and AR Cloud. We used _ dispaly.land _ and _ capture _ to demonstrate our photogrammetry idea. Lastly, we used Hololens2 to augment our environments!</p>\n<h2>Challenges I ran into</h2>\n<p></p><li>We only had one developer, and no professional 3D animator. So, we went to the workshops on Wednesday, and watched Youtube tutorials. \n</li><li>We wanted to have some interviews with elders, but the interviews were very limited to our family members. \n</li><li>We did not have enough time to use photogrammetry SDK on our mobile application, but we were able to visualize our ideas. \n</li><li>The entire team members didn\u2019t have experience in Hololens2. We struggled to get used to the equipment, and had to call mentors several times. We are glad that the AR environment is running and looking amazing!! <p></p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We are proud of our idea and concept. \nEach person got his /her role in the team and brought amazing creative inputs. </p>\n<h2>What we learned:</h2>\n<p>How to cooperate in a diverse environment, and how to develop it with Hololens2.\nA new understanding of family relationship.\nEye-tracking tool. </p>\n<h2>What's next for SeniAR</h2>\n<p>We want to further develop eye tracking AR for people with mobile disabilities. We also initially wanted to create a 3D layers of a Nursing Home (having layers of user's data, meditation room, fitness center like an AR Cloud 3D map), but later we decided to focus more on the social interaction. We want to create AR environment where elders can exercise, meditate, and enhance social interaction. </p>\n</li>\n</div>",
            "content_md": "\n## Inspiration\n\n\nLast Christmas, one of our team members saw her grandmother who now lives in a nursing home. She is now bedridden with a very limited mobility, but she is still so alive inside. Our teammate could see the liveliness in her eyes. She talked about her **hopes to move, travel and share memories with us**. Her vision is the inspiration of this project. Moreover, everyone has grandparents, and this application will let you **build shareable memories with elders in augmented spaces, and eventually create a new virtual heritage between you and them**.\n\n\n## Problems\n\n\nElders suffer from **social isolation** by staying in a limited space for a long time, which eventually causes many complications such as **loss of muscle strength and mental illness**. We decided to build AR Intimate messenger that connects younger generations with elders through technology. **How can we create an affective environment to reconnect generations, and fulfill their hopes to share memories with us?**\n\n\n## What it does\n\n\nSeniAR is an AR application for family members to create an AR space using **photogrammetry technique** and 3D templates, and send the new space to elders using **AR Cloud**. Elders receive the augmented reality messages along with text messages from their family. They can immerse into the environment, be relaxed, exercise, listen to music and sound and share memories with their family, creating intimate feelings. We believed that interaction in AR allows elders to be more engaged to situations, and exercise their bodies in a more entertaining way. For restricted and disabled people, eye-tracking on Hololens2 seems the resolution of this isolation.\n\n\nYou will craft a shareable memorial environment with landscape and registered audio message to send to your grandmother laying on her bed and using an HOLOLENS2 in her nursing home. \n\n\n**Here are some steps:**\n\n\nMobile users\n\n\n- A user choose from two options - Capture and Share.\n\n  \nCapture - 3D scan objects, plants, portraits and more using photogrammetry software.\n  \nShare - a user can create an AR environment from choosing templates on the application, then add 3d models such as plants and trees or your 3d scanned models.\n- Register an intimate audio message for your family members through a memory you have in common.\n\n- Send the message to your family or friends using AR Cloud!\n\nHololens users\n\n\n- An alarm pops up saying that you have a new message!\n\n- Open the message, and AR environment will appear along with the voice message. She can read it, if she can hear well.\n\n- Immerse into the AR environment by interacting with the objects.\n\n**Why AR Cloud?**\n\n\nWe believe that using AR cloud, we could create different layers of AR templates environments that can be easily shared and be shared by friends and family, and users to be immersed into the environments. Private shareable and open-source environments are provided. \n\n\n## How we built it\n\n\nWe built the project in Unity. The 3D assets were obtained for free on Sketchfab, free Unity assets, and video resources from Youtube. We experimented with some other ways to share the file with Aframe and AR Cloud. We used \\_ dispaly.land \\_ and \\_ capture \\_ to demonstrate our photogrammetry idea. Lastly, we used Hololens2 to augment our environments!\n\n\n## Challenges I ran into\n\n\n- We only had one developer, and no professional 3D animator. So, we went to the workshops on Wednesday, and watched Youtube tutorials.\n- We wanted to have some interviews with elders, but the interviews were very limited to our family members.\n- We did not have enough time to use photogrammetry SDK on our mobile application, but we were able to visualize our ideas.\n- The entire team members didn\u2019t have experience in Hololens2. We struggled to get used to the equipment, and had to call mentors several times. We are glad that the AR environment is running and looking amazing!! \n## Accomplishments that I'm proud of\n\n\nWe are proud of our idea and concept. \nEach person got his /her role in the team and brought amazing creative inputs. \n\n\n## What we learned:\n\n\nHow to cooperate in a diverse environment, and how to develop it with Hololens2.\nA new understanding of family relationship.\nEye-tracking tool. \n\n\n## What's next for SeniAR\n\n\nWe want to further develop eye tracking AR for people with mobile disabilities. We also initially wanted to create a 3D layers of a Nursing Home (having layers of user's data, meditation room, fitness center like an AR Cloud 3D map), but later we decided to focus more on the social interaction. We want to create AR environment where elders can exercise, meditate, and enhance social interaction.\n\n"
        },
        {
            "source": "https://devpost.com/software/shaken",
            "title": "Shaken - MIT Reality Hack",
            "blurb": "Puerto Rico has been hit with more than a 1000 earthquakes over the past three weeks and our goal was to raise awareness about the misfortune of the locals.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/blU8UPvUS5w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Daniel Sabio",
                    "about": "Creative Direction (Writing, Acting, music score, some 3D modeling)",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/199/datas/profile.jpg"
                },
                {
                    "name": "krispatl Pilcher",
                    "about": "Lead developer and Volumetric Film Capture",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/050/046/datas/profile.png"
                },
                {
                    "name": "Peter Lakatos",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/940/356/datas/profile.JPG"
                },
                {
                    "name": "Alexandros Lotsos",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/16785997?height=180&v=4&width=180"
                },
                {
                    "name": "Trilina",
                    "about": "",
                    "photo": "https://graph.facebook.com/2815659508498010/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "dephtkit",
                "kinect",
                "normal",
                "nreal",
                "oculus-quest",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>One of our teammate's family is from Puerto Rico, where over a 1000 earthquakes hit over the past three weeks. Thousands of homes were damaged and lost, forcing almost 10000 people to wait out the aftershocks in shelters. Electricity and water resources are also scarce in the region. Our project's goal is to raise awareness about the dire situation locals have been living in the past month and encourage viewers to help the affected. </p>\n<h2>What it does</h2>\n<p>We've built a narrative story piece VR experience that guides the viewer through the earthquake disaster in Ponce, Puerto Rico. Their journey goes through a sequence of scenes to grasp the graveness of the local's trauma and constant fear. There is also narration done by our Puerto Rican teammate Daniel Sabio, to fully immerse the viewer in the Caribbean experience. \nIn addition to the VR part, we also have an AR perspective, which is focused on data visualization and more factual information. </p>\n<h2>How we built it</h2>\n<p>We used the Unity game engine to build our VR experience on the Occulus Quest, and our AR experience on the Nreal mixed reality glasses. Daniel Sabio's narration was directed, acted, and captured live during the hackathon using a Microsoft Kinect and Depthkit, while all our 3D assets were also built live during the hackathon using Tiltbrush on a separate VR headset. A member of our team was dedicated to researching factual information and compiling their own report on the damages that Ponce suffered, highlights of which appear alongside Sabio's narration in our AR experience.</p>\n<h2>Challenges we ran into</h2>\n<p>Integration between Unity and GitHub did not work as seamlessly as we expected. Unity asset store plugin for GitHub and a mentor intending to help unfortunately ended up deleting our project files, which set us back quite significantly. But we did try to recreate our environment and restore the project. </p>\n<p>We also ran into some issues regarding video playback on nreal and no one could seemingly figure out what the problem was, but we ended up resolving the issue, the root of which turned out to be the transcoding upon video import.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of the fact that we were able to use this many technologies and create multiple platforms for the experience. We had some hiccups with github - some unrelated to our actions - but we managed to keep our cool, rebuild and finish the project. </p>\n<h2>What we learned</h2>\n<p>We learned new skills in Tilt Brush and got familiar with the nreal developer environment. We learned valuable lessons about keeping a backup of our project and making sure that we have checkpoints. </p>\n<h2>What's next for Shaken</h2>\n<p>The concept to raise awareness through VR installations can be applied to any story that is worth telling. The next step would be creating a general framework for communicating any type of experience, both with infographics and emotional footage. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOne of our teammate's family is from Puerto Rico, where over a 1000 earthquakes hit over the past three weeks. Thousands of homes were damaged and lost, forcing almost 10000 people to wait out the aftershocks in shelters. Electricity and water resources are also scarce in the region. Our project's goal is to raise awareness about the dire situation locals have been living in the past month and encourage viewers to help the affected. \n\n\n## What it does\n\n\nWe've built a narrative story piece VR experience that guides the viewer through the earthquake disaster in Ponce, Puerto Rico. Their journey goes through a sequence of scenes to grasp the graveness of the local's trauma and constant fear. There is also narration done by our Puerto Rican teammate Daniel Sabio, to fully immerse the viewer in the Caribbean experience. \nIn addition to the VR part, we also have an AR perspective, which is focused on data visualization and more factual information. \n\n\n## How we built it\n\n\nWe used the Unity game engine to build our VR experience on the Occulus Quest, and our AR experience on the Nreal mixed reality glasses. Daniel Sabio's narration was directed, acted, and captured live during the hackathon using a Microsoft Kinect and Depthkit, while all our 3D assets were also built live during the hackathon using Tiltbrush on a separate VR headset. A member of our team was dedicated to researching factual information and compiling their own report on the damages that Ponce suffered, highlights of which appear alongside Sabio's narration in our AR experience.\n\n\n## Challenges we ran into\n\n\nIntegration between Unity and GitHub did not work as seamlessly as we expected. Unity asset store plugin for GitHub and a mentor intending to help unfortunately ended up deleting our project files, which set us back quite significantly. But we did try to recreate our environment and restore the project. \n\n\nWe also ran into some issues regarding video playback on nreal and no one could seemingly figure out what the problem was, but we ended up resolving the issue, the root of which turned out to be the transcoding upon video import.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of the fact that we were able to use this many technologies and create multiple platforms for the experience. We had some hiccups with github - some unrelated to our actions - but we managed to keep our cool, rebuild and finish the project. \n\n\n## What we learned\n\n\nWe learned new skills in Tilt Brush and got familiar with the nreal developer environment. We learned valuable lessons about keeping a backup of our project and making sure that we have checkpoints. \n\n\n## What's next for Shaken\n\n\nThe concept to raise awareness through VR installations can be applied to any story that is worth telling. The next step would be creating a general framework for communicating any type of experience, both with infographics and emotional footage. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/liberty-city-vr",
            "title": "Liberty City VR",
            "blurb": "A VR experience that take users through the ups and downs of Miaimi\u2019s legendary neighborhood, from its heyday in the 1940s and 60s through its slow decline that culminated with the Miami riots in 1980",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/-t688v33rTE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Gameplay Screenshot 3",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/953/datas/original.png"
                },
                {
                    "title": "Gameplay Screenshot 4",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/954/datas/original.png"
                },
                {
                    "title": "Gameplay Screenshot 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/955/datas/original.png"
                },
                {
                    "title": "Gameplay Screenshot 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/956/datas/original.png"
                },
                {
                    "title": "Gameplay Screenshot 5",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/957/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Brandy Goodrick",
                    "about": "I was the main Unity dev on this project but my team mate picked up the engine very fast. It was a crunch for sure and  wish I had the time to make this look exactly the way I envisioned it, but I ran into issue with models (like you always do). Time constraints hindered me but I would love to continue working on this project. I learned that sleep is a bigger deal than I thought. Lol",
                    "photo": "https://www.gravatar.com/avatar/5044c84590709659ac4a2852b861efa0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Liberty City VR was inspired by a feature length documentary film two members of our team have been producing titled \"Razing Liberty Square\". The film documents the razing and redevelopment of the first segregated housing project (Liberty Square) which was constructed in the heart of Liberty City, a historic African-American neighborhood in Miami currently at risk of disappearing due to climate gentrification. During the last three years of filming we have collected a lot of oral history and memories from residents who remember the neighborhoods 'Golden Age'. We realized that VR would be the perfect platform to bring this 'Golden Age' back to life.</p>\n<p>The VR project invites users to experience Liberty City (and Liberty Square) as it was in its heyday, when segregation and Jim Crow disallowed black men and women to live freely, and yet in this small corner of the country, a community was able to support itself, construct itself, and thrive. We hoped to highlight this former black utopia with our build for this Hackathon. </p>\n<h2>What it does</h2>\n<p>Users are transported to Liberty Square, the first segregated public housing project, and the heart of historic Liberty City. Users are then guided by the voices of residents - past and present - explore the neighborhood, and the dichotomy of realities contrasting the experiences of adults who understand the horrid nature of the wall that surrounds the housing project, and the children who play and live at peace, oblivious to the ominous cement barrier which surrounds their home and defines the world around them.</p>\n<h2>How I built it</h2>\n<p>This project was built with the Unity engine. Some of our models were created and textured on-site, others were imported for free from the unity web store. We also got several free sound files from online. After we had all of the assets we needed, we arranged them together in unity and created the animations for the story.</p>\n<h2>Challenges I ran into</h2>\n<p>We didn't have a 3D modeler, or graphic artist, so our developers and producers had to fill in that role instead. \nWe also had some difficulty procuring our equipment, so we had to try to catch up by the end of Day 1 of the hack. \nWe had problems being able to model the assets exactly as we wanted to. It was also pretty hard to make sure the animation worked properly for all of the models. With some help from mentors and staff, we were able to do so, which was awesome.  From a creative point of view, the sound design was very complicated and our team did not have the experience nor the tools to actually create the design that we envisioned.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We are proud of the fact that we were able to make and texture the models despite the absence of a 3D modeler. This was a wonderful opportunity to learn something new for all of us. Moreover, our team dealt with a lot of limitations and still managed to make something that gave some of the reviewers goosebumps! </p>\n<h2>What I learned</h2>\n<p>We learned how to use Unity to create models. The filmmakers in our team learned to work in a different story telling environment, and also how to contribute to texturing models. Our Developer became a stronger advocate for creative storytelling, having experienced and led a team of experienced storytellers down a into a wildly different environment.</p>\n<h2>What's next for Liberty City VR</h2>\n<p>We are now moving forward to creating the entire VR experience. It is stand alone but also part of the larger impact campaign of the documentary \"Razing Liberty Square\" by Katja Esson, the project will be exhibited publicly in key communities where public partnerships have been established in conjunction with festival/theatrical screening of the documentary. Ultimately, the project will be housed in permanent exhibitions at the Hampton House, the African Heritage Cultural Arts Center and other Liberty City/Miami historical museums. </p>\n<p>We want to tell the story of this neighborhood that four generations of African-Americans have called their home. We want to debunk misconceptions about the impoverished and chronically violent area of today and educate users on the factors that caused the decline of this once vibrant neighborhood. </p>\n<p>The project intends to stimulate conversation among various Miami communities, as well as communities around the nation. The VR experience with its strong social justice content will be a tool to empower the community of Liberty City, and bring the neighborhood's history and struggle to the forefront of a citywide conversation. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nLiberty City VR was inspired by a feature length documentary film two members of our team have been producing titled \"Razing Liberty Square\". The film documents the razing and redevelopment of the first segregated housing project (Liberty Square) which was constructed in the heart of Liberty City, a historic African-American neighborhood in Miami currently at risk of disappearing due to climate gentrification. During the last three years of filming we have collected a lot of oral history and memories from residents who remember the neighborhoods 'Golden Age'. We realized that VR would be the perfect platform to bring this 'Golden Age' back to life.\n\n\nThe VR project invites users to experience Liberty City (and Liberty Square) as it was in its heyday, when segregation and Jim Crow disallowed black men and women to live freely, and yet in this small corner of the country, a community was able to support itself, construct itself, and thrive. We hoped to highlight this former black utopia with our build for this Hackathon. \n\n\n## What it does\n\n\nUsers are transported to Liberty Square, the first segregated public housing project, and the heart of historic Liberty City. Users are then guided by the voices of residents - past and present - explore the neighborhood, and the dichotomy of realities contrasting the experiences of adults who understand the horrid nature of the wall that surrounds the housing project, and the children who play and live at peace, oblivious to the ominous cement barrier which surrounds their home and defines the world around them.\n\n\n## How I built it\n\n\nThis project was built with the Unity engine. Some of our models were created and textured on-site, others were imported for free from the unity web store. We also got several free sound files from online. After we had all of the assets we needed, we arranged them together in unity and created the animations for the story.\n\n\n## Challenges I ran into\n\n\nWe didn't have a 3D modeler, or graphic artist, so our developers and producers had to fill in that role instead. \nWe also had some difficulty procuring our equipment, so we had to try to catch up by the end of Day 1 of the hack. \nWe had problems being able to model the assets exactly as we wanted to. It was also pretty hard to make sure the animation worked properly for all of the models. With some help from mentors and staff, we were able to do so, which was awesome. From a creative point of view, the sound design was very complicated and our team did not have the experience nor the tools to actually create the design that we envisioned.\n\n\n## Accomplishments that I'm proud of\n\n\nWe are proud of the fact that we were able to make and texture the models despite the absence of a 3D modeler. This was a wonderful opportunity to learn something new for all of us. Moreover, our team dealt with a lot of limitations and still managed to make something that gave some of the reviewers goosebumps! \n\n\n## What I learned\n\n\nWe learned how to use Unity to create models. The filmmakers in our team learned to work in a different story telling environment, and also how to contribute to texturing models. Our Developer became a stronger advocate for creative storytelling, having experienced and led a team of experienced storytellers down a into a wildly different environment.\n\n\n## What's next for Liberty City VR\n\n\nWe are now moving forward to creating the entire VR experience. It is stand alone but also part of the larger impact campaign of the documentary \"Razing Liberty Square\" by Katja Esson, the project will be exhibited publicly in key communities where public partnerships have been established in conjunction with festival/theatrical screening of the documentary. Ultimately, the project will be housed in permanent exhibitions at the Hampton House, the African Heritage Cultural Arts Center and other Liberty City/Miami historical museums. \n\n\nWe want to tell the story of this neighborhood that four generations of African-Americans have called their home. We want to debunk misconceptions about the impoverished and chronically violent area of today and educate users on the factors that caused the decline of this once vibrant neighborhood. \n\n\nThe project intends to stimulate conversation among various Miami communities, as well as communities around the nation. The VR experience with its strong social justice content will be a tool to empower the community of Liberty City, and bring the neighborhood's history and struggle to the forefront of a citywide conversation. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/panopticon-my25br",
            "title": "PANOPTICON",
            "blurb": "Eyes in the Back of Your Head",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/wOBmQ9UEebs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "View from Inside the Headset",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/205/datas/original.png"
                },
                {
                    "title": "Testing",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/206/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Rihn H",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/425/937/datas/profile.PNG"
                },
                {
                    "name": "James Koppel",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/1279592?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "camon",
                "magic-leap"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We wanted to explore extensions of the human senses, such as seeing more than that which is usually visible.</p>\n<h2>What it does</h2>\n<p>Provides the User with a view of a rearward facing camera without altering the orientation of one's head.</p>\n<h2>How we built it</h2>\n<p>In order to view the camera feed with good bandwidth through LAN and because of a bug mentioned below, we viewed it in-browser. </p>\n<h2>Challenges we ran into</h2>\n<p>We then encountered two bugs in Magic Leap which burnt the bulk of our remaining time. The first bug was that the Magic Leap browser, Helio, is by default configured to not access LAN, and would not prompt for permission upon accessing a LAN URL. The second bug, more serious, is that the Magic Leap Media Player is evidently unable to load videos from LAN, even with permissions properly configured. A Magic Leap employee confirmed this bug and filed it internally on our behalf.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Despite a lot of struggle with bugs in the Magic Leap, we managed to put together a cool demo pretty fast. </p>\n<h2>What we learned</h2>\n<p>From a cold start, we became comfortable with the Magic Leap's interface and its programmatic interface, as well as dabbles in Unity 3D.</p>\n<h2>What's next for PANOPTICON</h2>\n<p>The next step would be to create a more permanent hardware attachment, and to integrate cameras in more directions for true omnidirectional view.</p>\n<p>Team 72\n3A-12\nFuture Mobility; AR</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe wanted to explore extensions of the human senses, such as seeing more than that which is usually visible.\n\n\n## What it does\n\n\nProvides the User with a view of a rearward facing camera without altering the orientation of one's head.\n\n\n## How we built it\n\n\nIn order to view the camera feed with good bandwidth through LAN and because of a bug mentioned below, we viewed it in-browser. \n\n\n## Challenges we ran into\n\n\nWe then encountered two bugs in Magic Leap which burnt the bulk of our remaining time. The first bug was that the Magic Leap browser, Helio, is by default configured to not access LAN, and would not prompt for permission upon accessing a LAN URL. The second bug, more serious, is that the Magic Leap Media Player is evidently unable to load videos from LAN, even with permissions properly configured. A Magic Leap employee confirmed this bug and filed it internally on our behalf.\n\n\n## Accomplishments that we're proud of\n\n\nDespite a lot of struggle with bugs in the Magic Leap, we managed to put together a cool demo pretty fast. \n\n\n## What we learned\n\n\nFrom a cold start, we became comfortable with the Magic Leap's interface and its programmatic interface, as well as dabbles in Unity 3D.\n\n\n## What's next for PANOPTICON\n\n\nThe next step would be to create a more permanent hardware attachment, and to integrate cameras in more directions for true omnidirectional view.\n\n\nTeam 72\n3A-12\nFuture Mobility; AR\n\n\n"
        },
        {
            "source": "https://devpost.com/software/duet-oygkrx",
            "title": "Duet",
            "blurb": "Duet is a contemplative dance ritual for two people in shared space for Oculus Quest. The experience inspires new postures for VR, with people making physical contact in intimate and soft ways.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/b-5ptSwRSHA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Carla Cardenas",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/19547988?height=180&v=4&width=180"
                },
                {
                    "name": "Kevin Laibson",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C5603AQHK41TrBe_55Q/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=4SzkLCf-qlDYnWj7lyMOw4kHv2BDWHUONCSVFaq0NrY&v=beta&width=180"
                },
                {
                    "name": "Alisia Martinez",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/339/193/datas/profile.jpg"
                },
                {
                    "name": "Brandon Powers",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/696/951/datas/profile.JPG"
                }
            ],
            "built_with": [
                "github",
                "maya",
                "normcore",
                "oculus",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>To simplify the challenge of networking two players in a space we used Normcore. This made the complicated process much easier as we could easily use the plugin to launch our two users into the scene. </p>\n<h2>What it does</h2>\n<h2>How We Built It</h2>\n<h2>Challenges I ran into</h2>\n<ol>\n<li><p>Some challenges we faced included optimization for the Quest. The visual effects we imagined could not be replicated using Unity\u2019s new Shadergraph or high definition pipeline as we were developing for a mobile platform.</p></li>\n<li><p>A huge technical challenge we wanted to tackle was using two Oculus Quests in the same physical location so that users could interact by touching hands. There is no easy solution for this currently so we had to use Normcore plus some creative solving to make it work.</p></li>\n<li><p>Another obstacle was the instability of Oculus development with both Rift and Quest. At some points we could easily deploy to either, but at others we had to debug many processes that had no clear solution, including restarting the devices until it magically clicked. </p></li>\n</ol>\n<h2>Accomplishments that We're Proud Of</h2>\n<p>Even though the aesthetics of the experience for this Hack prototype didn't match our expectations, we were extremely proud of how the design functioned. </p>\n<h2>What We learned</h2>\n<h2>What's next for Duet</h2>\n<p>We are excited to continue to build on the experience, solving some technical challenges such a faulty animation when the Visitors reached a certain section of the dance. Then we'll playtest this new version before adding in the planned visuals and audio that didn't make it in the build at the Hack. We look forward to finding partners for funding and presentation to take DUET to festivals, theaters, and events around the world. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTo simplify the challenge of networking two players in a space we used Normcore. This made the complicated process much easier as we could easily use the plugin to launch our two users into the scene. \n\n\n## What it does\n\n\n## How We Built It\n\n\n## Challenges I ran into\n\n\n1. Some challenges we faced included optimization for the Quest. The visual effects we imagined could not be replicated using Unity\u2019s new Shadergraph or high definition pipeline as we were developing for a mobile platform.\n2. A huge technical challenge we wanted to tackle was using two Oculus Quests in the same physical location so that users could interact by touching hands. There is no easy solution for this currently so we had to use Normcore plus some creative solving to make it work.\n3. Another obstacle was the instability of Oculus development with both Rift and Quest. At some points we could easily deploy to either, but at others we had to debug many processes that had no clear solution, including restarting the devices until it magically clicked.\n\n\n## Accomplishments that We're Proud Of\n\n\nEven though the aesthetics of the experience for this Hack prototype didn't match our expectations, we were extremely proud of how the design functioned. \n\n\n## What We learned\n\n\n## What's next for Duet\n\n\nWe are excited to continue to build on the experience, solving some technical challenges such a faulty animation when the Visitors reached a certain section of the dance. Then we'll playtest this new version before adding in the planned visuals and audio that didn't make it in the build at the Hack. We look forward to finding partners for funding and presentation to take DUET to festivals, theaters, and events around the world. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/spacechangers",
            "title": "SpaceChanger",
            "blurb": "We're SpaceChangers, a team behind the SpaceChanger app - a WebAR utility to help with home renovation. Users can visualize wall, ceiling, and floor texture changes from their AR enabled smartphones.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Nay6ayVJiMc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "manokhinv Manokhin",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/4396449?height=180&v=4&width=180"
                },
                {
                    "name": "code-matt",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/17485082?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "aframe",
                "arkit",
                "css",
                "html",
                "javascript",
                "magic-leap",
                "magic-script",
                "react",
                "react-native"
            ],
            "content_html": "<div>\n<h1>SpaceChanger</h1>\n<p>SpaceChanger is a WebAR utility to help with home renovation. Users can visualize wall, ceiling, and floor texture changes from their AR enabled smartphone.</p>\n<h2>Source Code</h2>\n<p>SpaceChange is open source and hosted on GitHub:</p>\n<p>React Native ARKit app for creating rooms (RN-AR branch) - <a href=\"https://github.com/code-matt/spacechangers/tree/RN-AR\" rel=\"nofollow\">https://github.com/code-matt/spacechangers/tree/RN-AR</a>\nWebXR App For Viewing rooms (master branch) - <a href=\"https://github.com/code-matt/spacechangers/edit/master/\" rel=\"nofollow\">https://github.com/code-matt/spacechangers/edit/master/</a>\n(Note different git histories on branches above because of very late project tech pivot to ARKit)</p>\n<h2>Usage</h2>\n<ol>\n<li>Upon launch, users will add SpaceChanger Surfaces to their environment by placing 4 corners to define a surface. </li>\n<li>After placement, users can interact with any SpaceChanger Surface by selecting them. Users will be able to see texture/color/paint changes in real-time, and select from textures or paints provided by manufacturers.</li>\n<li>If in need of inspiration, the color palette selector will allow users to choose from our recommended Color Schemes.</li>\n<li>Furniture can also be placed in the space, allowing for a complete home renovation experience.</li>\n<li>Designs can be saved and uploaded to our server for sharing, downloading, and viewing on other devices in XR.</li>\n</ol>\n<h2>Development Story</h2>\n<p>We started by exploring different options for interacting with AR environments including A-Frame/WebXR, MagicScript/MagicLeap. After consulting a few WebXR mentors, we decided to commit to developing a MagicScript experience for the Magic Leap One - with hopes that the javascript and React.js based MagicScript would be a relatively easy-to-build solution.</p>\n<ul>\n<li><p>Day 1: our hopes were unfortunately dashed by a full day of development hell on Day 1. We spent the entire day trying to get the Hello World MagicScript example to build successfully to the ML headset. </p></li>\n<li><p>Day 2: we discovered that there are 3 different flavors of MagicScript, and only one of them works with React. The majority of Day 2 was spent trying to understand the differences between Immersive, Landscape, and MagicScript Components. After several false starts, we eventually got going to a point where MagicScript could not support what we were trying to do, so we were compelled to abandon MagicScript and start afresh. </p></li>\n<li><p>Day 3: we began afresh with A-Frame, and got a working prototype for placing surfaces and changing their texture. We'd call that a success!</p></li>\n</ul>\n<h2>Build Steps</h2>\n<h3>WebAR App</h3>\n<p>index.html is AR\nindex2.html is VR </p>\n<h3>RN ARKit App</h3>\n<pre class=\"language-nolang\"><code>clone\nnpm install\ncd iod\npod install\ncd ..\nreact-native start\n\n( start xcode select development team )\n( click play )\n</code></pre>\n<h2>Development Team</h2>\n<p>The SpaceChange team includes: </p>\n<ul>\n<li>  Cosmo Kramer</li>\n<li>  Jeffrey Lu</li>\n<li>  Matt Thompson</li>\n<li>  Vadim Manokhin</li>\n</ul>\n</div>",
            "content_md": "\n# SpaceChanger\n\n\nSpaceChanger is a WebAR utility to help with home renovation. Users can visualize wall, ceiling, and floor texture changes from their AR enabled smartphone.\n\n\n## Source Code\n\n\nSpaceChange is open source and hosted on GitHub:\n\n\nReact Native ARKit app for creating rooms (RN-AR branch) - <https://github.com/code-matt/spacechangers/tree/RN-AR>\nWebXR App For Viewing rooms (master branch) - <https://github.com/code-matt/spacechangers/edit/master/>\n(Note different git histories on branches above because of very late project tech pivot to ARKit)\n\n\n## Usage\n\n\n1. Upon launch, users will add SpaceChanger Surfaces to their environment by placing 4 corners to define a surface.\n2. After placement, users can interact with any SpaceChanger Surface by selecting them. Users will be able to see texture/color/paint changes in real-time, and select from textures or paints provided by manufacturers.\n3. If in need of inspiration, the color palette selector will allow users to choose from our recommended Color Schemes.\n4. Furniture can also be placed in the space, allowing for a complete home renovation experience.\n5. Designs can be saved and uploaded to our server for sharing, downloading, and viewing on other devices in XR.\n\n\n## Development Story\n\n\nWe started by exploring different options for interacting with AR environments including A-Frame/WebXR, MagicScript/MagicLeap. After consulting a few WebXR mentors, we decided to commit to developing a MagicScript experience for the Magic Leap One - with hopes that the javascript and React.js based MagicScript would be a relatively easy-to-build solution.\n\n\n* Day 1: our hopes were unfortunately dashed by a full day of development hell on Day 1. We spent the entire day trying to get the Hello World MagicScript example to build successfully to the ML headset.\n* Day 2: we discovered that there are 3 different flavors of MagicScript, and only one of them works with React. The majority of Day 2 was spent trying to understand the differences between Immersive, Landscape, and MagicScript Components. After several false starts, we eventually got going to a point where MagicScript could not support what we were trying to do, so we were compelled to abandon MagicScript and start afresh.\n* Day 3: we began afresh with A-Frame, and got a working prototype for placing surfaces and changing their texture. We'd call that a success!\n\n\n## Build Steps\n\n\n### WebAR App\n\n\nindex.html is AR\nindex2.html is VR \n\n\n### RN ARKit App\n\n\n\n```\nclone\nnpm install\ncd iod\npod install\ncd ..\nreact-native start\n\n( start xcode select development team )\n( click play )\n\n```\n\n## Development Team\n\n\nThe SpaceChange team includes: \n\n\n* Cosmo Kramer\n* Jeffrey Lu\n* Matt Thompson\n* Vadim Manokhin\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-flowjo",
            "title": "The FlowJo",
            "blurb": "The FlowJo is an interactive VR experience that encourages the participant to develop a curiosity for culture through expressive movement.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/sr7U-ZDvllw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Blair Photography. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/083/datas/original.jpg"
                },
                {
                    "title": "FlowJo&#39;s first meeting in the frozen depths of Atlantis, or Dante&#39;s Inferno (Dante&#39;s vision of hell was an icy tundra, not a fiery hell). ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/084/datas/original.jpg"
                },
                {
                    "title": "People trying to find people. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/085/datas/original.jpg"
                },
                {
                    "title": "Blair Photography. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/086/datas/original.jpg"
                },
                {
                    "title": "People doing stuff. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/091/datas/original.jpg"
                },
                {
                    "title": "People doing stuff. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/092/datas/original.jpg"
                },
                {
                    "title": "People doing stuff. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/093/datas/original.jpg"
                },
                {
                    "title": "Blair is in a state of flow getting inspiration from the most unlikely of places. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/912/094/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "et3rnald",
                    "about": "I came up with the original idea for the project: a musical nunchaku rhythm Virtual Reality game (which eventually pivoted to a calligraphic Japanese language learning VR game). I contributed to our project's design, development (in Unity/C#), project management, and user experience. I was thankful for a team that was open and honest with communication, respectful, and talented in our individual ways. The success of creating a fully fledged technical prototype is super inspiring, and I can't wait to continue designing and developing edifying VR projects after this amazing hackathon.",
                    "photo": "https://graph.facebook.com/10155917091572805/picture?height=180&width=180"
                },
                {
                    "name": "Adeeb Syed",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/945/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "canadians",
                "nocigarettes",
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Music brought the most geographically distannt group together.\nNunchucks threatened to chuck us apart.\nBut the slowmo flow of our dojo got us together though yo. </p>\n<h2>What it does</h2>\n<p>Enter the FlowJo is a calm, rhythm-based painting VR experience that helps players learn Japanese characters using their whole bodies. </p>\n<h2>How I built it</h2>\n<p>We built this in Unity using lots of googling, coffee, cookies, and Oculus Quests. </p>\n<h2>Challenges I ran into</h2>\n<p>We only had 1 real developer and 1 person that was a really good googler (2nd developer). We tried to do something really difficult, which was simulate the physics of nunchucks in VR. I would like to say we learned much about physics, but we mostly learned the importance of the \"v\" in virtual-- namely that it is virtual and not really real and therefore nunchuks in a virtual world won't behave exactly like nunchucks in VR. </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>The really-good-googler-developer felt very proud about all he learned in Unity. </p>\n<h2>What I learned</h2>\n<p>The \"really good googler\" learned a lot about developing in Unity. We learned a lot about physics. </p>\n<h2>What's next for The FlowJo</h2>\n<p>We may keep working on it as a team and improve the physics and adapt it to more Japanese characters or other script-based languages. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMusic brought the most geographically distannt group together.\nNunchucks threatened to chuck us apart.\nBut the slowmo flow of our dojo got us together though yo. \n\n\n## What it does\n\n\nEnter the FlowJo is a calm, rhythm-based painting VR experience that helps players learn Japanese characters using their whole bodies. \n\n\n## How I built it\n\n\nWe built this in Unity using lots of googling, coffee, cookies, and Oculus Quests. \n\n\n## Challenges I ran into\n\n\nWe only had 1 real developer and 1 person that was a really good googler (2nd developer). We tried to do something really difficult, which was simulate the physics of nunchucks in VR. I would like to say we learned much about physics, but we mostly learned the importance of the \"v\" in virtual-- namely that it is virtual and not really real and therefore nunchuks in a virtual world won't behave exactly like nunchucks in VR. \n\n\n## Accomplishments that I'm proud of\n\n\nThe really-good-googler-developer felt very proud about all he learned in Unity. \n\n\n## What I learned\n\n\nThe \"really good googler\" learned a lot about developing in Unity. We learned a lot about physics. \n\n\n## What's next for The FlowJo\n\n\nWe may keep working on it as a team and improve the physics and adapt it to more Japanese characters or other script-based languages. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/planet-protector-b18jfi",
            "title": "Planet Protector",
            "blurb": "So you think you recycle? Come test your skills with our immersive gameplay experience. You think you know, but you have no idea.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/nTH_bkvTseE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Carrie tests out the final product",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/902/datas/original.jpg"
                },
                {
                    "title": "Jay hard at work",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/903/datas/original.jpg"
                },
                {
                    "title": "Jose hard at work",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/905/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Jose E. Pimentel",
                    "about": "Wrote AFrame code for game mechanics. Most of it didnt make it into the main experience due to time constraints but there's an \"arcade\" mode that's just the game mechanics without any of the storytelling",
                    "photo": "https://avatars3.githubusercontent.com/u/6427690?height=180&v=4&width=180"
                },
                {
                    "name": "Jay Williamson",
                    "about": "Sound design, visual graphics and demo video",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQE7wGzLNM7F2w/profile-displayphoto-shrink_100_100/0?e=1547683200&height=180&t=FTT-eN86Xrbu-SPJc-vT92Vp3NkewK83UgQS_bBs5jU&v=beta&width=180"
                },
                {
                    "name": "Danielle Levin",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ef7d6481f45306765b58bdb399b15258?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Carrie Carter",
                    "about": "",
                    "photo": "https://media-exp2.licdn.com/dms/image/C5603AQEFgOJwGtUIIQ/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=Qq15XHiugTLrXKaEBSmK2rUghbK-Jgn4A4jU3nEO0JE&v=beta&width=180"
                }
            ],
            "built_with": [
                "3.js",
                "a-frame",
                "autodesk-fusion-360",
                "html",
                "javascript"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We came together around a shared passion for VR\u2019s ability to deepen empathy and create a sense of common humanity.  After brainstorming the many issues we hoped shine a light on, we settled on a topic close to all of us: sustainability.</p>\n<p>Every day, well-meaning people do their part to protect the planet by recycling their trash. It is one of the most prevalent daily sustainability behaviors in U.S., and it is the only one taught throughout the country\u2019s public and private school system. But do we actually know what happens to our trash when we recycle it? </p>\n<p>The reality is, we are devoting our energy to a practice that is stunningly inefficient, while ignoring other, more impactful behaviors we could focus on to protect and sustain natural resources.</p>\n<p>How could we help users who want to be responsible stewards of the earth, but don\u2019t have the bandwidth to identify the most impactful behaviors? Enter, Planet Protector!</p>\n<h2>What it does</h2>\n<p><strong>Planet Protector</strong> <em>Expand your awareness</em></p>\n<p>Planet Protector is an immersive educational experience, in which users learn the realities of waste management and its impact on the environment through a short gameplay and world exploring experience.  Planet Protector trains users to think more deeply about unseen elements of our consumption: what happens to trash after we throw it out? How can sustainability be incorporated farther up the production chain, before items arrive in our homes?  More than simply a device for educating users on the facts of recycling, Planet Protector trains users to extend their awareness across all kinds of behaviors, encouraging us to notice the impact of our choices beyond what is immediately visible to us. </p>\n<p>Ultimately, Planet Protector is about creating a world where each of us is aware of the downstream consequences of our choices, and we are equipped to make the most responsible choice possible.  </p>\n<h2>How we built it</h2>\n<p>We thought about each of our own daily behaviors, and we designed the experience for a user most similar to ourselves: people who aspired to act in a sustainably responsible fashion, but who didn\u2019t devote significant time to researching the implications of our consumption and waste choices. We decided to narrow our scope to focus on a single element of sustainability, recycling, because it was the most common daily habit for each of us.</p>\n<p>We educated ourselves, researching global trends in recycling and the impact of inefficient waste management processes in the context of an ongoing climate crisis. Having immersed ourselves in the data, we set out to design the user experience to mimic our own daily behavior in the real world: comfortably engaging in low-effort, validating behaviors like recycling (which is reflected in the easy gameplay in the first half of the experience), followed by an abrupt dose of reality about the relative ineffectiveness of those behaviors (reflected in the narration and scene change in the second half of the experience).* </p>\n<p>We wanted the experience to be accessible to all users, on any platform, so we built using Web XR to take advantage of the potential for Web-based VR experiences.</p>\n<ul>\n<li>We utilized the A-Frame library which is coded with Javascript</li>\n<li>Being web-based allowed us to iterate and build much more quickly than other environments</li>\n<li>Users can engage with our experience by simply viewing our site within a VR headset; no sideloading necessary</li>\n<li>We support any 6DOF headset with hand controllers now and in the future</li>\n<li>Full controller support allows for the users to interact with the environment</li>\n<li>We utilized a physics library to allow for a sandbox of interaction possibilities</li>\n</ul>\n<p>Throughout the process, we checked in on our design choices to ensure we stayed true to the goals for the user and the design principles we\u2019d outlined at the beginning of the hackathon. Those included:\n<em>Goals:</em></p>\n<ul>\n<li>Empathy-building, resonant experience</li>\n<li>Explore an unseen world\n<em>Principles:</em></li>\n<li>Don\u2019t shy away from discomfort, but don\u2019t create discomfort for the sake of it</li>\n<li>Stay true to the story and remain personal; elevate unheard voices or stories</li>\n<li>Responsible and caring toward user</li>\n<li>Accessible</li>\n</ul>\n<p>*Note: our intention is not to discourage users from recycling! Far from it, we hope this experience reinforces their existing efforts, while empowering them to take addition steps to reduce their waste footprint earlier in their consumption habits.</p>\n<h2>Challenges we ran into</h2>\n<p>We hit our most significant snag on the evening of Day 1. We had quickly landed on a topic that excited all of us that morning; we then spent the day storyboarding, meticulously outlining audio and visual assets, scripts and a project plan. Late that night, we discovered the Stanford Virtual Human Interaction Lab had already developed the same concept (presumably with more time and resources!). The story, style and user experience were all too close to our ideas to continue with our plan.  We were faced with a dilemma: keep the <a href=\"https://vhil.stanford.edu/becominghomeless/\" rel=\"nofollow\">same concept</a> and try to say something new on the topic, or throw it out and start with something fresh. </p>\n<p>Fortunately, at the beginning of the day, we\u2019d taken a moment to get to know one another before starting our planning. Carrie, our industrial design student, had shared something she\u2019d learned from her experiences in art school: be unafraid to throw it out and start all over again.</p>\n<p>Encouraged and emboldened by this, we gave ourselves the freedom to explore new concepts, and we quickly discovered we had a shared passion for sustainability.  The ideas started flowing, and we brainstormed until we were confident we\u2019d be able to pivot topics with the time left, while still staying true to the goals and principles we outlined in our first brainstorm.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We finished and shipped an MVP!</p>\n<p>Each of us brought a wide spectrum of skills: some are freelance or professional developers, some have art and design experience, and some had put on their first VR headset only a few weeks before the hackathon! We found a role for everybody on the team, and we were able to make use of and celebrate each person\u2019s individual strengths, while creating opportunities for learning for all. We approached our work with awareness of our own blindspots and with a desire to improve ourselves; we are proud to have worked on a team that lived the behaviors we hope to encourage in our users.</p>\n<h2>What we learned</h2>\n<p>We each learned a lot about our own recycling behaviors!  That was one of the hardest parts of the hackathon: the more we researched and built our product, the more we began to notice the unsustainable practices committed by us and others.</p>\n<p>We improved our skills with A-Frame and developed critical sprint skills: how to iterate quickly and when to be unafraid of starting over.  </p>\n<h2>What's next for Planet Protector</h2>\n<p>We have big visions for Planet Protector!  We\u2019d like to merge other elements into the game play to build out the initial scene, and we\u2019d like to expand the user\u2019s ability to explore and interact with their environment, including additional educational elements. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe came together around a shared passion for VR\u2019s ability to deepen empathy and create a sense of common humanity. After brainstorming the many issues we hoped shine a light on, we settled on a topic close to all of us: sustainability.\n\n\nEvery day, well-meaning people do their part to protect the planet by recycling their trash. It is one of the most prevalent daily sustainability behaviors in U.S., and it is the only one taught throughout the country\u2019s public and private school system. But do we actually know what happens to our trash when we recycle it? \n\n\nThe reality is, we are devoting our energy to a practice that is stunningly inefficient, while ignoring other, more impactful behaviors we could focus on to protect and sustain natural resources.\n\n\nHow could we help users who want to be responsible stewards of the earth, but don\u2019t have the bandwidth to identify the most impactful behaviors? Enter, Planet Protector!\n\n\n## What it does\n\n\n**Planet Protector** *Expand your awareness*\n\n\nPlanet Protector is an immersive educational experience, in which users learn the realities of waste management and its impact on the environment through a short gameplay and world exploring experience. Planet Protector trains users to think more deeply about unseen elements of our consumption: what happens to trash after we throw it out? How can sustainability be incorporated farther up the production chain, before items arrive in our homes? More than simply a device for educating users on the facts of recycling, Planet Protector trains users to extend their awareness across all kinds of behaviors, encouraging us to notice the impact of our choices beyond what is immediately visible to us. \n\n\nUltimately, Planet Protector is about creating a world where each of us is aware of the downstream consequences of our choices, and we are equipped to make the most responsible choice possible. \n\n\n## How we built it\n\n\nWe thought about each of our own daily behaviors, and we designed the experience for a user most similar to ourselves: people who aspired to act in a sustainably responsible fashion, but who didn\u2019t devote significant time to researching the implications of our consumption and waste choices. We decided to narrow our scope to focus on a single element of sustainability, recycling, because it was the most common daily habit for each of us.\n\n\nWe educated ourselves, researching global trends in recycling and the impact of inefficient waste management processes in the context of an ongoing climate crisis. Having immersed ourselves in the data, we set out to design the user experience to mimic our own daily behavior in the real world: comfortably engaging in low-effort, validating behaviors like recycling (which is reflected in the easy gameplay in the first half of the experience), followed by an abrupt dose of reality about the relative ineffectiveness of those behaviors (reflected in the narration and scene change in the second half of the experience).* \n\n\nWe wanted the experience to be accessible to all users, on any platform, so we built using Web XR to take advantage of the potential for Web-based VR experiences.\n\n\n* We utilized the A-Frame library which is coded with Javascript\n* Being web-based allowed us to iterate and build much more quickly than other environments\n* Users can engage with our experience by simply viewing our site within a VR headset; no sideloading necessary\n* We support any 6DOF headset with hand controllers now and in the future\n* Full controller support allows for the users to interact with the environment\n* We utilized a physics library to allow for a sandbox of interaction possibilities\n\n\nThroughout the process, we checked in on our design choices to ensure we stayed true to the goals for the user and the design principles we\u2019d outlined at the beginning of the hackathon. Those included:\n*Goals:*\n\n\n* Empathy-building, resonant experience\n* Explore an unseen world\n*Principles:*\n* Don\u2019t shy away from discomfort, but don\u2019t create discomfort for the sake of it\n* Stay true to the story and remain personal; elevate unheard voices or stories\n* Responsible and caring toward user\n* Accessible\n\n\n*Note: our intention is not to discourage users from recycling! Far from it, we hope this experience reinforces their existing efforts, while empowering them to take addition steps to reduce their waste footprint earlier in their consumption habits.\n\n\n## Challenges we ran into\n\n\nWe hit our most significant snag on the evening of Day 1. We had quickly landed on a topic that excited all of us that morning; we then spent the day storyboarding, meticulously outlining audio and visual assets, scripts and a project plan. Late that night, we discovered the Stanford Virtual Human Interaction Lab had already developed the same concept (presumably with more time and resources!). The story, style and user experience were all too close to our ideas to continue with our plan. We were faced with a dilemma: keep the [same concept](https://vhil.stanford.edu/becominghomeless/) and try to say something new on the topic, or throw it out and start with something fresh. \n\n\nFortunately, at the beginning of the day, we\u2019d taken a moment to get to know one another before starting our planning. Carrie, our industrial design student, had shared something she\u2019d learned from her experiences in art school: be unafraid to throw it out and start all over again.\n\n\nEncouraged and emboldened by this, we gave ourselves the freedom to explore new concepts, and we quickly discovered we had a shared passion for sustainability. The ideas started flowing, and we brainstormed until we were confident we\u2019d be able to pivot topics with the time left, while still staying true to the goals and principles we outlined in our first brainstorm.\n\n\n## Accomplishments that we're proud of\n\n\nWe finished and shipped an MVP!\n\n\nEach of us brought a wide spectrum of skills: some are freelance or professional developers, some have art and design experience, and some had put on their first VR headset only a few weeks before the hackathon! We found a role for everybody on the team, and we were able to make use of and celebrate each person\u2019s individual strengths, while creating opportunities for learning for all. We approached our work with awareness of our own blindspots and with a desire to improve ourselves; we are proud to have worked on a team that lived the behaviors we hope to encourage in our users.\n\n\n## What we learned\n\n\nWe each learned a lot about our own recycling behaviors! That was one of the hardest parts of the hackathon: the more we researched and built our product, the more we began to notice the unsustainable practices committed by us and others.\n\n\nWe improved our skills with A-Frame and developed critical sprint skills: how to iterate quickly and when to be unafraid of starting over. \n\n\n## What's next for Planet Protector\n\n\nWe have big visions for Planet Protector! We\u2019d like to merge other elements into the game play to build out the initial scene, and we\u2019d like to expand the user\u2019s ability to explore and interact with their environment, including additional educational elements. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/journalar",
            "title": "journalAR",
            "blurb": "An AR application for everyday people looking to take care of their mental health through active journaling.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/eqc9JRP_Ric?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Private journal view - mobile",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/456/datas/original.jpg"
                },
                {
                    "title": "Text message, video - mobile",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/457/datas/original.jpg"
                },
                {
                    "title": "Emotion selection - HMD",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/458/datas/original.png"
                },
                {
                    "title": "Audio clip - HMD",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/459/datas/original.png"
                },
                {
                    "title": "Map view - mobile",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/460/datas/original.jpg"
                },
                {
                    "title": "Image - HMD",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/461/datas/original.png"
                },
                {
                    "title": "Private journal view - HMD",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/462/datas/original.png"
                },
                {
                    "title": "Photo framing - HMD",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/466/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Tom\u00e1\u0161 Havl\u00edk",
                    "about": "I designed and hacked together a version of our app for Nreal AR HMD.",
                    "photo": "https://avatars1.githubusercontent.com/u/7209990?height=180&v=4&width=180"
                },
                {
                    "name": "nikkieto",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/46796781?height=180&v=4&width=180"
                },
                {
                    "name": "Sowmiya Nagarajan",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/20276568?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "nreal",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<p>Our project is journalAR. It\u2019s for everyday people looking to take care of their mental health through active journaling.</p>\n<p>Journaling is a helpful way to process, reflect, and express your thoughts and feelings. This can be especially helpful for people struggling with mental health to manage stress, anxiety, and depression in their everyday life.</p>\n<p>There are two versions of this project that are both made in Unity. The mobile version uses Vuforia and the HMD version uses the Nreal SDK. The UX differs quite heavily inbetween versions in order to best utilize their capabilities. For the HMD version, we went with UI built on gaze interaction in order to enable the user to use the application hands-free. So far, it enables the user to add new image and voice recordings, support for text is intended via an online transcribing tool to help mitigate poor input capabilities on AR HMDs. The current mobile version of our app augments text, image, audio and video posts recorded by passerbys around the scanned target. It includes a closed captions (CC) accessibility feature to enable transcription for audio posts.</p>\n<p>Our project allows people to journal in the world by creating a written text entry, a voice recording, taking a photo, or a video, and tagging it to a real place. Accessibility features have been added to ensure close captioning is available for individuals who are hard of hearing, smileys can be chosen to show the mood of the post, and the integration with places in the real world encourages active living.</p>\n<p>You\u2019ll also be able to see posts from others who have left a public journal entry. Often, the feeling of anxiety and depression comes with the feeling of isolation and loneliness. This public archive of journals is a reminder that many of these thoughts and feelings are shared. Users can see their personal journal history as well, and can look at the map to see where personal and public entries have been placed.</p>\n<p>Our team consisted of a UX interactions designer and two developers, one who worked solely on the mobile version and another focusing on the version for Nreal HMD. A number of challenges were encountered, most notably stemming from lack of prior experience of one of the authors with Nreal SDK. Our mobile developer had to redesign their application to work around markers in order to support indoor areas, outside areas are meant to be covered using geolocation.</p>\n<p>We are proud of creating two separate user interfaces, each tailored for a specific class of device. It was incredible being part of a team that went from idea to finish in mere 2.5 days. Since we were a team of three, we had to wear many hats, which has provided us with valuable experience in many areas of development. We have learned about amazing new AR devices and for one of the members, the application was their first forray into HMD-based AR development.</p>\n<p>In the future, we hope to integrate the functionality of the mobile version with the user experience of the Nreal version. We would also like to improve and add more accessibility features, and include sensitive content warnings.</p>\n<p>Come join us and start journaling!</p>\n</div>",
            "content_md": "\nOur project is journalAR. It\u2019s for everyday people looking to take care of their mental health through active journaling.\n\n\nJournaling is a helpful way to process, reflect, and express your thoughts and feelings. This can be especially helpful for people struggling with mental health to manage stress, anxiety, and depression in their everyday life.\n\n\nThere are two versions of this project that are both made in Unity. The mobile version uses Vuforia and the HMD version uses the Nreal SDK. The UX differs quite heavily inbetween versions in order to best utilize their capabilities. For the HMD version, we went with UI built on gaze interaction in order to enable the user to use the application hands-free. So far, it enables the user to add new image and voice recordings, support for text is intended via an online transcribing tool to help mitigate poor input capabilities on AR HMDs. The current mobile version of our app augments text, image, audio and video posts recorded by passerbys around the scanned target. It includes a closed captions (CC) accessibility feature to enable transcription for audio posts.\n\n\nOur project allows people to journal in the world by creating a written text entry, a voice recording, taking a photo, or a video, and tagging it to a real place. Accessibility features have been added to ensure close captioning is available for individuals who are hard of hearing, smileys can be chosen to show the mood of the post, and the integration with places in the real world encourages active living.\n\n\nYou\u2019ll also be able to see posts from others who have left a public journal entry. Often, the feeling of anxiety and depression comes with the feeling of isolation and loneliness. This public archive of journals is a reminder that many of these thoughts and feelings are shared. Users can see their personal journal history as well, and can look at the map to see where personal and public entries have been placed.\n\n\nOur team consisted of a UX interactions designer and two developers, one who worked solely on the mobile version and another focusing on the version for Nreal HMD. A number of challenges were encountered, most notably stemming from lack of prior experience of one of the authors with Nreal SDK. Our mobile developer had to redesign their application to work around markers in order to support indoor areas, outside areas are meant to be covered using geolocation.\n\n\nWe are proud of creating two separate user interfaces, each tailored for a specific class of device. It was incredible being part of a team that went from idea to finish in mere 2.5 days. Since we were a team of three, we had to wear many hats, which has provided us with valuable experience in many areas of development. We have learned about amazing new AR devices and for one of the members, the application was their first forray into HMD-based AR development.\n\n\nIn the future, we hope to integrate the functionality of the mobile version with the user experience of the Nreal version. We would also like to improve and add more accessibility features, and include sensitive content warnings.\n\n\nCome join us and start journaling!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/controlar",
            "title": "ControlAR",
            "blurb": "A Magic Leap tool that turns anything into an AR controller",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/IEIuP3kvSQY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "ControlAR Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/914/479/datas/original.jpg"
                },
                {
                    "title": "Team photo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/851/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Thomas Suarez",
                    "about": "AR / Magic Leap integration, video streaming, deep learning backend",
                    "photo": "https://avatars3.githubusercontent.com/u/5103968?height=180&v=4&width=180"
                },
                {
                    "name": "Weston Bell-Geddes",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/911/402/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "cuda",
                "magic-leap",
                "opencv",
                "posecnn",
                "python",
                "teleportal",
                "unity"
            ],
            "content_html": "<div>\n<p>ControlAR is an augmented reality tool built for the Magic Leap that allows the user to turn everyday objects into controllers that affect in-game objects.</p>\n<h2>What it does</h2>\n<p>ControlAR is diverse in its applications and can be applied to many different fields. In our demo, you can fly a plane around the room by simply pointing a banana in the direction you want the airplane to fly. As of now, this is done using the gyroscope inside a phone.</p>\n<h2>How we built it</h2>\n<p>This initial version of ControlAR is made with Unity, written in C#, and deployed to Magic Leap. The Unity client can also run on Android, allowing it to be used as an alternate controller. Camera frames (YUV data in byte buffers) are streamed via UDP to a custom Python server we wrote, which is intended to as a bridge between the Magic Leap 1 and the deep learning runtime. We were able to build and configure PoseCNN (an open-source pose estimation framework implemented from a research paper) from source and run basic images through it on an Nvidia GPU with CUDA support. We started work on converting YUV frame buffers to RGB data that is compatible with PoseCNN, but ran out of time for integration toward the end of the hackathon. We intend to finish the PoseCNN integration at some point in the near future.</p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>  Linux OS</li>\n<li>  Raspberry Pi</li>\n<li>  Deep learning framework - CUDA</li>\n<li>  Magic Leap OpenCV</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<ul>\n<li>  Streaming to/from a remote server for CUDA-based PoseCNN</li>\n<li>  Magic Leap deployment and feature exploration</li>\n<li>  Cohesive teamwork</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>  Networked streaming of camera data</li>\n<li>  Convolutional neural network integration</li>\n<li>  Have GPU environments set up ahead of a hackathon for deep learning, just in case</li>\n</ul>\n<h2>What's next for ControlAR</h2>\n<p>As aforementioned, ControlAR is a tool that can be used for a diverse array of applications. As we continue to implement machine learning, we plan to continue to optimize ControlAR, while adding more minigames showcasing potential functionalities.</p>\n</div>",
            "content_md": "\nControlAR is an augmented reality tool built for the Magic Leap that allows the user to turn everyday objects into controllers that affect in-game objects.\n\n\n## What it does\n\n\nControlAR is diverse in its applications and can be applied to many different fields. In our demo, you can fly a plane around the room by simply pointing a banana in the direction you want the airplane to fly. As of now, this is done using the gyroscope inside a phone.\n\n\n## How we built it\n\n\nThis initial version of ControlAR is made with Unity, written in C#, and deployed to Magic Leap. The Unity client can also run on Android, allowing it to be used as an alternate controller. Camera frames (YUV data in byte buffers) are streamed via UDP to a custom Python server we wrote, which is intended to as a bridge between the Magic Leap 1 and the deep learning runtime. We were able to build and configure PoseCNN (an open-source pose estimation framework implemented from a research paper) from source and run basic images through it on an Nvidia GPU with CUDA support. We started work on converting YUV frame buffers to RGB data that is compatible with PoseCNN, but ran out of time for integration toward the end of the hackathon. We intend to finish the PoseCNN integration at some point in the near future.\n\n\n## Challenges we ran into\n\n\n* Linux OS\n* Raspberry Pi\n* Deep learning framework - CUDA\n* Magic Leap OpenCV\n\n\n## Accomplishments that we're proud of\n\n\n* Streaming to/from a remote server for CUDA-based PoseCNN\n* Magic Leap deployment and feature exploration\n* Cohesive teamwork\n\n\n## What we learned\n\n\n* Networked streaming of camera data\n* Convolutional neural network integration\n* Have GPU environments set up ahead of a hackathon for deep learning, just in case\n\n\n## What's next for ControlAR\n\n\nAs aforementioned, ControlAR is a tool that can be used for a diverse array of applications. As we continue to implement machine learning, we plan to continue to optimize ControlAR, while adding more minigames showcasing potential functionalities.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bird-4ytr5a",
            "title": "Bird",
            "blurb": "Learning-focused introduction to VR creative tools",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/F3zA14blSYs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/166/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "jamil mehdaoui",
                    "about": "VR painter, I worked on visual aspects of bird, developping the environment directly in VR using Quill",
                    "photo": "https://www.gravatar.com/avatar/5fa62a5e3a461d2b12ecb07c2734a8f7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Aaron Santiago",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/47049060?height=180&v=4&width=180"
                },
                {
                    "name": "petersim",
                    "about": "",
                    "photo": "https://graph.facebook.com/3430511460354604/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>VR creation tools are a new paradigm of creation, but they are hard to learn and have intimidating interfaces. How can we onboard new creators in a way that will empower them to use the amazing tools available?</p>\n<h2>What it does</h2>\n<p>Bird uses an adaptive drawing guide alongside a storytelling environment to ease users into understanding spatial design. Bird's drawing tools reflect tools found in other apps, like Quill, Medium, and Tilt Brush, so that these skills will give creatives confidence in using VR to make art.</p>\n<h2>How we built it</h2>\n<p>We used Unity alongside some VR asset creation tools like Gravity Sketch and Quill to create a welcoming environment to draw in.</p>\n<h2>Challenges we ran into</h2>\n<p>Finding mechanics that could challenge new artists while not scaring them away took some time, iteration, and user testing. Additionally, recreating drawing tools within the hackathon constraints meant that we had to tailor a lot of our designs around what we could accomplish within the time frame.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Bird's main feature is its adaptive drawing guide. Users are prompted to \"fill in\" a model using a template, but as their skills progress, we can remove the guide while they are painting. This mechanic allows them to use it as a reference in-place, while easing the transition from using the guide to freehand painting.</p>\n<p>Our environments are a welcoming alternative to the empty spaces and grid environments in classic VR apps, and the concepts we have that take users drawings and weave them into the environments create a positive feedback loop that can keep new artists engaged with VR creativity.</p>\n<p>Bird's demo system is built for efficiency, so we could test users at the drop of the hat. It intelligently places the environment and the sculpt each time the headset is put on, so there is no down-time between demos.</p>\n<h2>What we learned</h2>\n<p>People are monsters! Through our user testing we learned intimately what kind of guidance people needed to be able to immerse themselves in their project.</p>\n<p>Additionally, the challenge of running demos throughout the hackathon meant that the app had to be streamlined from the beginning, meaning that our development approach was rigorously focused on MVP and iterate.</p>\n<h2>What's next for Bird</h2>\n<p>Currently, the demo is a vertical slice of one of the creation sessions. It features a brush that resembles the Quill sphere brush, however it is lacking many of the features. Bird's demo can teach new VR creatives how to draw in 3D space, but other techniques such as shading would be fantastic to be able to show as well.</p>\n<p>We want to have a full suite of lessons, from beginner to advanced, and have the related environmental and narrative rewards for completing them.</p>\n<p>Teaching skills related to other tools, like the clay tools in Medium and the hull brush in Tilt Brush, are also on Bird's roadmap.</p>\n<p>And of course, more user tests are on each day of the roadmap!</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nVR creation tools are a new paradigm of creation, but they are hard to learn and have intimidating interfaces. How can we onboard new creators in a way that will empower them to use the amazing tools available?\n\n\n## What it does\n\n\nBird uses an adaptive drawing guide alongside a storytelling environment to ease users into understanding spatial design. Bird's drawing tools reflect tools found in other apps, like Quill, Medium, and Tilt Brush, so that these skills will give creatives confidence in using VR to make art.\n\n\n## How we built it\n\n\nWe used Unity alongside some VR asset creation tools like Gravity Sketch and Quill to create a welcoming environment to draw in.\n\n\n## Challenges we ran into\n\n\nFinding mechanics that could challenge new artists while not scaring them away took some time, iteration, and user testing. Additionally, recreating drawing tools within the hackathon constraints meant that we had to tailor a lot of our designs around what we could accomplish within the time frame.\n\n\n## Accomplishments that we're proud of\n\n\nBird's main feature is its adaptive drawing guide. Users are prompted to \"fill in\" a model using a template, but as their skills progress, we can remove the guide while they are painting. This mechanic allows them to use it as a reference in-place, while easing the transition from using the guide to freehand painting.\n\n\nOur environments are a welcoming alternative to the empty spaces and grid environments in classic VR apps, and the concepts we have that take users drawings and weave them into the environments create a positive feedback loop that can keep new artists engaged with VR creativity.\n\n\nBird's demo system is built for efficiency, so we could test users at the drop of the hat. It intelligently places the environment and the sculpt each time the headset is put on, so there is no down-time between demos.\n\n\n## What we learned\n\n\nPeople are monsters! Through our user testing we learned intimately what kind of guidance people needed to be able to immerse themselves in their project.\n\n\nAdditionally, the challenge of running demos throughout the hackathon meant that the app had to be streamlined from the beginning, meaning that our development approach was rigorously focused on MVP and iterate.\n\n\n## What's next for Bird\n\n\nCurrently, the demo is a vertical slice of one of the creation sessions. It features a brush that resembles the Quill sphere brush, however it is lacking many of the features. Bird's demo can teach new VR creatives how to draw in 3D space, but other techniques such as shading would be fantastic to be able to show as well.\n\n\nWe want to have a full suite of lessons, from beginner to advanced, and have the related environmental and narrative rewards for completing them.\n\n\nTeaching skills related to other tools, like the clay tools in Medium and the hull brush in Tilt Brush, are also on Bird's roadmap.\n\n\nAnd of course, more user tests are on each day of the roadmap!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ar-community-garden",
            "title": "AR Community Garden",
            "blurb": "AR Community Garden creates more resilient individuals, groups, and communities highlighting urban edible plants. Participants plant seeds and pollinate plants as an interactive bee. ",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "AR Community Garden Mission Statement",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/101/datas/original.jpeg"
                },
                {
                    "title": "People who draw together, draw together for a better world. Participatory AR project includes almost 100 participants from MIT Reality Hack",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/103/datas/original.jpeg"
                },
                {
                    "title": "Paige Dansinger draws with participants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/096/datas/original.jpeg"
                },
                {
                    "title": "Partipant drawing in a VR Dandelion Patch in the AR Community Garden",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/104/datas/original.jpeg"
                },
                {
                    "title": "Participants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/098/datas/original.jpeg"
                },
                {
                    "title": "Partpants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/099/datas/original.jpeg"
                },
                {
                    "title": "AR  Edible and Medicinal Plant Targets that respond to app created with Unity + Vuforia",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/097/datas/original.jpeg"
                },
                {
                    "title": "AR Target for Community Dandelion Patch ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/102/datas/original.jpeg"
                },
                {
                    "title": "Edible and medicinal plants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/100/datas/original.jpeg"
                },
                {
                    "title": "Planting AR Seeds",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/107/datas/original.jpeg"
                },
                {
                    "title": "MIT Media Lab Sketches, Oliver Dansinger",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/488/datas/original.JPG"
                },
                {
                    "title": "Non-Edible Plants, Oliver Dansinger ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/929/795/datas/original.jpg"
                },
                {
                    "title": "Beebalm, Monarda",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/531/datas/original.JPG"
                },
                {
                    "title": "Purslane, Porticiclus",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/530/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Kritisha Jain",
                    "about": "I worked on crafting the narrative for the community garden. Through this process, I learned a lot - especially the power of technology to bring people together and to alleviate them of their fears, loneliness and lack of a real connection. ",
                    "photo": "https://www.gravatar.com/avatar/35e1eaa8d06c262a52bd7b7492fec813?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Paige Dansinger",
                    "about": "Idea proposal, Visual content creation in VR, AR Targets, and display materials. I facilitated the public practice element by drawing with MIT Reality Hackers us8ing Tilt Brush with an Oculus Quest, and presented project. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/166/datas/profile.JPG"
                },
                {
                    "name": "Oliver Dansinger",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8c69b913382eb452c61dc62c121c34c4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Taeyeon Kim",
                    "about": "",
                    "photo": "https://graph.facebook.com/1924651354269918/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "ios",
                "quest",
                "tiltbrush",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<p>AR Community Garden creates more resilient individuals, groups, and communities by using a mobile AR app created with Unity + Vuforia which highlights urban edible and medicinal plants. Participants use AR to actively plant seeds and pollinate plants with an interactive AR Bee.</p>\n<p>Creating AR Community Garden at MIT Hacker Community was an opportunity for connection. Almost 100 people participated in drawing in the Dandelion Patch. Dandelions provide edible and medicinal properties but people use poison to kill them. Like dandelions, Hackers are also outliers but can save the world! People loved participating in this  utopian change experience. Out team believes when people draw together, they draw together for a better world. </p>\n<p>We learned new software, how to be more inclusive, listening skills, and project management. But most of all learned to use our hearts as a guiding force in discussing community resiliency and using creative technology to crea=t more empathetic cities. Smart cities that are not heart and human centered are not smart enough without resilience, inclusion, participation, and love. </p>\n<p>Some challenges included syncing software, limiting ideas to the most successful project outcome, and providing more opportunities for the Hackers to draw in the garden -- people loved participating. In the future I would include </p>\n</div>",
            "content_md": "\nAR Community Garden creates more resilient individuals, groups, and communities by using a mobile AR app created with Unity + Vuforia which highlights urban edible and medicinal plants. Participants use AR to actively plant seeds and pollinate plants with an interactive AR Bee.\n\n\nCreating AR Community Garden at MIT Hacker Community was an opportunity for connection. Almost 100 people participated in drawing in the Dandelion Patch. Dandelions provide edible and medicinal properties but people use poison to kill them. Like dandelions, Hackers are also outliers but can save the world! People loved participating in this utopian change experience. Out team believes when people draw together, they draw together for a better world. \n\n\nWe learned new software, how to be more inclusive, listening skills, and project management. But most of all learned to use our hearts as a guiding force in discussing community resiliency and using creative technology to crea=t more empathetic cities. Smart cities that are not heart and human centered are not smart enough without resilience, inclusion, participation, and love. \n\n\nSome challenges included syncing software, limiting ideas to the most successful project outcome, and providing more opportunities for the Hackers to draw in the garden -- people loved participating. In the future I would include \n\n\n"
        },
        {
            "source": "https://devpost.com/software/utooth",
            "title": "uTooth",
            "blurb": "uTooth is a framework for Bluetooth communication between Unity and external devices. This is a new tool that allows XR creators to implement peripherals and external devices fast and efficiently.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/385824674?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Plant to Bluetooth to XR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/217/datas/original.png"
                },
                {
                    "title": "We&#39;ve connected a plant with uTooth",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/915/195/datas/original.jpg"
                },
                {
                    "title": "Who is this for?",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/222/datas/original.png"
                },
                {
                    "title": "Current Pipeline without our framework takes 1 to 3 days of work",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/220/datas/original.png"
                },
                {
                    "title": "Pipeline using our framework",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/221/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Pilar Aranda",
                    "about": "Mainly working in the UX design for the demo. I also contributed with some dev work with Unity and interaction design",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/764/datas/profile.jpg"
                },
                {
                    "name": "Taylor Waddell",
                    "about": "I worked on the serial communication between the Bluetooth/USB to Unity. This involved a lot of backend development and communication protocols... and caffeine ",
                    "photo": "https://avatars1.githubusercontent.com/u/8742247?height=180&v=4&width=180"
                },
                {
                    "name": "Alireza Bahremand",
                    "about": "I worked on the back-end with my teammate Taylor. We developed, tested, & debugged several .NET native Bluetooth libraries to integrate microcontroller communication with Unity for XR projects. Additionally, I merged our two separate scenes together (scene building done by M\u00f3nica & Pilar along with Unity BT scene).",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/483/165/datas/profile.jpg"
                },
                {
                    "name": "M\u00f3nica Ceisel",
                    "about": "I worked on the plant demo within Unity with my teammate Pilar. This included scripting visual effects and controller mechanics, scene building, debugging, and VR configuration of our project for the Oculus Quest.",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1484408586/lebi9kyxbe0hzs2vfrpa.jpg?height=180&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "c#",
                "c++",
                "oculus",
                "oculusquest",
                "plant",
                "raspberry-pi",
                "sensors",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>As hackers, as creators, we face the limits of technology often, although there are certain boundaries that we are ready to overcome. In our case, we found a problem we want to solve: we want to connect Reality one device at a time.</p>\n<h2>What it does</h2>\n<p>We\u2019ve created a framework in C++ and C# for Bluetooth communication between Unity and external devices and tested it with Arduino sensors. This is a tool that didn\u2019t exist and allows developers/creators to shorten the implementation time for Bluetooth dramatically. </p>\n<p>To test our framework we created a unique demo where we\u2019ve connected a living plant with different sensors (moisture, humidity, light\u2026) These sensors connect in real-time with a data visualization experience in VR.</p>\n<p>In the demo, the user is able to see and feel how the plant is feeling thanks to a series of particle systems that reflect in real-time the state of our plant.</p>\n<p>You may think that a plant can\u2019t make much difference, but our framework will allow developers, creators and makers to connect all kinds of systems from entire aquaponics vertical farms, new custom made controllers for diversely abled users or new tools for surgeons, among many other uses.</p>\n<h2>How we built it</h2>\n<p>We worked in parallel development. Taylor and Alireza developed, tested, &amp; debugged several .NET native Bluetooth libraries to integrate microcontroller communication with Unity for XR projects. While Pilar and M\u00f3nica worked on the demo scene, search and tweak assets,  and wrote the scripts that will interact with the demo plant. Finally, we merged our two separate scenes together with Unity BT scene and spent time debugging functionality.</p>\n<h2>Challenges we ran into</h2>\n<p>The main issue we ran into was the System.IO.Ports namespace for C#/Unity. For this class it was able to maintain an open BlueTooth connection, it would briefly connect and then drop. We believe this comes from a complex threading issue of how the Serial class maintains its connection. To make sure our project still succeeded we used a USB communication as windows binds these to COM ports the same way it does with BlueTooth. This allowed us to make a framework that works the exact same way. The other issue is since we had a lot of personal hardware there were many different ways in which we had to interface ranging from the sensor data to communication protocols.</p>\n<h2>What we learned</h2>\n<p>We learned a lot about the lower levels of C# programming, and how to use lower-level functions. </p>\n<h2>What's next for uTooth</h2>\n<p>What is next is to either wait till System.IO.Ports is updated our create our own serial communication libraries which may be easier. The data transfer was also much slower than it could be, sending smarter packers and moving around data in a more efficient way would allow for much high fidelity sensors such as videos and audio.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAs hackers, as creators, we face the limits of technology often, although there are certain boundaries that we are ready to overcome. In our case, we found a problem we want to solve: we want to connect Reality one device at a time.\n\n\n## What it does\n\n\nWe\u2019ve created a framework in C++ and C# for Bluetooth communication between Unity and external devices and tested it with Arduino sensors. This is a tool that didn\u2019t exist and allows developers/creators to shorten the implementation time for Bluetooth dramatically. \n\n\nTo test our framework we created a unique demo where we\u2019ve connected a living plant with different sensors (moisture, humidity, light\u2026) These sensors connect in real-time with a data visualization experience in VR.\n\n\nIn the demo, the user is able to see and feel how the plant is feeling thanks to a series of particle systems that reflect in real-time the state of our plant.\n\n\nYou may think that a plant can\u2019t make much difference, but our framework will allow developers, creators and makers to connect all kinds of systems from entire aquaponics vertical farms, new custom made controllers for diversely abled users or new tools for surgeons, among many other uses.\n\n\n## How we built it\n\n\nWe worked in parallel development. Taylor and Alireza developed, tested, & debugged several .NET native Bluetooth libraries to integrate microcontroller communication with Unity for XR projects. While Pilar and M\u00f3nica worked on the demo scene, search and tweak assets, and wrote the scripts that will interact with the demo plant. Finally, we merged our two separate scenes together with Unity BT scene and spent time debugging functionality.\n\n\n## Challenges we ran into\n\n\nThe main issue we ran into was the System.IO.Ports namespace for C#/Unity. For this class it was able to maintain an open BlueTooth connection, it would briefly connect and then drop. We believe this comes from a complex threading issue of how the Serial class maintains its connection. To make sure our project still succeeded we used a USB communication as windows binds these to COM ports the same way it does with BlueTooth. This allowed us to make a framework that works the exact same way. The other issue is since we had a lot of personal hardware there were many different ways in which we had to interface ranging from the sensor data to communication protocols.\n\n\n## What we learned\n\n\nWe learned a lot about the lower levels of C# programming, and how to use lower-level functions. \n\n\n## What's next for uTooth\n\n\nWhat is next is to either wait till System.IO.Ports is updated our create our own serial communication libraries which may be easier. The data transfer was also much slower than it could be, sending smarter packers and moving around data in a more efficient way would allow for much high fidelity sensors such as videos and audio.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ide-trae",
            "title": "Ide Tr\u00e6",
            "blurb": "The future of XR Hackathons: using social VR to help hackers more effectively communicate ideas, form teams, and develop the future.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/log_vdhgC7U?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Thomas William Mohr",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/11515396?height=180&v=4&width=180"
                },
                {
                    "name": "Michael Zhang",
                    "about": "",
                    "photo": "https://graph.facebook.com/2233076753626265/picture?height=180&width=180"
                },
                {
                    "name": "Valdemar Danry",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5603AQEytv4rb6kwPA/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=R5X7zvjdveVgzlTiSctta9EfA7vRhULcTfw-Ef--YbE&v=beta&width=180"
                },
                {
                    "name": "Ian Ralston",
                    "about": "",
                    "photo": "https://media-exp1.licdn.com/dms/image/C4E03AQF1RIlSYUULnA/profile-displayphoto-shrink_800_800/0?e=1585180800&height=180&t=IJZ-nVVN7qcfRppQ4CpWrXJ-5whTlT00htmpajNYHl4&v=beta&width=180"
                },
                {
                    "name": "Anna Brewer",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_0fkZG6-nbgnfbpukzfr4W66B6ONPkYOWrfpZk-gB_VL-kaxLrfYnoBxBT7NPXDOWgDyJWBKcESztCV2q1yTko-lRfSz1CVRLKyTU31k9Xuhxe2hMpDczTbEF74fiIVuqJ0LNQ3X96iH?height=180&width=180"
                }
            ],
            "built_with": [
                "abletonlive",
                "cinema4d",
                "maya",
                "normcore",
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Since we\u2019re at an XR hackathon, we wanted to get meta with our project, so we made a tool for creating ideas and forming teams in a virtual hackathon. Seeking to create a centralized environment for creating, storing, and quickly switching between 3d concepts, we took inspiration from the Danish \u201cSuttetr\u00e6et,\u201d (Google it if you\u2019re curious!) to create a tree of ideas (\u201cIde Tr\u00e6\u201d in Danish), that people can create and explore together.</p>\n<h2>What it does</h2>\n<p>Ide Tr\u00e6 is a networked multiplayer VR ideation and team formation space. It uses VR to solve three main problems:</p>\n<ul>\n<li>Accessibility: not everyone can afford to fly out to an XR hackathon</li>\n<li>Communication: it\u2019s hard to explain and collaborate on 3D, immersive concepts using 2D drawings</li>\n<li>Efficiency: there should not be a tradeoff between staying with your idea to explain it, and learning about the ideas of others\nIt consists of an atrium with a tree in the center where ideas, represented by VR headsets, hang from vines. Users can create their own ideas, and put on a headset to enter someone else\u2019s idea. Inside each headset is an \u201cideation space\u201d where users can draw together in 3d. The idea\u2019s creator can also record a voice pitch, so they don\u2019t have to physically be there to pitch the concept. Once a team is formed, they place their hands on a podium and press the button to confirm their team and idea.</li>\n</ul>\n<h2>How I built it</h2>\n<p>We built the app with Unity, Normcore, and the Oculus SDK. The visual assets were built with Maya and Cinema 4D. The audio was created with Ableton Live.</p>\n<h2>Challenges I ran into</h2>\n<ul>\n<li>Our team had to learn an entirely new multiplayer networking solution: Normcore. Because Normcore is a newer software solution, it was challenging to find resources to solve the bugs we encountered.</li>\n<li>It was a challenge not being able to use paid assets. We had two team members creating assets from scratch.</li>\n</ul>\n<h2>Accomplishments that I'm proud of</h2>\n<ul>\n<li>Our team built all the 3D models, from the giant atrium down to the marker and microphone in the ideation room, using Maya and Cinema 4D. </li>\n<li>We also created the music and audio for the experience from scratch</li>\n<li>We were able to get multiplayer working and solve lots of tricky bugs</li>\n</ul>\n<h2>What I learned</h2>\n<ul>\n<li>How to interact with objects in a social VR experience requires changing ownership controls of items, which requires additional steps and testing to make sure objects are synchronized across users.</li>\n<li>How to implement various advanced Unity features, such as particle systems and mesh generation</li>\n</ul>\n<h2>What's next for Ide Tr\u00e6</h2>\n<p>We plan on testing our product at the upcoming XR hackathon at UC Berkeley, after implementing some additional features. Specifically, we want to allow users to choose what roles and skills they are looking to add to their team, and to filter ideas and people based on those tags (something obviously not possible in the physical world). </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSince we\u2019re at an XR hackathon, we wanted to get meta with our project, so we made a tool for creating ideas and forming teams in a virtual hackathon. Seeking to create a centralized environment for creating, storing, and quickly switching between 3d concepts, we took inspiration from the Danish \u201cSuttetr\u00e6et,\u201d (Google it if you\u2019re curious!) to create a tree of ideas (\u201cIde Tr\u00e6\u201d in Danish), that people can create and explore together.\n\n\n## What it does\n\n\nIde Tr\u00e6 is a networked multiplayer VR ideation and team formation space. It uses VR to solve three main problems:\n\n\n* Accessibility: not everyone can afford to fly out to an XR hackathon\n* Communication: it\u2019s hard to explain and collaborate on 3D, immersive concepts using 2D drawings\n* Efficiency: there should not be a tradeoff between staying with your idea to explain it, and learning about the ideas of others\nIt consists of an atrium with a tree in the center where ideas, represented by VR headsets, hang from vines. Users can create their own ideas, and put on a headset to enter someone else\u2019s idea. Inside each headset is an \u201cideation space\u201d where users can draw together in 3d. The idea\u2019s creator can also record a voice pitch, so they don\u2019t have to physically be there to pitch the concept. Once a team is formed, they place their hands on a podium and press the button to confirm their team and idea.\n\n\n## How I built it\n\n\nWe built the app with Unity, Normcore, and the Oculus SDK. The visual assets were built with Maya and Cinema 4D. The audio was created with Ableton Live.\n\n\n## Challenges I ran into\n\n\n* Our team had to learn an entirely new multiplayer networking solution: Normcore. Because Normcore is a newer software solution, it was challenging to find resources to solve the bugs we encountered.\n* It was a challenge not being able to use paid assets. We had two team members creating assets from scratch.\n\n\n## Accomplishments that I'm proud of\n\n\n* Our team built all the 3D models, from the giant atrium down to the marker and microphone in the ideation room, using Maya and Cinema 4D.\n* We also created the music and audio for the experience from scratch\n* We were able to get multiplayer working and solve lots of tricky bugs\n\n\n## What I learned\n\n\n* How to interact with objects in a social VR experience requires changing ownership controls of items, which requires additional steps and testing to make sure objects are synchronized across users.\n* How to implement various advanced Unity features, such as particle systems and mesh generation\n\n\n## What's next for Ide Tr\u00e6\n\n\nWe plan on testing our product at the upcoming XR hackathon at UC Berkeley, after implementing some additional features. Specifically, we want to allow users to choose what roles and skills they are looking to add to their team, and to filter ideas and people based on those tags (something obviously not possible in the physical world). \n\n\n"
        },
        {
            "source": "https://devpost.com/software/x-waves",
            "title": "X-Waves",
            "blurb": "An Augmented Reality(AR) solution for Tourette Syndrome using Magic leap",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/igWwN06Wnvo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Sanjit Singh",
                    "about": "Worked on unity and integration with magic leap and mobile version of app",
                    "photo": "https://avatars.githubusercontent.com/u/3290334?height=180&v=3&width=180"
                },
                {
                    "name": "Kartik Kinge",
                    "about": "Unity3d game development. Contributed is development of AR experience for Magic Leap.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/914/754/datas/profile.jpg"
                },
                {
                    "name": "Tushar Purang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/768/datas/profile.png"
                },
                {
                    "name": "Charles Xu",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/563/124/datas/profile.jpg"
                },
                {
                    "name": "Yuan Xiao",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/6870c5e99e2b5d6af0ca48c11b2dff91?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "augmented-reality",
                "c#",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Tourette Syndrome is a nervous system disorder involving repetitive movements or unwanted sounds. There are more than 200,000 US cases per year and approximately 10-15 percent of those affected have a progressive or disabling course that lasts into adulthood. </p>\n<h2>What it does</h2>\n<p>X-Waves tries to teach a \u201ccompeting response,\u201d which is the essence of habit reversal therapy. Since the brain can\u2019t do two tics in the exact same instant, physical tics can essentially be neutralized over time.</p>\n<p>The user can associate sound patterns to the feeling of comfort felt after doing physical tics, hence creating a reinforcing action that can convert physical tics to mental tics</p>\n<h2>How we built it</h2>\n<p>The project is divided into two parts. The first part detects and engages the experience. The second part involves visuals, audio and haptic feedback to create an experience similar to ASMR. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTourette Syndrome is a nervous system disorder involving repetitive movements or unwanted sounds. There are more than 200,000 US cases per year and approximately 10-15 percent of those affected have a progressive or disabling course that lasts into adulthood. \n\n\n## What it does\n\n\nX-Waves tries to teach a \u201ccompeting response,\u201d which is the essence of habit reversal therapy. Since the brain can\u2019t do two tics in the exact same instant, physical tics can essentially be neutralized over time.\n\n\nThe user can associate sound patterns to the feeling of comfort felt after doing physical tics, hence creating a reinforcing action that can convert physical tics to mental tics\n\n\n## How we built it\n\n\nThe project is divided into two parts. The first part detects and engages the experience. The second part involves visuals, audio and haptic feedback to create an experience similar to ASMR. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/hyperviz",
            "title": "HyperViz",
            "blurb": "A VR medical visualization app that empowers physicians to better prepare for surgical procedures",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/L3ZVqKTJEyg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ashish Bakshi",
                    "about": "I originated the idea, recruited team members, and led the team in the development of the prototype and pitch presentation.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/741/datas/profile.jpg"
                },
                {
                    "name": "Beste Aydin",
                    "about": "I am an undergraduate student at the University of Michigan who is studying Computer Engineering. I was a developer for this project and was most excited to work with the hand tracking software/hardware.",
                    "photo": "https://avatars3.githubusercontent.com/u/35937246?height=180&v=4&width=180"
                },
                {
                    "name": "Gabriel Santa Maria",
                    "about": "I worked on general R&D, 3-D model optimization, shader and interaction programming.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/152/datas/profile.jpg"
                },
                {
                    "name": "Eric Tao",
                    "about": "I worked on user flow, UX/UI design and wrote and recorded the in-app script. I helped with scoping, user testing, model and environment creation, and lastly helped create the presentation materials.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/134/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "leap-motion",
                "unity",
                "uwp",
                "wmr"
            ],
            "content_html": "<div>\n<h2>Description</h2>\n<p>Every year, over 80 million CT scans are performed in the U.S. alone. Adding in MRI and other types of imaging, the number is even higher. This data is fundamentally in 3D, but is instead consumed by physicians as 2D, black and white images. To understand complex internal structures, doctors must flip through dozens or even hundreds of images in order to create a mental 3D reconstruction. This leads to a suboptimal understanding of patients\u2019 physiology and thus poor outcomes.</p>\n<p>HyperViz is a next-generation virtual reality medical visualization app that empowers physicians to better prepare for surgical procedures. We process 2D images like CT scans into 3D models, then visualize them in a VR environment, integrated with a photogrammetric scan of the patient. HyperViz segments the anatomical data to show different types of tissue, like skin, soft tissue, and bone.</p>\n<p>We built the app around a hand-tracking interface to maximize ease of use and enable physicians to rely on HyperViz without cumbersome controllers.</p>\n<h2>Team Members</h2>\n<p>Ashish Bakshi, Eric Tao, Gabriel Santa-Maria, Beste Aydin\n(Team 14)</p>\n<h2>Location</h2>\n<p>From day 2 onwards, we worked on the 3rd floor of the MIT Media Lab (Building E14), at table 3A-10.</p>\n<h2>Development</h2>\n<p>Platform: Windows 10 / HP Reverb Pro / Windows Mixed Reality + Leap Motion\nDevelopment Tools: Unity 2018.4.6f1, Microsoft Visual Studio 2019, Github\nSDKs: Mixed Reality Toolkit v2.0, Leap Motion SDK, Windows SDK\nAssets: Ashish Bakshi; Lazaroe\nPaid Assets: None</p>\n</div>",
            "content_md": "\n## Description\n\n\nEvery year, over 80 million CT scans are performed in the U.S. alone. Adding in MRI and other types of imaging, the number is even higher. This data is fundamentally in 3D, but is instead consumed by physicians as 2D, black and white images. To understand complex internal structures, doctors must flip through dozens or even hundreds of images in order to create a mental 3D reconstruction. This leads to a suboptimal understanding of patients\u2019 physiology and thus poor outcomes.\n\n\nHyperViz is a next-generation virtual reality medical visualization app that empowers physicians to better prepare for surgical procedures. We process 2D images like CT scans into 3D models, then visualize them in a VR environment, integrated with a photogrammetric scan of the patient. HyperViz segments the anatomical data to show different types of tissue, like skin, soft tissue, and bone.\n\n\nWe built the app around a hand-tracking interface to maximize ease of use and enable physicians to rely on HyperViz without cumbersome controllers.\n\n\n## Team Members\n\n\nAshish Bakshi, Eric Tao, Gabriel Santa-Maria, Beste Aydin\n(Team 14)\n\n\n## Location\n\n\nFrom day 2 onwards, we worked on the 3rd floor of the MIT Media Lab (Building E14), at table 3A-10.\n\n\n## Development\n\n\nPlatform: Windows 10 / HP Reverb Pro / Windows Mixed Reality + Leap Motion\nDevelopment Tools: Unity 2018.4.6f1, Microsoft Visual Studio 2019, Github\nSDKs: Mixed Reality Toolkit v2.0, Leap Motion SDK, Windows SDK\nAssets: Ashish Bakshi; Lazaroe\nPaid Assets: None\n\n\n"
        },
        {
            "source": "https://devpost.com/software/task-transfer",
            "title": "TASK TRANSFER",
            "blurb": "ENTERPRISE  TRAINING IN XR",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/HLyqFQoirRI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Lex Dreitser",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/274/391/datas/profile.png"
                },
                {
                    "name": "Maciek Jarosiewicz",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0156ac1680c8854bbc277baffce382eb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "megamindmo",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/23298720?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "magic",
                "magic-leap",
                "magic:-the-gathering",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Knowing that Magic Leap's corporate strategy has shifted to enterprise, we saw a great opportunity. </p>\n<h2>What it does</h2>\n<p>TASK TRANSFER \nENTERPRISE TRAINING in XR\ufffdEnd-to-end solution addressing \nthe world's aging workforce  </p>\n<ol>\n<li><p>Expert Wears Magic Leap and Performs Task - Magic Leap Records</p></li>\n<li><p>Novice Wears Magic Leap and Watches Recording - Performs Task Correctly - Magic Leap Records</p></li>\n<li><p>Expert Evaluates Novice Task Performance Remotely</p></li>\n</ol>\n<h2>How we built it</h2>\n<p>We used Unity and Magic Leap's developer resources to build an app that will spearhead Magic Leap's Enterprise offerings. </p>\n<h2>Challenges we ran into</h2>\n<p>Some of our team is developing on Magic Leap for the first time. There were some initial setup quirks.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were each able to build a new feature and commit to the repo on time.</p>\n<h2>What we learned</h2>\n<p>We learned a ton about teamwork and built great friendships. We each learned to build to Magic Leap</p>\n<h2>What's next for TASK TRANSFER</h2>\n<p>This is a very viable enterprise product. We will start pitching aerospace companies this week.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nKnowing that Magic Leap's corporate strategy has shifted to enterprise, we saw a great opportunity. \n\n\n## What it does\n\n\nTASK TRANSFER \nENTERPRISE TRAINING in XR\ufffdEnd-to-end solution addressing \nthe world's aging workforce \n\n\n1. Expert Wears Magic Leap and Performs Task - Magic Leap Records\n2. Novice Wears Magic Leap and Watches Recording - Performs Task Correctly - Magic Leap Records\n3. Expert Evaluates Novice Task Performance Remotely\n\n\n## How we built it\n\n\nWe used Unity and Magic Leap's developer resources to build an app that will spearhead Magic Leap's Enterprise offerings. \n\n\n## Challenges we ran into\n\n\nSome of our team is developing on Magic Leap for the first time. There were some initial setup quirks.\n\n\n## Accomplishments that we're proud of\n\n\nWe were each able to build a new feature and commit to the repo on time.\n\n\n## What we learned\n\n\nWe learned a ton about teamwork and built great friendships. We each learned to build to Magic Leap\n\n\n## What's next for TASK TRANSFER\n\n\nThis is a very viable enterprise product. We will start pitching aerospace companies this week.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/one-billion-lost-australia-s-wildfires",
            "title": "One Billion Lost: Australia's Wildfires VR Experience",
            "blurb": "Australia is on fire. How will you and your animal pals escape?",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/MlHis_Ln_lw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Rigging of origami style Rabbit Eared Bandicoots",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/503/datas/original.png"
                },
                {
                    "title": "Modelling of origami style Rabbit Eared Bandicoots",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/504/datas/original.png"
                },
                {
                    "title": "Modelling of origami style kangaroo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/511/datas/original.png"
                },
                {
                    "title": "Rigging of origami style kangaroo model",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/510/datas/original.png"
                },
                {
                    "title": "Code snippet",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/509/datas/original.png"
                },
                {
                    "title": "Overview of the scene in Unity",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/512/datas/original.png"
                },
                {
                    "title": "Interactive start menu",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/911/505/datas/original.PNG"
                },
                {
                    "title": "Overview of the scene in Unity",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/579/datas/original.PNG"
                },
                {
                    "title": "Ingame view of origami style animal models roaming the scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/588/datas/original.PNG"
                },
                {
                    "title": "The scene ends with a low poly style earth and a poignant audio experience",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/626/datas/original.PNG"
                },
                {
                    "title": "If contacted by fire, the origami style animals burn up as if they are paper",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/627/datas/original.PNG"
                },
                {
                    "title": "If contacted by fire, the origami style animals burn up as if they are paper",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/628/datas/original.PNG"
                },
                {
                    "title": "Ingame view of the fires consuming the animal models",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/916/629/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "MATTHEW CUDMORE",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c4a08c22a26d3e7401c3116935e07771?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "clipstudio",
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The rapidly advancing wildfires are an unstoppable force of nature, and you along with hundreds of helpless animals are trapped between that fire and a body of water. Where do you go?</p>\n<h2>What it does</h2>\n<p>We inject urgency into the scene with the mercilessly advancing wildfire. All around you the vibrant wildlife of Australia are disappearing by the second. Your natural inclination is to run to the water: this was the true reality for able bodied Australian animals and humans, many of whom had to escape to bodies of water to tread water for hours while waiting for wildfires to pass. Many were not so lucky.</p>\n<h2>How we built it</h2>\n<p>We sketched/modeled in Clip Studio and Blender before importing into Unity. Unity was the workhorse responsible for bringing together all assets into the VR environment. We scripted in C#. We prototyped with the Oculus Rift S acting as our HMD.</p>\n<h2>Challenges we ran into</h2>\n<p>We struggled with the tone and treatment of the subject matter, walking a careful balance between shock value, intimating a sense of helplessness and ending with a call to action to look after the animals of Australia and their environment.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>In order to make the VR experience suitable for all ages we made a conscious decision to abstract the animals in origami style. On contact with the fire, the origami animal models burn up and float skyward in plumes of ashes contributing to a growing sense of loss and helplessness.</p>\n<h2>What we learned</h2>\n<p>We learned a lot building a rapid prototyping and troubleshooting workflow with Unity and the Oculus especially around shaders, performance cost/optimization and spatial sound design.</p>\n<h2>What's next for One Billion Lost: Australia's Wildfires</h2>\n<p>With more time we'd like to apply more polish to animations, rigging and believable fire dynamics. We'd also like to better build out interactivity with the animals and perhaps inject small bits of gamification (ie carrying animals to safety in the water) to engage the user further.</p>\n<h2>Audio and photo attributions</h2>\n<p>News.com.au <a href=\"https://www.news.com.au/technology/environment/sir-david-attenborough-says-humans-have-overrun-the-world/news-story/279c49216accdcd3d55ddd49897f5f13\" rel=\"nofollow\">https://www.news.com.au/technology/environment/sir-david-attenborough-says-humans-have-overrun-the-world/news-story/279c49216accdcd3d55ddd49897f5f13</a></p>\n<p>CBS News <a href=\"https://www.cbsnews.com/news/australia-fires-over-1-billion-animals-feared-dead/\" rel=\"nofollow\">https://www.cbsnews.com/news/australia-fires-over-1-billion-animals-feared-dead/</a></p>\n<p>ITV news <a href=\"https://www.youtube.com/watch?v=EqZbfi3L1w0\" rel=\"nofollow\">https://www.youtube.com/watch?v=EqZbfi3L1w0</a></p>\n<p>ABC <a href=\"https://www.youtube.com/watch?v=LsiD5tB9yrc\" rel=\"nofollow\">https://www.youtube.com/watch?v=LsiD5tB9yrc</a></p>\n<p>Photograph by Carles Rabada <a href=\"https://unsplash.com/photos/v3v6uz-n-pQ\" rel=\"nofollow\">https://unsplash.com/photos/v3v6uz-n-pQ</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe rapidly advancing wildfires are an unstoppable force of nature, and you along with hundreds of helpless animals are trapped between that fire and a body of water. Where do you go?\n\n\n## What it does\n\n\nWe inject urgency into the scene with the mercilessly advancing wildfire. All around you the vibrant wildlife of Australia are disappearing by the second. Your natural inclination is to run to the water: this was the true reality for able bodied Australian animals and humans, many of whom had to escape to bodies of water to tread water for hours while waiting for wildfires to pass. Many were not so lucky.\n\n\n## How we built it\n\n\nWe sketched/modeled in Clip Studio and Blender before importing into Unity. Unity was the workhorse responsible for bringing together all assets into the VR environment. We scripted in C#. We prototyped with the Oculus Rift S acting as our HMD.\n\n\n## Challenges we ran into\n\n\nWe struggled with the tone and treatment of the subject matter, walking a careful balance between shock value, intimating a sense of helplessness and ending with a call to action to look after the animals of Australia and their environment.\n\n\n## Accomplishments that we're proud of\n\n\nIn order to make the VR experience suitable for all ages we made a conscious decision to abstract the animals in origami style. On contact with the fire, the origami animal models burn up and float skyward in plumes of ashes contributing to a growing sense of loss and helplessness.\n\n\n## What we learned\n\n\nWe learned a lot building a rapid prototyping and troubleshooting workflow with Unity and the Oculus especially around shaders, performance cost/optimization and spatial sound design.\n\n\n## What's next for One Billion Lost: Australia's Wildfires\n\n\nWith more time we'd like to apply more polish to animations, rigging and believable fire dynamics. We'd also like to better build out interactivity with the animals and perhaps inject small bits of gamification (ie carrying animals to safety in the water) to engage the user further.\n\n\n## Audio and photo attributions\n\n\nNews.com.au <https://www.news.com.au/technology/environment/sir-david-attenborough-says-humans-have-overrun-the-world/news-story/279c49216accdcd3d55ddd49897f5f13>\n\n\nCBS News <https://www.cbsnews.com/news/australia-fires-over-1-billion-animals-feared-dead/>\n\n\nITV news <https://www.youtube.com/watch?v=EqZbfi3L1w0>\n\n\nABC <https://www.youtube.com/watch?v=LsiD5tB9yrc>\n\n\nPhotograph by Carles Rabada <https://unsplash.com/photos/v3v6uz-n-pQ>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/mixed-reality-sign-language-learning-studio",
            "title": "Mixed Reality Sign Language Learning Studio",
            "blurb": "A mixed reality application that helps users quickly learn Sign Language, using modern learning techniques and the latest devices in VR.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/NeMCpeIceZ0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Tim Range",
                    "about": "I worked on the development of the application on the HoloLens 2 by Microsoft using Unity and Visual Studio and importing the MKRT, importing the various gestures and functions built into the device out of the box. ",
                    "photo": "https://avatars1.githubusercontent.com/u/42386765?height=180&v=4&width=180"
                },
                {
                    "name": "Derrick Wilson-Duncan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/514/038/datas/profile.jpg"
                },
                {
                    "name": "cthosul",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/60049559?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-creative-sdk",
                "c#",
                "microsoft-hololens",
                "nreal-sdk",
                "unity",
                "verbal-collective"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>A large number of Americans suffer from disabling hearing loss, causing problems with education, employment, and communication. With an improved method of teaching sign language people of all ages can regain their independence. </p>\n<h2>What it does</h2>\n<p>The Mixed Reality Sign Language Learning Studio or MRSLLS uses cutting edge education techniques to help students and users learn and understand sign language quickly.</p>\n<h2>How I built it</h2>\n<p>We built it for cross compatibility, targeting two platforms the Nreal Light and the Microsoft Hololens.</p>\n<h2>Challenges I ran into</h2>\n<p>A few build issues and a few missing libraries here and there.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I'm proud of our teamwork, personally speaking this is one of the most diverse teams I've worked with. </p>\n<h2>What I learned</h2>\n<p>Learned a lot about developing in Unity and the workflow for developing for mixed reality devices as a whole.</p>\n<h2>What's next for Mixed Reality Sign Language Learning Studio</h2>\n<p>Adding other sign languages like BSL.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nA large number of Americans suffer from disabling hearing loss, causing problems with education, employment, and communication. With an improved method of teaching sign language people of all ages can regain their independence. \n\n\n## What it does\n\n\nThe Mixed Reality Sign Language Learning Studio or MRSLLS uses cutting edge education techniques to help students and users learn and understand sign language quickly.\n\n\n## How I built it\n\n\nWe built it for cross compatibility, targeting two platforms the Nreal Light and the Microsoft Hololens.\n\n\n## Challenges I ran into\n\n\nA few build issues and a few missing libraries here and there.\n\n\n## Accomplishments that I'm proud of\n\n\nI'm proud of our teamwork, personally speaking this is one of the most diverse teams I've worked with. \n\n\n## What I learned\n\n\nLearned a lot about developing in Unity and the workflow for developing for mixed reality devices as a whole.\n\n\n## What's next for Mixed Reality Sign Language Learning Studio\n\n\nAdding other sign languages like BSL.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/giving-tree-eofiuq",
            "title": "DONO: Giving Tree",
            "blurb": "AR Donation Platform",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/G5aSEPYYZ9U?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Brodey Lajoie",
                    "about": "Developer",
                    "photo": "https://avatars2.githubusercontent.com/u/32546217?height=180&v=4&width=180"
                },
                {
                    "name": "Lauren Kam",
                    "about": "Researcher and learner",
                    "photo": "https://avatars1.githubusercontent.com/u/50049809?height=180&v=4&width=180"
                },
                {
                    "name": "Tiffany Lam",
                    "about": "Product designer / manager + lead creative (originated the idea)",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/911/398/datas/profile.jpg"
                },
                {
                    "name": "Dennis Henneman",
                    "about": "Developer",
                    "photo": "https://avatars0.githubusercontent.com/u/23065264?height=180&v=4&width=180"
                },
                {
                    "name": "Kexin Cha",
                    "about": "UX Researcher & Designer",
                    "photo": "https://www.gravatar.com/avatar/a8ab1389858a3ddeecd5158660e9ac1f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "arfoundation",
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Overview</h2>\n<p><em><strong>DONO: Giving Tree. A new way to experience giving.</strong></em></p>\n<p>DONO is an augmented reality donation platform available through mobile that helps boost fundraising for global causes by offering supporters a gamified, community experience once they donate. </p>\n<p>Using Unity AR Foundation, we challenged ourselves to design and build a prototype for an accessible and scalable fundraising tool that anyone could experience anytime and anywhere.</p>\n<h2>Inspiration</h2>\n<p>For this hackathon, we challenged ourselves to rethink the giving and donating experience and question what it looks like and how XR could be leveraged to improve this. Much fundraising for global causes -- like the Australian wildfires -- happens through traditional news and media, but little is focused on the user\u2019s experience when donating. So how can we change this and integrate components of gamification, excitement and sensation? </p>\n<h2>How does it work</h2>\n<ul>\n<li>With DONO Giving Tree, users can explore global causes (currently fundraising) and join them by donating and see how their donation affects the cause. </li>\n<li>Inspired by <a href=\"https://en.wikipedia.org/wiki/Wish_Tree_(Yoko_Ono_art_series)\" rel=\"nofollow\">Yoko Ono\u2019s Wish Tree</a>, supporters are invited to leave a personal message and place their donation leaf onto the tree. </li>\n<li>Once complete, Explore Mode is unlocked and supporters can walk around to discover and read personal messages left on the tree from fellow supporters around the world. </li>\n<li>When the cause reaches its funding goal, cause supporters are invited to come back and watch the AR tree in FULL bloom. </li>\n</ul>\n<h2>How can it be used and scaled</h2>\n<p>With the DONO AR platform, non-profit organizations and global causes will be able to sustainably boost fundraising efforts by offering everyday people with a donating experience that is fun and accessible. </p>\n<h2>Sourced Assets</h2>\n<p>Free pre-built 3D models (flowers &amp; mushrooms) were sourced as supporting materials surrounding the DONO tree to enhance the user\u2019s experience during the unlocked experience as they explore the AR environment. This serves to communicate the art of the possible and could be custom and specific to the cause selected, such as Australian wildlife. </p>\n<h2>Challenges Faced</h2>\n<ul>\n<li>With so many exciting and creative ideas from the entire team, it was not easy to prioritize features. We had to weigh them by function, time, passion and cohesion with the overall product story.</li>\n<li>We didn\u2019t have a graphic/UX designer or 3D modeler, but this was an open opportunity for exploration, resourcefulness &amp; learning</li>\n<li>We spent Day 1 building a marker-based AR using Vuforia until we realized end of day that UX flow makes more sense if it the user is not required to have a marker. We quickly adapted to markerless. </li>\n<li>Design and wireframing of experience was very iterative, but ultimately this helped optimize the user flow</li>\n<li>Diving into development before having final design wireframes ready was tough; this meant sometimes we\u2019d have to re-build features or re-design to ensure collective alignment</li>\n<li>Exploring Azure Spatial Anchors didn\u2019t work to our needs so we stuck with AR foundation plane detection in Unity</li>\n<li>Coding while exhausted</li>\n</ul>\n<h2>Learnings</h2>\n<ul>\n<li>3D modeling using Blender </li>\n<li>UX designing for AR experiences and environments</li>\n<li>Procedural generation of plant model using Unity</li>\n<li>Taking an idea and narrowing down to which XR technology to use (AR vs VR, what devices, what AR platform to use)</li>\n<li>Being resourceful and stretching team effort when you don\u2019t have sufficient technical experience</li>\n<li>How to size down a big idea and fit it into a weekend by continuous prioritization</li>\n<li>Scaling for both mobile Android &amp; IOS devices</li>\n<li>Designing something in XR that could be scalable to meet the masses</li>\n</ul>\n<h2>Future features</h2>\n<ul>\n<li>Explore other causes and their DONO trees in AR</li>\n<li>Non-profit cause registration and validation</li>\n<li>Donation payment processing and/or tie into other existing donation platforms</li>\n<li>Share with friends and when they join, prompt to create team branch on the DONO AR tree</li>\n<li>Mutil-player live session</li>\n<li>Monitor trees from past donations</li>\n<li>Port to VR or other AR platforms</li>\n<li>Sliding scale to see evolution of tree growth / fund growth </li>\n<li>Develop into learning/news platform for current causes that can be routinely explored by users</li>\n</ul>\n</div>",
            "content_md": "\n## Overview\n\n\n***DONO: Giving Tree. A new way to experience giving.***\n\n\nDONO is an augmented reality donation platform available through mobile that helps boost fundraising for global causes by offering supporters a gamified, community experience once they donate. \n\n\nUsing Unity AR Foundation, we challenged ourselves to design and build a prototype for an accessible and scalable fundraising tool that anyone could experience anytime and anywhere.\n\n\n## Inspiration\n\n\nFor this hackathon, we challenged ourselves to rethink the giving and donating experience and question what it looks like and how XR could be leveraged to improve this. Much fundraising for global causes -- like the Australian wildfires -- happens through traditional news and media, but little is focused on the user\u2019s experience when donating. So how can we change this and integrate components of gamification, excitement and sensation? \n\n\n## How does it work\n\n\n* With DONO Giving Tree, users can explore global causes (currently fundraising) and join them by donating and see how their donation affects the cause.\n* Inspired by [Yoko Ono\u2019s Wish Tree](https://en.wikipedia.org/wiki/Wish_Tree_(Yoko_Ono_art_series)), supporters are invited to leave a personal message and place their donation leaf onto the tree.\n* Once complete, Explore Mode is unlocked and supporters can walk around to discover and read personal messages left on the tree from fellow supporters around the world.\n* When the cause reaches its funding goal, cause supporters are invited to come back and watch the AR tree in FULL bloom.\n\n\n## How can it be used and scaled\n\n\nWith the DONO AR platform, non-profit organizations and global causes will be able to sustainably boost fundraising efforts by offering everyday people with a donating experience that is fun and accessible. \n\n\n## Sourced Assets\n\n\nFree pre-built 3D models (flowers & mushrooms) were sourced as supporting materials surrounding the DONO tree to enhance the user\u2019s experience during the unlocked experience as they explore the AR environment. This serves to communicate the art of the possible and could be custom and specific to the cause selected, such as Australian wildlife. \n\n\n## Challenges Faced\n\n\n* With so many exciting and creative ideas from the entire team, it was not easy to prioritize features. We had to weigh them by function, time, passion and cohesion with the overall product story.\n* We didn\u2019t have a graphic/UX designer or 3D modeler, but this was an open opportunity for exploration, resourcefulness & learning\n* We spent Day 1 building a marker-based AR using Vuforia until we realized end of day that UX flow makes more sense if it the user is not required to have a marker. We quickly adapted to markerless.\n* Design and wireframing of experience was very iterative, but ultimately this helped optimize the user flow\n* Diving into development before having final design wireframes ready was tough; this meant sometimes we\u2019d have to re-build features or re-design to ensure collective alignment\n* Exploring Azure Spatial Anchors didn\u2019t work to our needs so we stuck with AR foundation plane detection in Unity\n* Coding while exhausted\n\n\n## Learnings\n\n\n* 3D modeling using Blender\n* UX designing for AR experiences and environments\n* Procedural generation of plant model using Unity\n* Taking an idea and narrowing down to which XR technology to use (AR vs VR, what devices, what AR platform to use)\n* Being resourceful and stretching team effort when you don\u2019t have sufficient technical experience\n* How to size down a big idea and fit it into a weekend by continuous prioritization\n* Scaling for both mobile Android & IOS devices\n* Designing something in XR that could be scalable to meet the masses\n\n\n## Future features\n\n\n* Explore other causes and their DONO trees in AR\n* Non-profit cause registration and validation\n* Donation payment processing and/or tie into other existing donation platforms\n* Share with friends and when they join, prompt to create team branch on the DONO AR tree\n* Mutil-player live session\n* Monitor trees from past donations\n* Port to VR or other AR platforms\n* Sliding scale to see evolution of tree growth / fund growth\n* Develop into learning/news platform for current causes that can be routinely explored by users\n\n\n"
        },
        {
            "source": "https://devpost.com/software/signar-hw6rxf",
            "title": "SignAR ",
            "blurb": "Our goal is to break down barriers for non-verbal communicators. to have the experience of voice to voice conversation.  We believe everyone has an opinion and opportunities should be for all.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/nJkp6uROTvo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "UI MOCK UP 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/017/datas/original.png"
                },
                {
                    "title": "UI MOCK UP",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/018/datas/original.png"
                },
                {
                    "title": "UI MOCKUP 3",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/019/datas/original.png"
                },
                {
                    "title": "LOGO LOCK UP",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/031/datas/original.png"
                },
                {
                    "title": "LOGOTYPE 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/917/032/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Zhuoneng Wang",
                    "about": "I originated the idea of this project, worked on hand gesture recognition feature and general Unity issues to make sure the prototype works properly.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/918/487/datas/profile.png"
                },
                {
                    "name": "Runze Zhang",
                    "about": "I created a mini-base for recognizing sign language and sentences; built the interaction function in unity.",
                    "photo": "https://avatars1.githubusercontent.com/u/37249382?height=180&v=4&width=180"
                },
                {
                    "name": "Alistair Leyland",
                    "about": "I produced the project, helped with the video, lead aspects of the pitch presentation and tried to keep the team super positive during a very condensed timeline.  ",
                    "photo": "https://graph.facebook.com/10157386967244902/picture?height=180&width=180"
                },
                {
                    "name": "Soyoung Lim",
                    "about": "I worked on UX/UI and logo design. I tried to maximize usability of SignAR functions in situation of face-to-face conversation. ",
                    "photo": "https://media-exp1.licdn.com/dms/image/C5103AQHTT6IK0Q2-tA/profile-displayphoto-shrink_800_800/0?e=1584576000&height=180&t=lHQUeuaW5iCnQbiEXlLpBCam3ENOrIn0nNtN2cNP0MY&v=beta&width=180"
                },
                {
                    "name": "Ajinkya Hukerikar",
                    "about": "",
                    "photo": "https://lh4.googleusercontent.com/-l7EggvfklQE/AAAAAAAAAAI/AAAAAAAAQHw/AMZuucnRs70Wck9V0mGnmZTV5A2dYvtyvg/c/photo.jpg?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe",
                "adobe-illustrator",
                "augmented-reality",
                "c#",
                "interviews",
                "leap-motion",
                "oculus",
                "photoshop",
                "unity",
                "ux",
                "zed"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are inspired to augmented reality to create an experience for those who are hearing and speech impaired to be able to communicate via sound and text.</p>\n<h2>What it does</h2>\n<p>SignAR is the next generation of sign language application, providing non-verbal communicators new tools to communicate.  Core features include a gesture/sign language to voice/text translation, the ability to customize vocabulary and phrases [hot keys] to be able to verbally communicate with friends, family, coworkers and loved ones.</p>\n<h2>How we built it</h2>\n<ul>\n<li><p>We built this experience in Unity, utilizing leap motion for hand-tracking, oculus rift which was hacked with a camera to empower an AR function, several laptops, a Bluetooth speaker - as no directional audio speaker was available. </p></li>\n<li><p>Usability and readability were our main concerns of UX and UI. For UX, we worked on a user persona and sketched low fidelity prototyping with a functional flow. We focused on visual indicators/feedbacks for hearing impaired users and more comfortable face to face communication. To differentiate a user and a conversation partner, the complementary colour was used. We used a user's eye level as a reference for the position of UI so that, augmented reality UI harmonized well with reality and do not disturb a user's conversation. We leveraged Adobe Photoshop and Illustrator for the UI and branding elements.  </p></li>\n</ul>\n<h2>Challenges I ran into</h2>\n<ul>\n<li>We ran into challenges around teaching the experience to detect human hands and the associated meaning behind the gestures made by human hands.<br/>\nNone of the team previously knew American sign language and it was a fairly steep learning curve, but we learned enough to start the creation of a gestural vocabulary.  Machine learning would be a fantastic addition to the experience in order to build the vocabulary efficiently and create a meaningful tool.<br/></li>\n<li>This experience would lend itself very well to the hololens 2 - however due to hardware constraints one was not available, so we hacked together hardware solutions intended to emulate the hololens2.</li>\n</ul>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We are proud of numerous aspects of the project, but the top would be the technology created in a short time, thoughtfulness to the user experience and the development of the concept by scrambling to develop subject matter expertise within a sensitive area - non-verbal communication.  We sourced the hackathon crowd and leveraged mentors to unearth a wonderful person named Shannon, an educator who works with non-verbal communicators, whom we interviewed and talked with extensively in order to build a sensible and thoughtful approach to creating this product.  </p>\n<h2>What we learned</h2>\n<p>We learned the immense challenges and barriers that exist for non-verbal communicators.  98% of deaf people do not receive education in sign language.  72% of families do not sign with their deaf children - which is heartbreaking.  70% of deaf people don't work or are underemployed and this certainly doesn't bode well for their confidence or happiness.\n1 in 4 non-verbal communicators have left a job due to discrimination.  All of these facts drive us to create a better way for non-verbal communicators to express themselves in a personal and professional context.<br/>\nTechnically, we learned how to work with existing hardware limitations - lack of hololens2 - and work with oculus rift.  Ideally the next step for this project would be to make the product hardware agnostic in order to empower meaningful communication for any wearer of any headset.  </p>\n<h2>What's next for SignAR</h2>\n<ul>\n<li>This tool could be a very significant product for non-verbal communicators both locally and globally.  Exploring Watson integration will allow machine learnings to power up the vocabulary to scale the potential for meaningful communication around the world. \nFUTURE FEATURES //\u00a0</li>\n<li>INTEGRATE WITH OPEN SOURCE WATSON, LEVERAGE MACHINE LEARNINGS TO RAPIDLY BUILD OUT A ROBUST VOCABULARY DATABASE.\u00a0 \u00a0</li>\n<li>INTEGRATE WITH OPEN SOURCE PAST MIT HACKATHON PROJECT </li>\n<li>TO INTEGRATE EXISTING VOICE TO GESTURE PROJECT </li>\n<li>EFFECTIVELY CLOSING THE LOOP ON MEANINGFUL COMMUNICATION FOR NON-VERBAL COMMUNICATORS.\u00a0\u00a0</li>\n<li>DEVELOP DIRECTIONAL AUDIO FUNCTIONALITY, ALLOWING FOR PRESENTATION MODE OR MORE PRIVATE CONVERSATIONS TO TAKE PLACE.\u00a0</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are inspired to augmented reality to create an experience for those who are hearing and speech impaired to be able to communicate via sound and text.\n\n\n## What it does\n\n\nSignAR is the next generation of sign language application, providing non-verbal communicators new tools to communicate. Core features include a gesture/sign language to voice/text translation, the ability to customize vocabulary and phrases [hot keys] to be able to verbally communicate with friends, family, coworkers and loved ones.\n\n\n## How we built it\n\n\n* We built this experience in Unity, utilizing leap motion for hand-tracking, oculus rift which was hacked with a camera to empower an AR function, several laptops, a Bluetooth speaker - as no directional audio speaker was available.\n* Usability and readability were our main concerns of UX and UI. For UX, we worked on a user persona and sketched low fidelity prototyping with a functional flow. We focused on visual indicators/feedbacks for hearing impaired users and more comfortable face to face communication. To differentiate a user and a conversation partner, the complementary colour was used. We used a user's eye level as a reference for the position of UI so that, augmented reality UI harmonized well with reality and do not disturb a user's conversation. We leveraged Adobe Photoshop and Illustrator for the UI and branding elements.\n\n\n## Challenges I ran into\n\n\n* We ran into challenges around teaching the experience to detect human hands and the associated meaning behind the gestures made by human hands.  \n\nNone of the team previously knew American sign language and it was a fairly steep learning curve, but we learned enough to start the creation of a gestural vocabulary. Machine learning would be a fantastic addition to the experience in order to build the vocabulary efficiently and create a meaningful tool.\n* This experience would lend itself very well to the hololens 2 - however due to hardware constraints one was not available, so we hacked together hardware solutions intended to emulate the hololens2.\n\n\n## Accomplishments that I'm proud of\n\n\nWe are proud of numerous aspects of the project, but the top would be the technology created in a short time, thoughtfulness to the user experience and the development of the concept by scrambling to develop subject matter expertise within a sensitive area - non-verbal communication. We sourced the hackathon crowd and leveraged mentors to unearth a wonderful person named Shannon, an educator who works with non-verbal communicators, whom we interviewed and talked with extensively in order to build a sensible and thoughtful approach to creating this product. \n\n\n## What we learned\n\n\nWe learned the immense challenges and barriers that exist for non-verbal communicators. 98% of deaf people do not receive education in sign language. 72% of families do not sign with their deaf children - which is heartbreaking. 70% of deaf people don't work or are underemployed and this certainly doesn't bode well for their confidence or happiness.\n1 in 4 non-verbal communicators have left a job due to discrimination. All of these facts drive us to create a better way for non-verbal communicators to express themselves in a personal and professional context.  \n\nTechnically, we learned how to work with existing hardware limitations - lack of hololens2 - and work with oculus rift. Ideally the next step for this project would be to make the product hardware agnostic in order to empower meaningful communication for any wearer of any headset. \n\n\n## What's next for SignAR\n\n\n* This tool could be a very significant product for non-verbal communicators both locally and globally. Exploring Watson integration will allow machine learnings to power up the vocabulary to scale the potential for meaningful communication around the world. \nFUTURE FEATURES //\n* INTEGRATE WITH OPEN SOURCE WATSON, LEVERAGE MACHINE LEARNINGS TO RAPIDLY BUILD OUT A ROBUST VOCABULARY DATABASE.\n* INTEGRATE WITH OPEN SOURCE PAST MIT HACKATHON PROJECT\n* TO INTEGRATE EXISTING VOICE TO GESTURE PROJECT\n* EFFECTIVELY CLOSING THE LOOP ON MEANINGFUL COMMUNICATION FOR NON-VERBAL COMMUNICATORS.\n* DEVELOP DIRECTIONAL AUDIO FUNCTIONALITY, ALLOWING FOR PRESENTATION MODE OR MORE PRIVATE CONVERSATIONS TO TAKE PLACE.\n\n\n"
        }
    ],
    "realityfest-exhibition-hall": [
        {
            "source": "https://devpost.com/software/test-project-hello",
            "title": "Example Project",
            "blurb": "Use this as a starting point for planning your own post.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/eBTPq9bzWEc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Example image 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/705/423/datas/original.jpg"
                },
                {
                    "title": "Example image 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/705/375/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Wiley Corning",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7a33574e3f20ae3d61f83b92cc718e14?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#"
            ],
            "content_html": "<div>\n<h2>Motivation</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.</p>\n<p>Elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.</p>\n<h2>What it does</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.</p>\n<p>Elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.</p>\n<h2>How we built it</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.</p>\n<p>Elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.</p>\n</div>",
            "content_md": "\n## Motivation\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.\n\n\nElit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.\n\n\n## What it does\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.\n\n\nElit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.\n\n\n## How we built it\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Sed faucibus turpis in eu mi.\n\n\nElit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Massa eget egestas purus viverra accumsan in nisl nisi scelerisque.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/depress-the-depression",
            "title": "FightMind",
            "blurb": "FightMind is an all-in-one platform to fight depression in youngsters and to introduce them to augmented reality meditation, peer interaction, and receive guidance from our mental health therapists.",
            "awards": [
                "Participation Prize",
                "First Overall",
                "Best Domain Name from GoDaddy Registry [APAC Only]"
            ],
            "videos": [
                "https://www.youtube.com/embed/VJJg6YZvgjY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "AKSHITA GUPTA",
                    "about": "I worked on the Augmented reality part of the website and the video editing",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/100/297/datas/profile.jpeg"
                },
                {
                    "name": "Angelin Varghese",
                    "about": "I worked on the mental-health page of the website using html and CSS.It was great working as a team.\n",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/664/934/datas/profile.jpeg"
                },
                {
                    "name": "Anwisha Zaman",
                    "about": "I worked on the front end of the website using html ,css,js.\nAlso made the content of the website .",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/664/935/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "css3",
                "echoar",
                "html5",
                "javascript",
                "unity"
            ],
            "content_html": "<div>\n<h2>Our team code is 25689855</h2>\n<h2>Inspiration</h2>\n<p><strong>LIFE IS VALUABLE!!!</strong></p>\n<p>Lack of sleep, bad eating habits, and insufficient exercise are all factors that contribute to depression in college students. Academic stress, which includes financial concerns, pressure to find a decent career after graduation, and failed relationships, is enough to drive some students to drop out of college or worse. We must teach our people to be content with themselves and friendly with others, and we must free them from the mind's complicated psychological gymnastics.</p>\n<p>Peace is something that people who are living in the middle of terrible conflict yearn for and desire. \n                                       <strong>Pain is inevitable but suffering is optional</strong>\nHaving a broad view of life provides you the courage to get through difficult situations. Know that you are very much needed in this world.  This life, with all of its limitless possibilities, is a gift because it has the potential to become a source of joy and happiness not just for oneself but also for your own family and friends. So we're here to help you get rid of all your problems and show you that <em>life can still be a brilliant and sparkling star in the dark sky</em>.</p>\n<h2>What it does</h2>\n<p>We have come up with some of the best features such as the events section which includes <strong>Yoga</strong> which may help reduce stress, lower blood pressure and lower your heart rate and also better the body posture and spine health, flexibility, balance, strength, and coordination.</p>\n<p>Endorphins, the chemicals in your brain that reduce pain and tension, are released when you exercise. It also lowers stress chemicals such as cortisol and adrenaline. We've introduced a section where you may book tickets and socialize with your peers by attending events together and as we know  <strong>AEROBICS and GYMNASTICS</strong>  helps to relax our muscles and reduces all the stress.</p>\n<p>Your  <strong>mental health</strong> , which encompasses your emotional, psychological, and social well-being, is our primary emphasis. It has an impact on the way you think, feel, and act. It also influences how we deal with stress, interacts with people, and make good decisions. As a result, we provide you with a free psychologist with whom you may contact at any time and make appointments.</p>\n<p>Most importantly, we provide <strong>Augmented Reality Meditation</strong> , which is an active, immersive, and visual experience. It assists you in achieving a profound level of relaxation and a calm mind. Helps us focus and clear our minds of the muddled ideas that may clog our minds and cause tension.in Augmented reality, we have added peaceful music to calm your mind through this difficult situation.</p>\n<h2>How we built it</h2>\n<p>We have built using Html,CSS,javascript,echoAR,unity</p>\n<h2>Challenges we ran into</h2>\n<p>We had never used JavaScript to the depth that we did for this project, and we had trouble making a way to compare each separate user-inputted item to our small database. We faced issues in aligning our Echo-AR models.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud that we have made a website that can reduce the depression of people and helps them come out of it positively and helps reduce suicidal rates as it's a serious rising issue. We are happy that our website will make an impact on young minds and help them think deeply about their life and take the right decision and even face difficult situations boldly.</p>\n<h2>What we learned</h2>\n<p>We learned the use of echoAR and how to embed augmented reality into our website. We learned Html and CSS at advanced levels. We came across the use of animate.css which helped us add effects to CSS content which made to look our page even more attractive.</p>\n<h2>What's next for FightMind</h2>\n<p>We'd want to create a community for them where they can interact with their peers while also providing security conditions. We will add more features, such as organizing events and providing them with journals in which they may write and for which we will award prizes motivating them to improve their writing abilities. We would like to add some more graphics which will add even more effect to our page.</p>\n</div>",
            "content_md": "\n## Our team code is 25689855\n\n\n## Inspiration\n\n\n**LIFE IS VALUABLE!!!**\n\n\nLack of sleep, bad eating habits, and insufficient exercise are all factors that contribute to depression in college students. Academic stress, which includes financial concerns, pressure to find a decent career after graduation, and failed relationships, is enough to drive some students to drop out of college or worse. We must teach our people to be content with themselves and friendly with others, and we must free them from the mind's complicated psychological gymnastics.\n\n\nPeace is something that people who are living in the middle of terrible conflict yearn for and desire. \n **Pain is inevitable but suffering is optional**\nHaving a broad view of life provides you the courage to get through difficult situations. Know that you are very much needed in this world. This life, with all of its limitless possibilities, is a gift because it has the potential to become a source of joy and happiness not just for oneself but also for your own family and friends. So we're here to help you get rid of all your problems and show you that *life can still be a brilliant and sparkling star in the dark sky*.\n\n\n## What it does\n\n\nWe have come up with some of the best features such as the events section which includes **Yoga** which may help reduce stress, lower blood pressure and lower your heart rate and also better the body posture and spine health, flexibility, balance, strength, and coordination.\n\n\nEndorphins, the chemicals in your brain that reduce pain and tension, are released when you exercise. It also lowers stress chemicals such as cortisol and adrenaline. We've introduced a section where you may book tickets and socialize with your peers by attending events together and as we know **AEROBICS and GYMNASTICS** helps to relax our muscles and reduces all the stress.\n\n\nYour **mental health** , which encompasses your emotional, psychological, and social well-being, is our primary emphasis. It has an impact on the way you think, feel, and act. It also influences how we deal with stress, interacts with people, and make good decisions. As a result, we provide you with a free psychologist with whom you may contact at any time and make appointments.\n\n\nMost importantly, we provide **Augmented Reality Meditation** , which is an active, immersive, and visual experience. It assists you in achieving a profound level of relaxation and a calm mind. Helps us focus and clear our minds of the muddled ideas that may clog our minds and cause tension.in Augmented reality, we have added peaceful music to calm your mind through this difficult situation.\n\n\n## How we built it\n\n\nWe have built using Html,CSS,javascript,echoAR,unity\n\n\n## Challenges we ran into\n\n\nWe had never used JavaScript to the depth that we did for this project, and we had trouble making a way to compare each separate user-inputted item to our small database. We faced issues in aligning our Echo-AR models.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud that we have made a website that can reduce the depression of people and helps them come out of it positively and helps reduce suicidal rates as it's a serious rising issue. We are happy that our website will make an impact on young minds and help them think deeply about their life and take the right decision and even face difficult situations boldly.\n\n\n## What we learned\n\n\nWe learned the use of echoAR and how to embed augmented reality into our website. We learned Html and CSS at advanced levels. We came across the use of animate.css which helped us add effects to CSS content which made to look our page even more attractive.\n\n\n## What's next for FightMind\n\n\nWe'd want to create a community for them where they can interact with their peers while also providing security conditions. We will add more features, such as organizing events and providing them with journals in which they may write and for which we will award prizes motivating them to improve their writing abilities. We would like to add some more graphics which will add even more effect to our page.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/parc-instrumental-spatial",
            "title": "Parc Instrumental Spatial",
            "blurb": "Take a walk in the park of acoustic architecture... consisted of three sound sculptures and 80 spatial sound sources.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/XLx3wNTHSkY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Joanna Liu",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/936/177/datas/profile.jpeg"
                },
                {
                    "name": "Changbai Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/4966f41804d27bce4f7238904a7c5c6e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "yuening cai",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/70176057ce467e24fc8c19d3916706af?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jialin LIU",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/66630c4b4dc6b64d7f208731f5f8ca2f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "quixel",
                "udon",
                "unity",
                "unreal-engine",
                "vrchat"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>During the pandemic, we encountered the VRC rave scene. When we attended THE VRC club \"Ghost Club\", we were shocked by the production quality, energy and the creator community's consistent contribution to the space. For the first time we saw the potential of virtual worlds and the positive connection between people established in such space. It is an alive, supportive community of creators and audience not some zombie product made to impress VC.\nSo we started trying to build our own world and host a \"portal event\" where we had a simultaneous live performance in an underground club offline in Shanghai and also in VRC. </p>\n<p>The world is consisted of indoor live stages and outdoor acoustic architecture park. For this exhibition because there is no live performances we are exhibiting the outdoor area.</p>\n<h2>What it does</h2>\n<p>It's an acoustic architecture park, consisted of 3 parts: </p>\n<p><strong>Part 1 Tunnel:</strong>\nThis piece was inspired by the text \"Der Tunnel\" by Friedrich D\u00fcrrenmatt. The text describes a young man, 24 years old,  who took the train to the university, but the train entered an endless tunnel and fell into the center of the earth.</p>\n<p><strong>Part 2 Waterphone:</strong>\nThis instrument has a ghost like sound. Every steel bar here bring an interesting pitch to the mixture of sound.</p>\n<p><strong>Part 3 Spherical Network:</strong>\nIt is constructed with thousands of thin beams forming a shape of hemisphere. You will see the trajectory of the three comets (les trois m\u00e9t\u00e9ores). They are all directly along the beam of the spheric networks.\nThese trajectories are used as the trajectory of the sound elements. There are three kind of sound materials: noise, granular sounds like pulse-train, and language. The sounds choose the direction freely, and pass very quickly, like a meteor in the sky. </p>\n<p>The architecture is inspired by the text of Borg\u00e8s, le jardin aux sentiers qui bifurquent, with the idea of parallel possibility of life events, and the three kind of sound and three trajectory are metaphor of the parallel possibility.</p>\n<h2>How we built it</h2>\n<p>This is the first VRC world we built. We started by reading vrc documentation, and we go from there.</p>\n<p>Model: Unreal\nTexture, lighting, UI, Interation: Unity + VRC SDK\nSound: 64 channel audio is recorded in Modalys</p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Dynamic lighting that changes with video texture</li>\n<li>Network streaming issue within VRC</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Lighting, sound and post processing in Unity</li>\n<li>VRC SDK\n-Discord marketing and online community \"management\".</li>\n</ul>\n<h2>What's next for Parc Instrumental Spatial</h2>\n<p>We are constantly renovating the world, we are hoping to have the next portal event. We will update in this discord: <a href=\"https://discord.gg/j4NjQaXXJt\" rel=\"nofollow\">https://discord.gg/j4NjQaXXJt</a> </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nDuring the pandemic, we encountered the VRC rave scene. When we attended THE VRC club \"Ghost Club\", we were shocked by the production quality, energy and the creator community's consistent contribution to the space. For the first time we saw the potential of virtual worlds and the positive connection between people established in such space. It is an alive, supportive community of creators and audience not some zombie product made to impress VC.\nSo we started trying to build our own world and host a \"portal event\" where we had a simultaneous live performance in an underground club offline in Shanghai and also in VRC. \n\n\nThe world is consisted of indoor live stages and outdoor acoustic architecture park. For this exhibition because there is no live performances we are exhibiting the outdoor area.\n\n\n## What it does\n\n\nIt's an acoustic architecture park, consisted of 3 parts: \n\n\n**Part 1 Tunnel:**\nThis piece was inspired by the text \"Der Tunnel\" by Friedrich D\u00fcrrenmatt. The text describes a young man, 24 years old, who took the train to the university, but the train entered an endless tunnel and fell into the center of the earth.\n\n\n**Part 2 Waterphone:**\nThis instrument has a ghost like sound. Every steel bar here bring an interesting pitch to the mixture of sound.\n\n\n**Part 3 Spherical Network:**\nIt is constructed with thousands of thin beams forming a shape of hemisphere. You will see the trajectory of the three comets (les trois m\u00e9t\u00e9ores). They are all directly along the beam of the spheric networks.\nThese trajectories are used as the trajectory of the sound elements. There are three kind of sound materials: noise, granular sounds like pulse-train, and language. The sounds choose the direction freely, and pass very quickly, like a meteor in the sky. \n\n\nThe architecture is inspired by the text of Borg\u00e8s, le jardin aux sentiers qui bifurquent, with the idea of parallel possibility of life events, and the three kind of sound and three trajectory are metaphor of the parallel possibility.\n\n\n## How we built it\n\n\nThis is the first VRC world we built. We started by reading vrc documentation, and we go from there.\n\n\nModel: Unreal\nTexture, lighting, UI, Interation: Unity + VRC SDK\nSound: 64 channel audio is recorded in Modalys\n\n\n## Challenges we ran into\n\n\n* Dynamic lighting that changes with video texture\n* Network streaming issue within VRC\n\n\n## What we learned\n\n\n* Lighting, sound and post processing in Unity\n* VRC SDK\n-Discord marketing and online community \"management\".\n\n\n## What's next for Parc Instrumental Spatial\n\n\nWe are constantly renovating the world, we are hoping to have the next portal event. We will update in this discord: <https://discord.gg/j4NjQaXXJt> \n\n\n"
        },
        {
            "source": "https://devpost.com/software/firefighting-bot",
            "title": "Firefighting Bot",
            "blurb": "A crude and efficient solution to save many lives",
            "awards": [
                "Hacking - Runner Up",
                "Best Hardware Hack Sponsored by Digi-Key",
                "Third Overall"
            ],
            "videos": [
                "https://www.youtube.com/embed/b22K9MwNXhk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Circuit Schematic",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/661/764/datas/original.png"
                },
                {
                    "title": "Flow Chart of the System",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/661/896/datas/original.png"
                },
                {
                    "title": "PCB design of the Unit",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/661/719/datas/original.png"
                },
                {
                    "title": "3d Model of the Unit(Photodiodes are such placed, for omni directional flame sensing)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/661/760/datas/original.png"
                },
                {
                    "title": "One unit of Flame sensor",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/661/716/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Deblina Chattopadhyay",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GhoS-XoGENSIfNWQlstzOPHGTRfAAq-AdJ-hCqbLw=s96-c?height=180&width=180"
                },
                {
                    "name": "Avijit Das",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14Gi2CA5iEA_o7qLnU5oOnolBBTTEIWvmKWPlGoEGQQ=s96-c?height=180&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "autonomous",
                "c",
                "hardware",
                "iot",
                "mqtt",
                "pcb",
                "photodiode",
                "robot",
                "wifi"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The main inspiration behind building this project was that a few years back, one of our friends mistakenly set fire in his room while doing a science experiment. While he was able to put the fire out, many things were burnt down and were unrepairable.  Also, it was very noticeable that the number of fire breakout cases in factories and mines across the globe was quite high in 2021. </p>\n<p>We always find automatic fire extinguishers, like sprinklers in commercial places, like airports, offices, etc. But not so frequent in our houses, since they are quite expensive to set up, and sometimes the alarm going off at the wrong time can lead to things being destroyed because of the water. Us being electronics engineers, we thought of designing an inexpensive yet very consistent circuit to make an automatic fire alarm and sprinkler control, that can be easily placed in outhouses. </p>\n<h2>What it does</h2>\n<p>A firefighter robot control unit, that can actively sense the presence of fire and tries its best to put it off autonomously. It can sense fire no matter how the robot is oriented in a place. \nIoT connectivity to get you notified, if any mishap occurs, no matter where you are(of course not inside deep caves, or the middle of the ocean. </p>\n<h2>How we built it</h2>\n<p><img alt=\"alt text\" data-canonical-url=\"https://media.discordapp.net/attachments/792856016340910083/889024692495745044/Screenshot_2493.png?width=760&amp;height=427\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--o-698kym--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://media.discordapp.net/attachments/792856016340910083/889024692495745044/Screenshot_2493.png%3Fwidth%3D760%26height%3D427\"/></p>\n<p>The power supply receives an unregulated dc power supply from the outside, preferably a battery, and creates voltage levels with ample amount of current for different components. </p>\n<p>The sensor circuitry is responsible for sensing the flame. As we know, that fire emits infrared radiations(700 nm to 1 mm), we use photodiodes to measure peak wavelengths of around 650nm. With having 6 of them placed for omnidirectional sensing. An analog multiplexer, multiplex these signals and sends it to the MCU.</p>\n<p>The Main Control Unit (MCU) is the brain of the system. It interfaces with the sensor circuit by addressing the analog multiplexer since it has only one ADC. After reading the signals from the 6 sensors, it computes centroid and approximates the position of the burning fire. And after moving towards the fire, it switches to PID control to navigate the robot near to the fire. Then it actuates the relay. \nAlong with that, it has WiFi so that it can be connected to the network and act as a thing of IoT. It can notify you about the mishap instantly. Thus being a major player in critical situations like fire.</p>\n<p>And last but not the least, the driver circuit, which as the name suggests drives or actuates by receiving the control signal from the MCU. It can drive four to eight motors for the movement of an autonomous robot. It also contains a relay that can actuate any high power element such as a water pump to push water, or a strong electromagnet, or a solenoid to trigger a fire extinguisher.</p>\n<h2>Challenges we ran into</h2>\n<p>Sorting out and writing the centroid algorithm for detecting the position of the fire was a challenge. \nAnother challenge that we took upon ourselves was simplifying the circuit to a great extent for very optimized results and very low power consumption. Last but not least, routing the PCB in a very small area was also quite challenging.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Coming up with a very inexpensive design to such was one of our accomplishments. We were also very happy for being able to create and complete the entire PCB on time. </p>\n<h2>What we learned</h2>\n<p>We learned a lot about digital and analog circuitry. And along with that, we also learned about different components while designing the PCB. </p>\n<h2>What's next for Firefighting Bot</h2>\n<p>In the future, we would like to create a pathway for the circuitry to notify the owner as well as the Fire and Emergency services nearby, on the instance of a fire break out at the location using IoT.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe main inspiration behind building this project was that a few years back, one of our friends mistakenly set fire in his room while doing a science experiment. While he was able to put the fire out, many things were burnt down and were unrepairable. Also, it was very noticeable that the number of fire breakout cases in factories and mines across the globe was quite high in 2021. \n\n\nWe always find automatic fire extinguishers, like sprinklers in commercial places, like airports, offices, etc. But not so frequent in our houses, since they are quite expensive to set up, and sometimes the alarm going off at the wrong time can lead to things being destroyed because of the water. Us being electronics engineers, we thought of designing an inexpensive yet very consistent circuit to make an automatic fire alarm and sprinkler control, that can be easily placed in outhouses. \n\n\n## What it does\n\n\nA firefighter robot control unit, that can actively sense the presence of fire and tries its best to put it off autonomously. It can sense fire no matter how the robot is oriented in a place. \nIoT connectivity to get you notified, if any mishap occurs, no matter where you are(of course not inside deep caves, or the middle of the ocean. \n\n\n## How we built it\n\n\n![alt text](https://res.cloudinary.com/devpost/image/fetch/s--o-698kym--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://media.discordapp.net/attachments/792856016340910083/889024692495745044/Screenshot_2493.png%3Fwidth%3D760%26height%3D427)\n\n\nThe power supply receives an unregulated dc power supply from the outside, preferably a battery, and creates voltage levels with ample amount of current for different components. \n\n\nThe sensor circuitry is responsible for sensing the flame. As we know, that fire emits infrared radiations(700 nm to 1 mm), we use photodiodes to measure peak wavelengths of around 650nm. With having 6 of them placed for omnidirectional sensing. An analog multiplexer, multiplex these signals and sends it to the MCU.\n\n\nThe Main Control Unit (MCU) is the brain of the system. It interfaces with the sensor circuit by addressing the analog multiplexer since it has only one ADC. After reading the signals from the 6 sensors, it computes centroid and approximates the position of the burning fire. And after moving towards the fire, it switches to PID control to navigate the robot near to the fire. Then it actuates the relay. \nAlong with that, it has WiFi so that it can be connected to the network and act as a thing of IoT. It can notify you about the mishap instantly. Thus being a major player in critical situations like fire.\n\n\nAnd last but not the least, the driver circuit, which as the name suggests drives or actuates by receiving the control signal from the MCU. It can drive four to eight motors for the movement of an autonomous robot. It also contains a relay that can actuate any high power element such as a water pump to push water, or a strong electromagnet, or a solenoid to trigger a fire extinguisher.\n\n\n## Challenges we ran into\n\n\nSorting out and writing the centroid algorithm for detecting the position of the fire was a challenge. \nAnother challenge that we took upon ourselves was simplifying the circuit to a great extent for very optimized results and very low power consumption. Last but not least, routing the PCB in a very small area was also quite challenging.\n\n\n## Accomplishments that we're proud of\n\n\nComing up with a very inexpensive design to such was one of our accomplishments. We were also very happy for being able to create and complete the entire PCB on time. \n\n\n## What we learned\n\n\nWe learned a lot about digital and analog circuitry. And along with that, we also learned about different components while designing the PCB. \n\n\n## What's next for Firefighting Bot\n\n\nIn the future, we would like to create a pathway for the circuitry to notify the owner as well as the Fire and Emergency services nearby, on the instance of a fire break out at the location using IoT.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/saviour-cwdrmz",
            "title": "Saviour",
            "blurb": "A safety system with SOS signaling, Morse code converter, and GPS tracking for your next trip to a remote location",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/yJxWlMYwOS0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Complete working prototype",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/603/695/datas/original.jpg"
                },
                {
                    "title": "The latitude and longitude details of the location is being showed on the OLED display",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/603/696/datas/original.jpg"
                },
                {
                    "title": "The Longitude got converted into Morse Code",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/603/693/datas/original.jpg"
                },
                {
                    "title": "Earlier scenes when work was in progress",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/602/555/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Deblina Chattopadhyay",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GhoS-XoGENSIfNWQlstzOPHGTRfAAq-AdJ-hCqbLw=s96-c?height=180&width=180"
                },
                {
                    "name": "Avijit Das",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14Gi2CA5iEA_o7qLnU5oOnolBBTTEIWvmKWPlGoEGQQ=s96-c?height=180&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "buzzer",
                "fm",
                "gps",
                "led",
                "morse",
                "oled"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The countless stories and movies like the Lord of the Flies, Robinson Crusoe, Titanic, Cast Away, Six Days Seven Nights, etc. where the protagonist gets stuck on an island, even though it sounds very adventurous, but nonetheless the dangerous probability of it happening in real to people still remains. The idea behind Saviour was to create a hack that allows people to travel to remote locations without having to worry about getting lost or getting stuck there. </p>\n<h2>What it does</h2>\n<p>Saviour is an emergency tool that is very powerful, in times of real juncture. Saviour can do radio signalings like the popular SOS signal or any emergency signaling. If one gets stuck in a deserted area, say on an island or some mountain, where the Mobile gets useless, due to no network coverage, radios are still there, which can be a very effective way of communication in that scenario, and still, they are used all way. \nSaviour uses Morse Code signaling via a radio transmitter, also with a LED. It also has an OLED display, that can translate any sentence to its Morse Code, and display that code on the OLED simultaneously transmitting the code over the air, and also flashing it via a LED. It also has a buzzer to sound, how the Code will sound.\nNow, it also contains a GPS that can get your direct coordinates. Since GPS satellites revolve all around the world, they can connect and get your current location. So with that not only Savior can send the SOS signal using radio, but Saviour sends the location details i.e. the latitude and longitude of the signal's origin location using the GPS. This will make it a lot easier for the search party to find the lost individual thus speeding up the process of the rescue operation.\nThis hack is not only a very important tool that will come in handy for people going on a vacation or traveling because of work purposes but it can also be used as an educational tool for learning Morse code since it has a Morse Code translator. </p>\n<h2>How we built it</h2>\n<p>Saviour has four main parts - one is the Morse Code translator, indicator, transmitter, and the GPS.\nFirstly we created a menu-driven Graphics Interface for choosing the mode, whether the Morse Code or the GPS. This is done by a potentiometer to scroll, and a button to select.\nThe Morse code translator is built by parsing the String sentence character by character and creating its Morse equivalent. According to the supported Characters, the program translates it to a Morse String. To create a sentence, we reused the same button and the potentiometer. The potentiometer, scrolls through the characters, and the button increments the cursor.\nNext, the Indicator was built using a small Buzzer and a LED which can also be used for signaling, and it is still considered to be an effective way. The buzzer plays the Morse Code with a Farnsworth speed of 10 in sync with the LED flashes. \nAfter that, a simple single transistor FM transmitter was built using a coil, capacitor, and transistor. Its job is to transmit the Morse code over open Air.\nLast but not least, the life saviour, the GPS. As the name suggests, this part contains a GPS, through which the NMEA data is received as packets. Those get parsed and fetch the coordinates and finally get transmitted via the transmitter. \nNow all these interfaces are displayed over an OLED display, and all of them get controlled by the Arduino.</p>\n<h2>Challenges we ran into</h2>\n<p>Making the interface was a bit tricky. Stopping abnormal flow or increment of button pushes using flags. Getting ready with the GPS. Since the GPS works best over open air. Printing the translated Morse Code on the OLED was a bit tricky too. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were in a bit of a time crunch since a lot had to be done, but we decided to work on one aspect at a time and finish that first. We are really proud to have overcome all the challenges like making the interface, making the FM transmitter. Displaying the morse code and listening to the buzzer to beep in accordance was a very proud moment. Thus bringing a mere project idea into life and that too in very little time is what we are very happy about. Saviour is a hack that we are very proud of building this tool because it can really help people in times of emergency.</p>\n<h2>What we learned</h2>\n<p>We learned with help of simple components, such as the potentiometer and button we can create such a good graphical interface. We learned how to drive OLED displays. Learned about Morse Codes, speed. Learned a bit about GPS and the NMEA protocol. </p>\n<h2>What's next for Saviour</h2>\n<p>We are thinking of adding a digital compass, and an altimeter by which it can be used as a very powerful and absolute emergency item, and a normal human must-have.</p>\n<h2>Citations</h2>\n<p><a href=\"https://github.com/olikraus/u8g2\" rel=\"nofollow\">https://github.com/olikraus/u8g2</a></p>\n<p><a href=\"https://github.com/mikalhart/TinyGPSPlus\" rel=\"nofollow\">https://github.com/mikalhart/TinyGPSPlus</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe countless stories and movies like the Lord of the Flies, Robinson Crusoe, Titanic, Cast Away, Six Days Seven Nights, etc. where the protagonist gets stuck on an island, even though it sounds very adventurous, but nonetheless the dangerous probability of it happening in real to people still remains. The idea behind Saviour was to create a hack that allows people to travel to remote locations without having to worry about getting lost or getting stuck there. \n\n\n## What it does\n\n\nSaviour is an emergency tool that is very powerful, in times of real juncture. Saviour can do radio signalings like the popular SOS signal or any emergency signaling. If one gets stuck in a deserted area, say on an island or some mountain, where the Mobile gets useless, due to no network coverage, radios are still there, which can be a very effective way of communication in that scenario, and still, they are used all way. \nSaviour uses Morse Code signaling via a radio transmitter, also with a LED. It also has an OLED display, that can translate any sentence to its Morse Code, and display that code on the OLED simultaneously transmitting the code over the air, and also flashing it via a LED. It also has a buzzer to sound, how the Code will sound.\nNow, it also contains a GPS that can get your direct coordinates. Since GPS satellites revolve all around the world, they can connect and get your current location. So with that not only Savior can send the SOS signal using radio, but Saviour sends the location details i.e. the latitude and longitude of the signal's origin location using the GPS. This will make it a lot easier for the search party to find the lost individual thus speeding up the process of the rescue operation.\nThis hack is not only a very important tool that will come in handy for people going on a vacation or traveling because of work purposes but it can also be used as an educational tool for learning Morse code since it has a Morse Code translator. \n\n\n## How we built it\n\n\nSaviour has four main parts - one is the Morse Code translator, indicator, transmitter, and the GPS.\nFirstly we created a menu-driven Graphics Interface for choosing the mode, whether the Morse Code or the GPS. This is done by a potentiometer to scroll, and a button to select.\nThe Morse code translator is built by parsing the String sentence character by character and creating its Morse equivalent. According to the supported Characters, the program translates it to a Morse String. To create a sentence, we reused the same button and the potentiometer. The potentiometer, scrolls through the characters, and the button increments the cursor.\nNext, the Indicator was built using a small Buzzer and a LED which can also be used for signaling, and it is still considered to be an effective way. The buzzer plays the Morse Code with a Farnsworth speed of 10 in sync with the LED flashes. \nAfter that, a simple single transistor FM transmitter was built using a coil, capacitor, and transistor. Its job is to transmit the Morse code over open Air.\nLast but not least, the life saviour, the GPS. As the name suggests, this part contains a GPS, through which the NMEA data is received as packets. Those get parsed and fetch the coordinates and finally get transmitted via the transmitter. \nNow all these interfaces are displayed over an OLED display, and all of them get controlled by the Arduino.\n\n\n## Challenges we ran into\n\n\nMaking the interface was a bit tricky. Stopping abnormal flow or increment of button pushes using flags. Getting ready with the GPS. Since the GPS works best over open air. Printing the translated Morse Code on the OLED was a bit tricky too. \n\n\n## Accomplishments that we're proud of\n\n\nWe were in a bit of a time crunch since a lot had to be done, but we decided to work on one aspect at a time and finish that first. We are really proud to have overcome all the challenges like making the interface, making the FM transmitter. Displaying the morse code and listening to the buzzer to beep in accordance was a very proud moment. Thus bringing a mere project idea into life and that too in very little time is what we are very happy about. Saviour is a hack that we are very proud of building this tool because it can really help people in times of emergency.\n\n\n## What we learned\n\n\nWe learned with help of simple components, such as the potentiometer and button we can create such a good graphical interface. We learned how to drive OLED displays. Learned about Morse Codes, speed. Learned a bit about GPS and the NMEA protocol. \n\n\n## What's next for Saviour\n\n\nWe are thinking of adding a digital compass, and an altimeter by which it can be used as a very powerful and absolute emergency item, and a normal human must-have.\n\n\n## Citations\n\n\n<https://github.com/olikraus/u8g2>\n\n\n<https://github.com/mikalhart/TinyGPSPlus>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/pantomime-elaiub",
            "title": "Pantomime",
            "blurb": "Gestures to actions for disability aid and more",
            "awards": [
                "Best Lifestyle Improvement Hack for Drivers powered by Ford",
                "Health Access and Delivery, National Security or Mis/Dis Information by Mitre"
            ],
            "videos": [
                "https://www.youtube.com/embed/iIlG0HFebes?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Flow Chart of both the Units",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/705/datas/original.jpg"
                },
                {
                    "title": "Schematic of Sensing and Transmitter Circuitry",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/504/datas/original.png"
                },
                {
                    "title": "PCB of the Sensing and Transmitter Circuitry",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/503/datas/original.png"
                },
                {
                    "title": "3d Model of Sensing and Transmitter Circuitry",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/716/datas/original.png"
                },
                {
                    "title": "Schematic of Receiving and Control Unit",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/644/datas/original.png"
                },
                {
                    "title": "PCB of Receiving and Control Unit",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/643/datas/original.png"
                },
                {
                    "title": "3d Model of Receiving and Control Unit",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/670/545/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Deblina Chattopadhyay",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GhoS-XoGENSIfNWQlstzOPHGTRfAAq-AdJ-hCqbLw=s96-c?height=180&width=180"
                },
                {
                    "name": "Avijit Das",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14Gi2CA5iEA_o7qLnU5oOnolBBTTEIWvmKWPlGoEGQQ=s96-c?height=180&width=180"
                }
            ],
            "built_with": [
                "accelerometer",
                "arduino",
                "c",
                "filters",
                "hardware",
                "l293d",
                "rf"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Being in the field of electronic engineering, we wanted to explore and experiment with the different possibilities of working with several sensors and components. The idea of controlling mobile devices using simple hand gestures is very fascinating and useful. We also wanted to make a product that solves several problems, makes tasks easier and all-in-all be a useful tool for people.  </p>\n<h2>What it does</h2>\n<p>Pantomime is a configurable gesture controller unit that can be used for reliable human-machine interaction. For the demo purpose, we have shown a use case of the Pantomime in a chassis of 4WD, which here can be easily altered with a smart wheelchair. The Pantomime responds to the hand gestures to control the 4wd rover. The motions we were able to show using the prototype are forward, backward, left, right.</p>\n<h2>How we built it</h2>\n<p>The prototype of the project was built using 4 300 RPM BO motors, 4 wheels, an L293d motor driver, a 433mHz RF (transmitter and receiver) modules to transmit and receive the gesture commands, and two Arduino Uno (one for the receiver i.e. on the rover, and the other for the transmitter circuitry) for communication and controlling purposes. Along with that, to power the rover, we used 2 3.7v Li-On batteries. The chassis was built using an acrylic board. Some basic Arduino coding was done to communicate between the transmitter and receiver with checksums. For the gesture identifying wearable, we interfaced the transmitter with a three-axis accelerometer to calculate the angle of inclination in each direction. And lastly to show the utility of the receiver we incorporated it in a 4WD rover.</p>\n<h2>Challenges we ran into</h2>\n<p>Setting up the transmitter and receiver modules to send and receive the gesture commands was a bit tricky, there is a lag between the gesture made and the rover to perform the respective action, but later by modifying the code with some filters and adjusting the hardware circuitry as well, we got comparatively better results. We were also having some troubles with the 5v input port of the Arduino connected to the receiver that is on the rover, but we tackled that by moving the connections to the 5v port of the ICSP headers.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Completing the project on time including the PCB design, getting the prototype to work, and overcoming the obstacles faced while building it were some of our greatest achievements. </p>\n<h2>What we learned</h2>\n<p>We learned about RF communication and interfacing the modules with an accelerometer and a 4WD rover. We also learned about checksums, the eradication of errors, and haptic feedback. </p>\n<h2>What's next for Pantomime</h2>\n<p>Next, we would definitely try to make the interface much more smoother and efficient. We would also like to make Pantomime market-ready and user-configurable. We are planning to test the same gesture controller on different mobile devices like drones in the future. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nBeing in the field of electronic engineering, we wanted to explore and experiment with the different possibilities of working with several sensors and components. The idea of controlling mobile devices using simple hand gestures is very fascinating and useful. We also wanted to make a product that solves several problems, makes tasks easier and all-in-all be a useful tool for people. \n\n\n## What it does\n\n\nPantomime is a configurable gesture controller unit that can be used for reliable human-machine interaction. For the demo purpose, we have shown a use case of the Pantomime in a chassis of 4WD, which here can be easily altered with a smart wheelchair. The Pantomime responds to the hand gestures to control the 4wd rover. The motions we were able to show using the prototype are forward, backward, left, right.\n\n\n## How we built it\n\n\nThe prototype of the project was built using 4 300 RPM BO motors, 4 wheels, an L293d motor driver, a 433mHz RF (transmitter and receiver) modules to transmit and receive the gesture commands, and two Arduino Uno (one for the receiver i.e. on the rover, and the other for the transmitter circuitry) for communication and controlling purposes. Along with that, to power the rover, we used 2 3.7v Li-On batteries. The chassis was built using an acrylic board. Some basic Arduino coding was done to communicate between the transmitter and receiver with checksums. For the gesture identifying wearable, we interfaced the transmitter with a three-axis accelerometer to calculate the angle of inclination in each direction. And lastly to show the utility of the receiver we incorporated it in a 4WD rover.\n\n\n## Challenges we ran into\n\n\nSetting up the transmitter and receiver modules to send and receive the gesture commands was a bit tricky, there is a lag between the gesture made and the rover to perform the respective action, but later by modifying the code with some filters and adjusting the hardware circuitry as well, we got comparatively better results. We were also having some troubles with the 5v input port of the Arduino connected to the receiver that is on the rover, but we tackled that by moving the connections to the 5v port of the ICSP headers.\n\n\n## Accomplishments that we're proud of\n\n\nCompleting the project on time including the PCB design, getting the prototype to work, and overcoming the obstacles faced while building it were some of our greatest achievements. \n\n\n## What we learned\n\n\nWe learned about RF communication and interfacing the modules with an accelerometer and a 4WD rover. We also learned about checksums, the eradication of errors, and haptic feedback. \n\n\n## What's next for Pantomime\n\n\nNext, we would definitely try to make the interface much more smoother and efficient. We would also like to make Pantomime market-ready and user-configurable. We are planning to test the same gesture controller on different mobile devices like drones in the future. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/electool",
            "title": "ElecTool",
            "blurb": "An all-in-one handy hardware tool",
            "awards": [
                "Top Market Validation"
            ],
            "videos": [
                "https://www.youtube.com/embed/bsUqbQLBRdA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Height Measurement (Electronic Theodolites ?) ;)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/639/589/datas/original.jpg"
                },
                {
                    "title": "The Laser pointer and the LED for Stroboscope.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/639/614/datas/original.jpg"
                },
                {
                    "title": "Accelerometer and the Transistors for Live Wire detection",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/639/678/datas/original.jpg"
                },
                {
                    "title": "Metal Plate at the back for the Live wire detection.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/639/566/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Deblina Chattopadhyay",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GhoS-XoGENSIfNWQlstzOPHGTRfAAq-AdJ-hCqbLw=s96-c?height=180&width=180"
                },
                {
                    "name": "Avijit Das",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14Gi2CA5iEA_o7qLnU5oOnolBBTTEIWvmKWPlGoEGQQ=s96-c?height=180&width=180"
                }
            ],
            "built_with": [
                "arduino",
                "c++",
                "laser",
                "led",
                "oled"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Being engineering students, we realized how in industries and several other aspects of work, a person and especially an engineer needs to use different tools for the sole purpose of measuring. Hence we thought of building a hack that is an all-in-one solution for measuring requirements, which can also come in handy to beginner electricians. Many times we hear during the drilling of holes in walls, humans get shocked by rupturing live wires. So we thought of incorporating a live wire detector that can sense live wires. Along with that, we took inspiration from theodolites that were used to measure very tall structures, to build the height measuring device.</p>\n<h2>What it does</h2>\n<p>It is an electronic tool that can measure if a surface is leveled or not, measure pretty high heights using a laser pointer, the concept of theodolites, RPM of rotating objects using the stroboscopic effect. And a live wire detector can be useful for measuring live wires behind walls of concealing wiring.  The purpose of Laser theodolite was to give the electricians a rough Idea of how much wire to buy for tall buildings.  This can also be used by painter to estimate area of faces of buildings.</p>\n<h2>How we built it</h2>\n<p>ElecTool contains an accelerometer to measure the inclination angles. The level meter was pretty straight forward it senses the inclination on the X-axis. Upon tilting the board, the acceleration due to the gravity vector changes at an angle phi. Along with that using some trigonometry, we find the exact angle. \nThe next was the stroboscope, where it uses a potentiometer to change the flickering rate i.e how fast the LED oscillates and upon matching that with a moving or rotating object, we can see the object is still and steady, by calculating the period of oscillation and inversing it, we calculate the frequency of the object.  Here my camera's shutter speed is being approximated.\nNext is the Live wire detector, where it can detect the presence of live wires. Using transistors as Darlington pairs, it was picking up very small signals. Upon bringing that near an AC supply, there's a metallic plate behind the breadboard which acts as a surface capacitor and triggers the Arduino. \nFinally, the Height measurement tool is made by a laser pointer and the inclination readings. Using basic geometry and the tan(phi)=height/base, we were able to measure the height. The phi is the inclination angle measured by the accelerometer and the base is kept unity. Thus the height equals the tan(phi). \nThese were all controlled by the Arduino, and along with that, an OLED display is used for displaying the menu and a button and potentiometer for selecting. </p>\n<h2>Challenges we ran into</h2>\n<p>Completing the hardware prototype and making the code work for the different parts was the biggest challenge of them all. Mainly getting rid of noises that were coming from the accelerometer data. Which was resulting in very jittery measurements of the angles thus bad readings. The live wire detector was false triggering sometimes. Making the demo video finally was very constrained, as it contains a demo of 4 tools. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Being able to complete the project on time was the biggest achievement of them all. Watching the height measurements was pretty amazing, as it was a very crude device giving some accurate results. Removing the false triggering of the live wire detector.</p>\n<h2>What we learned</h2>\n<p>Learned about Theodolites and how previously people used to measure tall sculptures. </p>\n<h2>What's next for ElecTool</h2>\n<p>Making a perfect enclosure and a PCB for making a perfect handy tool for Engineers and as well as beginner Electricians.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nBeing engineering students, we realized how in industries and several other aspects of work, a person and especially an engineer needs to use different tools for the sole purpose of measuring. Hence we thought of building a hack that is an all-in-one solution for measuring requirements, which can also come in handy to beginner electricians. Many times we hear during the drilling of holes in walls, humans get shocked by rupturing live wires. So we thought of incorporating a live wire detector that can sense live wires. Along with that, we took inspiration from theodolites that were used to measure very tall structures, to build the height measuring device.\n\n\n## What it does\n\n\nIt is an electronic tool that can measure if a surface is leveled or not, measure pretty high heights using a laser pointer, the concept of theodolites, RPM of rotating objects using the stroboscopic effect. And a live wire detector can be useful for measuring live wires behind walls of concealing wiring. The purpose of Laser theodolite was to give the electricians a rough Idea of how much wire to buy for tall buildings. This can also be used by painter to estimate area of faces of buildings.\n\n\n## How we built it\n\n\nElecTool contains an accelerometer to measure the inclination angles. The level meter was pretty straight forward it senses the inclination on the X-axis. Upon tilting the board, the acceleration due to the gravity vector changes at an angle phi. Along with that using some trigonometry, we find the exact angle. \nThe next was the stroboscope, where it uses a potentiometer to change the flickering rate i.e how fast the LED oscillates and upon matching that with a moving or rotating object, we can see the object is still and steady, by calculating the period of oscillation and inversing it, we calculate the frequency of the object. Here my camera's shutter speed is being approximated.\nNext is the Live wire detector, where it can detect the presence of live wires. Using transistors as Darlington pairs, it was picking up very small signals. Upon bringing that near an AC supply, there's a metallic plate behind the breadboard which acts as a surface capacitor and triggers the Arduino. \nFinally, the Height measurement tool is made by a laser pointer and the inclination readings. Using basic geometry and the tan(phi)=height/base, we were able to measure the height. The phi is the inclination angle measured by the accelerometer and the base is kept unity. Thus the height equals the tan(phi). \nThese were all controlled by the Arduino, and along with that, an OLED display is used for displaying the menu and a button and potentiometer for selecting. \n\n\n## Challenges we ran into\n\n\nCompleting the hardware prototype and making the code work for the different parts was the biggest challenge of them all. Mainly getting rid of noises that were coming from the accelerometer data. Which was resulting in very jittery measurements of the angles thus bad readings. The live wire detector was false triggering sometimes. Making the demo video finally was very constrained, as it contains a demo of 4 tools. \n\n\n## Accomplishments that we're proud of\n\n\nBeing able to complete the project on time was the biggest achievement of them all. Watching the height measurements was pretty amazing, as it was a very crude device giving some accurate results. Removing the false triggering of the live wire detector.\n\n\n## What we learned\n\n\nLearned about Theodolites and how previously people used to measure tall sculptures. \n\n\n## What's next for ElecTool\n\n\nMaking a perfect enclosure and a PCB for making a perfect handy tool for Engineers and as well as beginner Electricians.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ikkuma",
            "title": "Ikkuma",
            "blurb": "Ikkuma is a story about a land being swallowed by the sea, where conflict cracks ice and fire tears families apart. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/lS_mSTLQJFo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Character",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/707/961/datas/original.png"
                },
                {
                    "title": "Polar Bear",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/707/962/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Yangli Liu",
                    "about": "I created all the models and the framework of this project.",
                    "photo": "https://www.gravatar.com/avatar/10a10f8eaa149decf69e9dab979880a7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "english",
                "github",
                "oculus",
                "tiltbrush",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The motivation of this work is making an immersive experience to help my audience to better understand the topic of climate change and its real impact on earth. While initially portrayed as a story of brutality and revenge, the narrative slowly pans out to show the futility of these personal struggles against the very real forces of nature and manmade climate change. Ikkuma is the Inuvialuit word for fire, a central element to this work and a stark visual contrast against the whites, greys, and blues of the frozen wastes. </p>\n<h2>What it does</h2>\n<p>A VR headset puts you in the mind of an orphan, witness to your mother\u2019s brutal execution and alone in a world of crumbling ice. The players must learn to tame the fire (ikkuma) in their hearts and the hunger in their belly if they hope to survive the harsh yet fragile Arctic tundra. </p>\n<h2>How we built it</h2>\n<p>We built it with Unity and Tilt Brush, Tilt Brush, Medium, Adobe series, etc.</p>\n<h2>Challenges we ran into</h2>\n<p>This is a project I started a while ago, unfortunately, due to the lack of support, I find it hard to continue working on the project on my own. I want to find someone who is interested in the topic of traditional culture/environment change. Now, the project only has some simple interactions and teleportation. I plan to find more collaborators and researchers from different backgrounds to work on some innovative interactive mechanisms. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>I feel that the visual contrast between fire and ice presents a striking representation of the changing climate, and by using the medium of VR we can create fantastic scenes that compose this contrast in unique ways.</p>\n<h2>What we learned</h2>\n<p>Even though this project is a fiction-based story, written by me, I will say it is not 100% baseless as I have learned a lot of interesting facts about Inuvialuit culture. The history of the Inuvialuit and their ancestors in the Beaufort region and Mackenzie Delta is long and complex. It extends far back in time to the arrival of the Thule Inuit, and perhaps even to their predecessors, the Dorset people. Inuvialuit have deep roots in the territory and a resulting vast, accumulated knowledge of its geography, fauna, weather, and ice conditions. This knowledge has made it possible for Inuvialuit to find food, create clothing, and enjoy a vibrant intellectual and emotional life for generations.</p>\n<h2>What's next for Ikkuma</h2>\n<p>The end goal for Ikkuma is to create a narrative experience that lets the players take on the role of our central \u201corphan\u201d character, abandoned in the arctic tundra. I\u2019d like this experience to be visceral but also educational, and I intend to do that by giving players a first person perspective of the impact of global warming on the North pole. The players will learn Inuit traditional knowledge (Qaujimajatuqangnit) by watching, listening to, and harvesting on the land, ice and water</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe motivation of this work is making an immersive experience to help my audience to better understand the topic of climate change and its real impact on earth. While initially portrayed as a story of brutality and revenge, the narrative slowly pans out to show the futility of these personal struggles against the very real forces of nature and manmade climate change. Ikkuma is the Inuvialuit word for fire, a central element to this work and a stark visual contrast against the whites, greys, and blues of the frozen wastes. \n\n\n## What it does\n\n\nA VR headset puts you in the mind of an orphan, witness to your mother\u2019s brutal execution and alone in a world of crumbling ice. The players must learn to tame the fire (ikkuma) in their hearts and the hunger in their belly if they hope to survive the harsh yet fragile Arctic tundra. \n\n\n## How we built it\n\n\nWe built it with Unity and Tilt Brush, Tilt Brush, Medium, Adobe series, etc.\n\n\n## Challenges we ran into\n\n\nThis is a project I started a while ago, unfortunately, due to the lack of support, I find it hard to continue working on the project on my own. I want to find someone who is interested in the topic of traditional culture/environment change. Now, the project only has some simple interactions and teleportation. I plan to find more collaborators and researchers from different backgrounds to work on some innovative interactive mechanisms. \n\n\n## Accomplishments that we're proud of\n\n\nI feel that the visual contrast between fire and ice presents a striking representation of the changing climate, and by using the medium of VR we can create fantastic scenes that compose this contrast in unique ways.\n\n\n## What we learned\n\n\nEven though this project is a fiction-based story, written by me, I will say it is not 100% baseless as I have learned a lot of interesting facts about Inuvialuit culture. The history of the Inuvialuit and their ancestors in the Beaufort region and Mackenzie Delta is long and complex. It extends far back in time to the arrival of the Thule Inuit, and perhaps even to their predecessors, the Dorset people. Inuvialuit have deep roots in the territory and a resulting vast, accumulated knowledge of its geography, fauna, weather, and ice conditions. This knowledge has made it possible for Inuvialuit to find food, create clothing, and enjoy a vibrant intellectual and emotional life for generations.\n\n\n## What's next for Ikkuma\n\n\nThe end goal for Ikkuma is to create a narrative experience that lets the players take on the role of our central \u201corphan\u201d character, abandoned in the arctic tundra. I\u2019d like this experience to be visceral but also educational, and I intend to do that by giving players a first person perspective of the impact of global warming on the North pole. The players will learn Inuit traditional knowledge (Qaujimajatuqangnit) by watching, listening to, and harvesting on the land, ice and water\n\n\n"
        }
    ],
    "realityvrhack": [
        {
            "source": "https://devpost.com/software/team-facilitator-test",
            "title": "Team Facilitator Test",
            "blurb": "Team Facilitator Test",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Scott W. Greenwald",
                    "about": "I really built absolutely everything. But I don't mind other people taking some credit, why do I need all the credit?",
                    "photo": "https://www.gravatar.com/avatar/051f3c1db1fcdcc359e7d9495e621804?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Luna Yuan",
                    "about": ":)",
                    "photo": "https://www.gravatar.com/avatar/8001043b1298ca7a66ff577b119507a5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "pauric",
                    "about": "I took the requirements from the customers and gave them to the developers, I'm a people person god-dammit",
                    "photo": "https://www.gravatar.com/avatar/c43ef26b500cbb00fa935d8d5a95d76f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ana Hurka-Robles",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0501d35cac8fdc611e4134fd2628dd09?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "ana-test ana-test",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/476ea40bc28a30112b13e0540c3f215e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Steven Max Patterson",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/464d2163bf0c74ffaa7d4eba24612e1f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Arpit  Gupta",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/772/062/datas/profile.png"
                },
                {
                    "name": "Praveen Aravamudham",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c28ea15260e57772a58db58711102c8f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "iphone-sdk"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Placeholder</p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for Team Facilitator Test</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nPlaceholder\n\n\n## What it does\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for Team Facilitator Test\n\n\n"
        },
        {
            "source": "https://devpost.com/software/sound-spheres",
            "title": "SoundSpheres",
            "blurb": "An audio memory visual puzzle where you combine different spheres to create music",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Screenshot of our Unity project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/142/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Thomas Crane",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ed103184c79fe6aa48af3efaf3479044?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Andy Tsen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2655f805abe120d26884e3a41b2c105e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "brian xavier",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ead91e6a4c6d3b4f84ae16a919f3ad4b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Charity Everett",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d0fcd55f8df1de83c1eebe8bbf7650d2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We love puzzle games and interacting with music and so made a music puzzle game that takes advantage of virtual reality's power of immersion!</p>\n<h2>What it does</h2>\n<p>A musical puzzle game where you have to place the right Sound Spheres on the right beat pillars to match a song.</p>\n<h2>How we built it</h2>\n<p>Utilizing Unity 3D, music creation software, and C# script.</p>\n<h2>Challenges we ran into</h2>\n<p>Our main challenges included how to get physics materials working, setting up the music to effect the environmental displays, and how to incorporate UI into the experience.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are exceptionally proud of the overall aesthetic, the gameplay mechanics, and the link between interactivity and sound. </p>\n<h2>What we learned</h2>\n<p>SO MUCH UNITY! This was a very good experience on </p>\n<h2>What's next for Sound Spheres</h2>\n<p>Maybe we'll get together and finish the experience; build it out into something really spectacular.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe love puzzle games and interacting with music and so made a music puzzle game that takes advantage of virtual reality's power of immersion!\n\n\n## What it does\n\n\nA musical puzzle game where you have to place the right Sound Spheres on the right beat pillars to match a song.\n\n\n## How we built it\n\n\nUtilizing Unity 3D, music creation software, and C# script.\n\n\n## Challenges we ran into\n\n\nOur main challenges included how to get physics materials working, setting up the music to effect the environmental displays, and how to incorporate UI into the experience.\n\n\n## Accomplishments that we're proud of\n\n\nWe are exceptionally proud of the overall aesthetic, the gameplay mechanics, and the link between interactivity and sound. \n\n\n## What we learned\n\n\nSO MUCH UNITY! This was a very good experience on \n\n\n## What's next for Sound Spheres\n\n\nMaybe we'll get together and finish the experience; build it out into something really spectacular.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ideals-67c1kl",
            "title": "Ideals - Focus",
            "blurb": "This project uses biometric feedback and contextual AR/MR to encourage positive behavior change.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Aaron Faucher",
                    "about": "Unity/C# development",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/522/064/datas/profile.jpg"
                },
                {
                    "name": "Liz Cormack",
                    "about": "UX/UI Design!",
                    "photo": "https://www.gravatar.com/avatar/420d954df0894edbe1fa9e9b464b5f15?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Leon Zhang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/689/datas/profile.JPG"
                },
                {
                    "name": "Akshay Mohan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/92dcb5f41f948d34756cbbfdbbb1e1e1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<p>We can all relate to the experience of sitting down to make progress on a task - whether reading a book, writing a proposal, or clearing through email - only to find ourselves wandering away from the work at hand. 'Focus' is a mixed reality application built for HoloLens that promotes positive attention and focus. The user begins by dynamically wrapping their work in a 'zone of focus.' If the user becomes distracted from their task, their attention is gently guided back to their prioritized activity.</p>\n<p><a href=\"http://afaucher.tumblr.com/post/152391823546/\" rel=\"nofollow\">View the video walkthrough here</a>.</p>\n</div>",
            "content_md": "\nWe can all relate to the experience of sitting down to make progress on a task - whether reading a book, writing a proposal, or clearing through email - only to find ourselves wandering away from the work at hand. 'Focus' is a mixed reality application built for HoloLens that promotes positive attention and focus. The user begins by dynamically wrapping their work in a 'zone of focus.' If the user becomes distracted from their task, their attention is gently guided back to their prioritized activity.\n\n\n[View the video walkthrough here](http://afaucher.tumblr.com/post/152391823546/).\n\n\n"
        },
        {
            "source": "https://devpost.com/software/educatvr",
            "title": "MOLECULVR",
            "blurb": "An educational interactive experience that allows you to pull atoms from a 3D periodic table to create molecules",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Taylor Gates",
                    "about": "Full-stack Unity developer, responsible for implementation of core features, overseeing unit testing and designing the interface mechanics.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/372/339/datas/profile.jpg"
                },
                {
                    "name": "Mariangely Iglesias Pena",
                    "about": "I worked on the design ideas, interactions and assets behind the project. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/380/datas/profile.jpg"
                },
                {
                    "name": "Jacki Hom",
                    "about": "Back-end developer working in C# and Unity, responsible for script creation of critical features.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/381/datas/profile.jpg"
                },
                {
                    "name": "Ryan Lee",
                    "about": "helped with the UX and design of the 3D room; learned Unity and C# to help with outlining script logic and developing assets in game space; helped run project demo with adults and kids at expo",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/421/044/datas/profile.jpg"
                },
                {
                    "name": "Michelle Wantuch",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/379/datas/profile.jpg"
                }
            ],
            "built_with": [
                "htc-vive",
                "unity"
            ],
            "content_html": "<div>\n<p>Team Lead: Taylor Gates\n<br/>Location: 4th by the elevator\n<br/>Category: Human Well-Being (education)\n<br/>\n<br/>GitHub Link: <a href=\"https://github.com/RyanChenLee/moleculvr\" rel=\"nofollow\">https://github.com/RyanChenLee/moleculvr</a>\n<br/>\n<br/></p>\n<h2>Inspiration</h2>\n<p>Not all students learn in the same way. Traditional teaching models assume that students will just listen to lectures or read books and that this is enough. This is not enough. Students needs are not being met. Often, students need a more immersive experience, one that allows for deeper thinking. The EDUCATVR team aims to tackle this issue by providing an engaging, immersive educational experience, MOLECULVR. We're allowing students to interact with the world in a way they are not able to in reality, making atoms and molecules tangible. Furthermore, students are not only able to see and interact with molecules, but they are also able to have the molecules interact with one another in ways either too unsafe or impossible in traditional classrooms and real life.</p>\n<h2>What it does</h2>\n<p>The game puts the player in a virtual lab with a 3D periodic table of elements. The player can reach into the periodic table, grab an atom, and put it into a \"combination chamber\" at their workbench. Students add molecules to the chamber one at a time, allowing them to explore possible combinations of elements. Once a student makes a combination of elements that is real-world molecule, the student is given visual feedback. The learning experience is discovery-based, allowing the student to try as many combinations as they desire. This is meant for a museum space setting.</p>\n<h2>How we built it</h2>\n<p>We started out by identifying the core problem we were trying to solve (see above) and the necessary features to solve those problems. Then, we prioritized those features to ensure a minimum viable product. MOLECULVR was created using the Unity3D game engine for the HTC Vive. The game uses a number of third party plugins and assets (from Yobi, Turbosquid, and the Unity asset store). The team workflow was divided into level design and feature implementation, with all members collaborating in real-time through the strategic use of the Unity Collaboration tool. We each worked in different Unity scenes to ensure that no conflicts  were made in our development workflow and putting everything together in the end. </p>\n<h2>Challenges we ran into</h2>\n<p>The HTC Vive had some hardware issues; at first, the PC did not detect the Vive hardware. Since the hardware is so new, much time was dedicated to fixing these issues which interfered with our ability to debug. Many of us are also new to game design (especially for VR) and ran into challenges surrounding the creation of a holistic immersive experience. Our developers were also very new to Unity and the C# language, but all were able to use their general coding backgrounds to solve problems effectively and collaboratively. Additionally, we have so many ideas that we constantly had to pre-evaluate what features were possible to make in such limited time.\n<br/>\n<br/>UPDATE: The biggest challenge of all...the entire project broke one hour before it was due. We had to search for the last working version and start again from there. Unfortunately, some UI elements were lost (e.g., color coded-periodic table with locked elements.)</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of our ability to learn useful tools in such a limited amount of time, communicate, collaborate, innovative, and iterate as a team, with the product and users always staying at the heart of our decisions. As a team, we felt that we accomplished our main goal of having various molecules and interactions available for the user to play with. </p>\n<h2>What we learned</h2>\n<p>We effectively used available resources to learn new skills on the fly. Through this, we learned Unity development and UX best practices for VR. We also learned about the constraints of design (i.e. letting the coders code instead of constantly feeding them ideas), teamwork, and team communication. One important aspect that we adopted throughout the hackathon was the ability to do \"check-ins\" so that we could all be aware of each other's progress and to-do list. </p>\n<h2>What's next for MOLECULVR</h2>\n<p>We have many plans for the future of MOLECULVR. Here are some features to add:\n<br/>\n<br/>- Color-code the periodic table of elements\n<br/>\n<br/>- Lock icon will appear in the bottom right corner of element blocks, restricting you from pulling out atoms (unlock the \"level\")\n<br/>\n<br/>- A curved periodic table (for a better user experience); animation of molecules once they are created (e.g., water molecule floating in water in the chamber once water molecule is made).\n<br/>\n<br/>- A working log of what you've put in the combination chamber, highlighting any successful combinations (on the clipboard that you can put on the workbench and carry around with you).\n<br/>\n<br/>- A larger list of molecules for combination (since many are \"locked\" right now on the periodic table).\n<br/>\n<br/>- Instruments and objects for interacting with the molecules in an creative way (test out different reactions). The bunsen burner should react with different molecules (e.g., water makes the flame go out and methane makes an explosion.)\n<br/>\n<br/>- A clear button on the workbench to clear contents in the combination chamber.\n<br/>\n<br/>- Different modes (free play vs. quest mode, which would give scaffolding for how to make certain molecules).\n<br/>\n<br/>- A dynamic poster with the molecules you've made, hanging on a side wall (like a pokedex of molecules) and/or glass jars to keep molecules in. \n<br/>\n<br/>- The capacity to pull molecules from objects in the environment and deconstruct them on the workbench. \n<br/>\n<br/>- Updated models of the lab scene (possibly including teleportation locomotion to allow for a larger lab scene). \n<br/>\n<br/>- An interactive intro tutorial (The experience is intended for a museum setting. This means that many students have not interacted with VR experiences before and have little time to be immersed in the experience. So, as students enter the lab, they would be provided with a short interactive tutorial, designed to be quick and seamlessly integrate into the virtual environment.)\n<br/>\n<br/>- Gloves would replace the controllers inside the environment\n<br/>\n<br/>- Make the \"You Have\" poster above the workbench functional (so that it tells you what molecules are in the chamber)</p>\n</div>",
            "content_md": "\nTeam Lead: Taylor Gates\n  \nLocation: 4th by the elevator\n  \nCategory: Human Well-Being (education)\n  \n\n  \nGitHub Link: <https://github.com/RyanChenLee/moleculvr>\n  \n\n  \n\n\n\n## Inspiration\n\n\nNot all students learn in the same way. Traditional teaching models assume that students will just listen to lectures or read books and that this is enough. This is not enough. Students needs are not being met. Often, students need a more immersive experience, one that allows for deeper thinking. The EDUCATVR team aims to tackle this issue by providing an engaging, immersive educational experience, MOLECULVR. We're allowing students to interact with the world in a way they are not able to in reality, making atoms and molecules tangible. Furthermore, students are not only able to see and interact with molecules, but they are also able to have the molecules interact with one another in ways either too unsafe or impossible in traditional classrooms and real life.\n\n\n## What it does\n\n\nThe game puts the player in a virtual lab with a 3D periodic table of elements. The player can reach into the periodic table, grab an atom, and put it into a \"combination chamber\" at their workbench. Students add molecules to the chamber one at a time, allowing them to explore possible combinations of elements. Once a student makes a combination of elements that is real-world molecule, the student is given visual feedback. The learning experience is discovery-based, allowing the student to try as many combinations as they desire. This is meant for a museum space setting.\n\n\n## How we built it\n\n\nWe started out by identifying the core problem we were trying to solve (see above) and the necessary features to solve those problems. Then, we prioritized those features to ensure a minimum viable product. MOLECULVR was created using the Unity3D game engine for the HTC Vive. The game uses a number of third party plugins and assets (from Yobi, Turbosquid, and the Unity asset store). The team workflow was divided into level design and feature implementation, with all members collaborating in real-time through the strategic use of the Unity Collaboration tool. We each worked in different Unity scenes to ensure that no conflicts were made in our development workflow and putting everything together in the end. \n\n\n## Challenges we ran into\n\n\nThe HTC Vive had some hardware issues; at first, the PC did not detect the Vive hardware. Since the hardware is so new, much time was dedicated to fixing these issues which interfered with our ability to debug. Many of us are also new to game design (especially for VR) and ran into challenges surrounding the creation of a holistic immersive experience. Our developers were also very new to Unity and the C# language, but all were able to use their general coding backgrounds to solve problems effectively and collaboratively. Additionally, we have so many ideas that we constantly had to pre-evaluate what features were possible to make in such limited time.\n  \n\n  \nUPDATE: The biggest challenge of all...the entire project broke one hour before it was due. We had to search for the last working version and start again from there. Unfortunately, some UI elements were lost (e.g., color coded-periodic table with locked elements.)\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of our ability to learn useful tools in such a limited amount of time, communicate, collaborate, innovative, and iterate as a team, with the product and users always staying at the heart of our decisions. As a team, we felt that we accomplished our main goal of having various molecules and interactions available for the user to play with. \n\n\n## What we learned\n\n\nWe effectively used available resources to learn new skills on the fly. Through this, we learned Unity development and UX best practices for VR. We also learned about the constraints of design (i.e. letting the coders code instead of constantly feeding them ideas), teamwork, and team communication. One important aspect that we adopted throughout the hackathon was the ability to do \"check-ins\" so that we could all be aware of each other's progress and to-do list. \n\n\n## What's next for MOLECULVR\n\n\nWe have many plans for the future of MOLECULVR. Here are some features to add:\n  \n\n  \n- Color-code the periodic table of elements\n  \n\n  \n- Lock icon will appear in the bottom right corner of element blocks, restricting you from pulling out atoms (unlock the \"level\")\n  \n\n  \n- A curved periodic table (for a better user experience); animation of molecules once they are created (e.g., water molecule floating in water in the chamber once water molecule is made).\n  \n\n  \n- A working log of what you've put in the combination chamber, highlighting any successful combinations (on the clipboard that you can put on the workbench and carry around with you).\n  \n\n  \n- A larger list of molecules for combination (since many are \"locked\" right now on the periodic table).\n  \n\n  \n- Instruments and objects for interacting with the molecules in an creative way (test out different reactions). The bunsen burner should react with different molecules (e.g., water makes the flame go out and methane makes an explosion.)\n  \n\n  \n- A clear button on the workbench to clear contents in the combination chamber.\n  \n\n  \n- Different modes (free play vs. quest mode, which would give scaffolding for how to make certain molecules).\n  \n\n  \n- A dynamic poster with the molecules you've made, hanging on a side wall (like a pokedex of molecules) and/or glass jars to keep molecules in. \n  \n\n  \n- The capacity to pull molecules from objects in the environment and deconstruct them on the workbench. \n  \n\n  \n- Updated models of the lab scene (possibly including teleportation locomotion to allow for a larger lab scene). \n  \n\n  \n- An interactive intro tutorial (The experience is intended for a museum setting. This means that many students have not interacted with VR experiences before and have little time to be immersed in the experience. So, as students enter the lab, they would be provided with a short interactive tutorial, designed to be quick and seamlessly integrate into the virtual environment.)\n  \n\n  \n- Gloves would replace the controllers inside the environment\n  \n\n  \n- Make the \"You Have\" poster above the workbench functional (so that it tells you what molecules are in the chamber)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/real-talk",
            "title": "Real Talk",
            "blurb": "When you speak a language in real-world situations you learn better. Real Talk helps you do just that. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/zPp9tXHjss4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ovetta Sampson",
                    "about": "I was the UX lead on this 36-hour VR/AR Hackathon project. Think you can't do user research in that short of time...think again. Ask me how. :) ",
                    "photo": "https://www.gravatar.com/avatar/153535964e8e5f7853f308f8c14ae98c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Zachery Schiller",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c461c3c0a82fdb8ed5d3a0d8a713f42e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Krystian Babilinski",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/267/721/datas/profile.jpg"
                },
                {
                    "name": "Jeffrey King",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/51c66f3ff5dfff45da0f08cb90b8b23f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Mark Kabban",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/23cf1a8caba9acaf95b628ef0158836d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<p>Real Talk uses virtual reality to create an immersive experience for children ages 9 to 12 who are learning to speak another language. Players learn English, in a contextual, real world environment as they interact with characters in the game. They compose and speak their responses, offering youth a fun and interactive way to learn.</p>\n</div>",
            "content_md": "\nReal Talk uses virtual reality to create an immersive experience for children ages 9 to 12 who are learning to speak another language. Players learn English, in a contextual, real world environment as they interact with characters in the game. They compose and speak their responses, offering youth a fun and interactive way to learn.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/directed-story-telling",
            "title": "\"Blue Note\" A 3D Audio Journey",
            "blurb": "Using multiple audio capture techniques to drive the user to follow the story forward, 3D audio to match the 3D video",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Video Sphere setup with spot audio in space",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/817/datas/original.png"
                },
                {
                    "title": "A framework was created to allow new video scenes to added by dropping them into two lists Light and Dark",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/819/datas/original.png"
                },
                {
                    "title": "The audio sprite moves throughout the space to help the user learn how transitions work for the story to progress",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/821/datas/original.png"
                },
                {
                    "title": "Audio and Video export from Adobe Premiere Pro, Video then needs conversion to .ogg",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/825/datas/original.png"
                },
                {
                    "title": "Binaural Audio Recorder",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/836/datas/original.jpg"
                },
                {
                    "title": "GoPro Omni",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/867/datas/original.JPG"
                },
                {
                    "title": "Henry Birddock",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/870/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Frank O&#39;Connor",
                    "about": "I was responsible for the software engineering side of the project. I worked with Unity/C# to assemble and display all the 360 videos, spatial audio, interactive events, and transition effects.",
                    "photo": "https://www.gravatar.com/avatar/c291f34a6b3addbb9e16fb1d0a52ef8c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Keith Bradley",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/101/datas/profile.jpg"
                },
                {
                    "name": "ryan silbert",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e2bafc7c28ac3d87dab706cadca9342f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Mike Sullivan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/eaea5add88d08fb98b8264a88c83ae27?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "4-channel-surround",
                "adobe-after-effects",
                "adobe-premiere-pro",
                "binaural",
                "c#",
                "ffmpeg",
                "garageband",
                "gopro",
                "gopro-omni",
                "hokusa2",
                "kolor",
                "samsung-gear-vr",
                "unity",
                "zoom-h2n"
            ],
            "content_html": "<div>\n<h2><strong>TEAM 10</strong></h2>\n<p>TEAM LEADER: Keith Bradley CELL: 7078490485</p>\n<h2>Entertainment and Story Telling</h2>\n<h2>Inspiration</h2>\n<p>Inspired by the sonic ambience around us. Binaural puts you in the exact sound field as originally intended. You can hear a bird taking off. You can hear the band exactly as they were positioned when playing. </p>\n<p><strong>The Verge writes:</strong>\n\"For decades, binaural recording was a novelty, and overlooked for less technically demanding methods. But with the rise of virtual reality hardware like the Oculus Rift, HTC Vive, PlayStation VR, and Samsung\u2019s Gear \u2014 systems dependent on realistic 3D audio to fully immerse their users \u2014 binaural audio is on the cusp of a renaissance. \nBinaural recording systems are unique because they emulate the workings of the human head. The architecture of our anatomy dictates how we understand the sounds we hear: with an ear on either side of a thick skull and spongy brain, we hear sounds enter our left and right ears at different times. If a dog barks by our left ear, it takes a few extra microseconds for the bark to reach the right ear; the sound will also be louder in one ear than the other. In addition, sound waves interact with the physical constitution of the listener \u2014 the pinna (or outer ear), the head, and the torso \u2014 and the surrounding space, creating listener-specific variations otherwise known as head-related transfer function. The brain scrutinizes these miniscule interaural differences of time and strength in order to localize sound with immaculate precision.\" via <a href=\"http://www.theverge.com/2015/2/12/8021733/3d-audio-3dio-binaural-immersive-vr-sound-times-square-new-york\" rel=\"nofollow\">link</a></p>\n<h2>What it does - The Story</h2>\n<p>An immersive Sci-Fi narrative experience using immersive sounds that we created to demonstrate the power of creating 360 video experiences with realistic visuals and sound.</p>\n<p>In that context we developed, Blue Note.  The Blue Note is an immersive narrative using only natural sounds that takes our viewer through the experience of what living in parallel dimensions could feel like here in Boston.  It\u2019s a proof-of-concept where Stranger Things meets Under The Skin where on your 18th birthday you are given the power of inter-dimensional sight.  Within the first few minutes of adulthood you either master your powers which manifest as a ring of fire, a tear you can create in the inter-dimensions, allowing you to see through space and time.  Or you risk being stuck in the alternate dimension for eternity.  </p>\n<p>In our world the alternate dimensions is a dark reflection of the same environment.  The audience will\nleave The Blue Note with a new understanding of the effect that sound has on our minds and consider the\npotential powers we can unlock within our ourself with a new way to view the world both visually and\nsonically.</p>\n<h2>How we built it - The Sound and Look of the Project</h2>\n<p>As a multi-disciplinary team consisting of a architectural designer, a software engineer, a 360 video entrepreneur, and filmmaker; we all came to this project to explore the full sensory immersion in virtual reality within a contained narrative. </p>\n<p>The project was shot on location in one day on the Charles River featuring Boston street musicians.  </p>\n<p>We shot the \u201cblue note dimension\u201d with the Samsung Gear VR at 4k to give it a rougher, more ethereal look before adding effects in After Effects and Premiere.  To capture the vividness of \u201cour dimension\u201d we captured it using the GoPro Omni Rig and gave it a saturated high contrast look.  All of the sound was captured using The Zoom H2N, custom-made binaural microphone.  For \u201cblue note world\u201d we mixed-down the binaural ambisonics to stereo using Hokusai 2 and finishing the sound design in Garageband.  All of the sounds were captured from the real world and designed specifically for each \u201cblue note\u201d scene.  </p>\n<p>The sound of the \u201creal world\u201d is a real-time binaural capture that has been spatially mapped in post production based on the sphere surface based on the horizontal positioning system of the Oculus.  </p>\n<p>The video editing was done in Adobe Premiere and AutoPano, the GoPro capture was done with Omni Importer.  The final product was built in Unity using eight spheres with the video as a movie texture of the sphere as well as extensive sound design.</p>\n<h2>Challenges we ran into</h2>\n<p>Because the audio needed to be spatial, each channel needed to be exported separately and mapped in location in each scene. This extended workflow adds a significant amount of time to exports and setup in Unity.</p>\n<p>The capture of the Omni rig wasn\u2019t as seamless as we had hoped and the importing took a lot of time and computing power. The Samsung Gear VR is more portable, but also captures at lower quality. Unity's limitations on video files as textures caused for some of the footage to be downgraded in this short time period. The equipment is capable of 4K or better, but videos needed to be downsized for this experiment.</p>\n<p>We had hoped that we would have more time to explore the story and really develop the idea that you are porting yourself between two dimensions.  </p>\n<p>Beyond time and technology as challenges, we also found it difficult to seamlessly blend the jazz instruments as an auditory cue to have the viewer turn around in the \u201cblue note\u201d universe. Ideally we would have started with the the darkness, silence and then have the sound cue drive the audience to turn their heads to experience \u201cour world\u201d.  To address that challenge we swapped the experience to start with the jazz musicians and then have the other \u201cblue note world\u201d be the contrast. We leveraged the idea of a Sci-Fi universe to overcome some of our technical disadvantages.</p>\n<p>We had hoped that we would have more time to explore the surrounding area.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of the the technique, result, and strong combination of a multidisciplinary team. We intended to explore a story that allows the audience to embody the same location but is moved to change their location by audio cues.  We think there is tremendous opportunity to explore cutting in virtual reality and using audio more specifically and pointedly.  If we start to focus on the audio, we believe the audience will be more forgiving of the video and this unlocks a tremendous opportunity for, immersive storytellers.  </p>\n<h2>What we learned</h2>\n<p>We learned a tremendous amount about the workflow of 360 video capture and trying to create an interactive element inside Unity. With the short timeline of the Hackathon, prerecorded assets would have been a huge benefit to the workflow.</p>\n<h2>What's next for \"Blue Note\"</h2>\n<p>We want to do stereo 360 video and further develop the narrative using sound as a primary conceptual driver. We also hope to be able to use higher resolution video files, an 8K capture rig would significantly improve immersion.</p>\n<p></p><blockquote> <p> <a href=\"https://www.instagram.com/p/BLWcixQjsN4/\" rel=\"nofollow\">\"Blue Note\" A 3D Audio Journey #realityvirtuallyhack</a></p> <p>A video posted by tkbrdly (@tkbrdly) on Oct 9, 2016 at 10:34am PDT</p> </blockquote> <p></p>\n</div>",
            "content_md": "\n## **TEAM 10**\n\n\nTEAM LEADER: Keith Bradley CELL: 7078490485\n\n\n## Entertainment and Story Telling\n\n\n## Inspiration\n\n\nInspired by the sonic ambience around us. Binaural puts you in the exact sound field as originally intended. You can hear a bird taking off. You can hear the band exactly as they were positioned when playing. \n\n\n**The Verge writes:**\n\"For decades, binaural recording was a novelty, and overlooked for less technically demanding methods. But with the rise of virtual reality hardware like the Oculus Rift, HTC Vive, PlayStation VR, and Samsung\u2019s Gear \u2014 systems dependent on realistic 3D audio to fully immerse their users \u2014 binaural audio is on the cusp of a renaissance. \nBinaural recording systems are unique because they emulate the workings of the human head. The architecture of our anatomy dictates how we understand the sounds we hear: with an ear on either side of a thick skull and spongy brain, we hear sounds enter our left and right ears at different times. If a dog barks by our left ear, it takes a few extra microseconds for the bark to reach the right ear; the sound will also be louder in one ear than the other. In addition, sound waves interact with the physical constitution of the listener \u2014 the pinna (or outer ear), the head, and the torso \u2014 and the surrounding space, creating listener-specific variations otherwise known as head-related transfer function. The brain scrutinizes these miniscule interaural differences of time and strength in order to localize sound with immaculate precision.\" via [link](http://www.theverge.com/2015/2/12/8021733/3d-audio-3dio-binaural-immersive-vr-sound-times-square-new-york)\n\n\n## What it does - The Story\n\n\nAn immersive Sci-Fi narrative experience using immersive sounds that we created to demonstrate the power of creating 360 video experiences with realistic visuals and sound.\n\n\nIn that context we developed, Blue Note. The Blue Note is an immersive narrative using only natural sounds that takes our viewer through the experience of what living in parallel dimensions could feel like here in Boston. It\u2019s a proof-of-concept where Stranger Things meets Under The Skin where on your 18th birthday you are given the power of inter-dimensional sight. Within the first few minutes of adulthood you either master your powers which manifest as a ring of fire, a tear you can create in the inter-dimensions, allowing you to see through space and time. Or you risk being stuck in the alternate dimension for eternity. \n\n\nIn our world the alternate dimensions is a dark reflection of the same environment. The audience will\nleave The Blue Note with a new understanding of the effect that sound has on our minds and consider the\npotential powers we can unlock within our ourself with a new way to view the world both visually and\nsonically.\n\n\n## How we built it - The Sound and Look of the Project\n\n\nAs a multi-disciplinary team consisting of a architectural designer, a software engineer, a 360 video entrepreneur, and filmmaker; we all came to this project to explore the full sensory immersion in virtual reality within a contained narrative. \n\n\nThe project was shot on location in one day on the Charles River featuring Boston street musicians. \n\n\nWe shot the \u201cblue note dimension\u201d with the Samsung Gear VR at 4k to give it a rougher, more ethereal look before adding effects in After Effects and Premiere. To capture the vividness of \u201cour dimension\u201d we captured it using the GoPro Omni Rig and gave it a saturated high contrast look. All of the sound was captured using The Zoom H2N, custom-made binaural microphone. For \u201cblue note world\u201d we mixed-down the binaural ambisonics to stereo using Hokusai 2 and finishing the sound design in Garageband. All of the sounds were captured from the real world and designed specifically for each \u201cblue note\u201d scene. \n\n\nThe sound of the \u201creal world\u201d is a real-time binaural capture that has been spatially mapped in post production based on the sphere surface based on the horizontal positioning system of the Oculus. \n\n\nThe video editing was done in Adobe Premiere and AutoPano, the GoPro capture was done with Omni Importer. The final product was built in Unity using eight spheres with the video as a movie texture of the sphere as well as extensive sound design.\n\n\n## Challenges we ran into\n\n\nBecause the audio needed to be spatial, each channel needed to be exported separately and mapped in location in each scene. This extended workflow adds a significant amount of time to exports and setup in Unity.\n\n\nThe capture of the Omni rig wasn\u2019t as seamless as we had hoped and the importing took a lot of time and computing power. The Samsung Gear VR is more portable, but also captures at lower quality. Unity's limitations on video files as textures caused for some of the footage to be downgraded in this short time period. The equipment is capable of 4K or better, but videos needed to be downsized for this experiment.\n\n\nWe had hoped that we would have more time to explore the story and really develop the idea that you are porting yourself between two dimensions. \n\n\nBeyond time and technology as challenges, we also found it difficult to seamlessly blend the jazz instruments as an auditory cue to have the viewer turn around in the \u201cblue note\u201d universe. Ideally we would have started with the the darkness, silence and then have the sound cue drive the audience to turn their heads to experience \u201cour world\u201d. To address that challenge we swapped the experience to start with the jazz musicians and then have the other \u201cblue note world\u201d be the contrast. We leveraged the idea of a Sci-Fi universe to overcome some of our technical disadvantages.\n\n\nWe had hoped that we would have more time to explore the surrounding area.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of the the technique, result, and strong combination of a multidisciplinary team. We intended to explore a story that allows the audience to embody the same location but is moved to change their location by audio cues. We think there is tremendous opportunity to explore cutting in virtual reality and using audio more specifically and pointedly. If we start to focus on the audio, we believe the audience will be more forgiving of the video and this unlocks a tremendous opportunity for, immersive storytellers. \n\n\n## What we learned\n\n\nWe learned a tremendous amount about the workflow of 360 video capture and trying to create an interactive element inside Unity. With the short timeline of the Hackathon, prerecorded assets would have been a huge benefit to the workflow.\n\n\n## What's next for \"Blue Note\"\n\n\nWe want to do stereo 360 video and further develop the narrative using sound as a primary conceptual driver. We also hope to be able to use higher resolution video files, an 8K capture rig would significantly improve immersion.\n\n\n\n>   [\"Blue Note\" A 3D Audio Journey #realityvirtuallyhack](https://www.instagram.com/p/BLWcixQjsN4/)\n> \n>  A video posted by tkbrdly (@tkbrdly) on Oct 9, 2016 at 10:34am PDT\n> \n>  \n\n \n"
        },
        {
            "source": "https://devpost.com/software/vrchitecture",
            "title": "A",
            "blurb": "A",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Sean Dekkers",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7658834e51ac4850b1ccc2d10739d190?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Patrick Murray",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fcca186ffd8ff4726e83b8bed2f2c758?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Liam Tuohy",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQG-pO0dYpPokA/profile-displayphoto-shrink_800_800/0?e=1577923200&height=180&t=YgX6qpDavOcq-ifrSROIfAQPfXFsugRZMBjOe8qa6h0&v=beta&width=180"
                },
                {
                    "name": "Thiyagarajan Adi Raman",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/327fe3efca12609c80c7c7207bd517b7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kelly Wang",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/78dca0c3d35d7ca668b88d7526e83d56?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c-sharp",
                "javascript",
                "json",
                "maya",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<h2>What it does</h2>\n<p>A</p>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n## What it does\n\n\nA\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for\n\n\n"
        },
        {
            "source": "https://devpost.com/software/silver-unicorn",
            "title": "Silver Unicorn",
            "blurb": "We have robots. Friendly robots. ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Swaroop Pal",
                    "about": "+ Programming the mixed reality interface and the networking components on Hololens. ",
                    "photo": "https://media.licdn.com/dms/image/C5603AQFVPdeacBYpAw/profile-displayphoto-shrink_100_100/0?e=1545264000&height=180&t=HyfG0Mr4quzjuUV2RTiKTX-NmjJtjHUosseMZ86GIL8&v=beta&width=180"
                },
                {
                    "name": "Ian Sterling",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0__bBK7OLYNySePLgn5liKCR2YKd2e9z0Uh_iKdodxlyAd9L2nL_7Kaabx1e7UvLOc8_iAX7e04IaWrAiQ7qtuboL1gIaHrAeVWqtPuIrOnoRkVFPQL5Xj2xOfjYq4MA0IhB9yIzKdDrD?height=180&width=180"
                },
                {
                    "name": "Albert Schweitzer",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c5d805c5862bac69139d705e75f789b3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<h2>About</h2>\n<p><a href=\"https://github.com/aschwtzr/SilverUnicorn/wiki\" rel=\"nofollow\">Silver Unicorn</a> is a collection of hacks and APIs that aims to serve as a prototyping platform for Augmented Reality by handling human-digital-physical interactions. The original project includes a hack for Microsoft Hololens that set up a complete loop of interactions between a human using voice to control a Sphero via a TCP server running on a Mac OS app. The Sphero was contained in a box with an image that was marked and recognized by the Vuforia engine running on Hololens, which then rendered a digital object for the user to interact with.</p>\n<p>After not making it to semifinals the team continued hacking with the aim of moving an Arduino powered robot through the physical world using the tracking features in the HTC Vive Wand. A UDP network would then pass that information to Hololens which could project the movements of the robot in a digital rendering of the Vive's physical space. Unfortunately, networking with Unity is really hard.</p>\n<p>This is where you can come in. We're really excited about the potential this type of technology gives us to redefine the relationships between physical and digital objects. We plan to continue these efforts and will expand the work we've done to continue building out usable APIs and hacks that integrate physical objects into virtual environments or give us new ways to interact with them through digital means.</p>\n<h2>Fun Facts:</h2>\n<p>The original project was built over the course of 67 hours at the Reality, Virtually, Hackathon! sponsored by the MIT Media Lab.\nAlthough all group members slept at some point, at no point since first convening on Saturday did all participants sleep at the same time.\nNo robots were harmed in the making of this project, no matter what Erlich Bachman says.</p>\n</div>",
            "content_md": "\n## About\n\n\n[Silver Unicorn](https://github.com/aschwtzr/SilverUnicorn/wiki) is a collection of hacks and APIs that aims to serve as a prototyping platform for Augmented Reality by handling human-digital-physical interactions. The original project includes a hack for Microsoft Hololens that set up a complete loop of interactions between a human using voice to control a Sphero via a TCP server running on a Mac OS app. The Sphero was contained in a box with an image that was marked and recognized by the Vuforia engine running on Hololens, which then rendered a digital object for the user to interact with.\n\n\nAfter not making it to semifinals the team continued hacking with the aim of moving an Arduino powered robot through the physical world using the tracking features in the HTC Vive Wand. A UDP network would then pass that information to Hololens which could project the movements of the robot in a digital rendering of the Vive's physical space. Unfortunately, networking with Unity is really hard.\n\n\nThis is where you can come in. We're really excited about the potential this type of technology gives us to redefine the relationships between physical and digital objects. We plan to continue these efforts and will expand the work we've done to continue building out usable APIs and hacks that integrate physical objects into virtual environments or give us new ways to interact with them through digital means.\n\n\n## Fun Facts:\n\n\nThe original project was built over the course of 67 hours at the Reality, Virtually, Hackathon! sponsored by the MIT Media Lab.\nAlthough all group members slept at some point, at no point since first convening on Saturday did all participants sleep at the same time.\nNo robots were harmed in the making of this project, no matter what Erlich Bachman says.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/realitysampler",
            "title": "Let it Snow!",
            "blurb": "A last chance to experience snowfall in the era of global warming.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/lkq_TUulxHw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Let it Snow!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/217/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Adrian Sas",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/29203c47f19f2dad330b5754e9ce972b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Hao Chen Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/739bf20fdbb3fb668808ff42d1a3d1d8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sam L",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/74eb60334766f1c42dd104cd1728c9db?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "KR Saxton",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a53f78c536bae228fc70fbdb3b4b7b9c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<p>In this fanciful mixed reality experience, snow falls from above while a snowman serenades you.</p>\n<p>We created the winter wonderland effect from a Unity particle system which simulates snow, and worked with Hololens' spatial mapping function to locate surfaces and form a mesh which catches the accumulating snowfall. </p>\n<p>Let it Snow! transposes the magic of winter onto unlikely environments for a delightful, wintry experience everyone can enjoy. </p>\n</div>",
            "content_md": "\nIn this fanciful mixed reality experience, snow falls from above while a snowman serenades you.\n\n\nWe created the winter wonderland effect from a Unity particle system which simulates snow, and worked with Hololens' spatial mapping function to locate surfaces and form a mesh which catches the accumulating snowfall. \n\n\nLet it Snow! transposes the magic of winter onto unlikely environments for a delightful, wintry experience everyone can enjoy. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/project-playbox",
            "title": "Project PlayBox",
            "blurb": "A virtual reality playground for pediatric patients",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/n---wFVsP3c?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jackson Tam",
                    "about": "Concept and implementation",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/843/datas/profile.png"
                },
                {
                    "name": "Anagha Todalbagi",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/18bcb402477b269f64b326c0dcfbb5fa?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Maximiliano Madrid",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3db42751d2077b22632d331f8dfc36f2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sabrina De Los Santos",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/995d6ebba65accd8a4de49753fa6ced0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "oculus-vr-sdk",
                "rift",
                "unity"
            ],
            "content_html": "<div>\n<p>We hope this app will assist in pain therapy for pediatric patients by providing an engaging way for kids to design, build, and share their own VR playground.  </p>\n</div>",
            "content_md": "\nWe hope this app will assist in pain therapy for pediatric patients by providing an engaging way for kids to design, build, and share their own VR playground. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/mindflow-vr",
            "title": "MindFlow VR",
            "blurb": "Guided meditation in VR with realtime respiratory feedback.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Sourabh Jain",
                    "about": "Product management, development of business case, programming and ideation support.",
                    "photo": "https://www.gravatar.com/avatar/439c20cd1911a18a52e7adba042c7314?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tim Gallati",
                    "about": "Team lead, wrote and produced the guided meditation audio, 360 image content",
                    "photo": "https://www.gravatar.com/avatar/999e430e34c28c251b105dea3edb794c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Shea Rembold",
                    "about": "Unity programming",
                    "photo": "https://www.gravatar.com/avatar/e4607be2b082e4861f4f5566cc45c2fc?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "shylo shepherd",
                    "about": "I worked on the environment,  animations and UI. ",
                    "photo": "https://avatars3.githubusercontent.com/u/5191443?height=180&v=4&width=180"
                },
                {
                    "name": "Angelica Tinga",
                    "about": "I worked on the BIOPAC respiratory measurements and testing other physiological measures and integrating the measurements using iMotions software. ",
                    "photo": "https://www.gravatar.com/avatar/d306ff29cdd1091fb265c8dc9330551f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>ZenZone Gear VR app, Spire respiratory feedback device.</p>\n<h2>What it does</h2>\n<p>Provides realtime respiratory feedback during guided meditations in a VR environment.</p>\n<h2>How we built it</h2>\n<p>We connected a BIOPAC respiratory monitor to an Oculus Rift using iMotions software, and created an immersive VR environment with a soothing guided meditation.</p>\n<h2>Challenges we ran into</h2>\n<p>EEG headset hardware from iMotion did not provide fully accurate readings.  </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Getting live respiratory biometric data and applying it directly into visualisations in virtual reality.</p>\n<h2>What we learned</h2>\n<h2>What's next for MindFlow VR</h2>\n<p>Engagement with communities in medicine and wellness research.</p>\n<h2>Other Details:</h2>\n<p>Lead name: Tim Gallati\nLead person\u2019s telephone number: 310.442.9391\nLocation: Media Lab\nRoom: E14 - 674\nRoughly where in the room: In the center of the room\nFloor number: 6th floor\nVertical: Human Wellbeing\nBackground Photo and Audio captured and edited by Tim Gallati</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nZenZone Gear VR app, Spire respiratory feedback device.\n\n\n## What it does\n\n\nProvides realtime respiratory feedback during guided meditations in a VR environment.\n\n\n## How we built it\n\n\nWe connected a BIOPAC respiratory monitor to an Oculus Rift using iMotions software, and created an immersive VR environment with a soothing guided meditation.\n\n\n## Challenges we ran into\n\n\nEEG headset hardware from iMotion did not provide fully accurate readings. \n\n\n## Accomplishments that we're proud of\n\n\nGetting live respiratory biometric data and applying it directly into visualisations in virtual reality.\n\n\n## What we learned\n\n\n## What's next for MindFlow VR\n\n\nEngagement with communities in medicine and wellness research.\n\n\n## Other Details:\n\n\nLead name: Tim Gallati\nLead person\u2019s telephone number: 310.442.9391\nLocation: Media Lab\nRoom: E14 - 674\nRoughly where in the room: In the center of the room\nFloor number: 6th floor\nVertical: Human Wellbeing\nBackground Photo and Audio captured and edited by Tim Gallati\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-story-tellers",
            "title": "VR Story Tellers",
            "blurb": "Let your story come alive into an Interactive VR, real time, right from the pages of a book or any text input.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/0TdY-7lcVP8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Yuta Toga",
                    "about": "sound design, sound part of unity develop ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/695/datas/profile.jpg"
                },
                {
                    "name": "Nabanita De",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/582/datas/profile.jpg"
                },
                {
                    "name": "Adrian Babilinski",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/691/datas/profile.jpg"
                },
                {
                    "name": "pat pataranutaporn",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/59f516b4fcf9f48508883070ce4aefde?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Biswaraj Kar",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1476193365/xetevcinokz23blvoqgz.jpg?height=180&width=180"
                }
            ],
            "built_with": [
                "amazon-web-services",
                "google-cardboard",
                "ibm-watson",
                "ibmwatsontoneanalyzer",
                "microsoft-azure-cognitive-services",
                "microsoftcognitiveservices",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<p>The Inception of the project was at the MIT Media Lab Reality Virtually Hackathon, where all of our team members brainstormed &amp; came up with this idea of turning text to a Virtual Reality Experience in real time.</p>\n<h2>Main Idea:</h2>\n<p>On entering your story on our website, the app generates the viewable VR content including real life characters(images), objects in proper locations - scenes(background), coupled with a background sound based on the characters, actions and mood on the Story plot.</p>\n<h2>Interface:</h2>\n<p>We have built a website to enter your own story (as plain text). As you click on submit, we analyze and figure out the main objects,  their descriptions, the background and the overall mood of the story using machine learning and natural language processing.\nWe used Machine learning APIs of Microsoft Cognitive Services and the IBM Watson Tone Analyzer. We combine the outputs of the APIs into a single JSON file, which is then stored on the Cloud (Amazon Web Services). The file is picked up by Unity and analyzed to pull up assets and related animations based on the objects and background sounds based on the mood of the plot. The objects are placed relative to each other with the appropriate background and a background music is played real time. This entire experience is viewable on Google Cardboard. We typically chose Cardboard as it is one of the cheapest VR device available with the best experience, hence more users would get access to the app.</p>\n<h2>Challenges:</h2>\n<p>Integrating and mapping all the components to each other: Cloud, Machine Learning, VR and Web Development especially since its our first hackathon for most of us and none of us had any experience with prior AWS and most of us had no prior VR experience and how we learnt everything on the fly at the Hackathon and implemented it end-to-end within one and half days was a tough job, done well, with extreme dedication.</p>\n<h2>UseCases:</h2>\n<ul>\n<li>Can be used by educational institutes heavily, to engage students in the learning experience, can be used by parents (best when they are away), engaging kids in a great learning experience etc. Best used for Entertainment purposes.</li>\n<li>Can be used to help children with learning disabilities (dyslexia, autism) to read stories and experience content.</li>\n<li>Can be used by Writers, to visualize their plots to see how it would be like to have a movie out of it. Also to engage more users, to their writing skills through this brilliant experience. </li>\n<li>Can be used by Directors to visualize plots and do cost estimates based on assets and locations and also hire the best actors for the movies. </li>\n<li>Can be used by book lovers who think movies made, donot do justice to their amazing books. </li>\n</ul>\n<h2>Future Scope:</h2>\n<ul>\n<li>We can use EEG emotion sensors built into a simple headset to monitor the user's emotion and dynamically change the content. (specially make custom apps for children with this)</li>\n<li>The machine learning algorithm can be enhanced to more effectively find the subject, object and the scene, which will enhance the overall text to VR quality.</li>\n<li>We can have a database of assets based on the stories we evaluate and keep adding assets to it as more stories are read which have new assets in it.</li>\n<li>We can automate the asset-pull method which dynamically downloads new assets if not present in the current asset database.</li>\n</ul>\n<h2>Vertical:</h2>\n<p>Entertainment/Storytelling/Education</p>\n</div>",
            "content_md": "\nThe Inception of the project was at the MIT Media Lab Reality Virtually Hackathon, where all of our team members brainstormed & came up with this idea of turning text to a Virtual Reality Experience in real time.\n\n\n## Main Idea:\n\n\nOn entering your story on our website, the app generates the viewable VR content including real life characters(images), objects in proper locations - scenes(background), coupled with a background sound based on the characters, actions and mood on the Story plot.\n\n\n## Interface:\n\n\nWe have built a website to enter your own story (as plain text). As you click on submit, we analyze and figure out the main objects, their descriptions, the background and the overall mood of the story using machine learning and natural language processing.\nWe used Machine learning APIs of Microsoft Cognitive Services and the IBM Watson Tone Analyzer. We combine the outputs of the APIs into a single JSON file, which is then stored on the Cloud (Amazon Web Services). The file is picked up by Unity and analyzed to pull up assets and related animations based on the objects and background sounds based on the mood of the plot. The objects are placed relative to each other with the appropriate background and a background music is played real time. This entire experience is viewable on Google Cardboard. We typically chose Cardboard as it is one of the cheapest VR device available with the best experience, hence more users would get access to the app.\n\n\n## Challenges:\n\n\nIntegrating and mapping all the components to each other: Cloud, Machine Learning, VR and Web Development especially since its our first hackathon for most of us and none of us had any experience with prior AWS and most of us had no prior VR experience and how we learnt everything on the fly at the Hackathon and implemented it end-to-end within one and half days was a tough job, done well, with extreme dedication.\n\n\n## UseCases:\n\n\n* Can be used by educational institutes heavily, to engage students in the learning experience, can be used by parents (best when they are away), engaging kids in a great learning experience etc. Best used for Entertainment purposes.\n* Can be used to help children with learning disabilities (dyslexia, autism) to read stories and experience content.\n* Can be used by Writers, to visualize their plots to see how it would be like to have a movie out of it. Also to engage more users, to their writing skills through this brilliant experience.\n* Can be used by Directors to visualize plots and do cost estimates based on assets and locations and also hire the best actors for the movies.\n* Can be used by book lovers who think movies made, donot do justice to their amazing books.\n\n\n## Future Scope:\n\n\n* We can use EEG emotion sensors built into a simple headset to monitor the user's emotion and dynamically change the content. (specially make custom apps for children with this)\n* The machine learning algorithm can be enhanced to more effectively find the subject, object and the scene, which will enhance the overall text to VR quality.\n* We can have a database of assets based on the stories we evaluate and keep adding assets to it as more stories are read which have new assets in it.\n* We can automate the asset-pull method which dynamically downloads new assets if not present in the current asset database.\n\n\n## Vertical:\n\n\nEntertainment/Storytelling/Education\n\n\n"
        },
        {
            "source": "https://devpost.com/software/schwifty-8ib1jy",
            "title": "revolVR",
            "blurb": "A VR game that showcases the intersection of a novel locomotion mechanic with immersive low poly graphics.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/b_wYpHka4e8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "revolVR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/352/datas/original.jpg"
                },
                {
                    "title": "An inspiration for the wheel idea",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/015/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Michael Ballentine",
                    "about": "Created and helped implement art assets and assisted on technical aspects",
                    "photo": "https://www.gravatar.com/avatar/47e8d94fcd15857a8f087f3080c16515?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Noor Eddin Amer",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d751ddfe6e6923935cfb78605d95aca1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Gabe Fields",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/cc0aa599bfb01dd980cdeb259daa723b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "John Mikhail",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ca1c3539bac582e1fe152e44df013ac0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Elisa Young",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/1c3827d51936e6b76895d3f93926b541?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "audacity",
                "maya",
                "python",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>While VR attempts to curate some very immersive experiences, the inputs that users provide feel very unnatural and unfitting to the immersive experience. All the buttons that you have to press and the way that you traverse the world (through teleporting for instance) are not natural. What if you can run in place rather than using the controllers to traverse a large space? This was the initial thought that started the whole project.</p>\n<p>The lack of good locomotion input mechanics for VR prompted us to explore that space. In particular the merge of physical motion with engaging virtual experiences can be a strong contributing factor towards human well being and fitness. Users are a provided with an engaging experience that prompts them to exercise and stay fit.</p>\n<p>We wanted to couple our vision for a new locomotion mechanic with an interesting creative experience that you can only get in VR. With such an engaging setting, we hoped to prompt users to exercise by running in place and stay fit. In particular, we wanted the users to feel the sense of scale and explore the notion of scale in VR through peculiar and fascinating graphics. </p>\n<h2>What it is</h2>\n<p>In this hackathon, we developed a locomotion mechanic for Vive that allows you to run in place to traverse the space in VR. In addition to that mechanic, we created our own stylized art assets to curate an otherworldly and peculiar experience, with scale as a main feature to be explored in VR. We merged these two assets into one virtual experience that promotes fitness and human well being through cardiovascular exercise that is packaged as a fun and engaging game. </p>\n<p>The game is set in 2035. Earth is quickly running out of natural resources. Desperate to find the energy needed to sustain life, scientists and aerospace engineers have been exploring nearby planets, and to their surprise, they have discovered a strange, glowing orb (an anomaly) floating near Mars. After building a space station around it to study it, scientists have discovered that it absorbs everything close by but emits crystals containing energy precious to sustaining human life on Earth. In an effort to collect what energy they can before the station collapses into the anomaly, they have sent you there on a mission to collect as many crystals as possible to bring back to Earth. </p>\n<h2>How we built it</h2>\n<p>We used Python analytics libraries to develop the running algorithm and then implemented it in C# for use in unity. We used Maya to create our own stylize low poly graphics. The graphics emphasized scale and curated an other worldly and fascinating experience. We then imported these graphics into unity and developed the game mechanics around them, with running in place as the main source of input. Finally we downloaded relevant sounds from freesound.org, edited them in audacity and used sound engineering to incorporate these sounds into the game play. All of this came together to provide an enchanting experience with HTC Vive. </p>\n<h2>Challenges we ran into</h2>\n<p>The locomotion algorithm took a long time to develop. The collected data was very noisy and it was quite challenging to smooth the data in order to accurately predict whether the person wearing the headset is running in place or not. </p>\n<p>As a team it took us a long time to converge on an idea to utilize the locomotion algorithm and the graphics at our disposal. In particular the MVP of the game only came together on Sunday morning.</p>\n<p>Finally, development collaboration proved to be challenging with github and unity. It was very hard to maintain a running version of the code on all of our machines and eventually we turned to peer programing on one machine and having a single working copy of the source code. </p>\n<h2>Accomplishments that we are proud of</h2>\n<p>We are really proud of being able to create a fully functioning game with a new input mechanism and high quality graphics. The experience is truly enchanting and it makes us very proud of our work and keen to see what other worlds we can model in VR.</p>\n<h2>What we learned</h2>\n<p>We learned how to collaborate with developers and designers to create really powerful content. We explored developing with unity for the HTC Vive.</p>\n<h2>What's next for revolVR</h2>\n<p>The next step is to publish revolVR on steam. It is already a great product with nice appeal and great potential. Moreover, the locomotion mechanic that we have built can be used for many other experiences and may even become a common mechanism for getting user input.</p>\n<h2>Notes</h2>\n<p>1.) Team Lead: John Mikhail</p>\n<p>3.) Team's Location: E14-445</p>\n<p>4.) Vertical Category: Entertainment/Human Well Being</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWhile VR attempts to curate some very immersive experiences, the inputs that users provide feel very unnatural and unfitting to the immersive experience. All the buttons that you have to press and the way that you traverse the world (through teleporting for instance) are not natural. What if you can run in place rather than using the controllers to traverse a large space? This was the initial thought that started the whole project.\n\n\nThe lack of good locomotion input mechanics for VR prompted us to explore that space. In particular the merge of physical motion with engaging virtual experiences can be a strong contributing factor towards human well being and fitness. Users are a provided with an engaging experience that prompts them to exercise and stay fit.\n\n\nWe wanted to couple our vision for a new locomotion mechanic with an interesting creative experience that you can only get in VR. With such an engaging setting, we hoped to prompt users to exercise by running in place and stay fit. In particular, we wanted the users to feel the sense of scale and explore the notion of scale in VR through peculiar and fascinating graphics. \n\n\n## What it is\n\n\nIn this hackathon, we developed a locomotion mechanic for Vive that allows you to run in place to traverse the space in VR. In addition to that mechanic, we created our own stylized art assets to curate an otherworldly and peculiar experience, with scale as a main feature to be explored in VR. We merged these two assets into one virtual experience that promotes fitness and human well being through cardiovascular exercise that is packaged as a fun and engaging game. \n\n\nThe game is set in 2035. Earth is quickly running out of natural resources. Desperate to find the energy needed to sustain life, scientists and aerospace engineers have been exploring nearby planets, and to their surprise, they have discovered a strange, glowing orb (an anomaly) floating near Mars. After building a space station around it to study it, scientists have discovered that it absorbs everything close by but emits crystals containing energy precious to sustaining human life on Earth. In an effort to collect what energy they can before the station collapses into the anomaly, they have sent you there on a mission to collect as many crystals as possible to bring back to Earth. \n\n\n## How we built it\n\n\nWe used Python analytics libraries to develop the running algorithm and then implemented it in C# for use in unity. We used Maya to create our own stylize low poly graphics. The graphics emphasized scale and curated an other worldly and fascinating experience. We then imported these graphics into unity and developed the game mechanics around them, with running in place as the main source of input. Finally we downloaded relevant sounds from freesound.org, edited them in audacity and used sound engineering to incorporate these sounds into the game play. All of this came together to provide an enchanting experience with HTC Vive. \n\n\n## Challenges we ran into\n\n\nThe locomotion algorithm took a long time to develop. The collected data was very noisy and it was quite challenging to smooth the data in order to accurately predict whether the person wearing the headset is running in place or not. \n\n\nAs a team it took us a long time to converge on an idea to utilize the locomotion algorithm and the graphics at our disposal. In particular the MVP of the game only came together on Sunday morning.\n\n\nFinally, development collaboration proved to be challenging with github and unity. It was very hard to maintain a running version of the code on all of our machines and eventually we turned to peer programing on one machine and having a single working copy of the source code. \n\n\n## Accomplishments that we are proud of\n\n\nWe are really proud of being able to create a fully functioning game with a new input mechanism and high quality graphics. The experience is truly enchanting and it makes us very proud of our work and keen to see what other worlds we can model in VR.\n\n\n## What we learned\n\n\nWe learned how to collaborate with developers and designers to create really powerful content. We explored developing with unity for the HTC Vive.\n\n\n## What's next for revolVR\n\n\nThe next step is to publish revolVR on steam. It is already a great product with nice appeal and great potential. Moreover, the locomotion mechanic that we have built can be used for many other experiences and may even become a common mechanism for getting user input.\n\n\n## Notes\n\n\n1.) Team Lead: John Mikhail\n\n\n3.) Team's Location: E14-445\n\n\n4.) Vertical Category: Entertainment/Human Well Being\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-virtual-memory-palace",
            "title": "The Virtual Memory Palace",
            "blurb": "A training tool that allows the user to practice spatial cognition and visual memory",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/EA1gDCJ9OtI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Gwan Yip",
                    "about": "Team lead, research, product strategy, UX, UI development",
                    "photo": "https://www.gravatar.com/avatar/7b33cfbe47d51f0202e9e17d0741957a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tycho de Back",
                    "about": "Scientific validation of concept and user testing, quality control.",
                    "photo": "https://www.gravatar.com/avatar/cf8fc1ed1eeac99a1fb81dd6011890c0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Fernando Nazario",
                    "about": "Developer",
                    "photo": "https://avatars.githubusercontent.com/u/21963366?height=180&v=3&width=180"
                },
                {
                    "name": "Mehedi Ahmed",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/346e82af026edd7ca4688eb7a6fafc20?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "daydream",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The Memory Palace technique, originally devised by the Greek poet Simonides (556 \u2013 468 BC) has been used to great success to quickly and easily commit enormous amounts of facts, faces and numbers to memory. The method works by leveraging the strength of visualization and spatial memory to first vividly visualize the items to be memorized and then mentally putting them along an imagined route.</p>\n<h2>What it does</h2>\n<p>A training tool that allows the user to practice the concepts behind the \u2018Memory Palace\u2019 utilizing spatial cognition and visual memory. Users are transported into a virtual memory palace where they traverse along a pre-determined route placing objects on platforms within different rooms of the 'palace'. Once the user has finished they can test their memory through a 3D UI that responds to voice recognition. For each object the user gets correct they receive a point allowing them to track their progress and challenge other people.</p>\n<h2>How we built it</h2>\n<p>We built the application for the DayDream specifically because we wanted to utilize a platform that could reach as many people as we could but also take advantage of the 3DOF controller which makes navigating and placing objects around a 3D environment possible on mobile that wasn't previous.</p>\n<h2>Challenges we ran into</h2>\n<p>We ran into a lot of issues trying to use and learn the DayDream device, specifically trying to get the controller emulator to work. We also had issues trying to integrate the main game level with the voice recognition test level. We also ran into issues getting the Movie mesh to work in android, the opening video we created that would function as the on-boarding video to the experience is attached in the video link.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our core goal was to create a virtual reality product that can statistically demonstrate value to the end user. We believe that in order for the VR industry to truely reach it's potential we need to engage users outside of the gaming industry and with this application we feel we can do that by tapping into the human well being industry. We conducted several rounds of user research and testing, establishing a baseline to test our application against. One of the things we're most proud of was applying design thinking and a research driven development approach to VR. You can see our findings here: <a href=\"https://www.dropbox.com/s/jszpr6nmnjdwm1u/VR-Memory-Palace%20-%20V3.pdf?dl=1\" rel=\"nofollow\">https://www.dropbox.com/s/jszpr6nmnjdwm1u/VR-Memory-Palace%20-%20V3.pdf?dl=1</a></p>\n<h2>What we learned</h2>\n<p>Working with such a new platform will make some of the most fundamental and straightforward tasks very challenging</p>\n<h2>What's next for The Virtual Memory Palace</h2>\n<p>Expand the content into different verticals and industries so users can have multiple different Virtual Memory Palaces and practice learning specific items related to languages, professions, etc. This could potentially expand into a market place where content creators can design their own Memory Palaces and objects to them sell to the community</p>\n<h2>The vertical category in which you are competing</h2>\n<p>Human Well-Being (Education/Health/Wellness/Activism)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe Memory Palace technique, originally devised by the Greek poet Simonides (556 \u2013 468 BC) has been used to great success to quickly and easily commit enormous amounts of facts, faces and numbers to memory. The method works by leveraging the strength of visualization and spatial memory to first vividly visualize the items to be memorized and then mentally putting them along an imagined route.\n\n\n## What it does\n\n\nA training tool that allows the user to practice the concepts behind the \u2018Memory Palace\u2019 utilizing spatial cognition and visual memory. Users are transported into a virtual memory palace where they traverse along a pre-determined route placing objects on platforms within different rooms of the 'palace'. Once the user has finished they can test their memory through a 3D UI that responds to voice recognition. For each object the user gets correct they receive a point allowing them to track their progress and challenge other people.\n\n\n## How we built it\n\n\nWe built the application for the DayDream specifically because we wanted to utilize a platform that could reach as many people as we could but also take advantage of the 3DOF controller which makes navigating and placing objects around a 3D environment possible on mobile that wasn't previous.\n\n\n## Challenges we ran into\n\n\nWe ran into a lot of issues trying to use and learn the DayDream device, specifically trying to get the controller emulator to work. We also had issues trying to integrate the main game level with the voice recognition test level. We also ran into issues getting the Movie mesh to work in android, the opening video we created that would function as the on-boarding video to the experience is attached in the video link.\n\n\n## Accomplishments that we're proud of\n\n\nOur core goal was to create a virtual reality product that can statistically demonstrate value to the end user. We believe that in order for the VR industry to truely reach it's potential we need to engage users outside of the gaming industry and with this application we feel we can do that by tapping into the human well being industry. We conducted several rounds of user research and testing, establishing a baseline to test our application against. One of the things we're most proud of was applying design thinking and a research driven development approach to VR. You can see our findings here: <https://www.dropbox.com/s/jszpr6nmnjdwm1u/VR-Memory-Palace%20-%20V3.pdf?dl=1>\n\n\n## What we learned\n\n\nWorking with such a new platform will make some of the most fundamental and straightforward tasks very challenging\n\n\n## What's next for The Virtual Memory Palace\n\n\nExpand the content into different verticals and industries so users can have multiple different Virtual Memory Palaces and practice learning specific items related to languages, professions, etc. This could potentially expand into a market place where content creators can design their own Memory Palaces and objects to them sell to the community\n\n\n## The vertical category in which you are competing\n\n\nHuman Well-Being (Education/Health/Wellness/Activism)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/attraction",
            "title": "Attraction",
            "blurb": "sciences of magnets in VR",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Yen-Ling Kuo",
                    "about": "",
                    "photo": "https://graph.facebook.com/707870927/picture?height=180&width=180"
                },
                {
                    "name": "&quot;Ryan&quot; Yan Zhang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/539/datas/profile.jpg"
                },
                {
                    "name": "Muso Fun",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1475966827/yoz0um09vjx0fkahcgxy.jpg?height=180&width=180"
                },
                {
                    "name": "Jackie Lee",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/149/610/datas/profile.jpg"
                }
            ],
            "built_with": [
                "htc-vive",
                "unreal-engine"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We'd like to train people's intuitions about the invisible forces of magnetic fields</p>\n<h2>What it does</h2>\n<p>It's a short VR experience for seeing magnetic fields.</p>\n<h2>How I built it</h2>\n<p>We use Unreal Engine and Vive</p>\n<h2>Challenges I ran into</h2>\n<p>Making it a game is not easy. Visualizing with math and vector field is tricky.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Magnetic fields look really great in VR</p>\n<h2>What I learned</h2>\n<p>There are still lots fun eletromagnetic interaction we have not covered.</p>\n<h2>What's next for Attraction</h2>\n<p>We will see...</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe'd like to train people's intuitions about the invisible forces of magnetic fields\n\n\n## What it does\n\n\nIt's a short VR experience for seeing magnetic fields.\n\n\n## How I built it\n\n\nWe use Unreal Engine and Vive\n\n\n## Challenges I ran into\n\n\nMaking it a game is not easy. Visualizing with math and vector field is tricky.\n\n\n## Accomplishments that I'm proud of\n\n\nMagnetic fields look really great in VR\n\n\n## What I learned\n\n\nThere are still lots fun eletromagnetic interaction we have not covered.\n\n\n## What's next for Attraction\n\n\nWe will see...\n\n\n"
        },
        {
            "source": "https://devpost.com/software/zeegeeball",
            "title": "ZeeGee",
            "blurb": "Have you ever wanted to fly in VR? This experience allows you to float from area to area with the help of frisbees",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Leonard Wedderburn",
                    "about": "I worked on the tools management",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/958/datas/profile.jpg"
                },
                {
                    "name": "Alan Foster",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/474/142/datas/profile.jpg"
                },
                {
                    "name": "Daniel Bryand",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c854e9372564492cef7fd0b692e9bf4c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Louis DeScioli",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/4272e3cb52eac9d4105917246b8d71a7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Max Rose",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/69bf1b1af83c5df48e93578217a8d089?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Zero Gravity</p>\n<h2>What it does</h2>\n<p>The player is able to float from area to area, they use their controllers like wings to guide themselves around</p>\n<h2>How I built it</h2>\n<p>Using Unity and Steam plugins</p>\n<h2>Challenges I ran into</h2>\n<p>Figuring out what to use the movement for</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Creating a movement mechanic that is smooth and simple to use</p>\n<h2>What I learned</h2>\n<p>Breaking the rules in VR allows for creative possibilites</p>\n<h2>What's next for ZeeGee</h2>\n<p>Possible development into a full game</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nZero Gravity\n\n\n## What it does\n\n\nThe player is able to float from area to area, they use their controllers like wings to guide themselves around\n\n\n## How I built it\n\n\nUsing Unity and Steam plugins\n\n\n## Challenges I ran into\n\n\nFiguring out what to use the movement for\n\n\n## Accomplishments that I'm proud of\n\n\nCreating a movement mechanic that is smooth and simple to use\n\n\n## What I learned\n\n\nBreaking the rules in VR allows for creative possibilites\n\n\n## What's next for ZeeGee\n\n\nPossible development into a full game\n\n\n"
        },
        {
            "source": "https://devpost.com/software/asymmetrical-d-d-vr",
            "title": "Asymmetrical D&D VR",
            "blurb": "VR player explores a dungeon, PC player determines their map location by watching them turn their head",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Noah Senzel",
                    "about": "Camera movement and controller input",
                    "photo": "https://www.gravatar.com/avatar/275b1915ce391028ce9762ab390f2c1d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dan Donato",
                    "about": "Networking",
                    "photo": "https://avatars2.githubusercontent.com/u/10944648?height=180&v=4&width=180"
                },
                {
                    "name": "Zoey Zongyu Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5b05a58c48057d625bb75d042b3251c9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Asymmetrical D&amp;D VR is a competitive multiplayer game that has been done in non-VR (Dungeon Keeper, etc) but has exciting VR potential.</p>\n<h2>What it does</h2>\n<p>One player has an overview of the dungeon but no indicator of the players movements. They must watch the adventurer who is playing in VR (using a controller to go forward and turning their head to turn) to approximate their location and spring traps appropriately.</p>\n<h2>How we built it</h2>\n<p>We examined sample files from Google's documentation and used them to create a basic scene, and used our previous networking knowledge for the pc to android setup. </p>\n<h2>Challenges we ran into</h2>\n<p>The initial setup of the sample Android project and Unity networking took us longer than expected.\nThe build crashed or didn't install on Android for hours of debugging</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The game uses multiple technologies (bluetooth controller, cardboard, and pc/android networking), but is easily accessible with devices that many people already own.</p>\n<h2>What we learned</h2>\n<p>Update your SDKs early, and don't get bogged down trying to make something work that <em>should</em> work, if it's taking too long.</p>\n<p>Test your build on the platform you're creating for, not in the Unity editor...</p>\n<h2>What's next for Asymmetrical D&amp;D VR</h2>\n<p>With a viewer we picked up during the hackathon, we hope to add more gameplay elements (maybe more trap types or more players?) and possibly handmade assets.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAsymmetrical D&D VR is a competitive multiplayer game that has been done in non-VR (Dungeon Keeper, etc) but has exciting VR potential.\n\n\n## What it does\n\n\nOne player has an overview of the dungeon but no indicator of the players movements. They must watch the adventurer who is playing in VR (using a controller to go forward and turning their head to turn) to approximate their location and spring traps appropriately.\n\n\n## How we built it\n\n\nWe examined sample files from Google's documentation and used them to create a basic scene, and used our previous networking knowledge for the pc to android setup. \n\n\n## Challenges we ran into\n\n\nThe initial setup of the sample Android project and Unity networking took us longer than expected.\nThe build crashed or didn't install on Android for hours of debugging\n\n\n## Accomplishments that we're proud of\n\n\nThe game uses multiple technologies (bluetooth controller, cardboard, and pc/android networking), but is easily accessible with devices that many people already own.\n\n\n## What we learned\n\n\nUpdate your SDKs early, and don't get bogged down trying to make something work that *should* work, if it's taking too long.\n\n\nTest your build on the platform you're creating for, not in the Unity editor...\n\n\n## What's next for Asymmetrical D&D VR\n\n\nWith a viewer we picked up during the hackathon, we hope to add more gameplay elements (maybe more trap types or more players?) and possibly handmade assets.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/embiggen-the-embiggening",
            "title": "Embiggen: The Embiggening",
            "blurb": "Utilizing scaling to traverse large game maps while growing and shrinking items to interact with environment.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Chris Goodwin",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2ef668fd13c15474ac1fc8c2c97db4bd?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Martin E",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a5cd34672e0b9a6463cbcf551e5bcdb8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Murch Ewings",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQG9GNAI670qHg/profile-displayphoto-shrink_800_800/0?e=1564617600&height=180&t=tPGni7nmNoE-fqPo1mjUKNBZJ2C1rnKRTNW85cOXjAU&v=beta&width=180"
                },
                {
                    "name": "Thomas Rind",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/1bfec425f8b29eef6ecad3d906d4cbf1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "liz Pasek-Allen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/869cafc40b8b96beb241e51ee28f8946?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Charlie Pennington",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9830a49a8f08e29994c8c8e9df0b74c0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>What began as detestation for teleportation as a lazy means\nof travel embiggenned into something more. By introducing\nthe scale mechanic, we aim to engender in the player a\ndeeper appreciation for his or her surroundings. We allow the\nplayer to move about the environment natrually.  We beleive\nthat expanding upon the preexisting human experience is the\nbest way to interact with the virtual world. This retains comfort\nwhile allowing the user to experience the amazing.</p>\n<h2>What it does</h2>\n<p>Allows you to traverse long distance by walking instead of\nteleporting. Also allows the user to either grow or shrink their\ncharacter and found items to problem solve and find\nalternative and creative use cases as they progress through\nthe world.</p>\n<h2>How we built it</h2>\n<p>With unity, the vive, and a little bit of luck.</p>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for Embiggen: The Embiggening</h2>\n<h2>The name of your team lead</h2>\n<p>Chris Goodwin</p>\n<h2>The team lead's telephone number</h2>\n<p>805-602-7848</p>\n<h2>The location where the judges can find your team including the floor number</h2>\n<p>6th floor.  Table 16/15</p>\n<h2>The vertical category in which you are competing</h2>\n<p>Entertainment/Gaming</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWhat began as detestation for teleportation as a lazy means\nof travel embiggenned into something more. By introducing\nthe scale mechanic, we aim to engender in the player a\ndeeper appreciation for his or her surroundings. We allow the\nplayer to move about the environment natrually. We beleive\nthat expanding upon the preexisting human experience is the\nbest way to interact with the virtual world. This retains comfort\nwhile allowing the user to experience the amazing.\n\n\n## What it does\n\n\nAllows you to traverse long distance by walking instead of\nteleporting. Also allows the user to either grow or shrink their\ncharacter and found items to problem solve and find\nalternative and creative use cases as they progress through\nthe world.\n\n\n## How we built it\n\n\nWith unity, the vive, and a little bit of luck.\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for Embiggen: The Embiggening\n\n\n## The name of your team lead\n\n\nChris Goodwin\n\n\n## The team lead's telephone number\n\n\n805-602-7848\n\n\n## The location where the judges can find your team including the floor number\n\n\n6th floor. Table 16/15\n\n\n## The vertical category in which you are competing\n\n\nEntertainment/Gaming\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vision",
            "title": "Vision",
            "blurb": "Using mixed reality for natural world color reassignment for people with color deficiencies",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Kay Igwe",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/209/306/datas/profile.png"
                },
                {
                    "name": "Snowonbamboo Du",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/59c9ddf768ffdecea4c99f32683bbdf6?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<p>Kay Igwe\n<a href=\"mailto:kci2104@columbia.edu\" rel=\"nofollow\">kci2104@columbia.edu</a>\n2813006376\nLocation: Room: 674\nGithub Link: <a href=\"https://github.com/igweckay/HoloLensVideo\" rel=\"nofollow\">https://github.com/igweckay/HoloLensVideo</a></p>\n</div>",
            "content_md": "\nKay Igwe\n[kci2104@columbia.edu](mailto:kci2104@columbia.edu)\n2813006376\nLocation: Room: 674\nGithub Link: <https://github.com/igweckay/HoloLensVideo>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/kickalz",
            "title": "KickALZ",
            "blurb": "Using VR to improve the quality of life for persons living with Alzheimer's Disease",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Memory Garden ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/439/963/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Meredith Wilson",
                    "about": "I developed the interactive game object behaviors and triggers in Unity and C# (e.g. the growing flowers and water balls that drop on them)",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/429/240/datas/profile.png"
                },
                {
                    "name": "Jacob Hamman",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/f367da7853266d3c6db7c68b6ac049d7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dig Vijay Kumar Yarlagadda",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_OB6ZZiF1Lv7jrgT04TOZ18S1ThIjvy-mB6p9On81T-Ryv2P0gTOZ9L31GlKjvg30NBKZ0zaPdl3p94cmqXikKcfxul3g94cOlXiUU9VtbKqApa2CjvvzRCq_2XRSx48f4bbNpeOQgtJ?height=180&width=180"
                },
                {
                    "name": "Alice Kra",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/757/datas/profile.jpg"
                },
                {
                    "name": "Akshay Mohan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/92dcb5f41f948d34756cbbfdbbb1e1e1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "android-studio",
                "daydream",
                "ibm-watson",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Alzheimer's Disease is a horrible disease that robs a person of their memory, independence, and overall quality of life.  We want to use virtual reality to help reduce stress, improve cognition, and retain memories.   </p>\n<h2>What it does</h2>\n<p>Immerses a user in a garden where they can grown memory flowers and engage with stimulating music and atmosphere.</p>\n<h2>How we built it</h2>\n<p>Unity </p>\n<h2>Challenges we ran into</h2>\n<p>Implementing voice recognition and painstakingly little details</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Built one of the first apps built on Daydream platform to help Alzheimer's patients.</p>\n<h2>What we learned</h2>\n<p>We learned about daydream platform and Watson SDK for Daydream 3D.</p>\n<h2>What's next for KickALZ</h2>\n<p>Get as many people to use as possible</p>\n<p>Notes:</p>\n<p>1.) The name of your team lead\nJacob Hamman</p>\n<p>2.) The team lead's telephone number\n213-999-9272</p>\n<p>3.) The location where the judges can find your team including the floor number\nRoom E15-359a on 3rd Floor</p>\n<p>4.) The vertical category in which you are competing:\nHuman Well-Being (Education/Health/Wellness/Activism)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAlzheimer's Disease is a horrible disease that robs a person of their memory, independence, and overall quality of life. We want to use virtual reality to help reduce stress, improve cognition, and retain memories. \n\n\n## What it does\n\n\nImmerses a user in a garden where they can grown memory flowers and engage with stimulating music and atmosphere.\n\n\n## How we built it\n\n\nUnity \n\n\n## Challenges we ran into\n\n\nImplementing voice recognition and painstakingly little details\n\n\n## Accomplishments that we're proud of\n\n\nBuilt one of the first apps built on Daydream platform to help Alzheimer's patients.\n\n\n## What we learned\n\n\nWe learned about daydream platform and Watson SDK for Daydream 3D.\n\n\n## What's next for KickALZ\n\n\nGet as many people to use as possible\n\n\nNotes:\n\n\n1.) The name of your team lead\nJacob Hamman\n\n\n2.) The team lead's telephone number\n213-999-9272\n\n\n3.) The location where the judges can find your team including the floor number\nRoom E15-359a on 3rd Floor\n\n\n4.) The vertical category in which you are competing:\nHuman Well-Being (Education/Health/Wellness/Activism)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ice-breakers",
            "title": "Ice Breakers",
            "blurb": "Travel through (literally) Greenland go to places that the laws of physics won't allow you to travel to in real life!",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Alexandra Boghosian",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9e812f84667beae23528ad1d6cd0b3e8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Shaash S",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/92ef2056cb056fc7ab80f8e75c91da4b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Michael Wissner",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/969/datas/profile.jpg"
                },
                {
                    "name": "Qiushuo Wang",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fcb6bf9669232c17664aa009c44a875d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Important information requested</h2>\n<p>Alexandra Boghosian,\n9176700235,\n6th floor, table 13,\nEducation</p>\n<h2>Inspiration</h2>\n<p>I've been sending scientists into the field with 360 cameras to document their work, and I realized that eventually we are going to photograph the planet in 360. and that's going to get boring.</p>\n<p><strong>we are taking people through the ice sheets</strong>\nbecause...</p>\n<p>1) they are remote\n2) they are dangerous\n3) they are dark half the year\n4) they are beautiful\n5) and they are important for sea level rise and climate change</p>\n<h2>What it does</h2>\n<p>We built a model of Greenland from real data that have been collected via satellite and airborne missions. We move around the surface of Greenland in a plane, the way that the majority of data are collected, pointing out important features along the way. We also take you below the ice, so you can discover a place that hasn't seen the atmosphere in over 100,000 years!</p>\n<h2>How I built it</h2>\n<p>Working on it! With real data, and unity</p>\n<h2>Future plans</h2>\n<p>So many... we will tell you about them!</p>\n</div>",
            "content_md": "\n## Important information requested\n\n\nAlexandra Boghosian,\n9176700235,\n6th floor, table 13,\nEducation\n\n\n## Inspiration\n\n\nI've been sending scientists into the field with 360 cameras to document their work, and I realized that eventually we are going to photograph the planet in 360. and that's going to get boring.\n\n\n**we are taking people through the ice sheets**\nbecause...\n\n\n1) they are remote\n2) they are dangerous\n3) they are dark half the year\n4) they are beautiful\n5) and they are important for sea level rise and climate change\n\n\n## What it does\n\n\nWe built a model of Greenland from real data that have been collected via satellite and airborne missions. We move around the surface of Greenland in a plane, the way that the majority of data are collected, pointing out important features along the way. We also take you below the ice, so you can discover a place that hasn't seen the atmosphere in over 100,000 years!\n\n\n## How I built it\n\n\nWorking on it! With real data, and unity\n\n\n## Future plans\n\n\nSo many... we will tell you about them!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dragon-accord",
            "title": "Dragon Accord",
            "blurb": "Augmented Reality meets spatially aware audio painting and sequencing tool for artists.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/195568207?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Scott Niejadlik",
                    "about": "The root of this project was an Idea I came up with at about 4am at the previous hololens hackathon.  It's been stuck in my head and I really wanted to make it a reality.  I was the primary developer, and hololens engineer.  I was working in Unity and C# to help create the immersive audible and visual experience that I imagined.",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GiY3WX8vYskRERZ8mqDEx8TqszGipo1hZzuyH3R4A?height=180&width=180"
                },
                {
                    "name": "Costas Frost",
                    "about": "I worked on the graphics, I created UI elements, animations, particle effects.  Sweet butterflies.",
                    "photo": "https://avatars2.githubusercontent.com/u/7755346?height=180&v=3&width=180"
                },
                {
                    "name": "Matthew Silverstein",
                    "about": "Created all audio clips. ",
                    "photo": "https://www.gravatar.com/avatar/db0f7e561e538ef1c858dcb591e11bc6?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jameson Nash",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8af4d5971a2308b86a94f58fb98129c5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Max Harper",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_tYf_g3ZFZ5rgIhzYRfzS1v-FMGuhDTV1tsUDRND6p-NTelI1OYBD9nzFs-Pgw-V0teM_rFVbr5_3I5hPZDoxvzRwv5_8I570YDo8YqaQYkn23PN_-yyaO8hBzrMKC54acmm7JEV_XbQ?height=180&width=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "logic",
                "microsoft-hololens",
                "midi.js",
                "photoshop",
                "substance",
                "unity",
                "zbrush"
            ],
            "content_html": "<div>\n<p>We felt inspired to create a tool that would allow an artist to paint music in a real world space.  To not only hear the music that they create but also to see it.  </p>\n<p>It utilizes the Hololenses spatial mapping, voice recognition, gesture control, and most importantly 3d spatial audio to create a multi textured environment of audio and colors.  We are basing the audio beats and colors on natural wave forms and color theory to help simulate a visual approach to hearing music.  Mix the with some kick ass animations and effects to tickle your visual and audible cortex</p>\n<p>We built it primarily within Unity and Visual Studio, utilizing Blender for modelling, and Logic Pro X for midi control and pre processing of audio clips.</p>\n<p><a href=\"https://vimeo.com/195568334\" rel=\"nofollow\">https://vimeo.com/195568334</a> -- quick pitch video</p>\n<p><a href=\"https://vimeo.com/195568207\" rel=\"nofollow\">https://vimeo.com/195568207</a> -- raw footage captured during monday demonstrations.</p>\n</div>",
            "content_md": "\nWe felt inspired to create a tool that would allow an artist to paint music in a real world space. To not only hear the music that they create but also to see it. \n\n\nIt utilizes the Hololenses spatial mapping, voice recognition, gesture control, and most importantly 3d spatial audio to create a multi textured environment of audio and colors. We are basing the audio beats and colors on natural wave forms and color theory to help simulate a visual approach to hearing music. Mix the with some kick ass animations and effects to tickle your visual and audible cortex\n\n\nWe built it primarily within Unity and Visual Studio, utilizing Blender for modelling, and Logic Pro X for midi control and pre processing of audio clips.\n\n\n<https://vimeo.com/195568334> -- quick pitch video\n\n\n<https://vimeo.com/195568207> -- raw footage captured during monday demonstrations.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/superbugs-fighting-antimicrobial-resistance-no9zej",
            "title": "Superbugs: fighting antimicrobial resistance ",
            "blurb": "Educational immersive experience which teaches antimicrobial resistance ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/qkj3MDvuB0I?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "antibiotic model extracted from  Protein databank",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/420/904/datas/original.JPG"
                },
                {
                    "title": "unity game play of immersive 360  scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/420/905/datas/original.JPG"
                },
                {
                    "title": "Dr. Shu.  researcher of the star shaped protein ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/420/906/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "Steve Doroff",
                    "about": "Built out Unity environment and developed and modified all C# code.",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1471804548/fyik2qhcgspzfww7xokn.jpg?height=180&width=180"
                },
                {
                    "name": "Sam Spaeth",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/401/938/datas/profile.jpg"
                },
                {
                    "name": "Karen Zastudil",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_FiVC9702c3w4U3y1W5Eg92e2cbO4RCD15Gaj92mlHCodX6VPwLomZuVxJdY6Z5206h4yJwyWo7tP?height=180&width=180"
                }
            ],
            "built_with": [
                "3dstudiomax",
                "unity",
                "weblabviewerlit",
                "weblabviewerlite",
                "zbrush"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Mankind is under attack. D r  Shu Jie LAM. PhD  is researching a new line of treatment for antimicrobial therapy. Her research is a new hope for the growing MRSA like bugs. Her recent discoveries inspired our immersive experience.  </p>\n<h2>What it does</h2>\n<p>Immersive Experience where a video of a Microbiologist  open and closes with some educational information. The viewer experiences an immersive environment where they are represented as a bacterial entity (rod shaped gram negative bacteria)  treated with antibiotics. We (the bacteria) are invaded by antibiotics, which we have learned to be resistant to. Unfortunately a new line of nanotechnology of tiny star shaped protein molecules, have found a way to destroy our cell walls. <em>*<em>They won this battle but is this the end?  *</em></em>\n<a href=\"https://www.researchgate.net/publication/308024491_Combating_multidrug-resistant_Gram-negative_bacteria_with_structurally_nanoengineered_antimicrobial_peptide_polymers\" rel=\"nofollow\">link</a></p>\n<h2>How I built</h2>\n<p>Unity build for a Tango.\nVirtual environment was built from Unity's Survival Shooter game and the project Tango dev kit. Codebase has  than 50% of code and all assets modeled and built for this project. Within the virtual environment a customized video component of Dr. Shu has been added for educational value.<br/>\nUnity and C# were used for codebase, digital assets were built in 3DSmax and Zbrush. Textures created in Zbrush and photoshop.  </p>\n<h2>Challenges I ran into</h2>\n<p>Small team working together to create a short immersive experience. Learning functions and capabilities of various part of software and hardware to complete the goals in the allotted time available. </p>\n<ul>\n<li>resources with appropriate skills</li>\n<li>complexity of the technology component integration </li>\n<li>documentation of emerging technologies</li>\n</ul>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Using emerging technologies to help educate people regarding an issue that effects all of Mankind. We developed educational material that used new technology to make engaging immersive content. Exploring way to develop an engaging story for microbial resistance. Being an immersive experience, it is platform agnostic. The material also presented in a multimodal manner that is not restricted to demographic or previous knowledge.</p>\n<h2>What I learned</h2>\n<p>Unity development. Thinking of ways of making learning engaging. </p>\n<h2>What's next for Superbugs: fighting antimicrobial resistance</h2>\n<p>Scene 2 where the next line of antibiotics are effective against the bacteria</p>\n<p>The name of your team lead\n      -Sam Spaeth </p>\n<p>2.) The team lead's telephone number\n        914-588-6149</p>\n<p>3.) The location where the judges can find your team including the floor number\n       - Foor 6, table 5 </p>\n<p>4.) The vertical category in which you are competing\n       - Human Well-Being (Education/Health/Wellness/Activism)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMankind is under attack. D r Shu Jie LAM. PhD is researching a new line of treatment for antimicrobial therapy. Her research is a new hope for the growing MRSA like bugs. Her recent discoveries inspired our immersive experience. \n\n\n## What it does\n\n\nImmersive Experience where a video of a Microbiologist open and closes with some educational information. The viewer experiences an immersive environment where they are represented as a bacterial entity (rod shaped gram negative bacteria) treated with antibiotics. We (the bacteria) are invaded by antibiotics, which we have learned to be resistant to. Unfortunately a new line of nanotechnology of tiny star shaped protein molecules, have found a way to destroy our cell walls. ***They won this battle but is this the end? ***\n[link](https://www.researchgate.net/publication/308024491_Combating_multidrug-resistant_Gram-negative_bacteria_with_structurally_nanoengineered_antimicrobial_peptide_polymers)\n\n\n## How I built\n\n\nUnity build for a Tango.\nVirtual environment was built from Unity's Survival Shooter game and the project Tango dev kit. Codebase has than 50% of code and all assets modeled and built for this project. Within the virtual environment a customized video component of Dr. Shu has been added for educational value.  \n\nUnity and C# were used for codebase, digital assets were built in 3DSmax and Zbrush. Textures created in Zbrush and photoshop. \n\n\n## Challenges I ran into\n\n\nSmall team working together to create a short immersive experience. Learning functions and capabilities of various part of software and hardware to complete the goals in the allotted time available. \n\n\n* resources with appropriate skills\n* complexity of the technology component integration\n* documentation of emerging technologies\n\n\n## Accomplishments that I'm proud of\n\n\nUsing emerging technologies to help educate people regarding an issue that effects all of Mankind. We developed educational material that used new technology to make engaging immersive content. Exploring way to develop an engaging story for microbial resistance. Being an immersive experience, it is platform agnostic. The material also presented in a multimodal manner that is not restricted to demographic or previous knowledge.\n\n\n## What I learned\n\n\nUnity development. Thinking of ways of making learning engaging. \n\n\n## What's next for Superbugs: fighting antimicrobial resistance\n\n\nScene 2 where the next line of antibiotics are effective against the bacteria\n\n\nThe name of your team lead\n -Sam Spaeth \n\n\n2.) The team lead's telephone number\n 914-588-6149\n\n\n3.) The location where the judges can find your team including the floor number\n - Foor 6, table 5 \n\n\n4.) The vertical category in which you are competing\n - Human Well-Being (Education/Health/Wellness/Activism)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/blabla-bw4a0n",
            "title": "Hamsa",
            "blurb": "Asymmetrical social experience that plays with two different perspectives",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Nai-chen Yang",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3ee982465305cf9fe73f5d9bc827ae7e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kylin Chen",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/7727275?height=180&v=4&width=180"
                },
                {
                    "name": "Yun Li",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5103AQEGfo_z8NBoIQ/profile-displayphoto-shrink_100_100/0?e=1545868800&height=180&t=_mr_JHs9ipQRxbUHeAQvxXjgYNH0T4qvKtep7wOxKBU&v=beta&width=180"
                },
                {
                    "name": "Yue(Melody) Hu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2b3357e35ec39e92e99dbf0712c36ad7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "htc-vive",
                "leap-motion",
                "unity"
            ],
            "content_html": "<div>\n<p>1)Team Lead : Yun Li</p>\n<p>2)Contact : 2012755569</p>\n<p>3)MIT Media Lab 6th floor table 55</p>\n<p>4) Entertainment story telling+ Human Connection</p>\n<p>Github Link : <a href=\"https://github.com/yunllii/Hamsa_AllCodes\" rel=\"nofollow\">https://github.com/yunllii/Hamsa_AllCodes</a></p>\n<p>Public Assets Used : </p>\n<p>VRTK : <a href=\"https://github.com/thestonefox/SteamVR_Unity_Toolkit\" rel=\"nofollow\">https://github.com/thestonefox/SteamVR_Unity_Toolkit</a> 20%</p>\n<h2>Inspiration</h2>\n<p>There are something unique about our hands. We are able to express a lot of emotions from hand gestures a long. By isolating gestural movements and combine it with VR, we want to see how people would interact with each other through two uniquely different perspectives.</p>\n<h2>What it does</h2>\n<p>Masma is an asymmetrical social experience between two people in both reality and VR world. One person, using HTC Vive, is connected with another leap motion user. That person in VR can move around and have a more detailed knowledge of the surround while person with leap motion have broader view of the environment.  Utilizing this unbalanced relationship, our team aimed to create interactive objects and tasks for both players to collaborate, to compete and to bond with each other in this short yet intimating experience. </p>\n<h2>How I built it</h2>\n<p>Our team first connected leap motion's control with HTC vive using unity, so leap motion user can have a bird view of the virtual world and the Vive player, while the Vive player can be immersed in the virtual reality and see the giant hand movement of leap motion user, simultaneously. Then, our team designed a series of experiment of interactive spaces utilizing this set up while our artists created an aesthetic and narrative that fit. Our developer would take these ideas and materialize them in code, using Unity and C Sharp. After some iterations and user-testing, our project was finally ready for presentation.</p>\n<h2>Challenges I ran into</h2>\n<p>The first challenges our team faced was time. Our team have many brillant ideas that we simply don't have enough time to implement all. We had to make choices in priorities and created something essential to our core idealogy for people to experience, in 48 hours.</p>\n<p>The other challenge is a bit silly. We were using a developer's version of HTC Vive. Despite of its cool name and all the swags it brings, the dev kit is highly unstable and increases our time cost dramatically. If we would do this over again, it would be a better idea to just borrow someone else's kit.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We created a very smooth transition between the real world and virtual reality. The leap motion user and the vive user can read each other's movement perfectly and are able to express themselves through body movements.</p>\n<p>We also managed to create a semi-realistic low poly style that's easy to work with but high expressive. We were able to create compelling and enjoyable visuals.</p>\n<h2>What I learned</h2>\n<ol>\n<li>It requires a very unique way of thinking when design in VR. Something can't simply being imagined without experiences and the only way of learning is through rapid prototyping and iterating. </li>\n<li>Our developer learnt to create a perfectly smooth communication between leap motion and HTC vive through Unity.</li>\n<li>For our team, it was the best approach to design our developer's schedule. We learnt it the hard way and realized that it is not a good design if we can't implement it. </li>\n</ol>\n<h2>What's next for Hamsa</h2>\n<p>We hope to improve the complexity and diversity of our scenery and add more interactive objects for people to play with. We also want to create a more elaborate tutorial and introduction of our world setting so people can be more invested to the roles they play. </p>\n</div>",
            "content_md": "\n1)Team Lead : Yun Li\n\n\n2)Contact : 2012755569\n\n\n3)MIT Media Lab 6th floor table 55\n\n\n4) Entertainment story telling+ Human Connection\n\n\nGithub Link : <https://github.com/yunllii/Hamsa_AllCodes>\n\n\nPublic Assets Used : \n\n\nVRTK : <https://github.com/thestonefox/SteamVR_Unity_Toolkit> 20%\n\n\n## Inspiration\n\n\nThere are something unique about our hands. We are able to express a lot of emotions from hand gestures a long. By isolating gestural movements and combine it with VR, we want to see how people would interact with each other through two uniquely different perspectives.\n\n\n## What it does\n\n\nMasma is an asymmetrical social experience between two people in both reality and VR world. One person, using HTC Vive, is connected with another leap motion user. That person in VR can move around and have a more detailed knowledge of the surround while person with leap motion have broader view of the environment. Utilizing this unbalanced relationship, our team aimed to create interactive objects and tasks for both players to collaborate, to compete and to bond with each other in this short yet intimating experience. \n\n\n## How I built it\n\n\nOur team first connected leap motion's control with HTC vive using unity, so leap motion user can have a bird view of the virtual world and the Vive player, while the Vive player can be immersed in the virtual reality and see the giant hand movement of leap motion user, simultaneously. Then, our team designed a series of experiment of interactive spaces utilizing this set up while our artists created an aesthetic and narrative that fit. Our developer would take these ideas and materialize them in code, using Unity and C Sharp. After some iterations and user-testing, our project was finally ready for presentation.\n\n\n## Challenges I ran into\n\n\nThe first challenges our team faced was time. Our team have many brillant ideas that we simply don't have enough time to implement all. We had to make choices in priorities and created something essential to our core idealogy for people to experience, in 48 hours.\n\n\nThe other challenge is a bit silly. We were using a developer's version of HTC Vive. Despite of its cool name and all the swags it brings, the dev kit is highly unstable and increases our time cost dramatically. If we would do this over again, it would be a better idea to just borrow someone else's kit.\n\n\n## Accomplishments that I'm proud of\n\n\nWe created a very smooth transition between the real world and virtual reality. The leap motion user and the vive user can read each other's movement perfectly and are able to express themselves through body movements.\n\n\nWe also managed to create a semi-realistic low poly style that's easy to work with but high expressive. We were able to create compelling and enjoyable visuals.\n\n\n## What I learned\n\n\n1. It requires a very unique way of thinking when design in VR. Something can't simply being imagined without experiences and the only way of learning is through rapid prototyping and iterating.\n2. Our developer learnt to create a perfectly smooth communication between leap motion and HTC vive through Unity.\n3. For our team, it was the best approach to design our developer's schedule. We learnt it the hard way and realized that it is not a good design if we can't implement it.\n\n\n## What's next for Hamsa\n\n\nWe hope to improve the complexity and diversity of our scenery and add more interactive objects for people to play with. We also want to create a more elaborate tutorial and introduction of our world setting so people can be more invested to the roles they play. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/holocaptions",
            "title": "HoloCaptions",
            "blurb": "Sense of Sound Direction & Real Life Closed Captioning for the Hearing Impaired",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "David Tran",
                    "about": "I worked on facial recognition, the UI of the application in Unity, and on overall integration in the application.",
                    "photo": "https://www.gravatar.com/avatar/ddf01b72f816e519d4c73ef5ffbc8553?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Johan Ospina",
                    "about": "I worked on the backend for voice dictation and HoloLens gaze interaction. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/678/datas/profile.jpg"
                },
                {
                    "name": "Tess Gauthier",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/04810efd671a2f60d187282647f7d97e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Makivic",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/9288823?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "love",
                "microsoft-hololens",
                "unity",
                "universal-windows-platform"
            ],
            "content_html": "<div>\n<p>Those with hearing impairments might not be able to utilize the same auditory cues that we use every day, or experience conversations in the same way. We want to provide an auditory compass and real life closed captioning for indoor use for those with hearing impairments. Utilizing the multiple microphones and the AR capabilities of the HoloLens, as someone approaches the user from behind, the hololens could provide visual indication of sound coming from the sides or behind, and as the user turns to face them, closed captioning could be displayed by the speaker for an improved conversational experience.</p>\n<p>Our Team is at Table 30 on the 6th Floor, and our vertical is Human Connection/Social. </p>\n<p>Team Lead: Johan Ospina</p>\n<p><a href=\"https://github.com/johanos/ProjectMurphy\" rel=\"nofollow\">https://github.com/johanos/ProjectMurphy</a></p>\n</div>",
            "content_md": "\nThose with hearing impairments might not be able to utilize the same auditory cues that we use every day, or experience conversations in the same way. We want to provide an auditory compass and real life closed captioning for indoor use for those with hearing impairments. Utilizing the multiple microphones and the AR capabilities of the HoloLens, as someone approaches the user from behind, the hololens could provide visual indication of sound coming from the sides or behind, and as the user turns to face them, closed captioning could be displayed by the speaker for an improved conversational experience.\n\n\nOur Team is at Table 30 on the 6th Floor, and our vertical is Human Connection/Social. \n\n\nTeam Lead: Johan Ospina\n\n\n<https://github.com/johanos/ProjectMurphy>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/lazyeye",
            "title": "LazyEye",
            "blurb": "An automated health care study  providing set of VR Apps that can be used to assess and of Amblyopia",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Ken Yee",
                    "about": "Modified the Google Daydream SDK so you can run the left/right cameras w/ separate layers and modified one of their demos to help improve LazyEye.",
                    "photo": "https://www.gravatar.com/avatar/e1b16b34cd4439cd457ce15a493e8280?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "preetish Kakkar",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/888/datas/profile.jpg"
                },
                {
                    "name": "ildar iakubov",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/14405387?height=180&v=4&width=180"
                },
                {
                    "name": "Paul Senatillaka",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/826073?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "touchdesigner",
                "unity"
            ],
            "content_html": "<div>\n<h2>Vertical Name - HealthCare</h2>\n<p><br/></p>\n<h2>Automated health care VR Apps for evaluation and treatment for Amblyopia</h2>\n<p>Amblyopia is the most common cause of visual impairment among children and the most common cause of monocular visual impairment among young and middle-aged adults.\n<br/><br/>\nCommon treatment is patching (wearing a patch over a stronger eye) or Atropine (drops of drug ). Those treatment have several drawbacks - age limitation and so-called reverse amblyopia among them.\n<br/><br/>\nAn attempt to use VR to deliver specific stimuli without harming the healthy eye, or overcome the age limit by increasing the effect with all those incredible tools is evident for anyone familiar with both VR technology and ambylopia. There is even one startup, Vivid Vision (ex Diplopia ). They created special VR games, and they sell licenses to clinics, so the clinics could provide a service to their clients. \n<br/><br/>\nOur first obvious goal is to make such treatment accessible, so that any kid or adult with amblyopia could use it. Our multi platform solution includes GearVR, HTC Vive and Cardboard.\n<br/><br/>\nTo make our solution a full automated healthcare case we need automated diagnosis. Lang stereo test, a broadly used amblyopia test can be implemented inside VR, and so we did, adding this feature to our app. But this test is reliably exclusive only to one of the three amblyopia types. The most advanced technique to test other types is retinal birefringence scanning. We read two science papers describing this method, and both of them note that it is similar to the techniques you would use for eye-tracking interfaces. That means that if we hack an eye-tracking system we could use it for amblyopia diagnostics.\n<br/><br/>\nOur environment adds many tools to collaborate on amblyopia research projects, including testing, same treatment games mentioned above, adding treatment using any user content (eg movies, pictures, books) and treatment using any app on your computer. All those procedures have statistics and are programmable to follow treatment schedule. \n<br/><br/>\nOur system consist of App that lets user play games, watch youtube video and use any desktop app in VR, Based upon diagnosis and user inputs, we apply various effect to left or right eye. Effects such as Blur, reduced brightness, saturation are applied to strong eye, this helps our brains to concentrate more on side which has better view helping the weaker eye to be used more. \n<br/><br/>\nWe used TouchDesigner to create a demo for HTC Vive and Unity3D to demo for cardboard and GearVR. We used Unity3D post processing image effects as well as added our own effects to control brightness, saturation, Hue. Most of the challenge was writing Shaders that could do custom effects based upon research paper we read. \n<br/><br/>\nWe learned more about lazy eye problem and how doctors used to solve it in past. We also read various research paper that lead us to use Unity3D image effects and required us to learn various shading techniques and integrating shaders in Unity3D. \n<br/><br/>\nFinally we want our App system to act as a platform that can be used to watch any VR content and play any VR games so that it can be used for treatment of Amblyopia while users can enjoy their favorite VR content.\n<br/><br/></p>\n<p><b>Github links</b>\n<br/>\nWe have used two separate tools for prototyping <br/>\n<br/>\nHTC Vive &amp; Oculus Rift demo using TouchDesigner<br/>\n<a href=\"https://github.com/githubildar/LazyEye\" rel=\"nofollow\">https://github.com/githubildar/LazyEye</a>\n<br/>\nGearVR and Cardboard app using Unity3D used as VR Video Player (With Various ImageShader effects)<br/>\n<a href=\"https://github.com/preetishkakkar/VRCinemaHackton\" rel=\"nofollow\">https://github.com/preetishkakkar/VRCinemaHackton</a>\n<br/>\nSimple VR Demo with Blur and Brightness Shader effects<br/>\n<a href=\"https://github.com/kenyee/vrhackathonOct2016\" rel=\"nofollow\">https://github.com/kenyee/vrhackathonOct2016</a></p>\n<p><br/><br/></p>\n<p><b>Contact Info</b></p>\n<p>ildar iakubov : 857-247-9473</p>\n<p>Preetish Kakkar : 515-771-5771</p>\n</div>",
            "content_md": "\n## Vertical Name - HealthCare\n\n\n  \n\n\n\n## Automated health care VR Apps for evaluation and treatment for Amblyopia\n\n\nAmblyopia is the most common cause of visual impairment among children and the most common cause of monocular visual impairment among young and middle-aged adults.\n  \n  \n\nCommon treatment is patching (wearing a patch over a stronger eye) or Atropine (drops of drug ). Those treatment have several drawbacks - age limitation and so-called reverse amblyopia among them.\n  \n  \n\nAn attempt to use VR to deliver specific stimuli without harming the healthy eye, or overcome the age limit by increasing the effect with all those incredible tools is evident for anyone familiar with both VR technology and ambylopia. There is even one startup, Vivid Vision (ex Diplopia ). They created special VR games, and they sell licenses to clinics, so the clinics could provide a service to their clients. \n  \n  \n\nOur first obvious goal is to make such treatment accessible, so that any kid or adult with amblyopia could use it. Our multi platform solution includes GearVR, HTC Vive and Cardboard.\n  \n  \n\nTo make our solution a full automated healthcare case we need automated diagnosis. Lang stereo test, a broadly used amblyopia test can be implemented inside VR, and so we did, adding this feature to our app. But this test is reliably exclusive only to one of the three amblyopia types. The most advanced technique to test other types is retinal birefringence scanning. We read two science papers describing this method, and both of them note that it is similar to the techniques you would use for eye-tracking interfaces. That means that if we hack an eye-tracking system we could use it for amblyopia diagnostics.\n  \n  \n\nOur environment adds many tools to collaborate on amblyopia research projects, including testing, same treatment games mentioned above, adding treatment using any user content (eg movies, pictures, books) and treatment using any app on your computer. All those procedures have statistics and are programmable to follow treatment schedule. \n  \n  \n\nOur system consist of App that lets user play games, watch youtube video and use any desktop app in VR, Based upon diagnosis and user inputs, we apply various effect to left or right eye. Effects such as Blur, reduced brightness, saturation are applied to strong eye, this helps our brains to concentrate more on side which has better view helping the weaker eye to be used more. \n  \n  \n\nWe used TouchDesigner to create a demo for HTC Vive and Unity3D to demo for cardboard and GearVR. We used Unity3D post processing image effects as well as added our own effects to control brightness, saturation, Hue. Most of the challenge was writing Shaders that could do custom effects based upon research paper we read. \n  \n  \n\nWe learned more about lazy eye problem and how doctors used to solve it in past. We also read various research paper that lead us to use Unity3D image effects and required us to learn various shading techniques and integrating shaders in Unity3D. \n  \n  \n\nFinally we want our App system to act as a platform that can be used to watch any VR content and play any VR games so that it can be used for treatment of Amblyopia while users can enjoy their favorite VR content.\n  \n  \n\n\n\n**Github links**\n  \n\nWe have used two separate tools for prototyping   \n\n  \n\nHTC Vive & Oculus Rift demo using TouchDesigner  \n\n<https://github.com/githubildar/LazyEye>\n  \n\nGearVR and Cardboard app using Unity3D used as VR Video Player (With Various ImageShader effects)  \n\n<https://github.com/preetishkakkar/VRCinemaHackton>\n  \n\nSimple VR Demo with Blur and Brightness Shader effects  \n\n<https://github.com/kenyee/vrhackathonOct2016>\n\n\n  \n  \n\n\n\n**Contact Info**\n\n\nildar iakubov : 857-247-9473\n\n\nPreetish Kakkar : 515-771-5771\n\n\n"
        },
        {
            "source": "https://devpost.com/software/annexnation-0hcrqv",
            "title": "AnnexNation",
            "blurb": "A game in which you work collaboratively or competitvely to layer AR art over the real world.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Marcel .",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/959/177/datas/profile.png"
                },
                {
                    "name": "Heather Albano",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5d02a13c61098e8226505fa6cb5d912c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<p>Team Lead: Arya Afshin Perjia Mahini\n949-701-1259\nFloor 6 Table 5</p>\n<p>All the world's a stage...or it could be. AR gives us the opportunity to add a layer of shared fantasy to the physical world we already have. </p>\n<p>Since the world's a bigger stage than we can conquer in one weekend, let's prove the concept in one room. </p>\n<p>In AnnexNation, you get to wield magic. Take control of an element - fire, water, or nature. Claim a space in the room - surface, wall, floor, or midair. Layer illusion over the mundane - create waterfalls, vines, volcanoes. Then let the next player make their mark...and see what happens when your worlds collide. Struggle for dominance, or make art together. </p>\n</div>",
            "content_md": "\nTeam Lead: Arya Afshin Perjia Mahini\n949-701-1259\nFloor 6 Table 5\n\n\nAll the world's a stage...or it could be. AR gives us the opportunity to add a layer of shared fantasy to the physical world we already have. \n\n\nSince the world's a bigger stage than we can conquer in one weekend, let's prove the concept in one room. \n\n\nIn AnnexNation, you get to wield magic. Take control of an element - fire, water, or nature. Claim a space in the room - surface, wall, floor, or midair. Layer illusion over the mundane - create waterfalls, vines, volcanoes. Then let the next player make their mark...and see what happens when your worlds collide. Struggle for dominance, or make art together. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/data-tree-modeler",
            "title": "Data Tree Modeler",
            "blurb": "A program to visualize large scale collaborative data",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/LXIt47NgGdQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Bonsai Data Tree 1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/455/927/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Adam Sauer",
                    "about": "I contributed to the ideation of the idea, and the scripting for the information into roots and branches.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/544/762/datas/profile.jpg"
                },
                {
                    "name": "Sarthak Giri",
                    "about": "I helped with the ideation process. I also worked on interaction in the 3d environment.",
                    "photo": "https://www.gravatar.com/avatar/db9f3d2f8d170ac4e8cd8c4519fa3b10?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ethan Anderson",
                    "about": "I created the visual effects for our application including the 3D environment. I also contributed to the ideation of the application and it's functionality.",
                    "photo": "https://www.gravatar.com/avatar/cecbb436165fdef0373775a84cb263a4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Aravind Elangovan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8a2d93ccac2f66781d6c740b342d6b30?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tim Besard",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/311f28d70bb1de3b0e9bb55e9d5fd26d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<p>Our modeler takes coding data structures and visualizes it as a tree. By doing so we are eliminating the complexity and relating it to something we see everyday. The tree structure is something that we inherently understand because we have evolved to understand objects and structures in nature. Our modeler also interacts with the tree in a very meaningful way. Like the roots of a tree, some of which are filament like and some very thick and strong, the data structures are the same way. We have rendered that effect by changing the opacity of the trace code depending on the use. Similarly as a plant/leaves grows old, it slowly turns from green (young, new) to brown (old, dead), the user is able to see that in the tree modeler. Zooming in and out gives the depth of information that the person is looking obtain.</p>\n</div>",
            "content_md": "\nOur modeler takes coding data structures and visualizes it as a tree. By doing so we are eliminating the complexity and relating it to something we see everyday. The tree structure is something that we inherently understand because we have evolved to understand objects and structures in nature. Our modeler also interacts with the tree in a very meaningful way. Like the roots of a tree, some of which are filament like and some very thick and strong, the data structures are the same way. We have rendered that effect by changing the opacity of the trace code depending on the use. Similarly as a plant/leaves grows old, it slowly turns from green (young, new) to brown (old, dead), the user is able to see that in the tree modeler. Zooming in and out gives the depth of information that the person is looking obtain.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/helpinghand-nwm0k6",
            "title": "HelpingHand",
            "blurb": "Portable therapeutic AR application for amputees for treatment of phantom limb pain",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Gf1Cn7w-qfQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Julien BOUVIER-VOLAILLE",
                    "about": "I worked on java module to collecte iMotion data extracted from Shimmer devices, filter and generate events. I integrated those event in an Unity demo.",
                    "photo": "https://www.gravatar.com/avatar/4d3a070fe16c728138c36ff037328e29?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Mehdi Lefouili",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7001f02a5195e4e9015b2eff6155df83?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lindsay Lin",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/776cf39df980711e80fc02317eb64649?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jingru Guo",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8f94bd1b7b4449f359bfed93d62b53a1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "emg",
                "imotion",
                "java",
                "microsoft-hololens",
                "shimmer3",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>There are nearly 2 million people living with limb loss in the United States. Approximately 60 to 80% of such amputees experience phantom sensations in their amputated limb, and the majority of the sensations are painful (this pain is called \"phantom limb pain\" (PLP)). Pharmacotherapy, surgery, and traditional adjuvant therapy (e.g. physiotherapy, massage, and ultrasound) are also not consistently effective. There has been clinical research showing that mirror therapy -- a way of positioning mirrors so that it visually convinces the amputee that he/she still has the missing limb -- is effective for some patients. We believe that an AR system that enables myoelectric control of a virtual limb and provides a visual simulation of the missing limb will be an effective and useful tool in the arsenal of treatments against PLP. There have been clinical studies that demonstrate high levels of effectiveness for AR in reducing PLP (See, e.g., <a href=\"http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full\" rel=\"nofollow\">http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19191061\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pubmed/19191061</a>) but there are currently no commercial, public  implementations out there. Moreover, we wanted to leverage the benefits of computational rehabilitation systems in being able to track patient progress, adjust task difficulty, and engage patients.</p>\n<p>In addition to visualization of the lost limb, we also want to enable the virtual arm to interact with the real world in useful applications. We believe this will be more convincing and effective for patients who are suffering PLP. Also, in the long run, perhaps the AR system will be able to transcend therapy and enhance day-to-day functionality of people who are missing limbs.</p>\n<h2>What it does</h2>\n<p>Our HoloLens application uses myoelectric input from the iMotions Shimmer3 ECG/EMG sensor to predict the amputee's intended muscle movements. We use this input to create an AR visualization of a hand that moves in interaction with the real world. The ultimate experience gives amputees the illusion that they have \"regained\" their lost limb. This responsive visual feedback helps patients slowly change their inner cortical map (an internal brain map that keeps track of limbs) and reduce PLP. </p>\n<p>We also enabled the virtual limb to be able to impact the real world. For example, we can launch music from the virtual limb directly on a computer in the real world.</p>\n<h2>How we built it</h2>\n<p>Our project is open source. We've created a backend for the iMotion platform to send data to the HoloLens. This backend can be used in conjunction with any sensor supported by the iMotion platform and is used to push events to the HoloLens. This technology can be applied ubiquitously by other developers to create  projects in the HoloLens featuring iMotion biometric sensors. We also created an algorithm to remove noise and detect changes in muscle movements for the iMotion Shimmer3 ECG/EMG. This algorithm can also be used by other developers to create an AR experience that is linked to myoelectric signals. </p>\n<p>We wanted the graphics to be immersive thus we used a realistic looking hand. Our Hololens application is built with Unity and is able to pull and push events from and to the real world. We have a complete chain of event management that relies on an local backend, a local server and external server.</p>\n<h2>Challenges we ran into</h2>\n<p>The iMotion Shimmer3 did not work at the beginning \u2014 it took a lot of tinkering to get it to start working. \nInherant complexity to connect all application modules: Shimmer, iMotion plateform, Hololens, backends and servers \nWe were not experts in Unity or the HoloLens so we had a steep learning curve. \nThe HoloLens SDK runs only on Windows and we only had one Windows computer. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We made the end to end connectivity from the iMotion sensor to the HoloLens!\nWe have a working demo!</p>\n<h2>What we learned</h2>\n<p>We started off knowing very little about Unity and the HoloLens. In the past couple of days, we learned a lot about Unity and the HoloLens! We also learned how to incorporate iMotion sensors with the HoloLens.</p>\n<h2>What's next for HelpingHand</h2>\n<p>We did not have time to build all the features that we wanted. We currently have simple actions and only one interaction between the virtual arm and a real object. We want to add: (1) a full range of motions and (2) more actions between virtual arm and real objects. Ultimately, it would be cool to have the virtual limb be able to interact with many real world smart objects.</p>\n<h2>Notes</h2>\n<p><strong>Team Lead:</strong> Julien Bouvier</p>\n<p><strong>Phone:</strong> 781-298-8165</p>\n<p><strong>Location:</strong> Room E14-674, Table 38</p>\n<p><strong>Vertical Category:</strong> Human Well-Being (Education/Health/Wellness/Activism)</p>\n<p><strong>GitHub:</strong> <a href=\"https://github.com/bouviervj/RealityVirtually_HelpingHand\" rel=\"nofollow\">https://github.com/bouviervj/RealityVirtually_HelpingHand</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThere are nearly 2 million people living with limb loss in the United States. Approximately 60 to 80% of such amputees experience phantom sensations in their amputated limb, and the majority of the sensations are painful (this pain is called \"phantom limb pain\" (PLP)). Pharmacotherapy, surgery, and traditional adjuvant therapy (e.g. physiotherapy, massage, and ultrasound) are also not consistently effective. There has been clinical research showing that mirror therapy -- a way of positioning mirrors so that it visually convinces the amputee that he/she still has the missing limb -- is effective for some patients. We believe that an AR system that enables myoelectric control of a virtual limb and provides a visual simulation of the missing limb will be an effective and useful tool in the arsenal of treatments against PLP. There have been clinical studies that demonstrate high levels of effectiveness for AR in reducing PLP (See, e.g., <http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full>, <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/>, <https://www.ncbi.nlm.nih.gov/pubmed/19191061>) but there are currently no commercial, public implementations out there. Moreover, we wanted to leverage the benefits of computational rehabilitation systems in being able to track patient progress, adjust task difficulty, and engage patients.\n\n\nIn addition to visualization of the lost limb, we also want to enable the virtual arm to interact with the real world in useful applications. We believe this will be more convincing and effective for patients who are suffering PLP. Also, in the long run, perhaps the AR system will be able to transcend therapy and enhance day-to-day functionality of people who are missing limbs.\n\n\n## What it does\n\n\nOur HoloLens application uses myoelectric input from the iMotions Shimmer3 ECG/EMG sensor to predict the amputee's intended muscle movements. We use this input to create an AR visualization of a hand that moves in interaction with the real world. The ultimate experience gives amputees the illusion that they have \"regained\" their lost limb. This responsive visual feedback helps patients slowly change their inner cortical map (an internal brain map that keeps track of limbs) and reduce PLP. \n\n\nWe also enabled the virtual limb to be able to impact the real world. For example, we can launch music from the virtual limb directly on a computer in the real world.\n\n\n## How we built it\n\n\nOur project is open source. We've created a backend for the iMotion platform to send data to the HoloLens. This backend can be used in conjunction with any sensor supported by the iMotion platform and is used to push events to the HoloLens. This technology can be applied ubiquitously by other developers to create projects in the HoloLens featuring iMotion biometric sensors. We also created an algorithm to remove noise and detect changes in muscle movements for the iMotion Shimmer3 ECG/EMG. This algorithm can also be used by other developers to create an AR experience that is linked to myoelectric signals. \n\n\nWe wanted the graphics to be immersive thus we used a realistic looking hand. Our Hololens application is built with Unity and is able to pull and push events from and to the real world. We have a complete chain of event management that relies on an local backend, a local server and external server.\n\n\n## Challenges we ran into\n\n\nThe iMotion Shimmer3 did not work at the beginning \u2014 it took a lot of tinkering to get it to start working. \nInherant complexity to connect all application modules: Shimmer, iMotion plateform, Hololens, backends and servers \nWe were not experts in Unity or the HoloLens so we had a steep learning curve. \nThe HoloLens SDK runs only on Windows and we only had one Windows computer. \n\n\n## Accomplishments that we're proud of\n\n\nWe made the end to end connectivity from the iMotion sensor to the HoloLens!\nWe have a working demo!\n\n\n## What we learned\n\n\nWe started off knowing very little about Unity and the HoloLens. In the past couple of days, we learned a lot about Unity and the HoloLens! We also learned how to incorporate iMotion sensors with the HoloLens.\n\n\n## What's next for HelpingHand\n\n\nWe did not have time to build all the features that we wanted. We currently have simple actions and only one interaction between the virtual arm and a real object. We want to add: (1) a full range of motions and (2) more actions between virtual arm and real objects. Ultimately, it would be cool to have the virtual limb be able to interact with many real world smart objects.\n\n\n## Notes\n\n\n**Team Lead:** Julien Bouvier\n\n\n**Phone:** 781-298-8165\n\n\n**Location:** Room E14-674, Table 38\n\n\n**Vertical Category:** Human Well-Being (Education/Health/Wellness/Activism)\n\n\n**GitHub:** <https://github.com/bouviervj/RealityVirtually_HelpingHand>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/wanderers",
            "title": "Wanderer",
            "blurb": "We are set out to alleviate anxiety on transportation through sight, sound, and smell. ",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Birdseye Screenshot of Scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/598/datas/original.PNG"
                },
                {
                    "title": "Birdseye Screenshot of Scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/608/datas/original.PNG"
                },
                {
                    "title": "This is a picture of the headset before we added the scent capsules to the plates. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/875/datas/original.jpg"
                },
                {
                    "title": "Manisha combining scents. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/876/datas/original.jpg"
                },
                {
                    "title": "The early version of the scent release bead. This is a small bead that easily dissolves. We instead used gelatin pill capsules. ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/877/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Orhan Celiker",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/95faf06a8c2801ddf1cf2d515880c209?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Peter Seger",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/998/datas/profile.jpg"
                },
                {
                    "name": "Wiley Corning",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7a33574e3f20ae3d61f83b92cc718e14?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Caitlin Kearney",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/243/208/datas/profile.jpg"
                }
            ],
            "built_with": [
                "arduino",
                "c#",
                "daydream",
                "unity",
                "vr"
            ],
            "content_html": "<div>\n<p><strong>Judging Info</strong><br/>\n-Team Lead: Caitlin Kearney<br/>\n-Team lead's phone number: three-five-too-281-7901<br/>\n-Location: E15-341 (Nagashima Room)<br/>\n-Vertical category: Human Well-Being<br/></p>\n<h1>WANDERER</h1>\n<h2>The Inspiration</h2>\n<p>While getting off the T at Kendall/MIT Station in route to the hackathon, Caitlin met a women in tears who was experiencing a panic attack on the train...</p>\n<p>The woman's name was Camille, a Boston local who immigrated from Haiti years ago. Camille described her experience on the train. Her attack showed no warning but overwhelmed her to the point where she had to stop her journey on the train before reaching work, and in doing so, upsetting her entire day. </p>\n<p>Camille, Caitlin, and many other people share anxiety. Our team wants help commuters and travelers have an easier journey so they can enjoy their destination. </p>\n<h2>The Project</h2>\n<p>We are solving this problem with a Google Daydream experience that places the user in a hot air balloon over a canyon gorge. The locomotion in the virtual reality environment is based of the users movement in transit, and also alerts the user when they have reached their destination. Wanderer combines aroma therapy and a calming audio-visual environment as two forms of anxiety treatment. </p>\n<h2>User Experience</h2>\n<p>Ideally our users will mount the device at the on-set of a panic attack or as a preventative measure after sitting down on the train/bus. \nSince Wanderer is a VR experience on the go, we designed the project for Daydream portability and comfort in mind. Manisha devised the scent head mount attachment to be as light weight as possible with changeable parts for the scent beads. It is fastened with a headband around the tip of the Daydream HMD with the scent beads located near the user's nose. \nLocomotion is a very large design challenge in the virtual reality development community. Moving the user in a virtual space needs to mimic or trick their physical movement or lack there of. The hot air balloon ride we provide moves with the user through the phone's accelerometer and GPS information in hopes of limiting nausea. The user is also placed inside the balloon basket with some stationary objects to limit inner ear confusion and mimic the feeling of travel. \nSince commutes can be lengthy, the user is able to interact with the environment by blowing bubbles using the Daydream Controller. </p>\n<h2>Impact</h2>\n<p>For someone who suffers with anxiety in cars, trains, buses, etc. Wanderer allows them to find comfort on their journey. For all users of public transit this provides a possible escape from dark subways and old train advertisements. This project is unique in that it challenges virtual reality design for users in transit, and can hopefully inspire other projects that work to improve user experience in public transit. </p>\n<h2>How it's Made</h2>\n<p>Wanderer is made with love and Unity. The Unity scene is a basic endless runner idea but generates terrain in the projected path of the user. The motion path and velocity is tracked with a mixture of the phone's gyroscope, accelerometer, and GPS location. Aroma therapy is triggered via an Arduino Mini Pro sewn to a headband attached to the headset viewer. The scent is contained in small beads that are gently heated to release the smell to the user. </p>\n<h2>Innovation</h2>\n<p>Wanderer also explores the use of scent to build more immersive experiences in virtual reality; a utility with little exploration. It also tests a new idea for locomotion in motion in virtual reality which is only possible with mobile VR at this time. </p>\n<h2>Collaboration</h2>\n<p>The team met each other for the first time at the Reality Virtually Hackathon, where only 2 of the 5 members had previous experience designing for virtual reality. Together they learned the new platform of Daydream. :) Bringing scent into the project was the efforts of Manisha. Thankfully Orhan also has a background in hardware and was able to help finalize the scent device. Wiley and Caitlin worked with Unity before, but Orhan and Peter learned over the weekend and kicked butt. Peter has a background in audio design and worked on music and tag teamed aesthetics with Caitlin who did the environment modeling and design. Wiley magically made the terrain and all the game pieces play nicely together. </p>\n<h2>Aesthetic</h2>\n<p>We can proudly say all our art was made during the hackathon and we didn't download any textures or models.The team agreed to stick to a simple low poly design to make the most our time and experience. Even though art normally doesn't matter at hackathons, our experience deeply relies on creating another world for the user to escape. So we spent some serious time making the environment art, audio, and scents work together. Since we are developing for mobile VR low poly worked better for our needs. </p>\n<h2>Design Flaws and Challenges</h2>\n<p>Making the GPS and accelerometer and gyroscope play nicely and give accurate feedback was a struggle. Something we are still working on fixing with the help of mentors. The original material used to create the scent beads dissolved with alcohol, although Manisha was constantly looking for the hardware needed to implement a reliable system to execute so we switched to a different material and mechanism to allow the scent to release. </p>\n<h2>Next Steps</h2>\n<p>Get the terrain generation working to scope with the projected user path and making the scents execute on trigger from Unity. </p>\n<h2>Thank you!</h2>\n<p>This wouldn't have been possible without the guidance, coffee, hardware, and ninja help of the Reality Virtually Hackathon Team, Sponsors, and Volunteers. \nWith special thanks to Chuck, Nigel, and Dylan.  </p>\n</div>",
            "content_md": "\n**Judging Info**  \n\n-Team Lead: Caitlin Kearney  \n\n-Team lead's phone number: three-five-too-281-7901  \n\n-Location: E15-341 (Nagashima Room)  \n\n-Vertical category: Human Well-Being  \n\n\n\n# WANDERER\n\n\n## The Inspiration\n\n\nWhile getting off the T at Kendall/MIT Station in route to the hackathon, Caitlin met a women in tears who was experiencing a panic attack on the train...\n\n\nThe woman's name was Camille, a Boston local who immigrated from Haiti years ago. Camille described her experience on the train. Her attack showed no warning but overwhelmed her to the point where she had to stop her journey on the train before reaching work, and in doing so, upsetting her entire day. \n\n\nCamille, Caitlin, and many other people share anxiety. Our team wants help commuters and travelers have an easier journey so they can enjoy their destination. \n\n\n## The Project\n\n\nWe are solving this problem with a Google Daydream experience that places the user in a hot air balloon over a canyon gorge. The locomotion in the virtual reality environment is based of the users movement in transit, and also alerts the user when they have reached their destination. Wanderer combines aroma therapy and a calming audio-visual environment as two forms of anxiety treatment. \n\n\n## User Experience\n\n\nIdeally our users will mount the device at the on-set of a panic attack or as a preventative measure after sitting down on the train/bus. \nSince Wanderer is a VR experience on the go, we designed the project for Daydream portability and comfort in mind. Manisha devised the scent head mount attachment to be as light weight as possible with changeable parts for the scent beads. It is fastened with a headband around the tip of the Daydream HMD with the scent beads located near the user's nose. \nLocomotion is a very large design challenge in the virtual reality development community. Moving the user in a virtual space needs to mimic or trick their physical movement or lack there of. The hot air balloon ride we provide moves with the user through the phone's accelerometer and GPS information in hopes of limiting nausea. The user is also placed inside the balloon basket with some stationary objects to limit inner ear confusion and mimic the feeling of travel. \nSince commutes can be lengthy, the user is able to interact with the environment by blowing bubbles using the Daydream Controller. \n\n\n## Impact\n\n\nFor someone who suffers with anxiety in cars, trains, buses, etc. Wanderer allows them to find comfort on their journey. For all users of public transit this provides a possible escape from dark subways and old train advertisements. This project is unique in that it challenges virtual reality design for users in transit, and can hopefully inspire other projects that work to improve user experience in public transit. \n\n\n## How it's Made\n\n\nWanderer is made with love and Unity. The Unity scene is a basic endless runner idea but generates terrain in the projected path of the user. The motion path and velocity is tracked with a mixture of the phone's gyroscope, accelerometer, and GPS location. Aroma therapy is triggered via an Arduino Mini Pro sewn to a headband attached to the headset viewer. The scent is contained in small beads that are gently heated to release the smell to the user. \n\n\n## Innovation\n\n\nWanderer also explores the use of scent to build more immersive experiences in virtual reality; a utility with little exploration. It also tests a new idea for locomotion in motion in virtual reality which is only possible with mobile VR at this time. \n\n\n## Collaboration\n\n\nThe team met each other for the first time at the Reality Virtually Hackathon, where only 2 of the 5 members had previous experience designing for virtual reality. Together they learned the new platform of Daydream. :) Bringing scent into the project was the efforts of Manisha. Thankfully Orhan also has a background in hardware and was able to help finalize the scent device. Wiley and Caitlin worked with Unity before, but Orhan and Peter learned over the weekend and kicked butt. Peter has a background in audio design and worked on music and tag teamed aesthetics with Caitlin who did the environment modeling and design. Wiley magically made the terrain and all the game pieces play nicely together. \n\n\n## Aesthetic\n\n\nWe can proudly say all our art was made during the hackathon and we didn't download any textures or models.The team agreed to stick to a simple low poly design to make the most our time and experience. Even though art normally doesn't matter at hackathons, our experience deeply relies on creating another world for the user to escape. So we spent some serious time making the environment art, audio, and scents work together. Since we are developing for mobile VR low poly worked better for our needs. \n\n\n## Design Flaws and Challenges\n\n\nMaking the GPS and accelerometer and gyroscope play nicely and give accurate feedback was a struggle. Something we are still working on fixing with the help of mentors. The original material used to create the scent beads dissolved with alcohol, although Manisha was constantly looking for the hardware needed to implement a reliable system to execute so we switched to a different material and mechanism to allow the scent to release. \n\n\n## Next Steps\n\n\nGet the terrain generation working to scope with the projected user path and making the scents execute on trigger from Unity. \n\n\n## Thank you!\n\n\nThis wouldn't have been possible without the guidance, coffee, hardware, and ninja help of the Reality Virtually Hackathon Team, Sponsors, and Volunteers. \nWith special thanks to Chuck, Nigel, and Dylan. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/ar-construct",
            "title": "AR.Construct",
            "blurb": "AR.Construct seeks to demonstrate assisted 3D assembly through the use of LEGO bricks",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Tim Marquart",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/03d32e1653c054a2b59b07fbc6090365?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Gabe Evans",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/81a85c50965fc057e2b22ab1e7231566?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jarrett Linowes",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/f25a02a329830261c375fc15bffa7dc8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jonathan Stets",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/451893ba5c7b18da8bb91fa42bdbf5a9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity",
                "visual-studio",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Today, almost everything that is built is created using 2D drawings, from bridges, to furniture, to rockets. Augmented reality presents an opportunity to view building instructions superimposed upon the item being built. This will improve efficiency for several industries and allow for more effective assembly instructions.</p>\n<h2>What it does</h2>\n<p>The project creates virtual LEGO bricks located in the user's field of vision to instruct them on the next step of the building process. </p>\n<h2>How We built it</h2>\n<p>Vuforia tracks an image that is used to pinpoint the location of the LEGO bricks. Each step in the process is triggered by user input, showing the user the next step. We used Unity to arrange the objects in 3D space, and the Hololens to project this image into the user's field of view.</p>\n<h2>Challenges I ran into</h2>\n<p>The software proved to be extremely challenging and had several bugs. The image tracking was inconsistent in the level of tracking accuracy it provided, and several issues with Visual Studio slowed development.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I am proud that the team has come together and we have solved many of our issues. The product is working well, but there is some work left before we are ready for tomorrow.</p>\n<h2>What I learned</h2>\n<p>Personally, I learned a lot more about Unity, got to see firsthand many of the issues that can come up, and improved my ability to work with a team</p>\n<h2>What's next for AR.Construct</h2>\n<p>This tool is a proof of concept to show what is possible with the technology. By extending the capabilities of the program, it can be used to construct more complex assemblies.</p>\n<h2>Notes</h2>\n<p>Team Lead - Tim Marquart\n832-940-4477\nFloor 6, Table 9\nCommerce/Industry (Engineering / Construction) </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nToday, almost everything that is built is created using 2D drawings, from bridges, to furniture, to rockets. Augmented reality presents an opportunity to view building instructions superimposed upon the item being built. This will improve efficiency for several industries and allow for more effective assembly instructions.\n\n\n## What it does\n\n\nThe project creates virtual LEGO bricks located in the user's field of vision to instruct them on the next step of the building process. \n\n\n## How We built it\n\n\nVuforia tracks an image that is used to pinpoint the location of the LEGO bricks. Each step in the process is triggered by user input, showing the user the next step. We used Unity to arrange the objects in 3D space, and the Hololens to project this image into the user's field of view.\n\n\n## Challenges I ran into\n\n\nThe software proved to be extremely challenging and had several bugs. The image tracking was inconsistent in the level of tracking accuracy it provided, and several issues with Visual Studio slowed development.\n\n\n## Accomplishments that I'm proud of\n\n\nI am proud that the team has come together and we have solved many of our issues. The product is working well, but there is some work left before we are ready for tomorrow.\n\n\n## What I learned\n\n\nPersonally, I learned a lot more about Unity, got to see firsthand many of the issues that can come up, and improved my ability to work with a team\n\n\n## What's next for AR.Construct\n\n\nThis tool is a proof of concept to show what is possible with the technology. By extending the capabilities of the program, it can be used to construct more complex assemblies.\n\n\n## Notes\n\n\nTeam Lead - Tim Marquart\n832-940-4477\nFloor 6, Table 9\nCommerce/Industry (Engineering / Construction) \n\n\n"
        },
        {
            "source": "https://devpost.com/software/pitch-paint",
            "title": "Vream Team",
            "blurb": "Abstract audio composition in a simple virtual reality toy with music visualization",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Zach",
                    "about": "I helped with visual design, mostly coded a lot of the shaders and user interaction.  We all pushed for the final vision of the project together.",
                    "photo": "https://www.gravatar.com/avatar/f58f5fb3e1ad44b6cacbc5635a996feb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Emily",
                    "about": "I worked on front end design and development. Wrote code and designed",
                    "photo": "https://www.gravatar.com/avatar/a52461386042883cead0002af13f86bd?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tin Chag",
                    "about": "I primarily worked on the backend coding. ",
                    "photo": "https://www.gravatar.com/avatar/de2a1e57672f86438dbae1a0c307ad45?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Rebecca Kleinberger",
                    "about": "I worked on audio design and some parts of the UI visual design. I participated with the rest of the team in the definition of the vision",
                    "photo": "https://www.gravatar.com/avatar/13ae7fb8464346c9ebba0276ea1b0856?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "casper21",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/dbc735e2eb58cbd4f77ebfe60f253dec?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Team  Lead:</h2>\n<p>Emily  Van Belleghem, 408 859 5845</p>\n<h2>Location:</h2>\n<p>Floor two, left side Vive room</p>\n<h2>Vertcal Category:</h2>\n<p>Entertainment, storytelling</p>\n<h2>Inspiration</h2>\n<p>This project was inspired by synesthesia. As artists we wanted to try and blend the senses to experience more of the world we live in; in this case, to see sound. It was also inspired by our team leaders experiences with composition. There are many people who would love to write music, and have a terrific sense of melody, but don't have any way to write it down. This is because music has an incredible amount of syntax, and requires years of training to completely understand. This application was intended to put the power of music in the hands of people who would like to explore a virtual world where it might be a bit easier to experiment with sound. It immerses the user in their work in an interactive and simple atmosphere while also creating incredible musical sculptures. </p>\n<h2>What it does</h2>\n<p>The user can compose, listen to, and visualize their own music through a wholly new abstract vision. Basically the user can draw music with their hands and watch it play before their very eyes. The notes light up in the color of their pitch, where pitch varies on the vertical axis. So the user can better see how notes might relate to each other the way colors and relative space does. There are several different instruments to choose and test with, and the user can draw with both hands if they would like. </p>\n<h2>How we built it</h2>\n<p>The five of us collaborated on music, design and unity development to produce this experience. It took many hours but in the end were very proud of our work and the abilities this VR project brings forth. </p>\n<h2>Challenges we ran into</h2>\n<p>Finding an impressive way to visualize our physical music system is difficult. We spent much of the first day prototyping and testing different aesthetics in search of one that fit our vision. As you could imagine rethinking music in a 3D space is very difficult has many different options, we needed something simple, intuitive and fun. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Were proud of the notes lighting up in the correct color when they're hit . It really brings motion to our artistic piece and gives the user a much better sense of timing. We're also very proud of our intuitive UI. We feel it was extremely different traditional music composition and it gave user lots of freedom. We also tested it with composor's outside our group to see their thoughts, and they felt it would greatly enhance understanding of music theory. We're proud we have a new way to experience music and how different it is than the user might expect. </p>\n<h2>What we learned</h2>\n<p>We learned that key really matters. #HarmonyIsKey. Zach learned a lot about shaders, I learned a lot about how the Vive's controls. Our group was one of many people with very different backgrounds, and we we're to take input from everyone and blend ideas. We also learned that this project has a lot of potential to go deep into music theory into many different abstract ways. </p>\n<h2>What's next for Vream Team</h2>\n<p>There's a lot of ways we intend to take this project. In general we would love to keep exploring the relations of space and visualizing music. Hopefully we could add different effects to the music such as distortion, reverb etc. Better visuals and animations would also be amazing to truly create a performance of art. We also would like to add an editing feature to change the music already written. We'd also like to pause music, change volume of music, give better control on the looping aspect. Vream Team definitely proved a proof of concept that could go very far in terms of VR application.</p>\n</div>",
            "content_md": "\n## Team Lead:\n\n\nEmily Van Belleghem, 408 859 5845\n\n\n## Location:\n\n\nFloor two, left side Vive room\n\n\n## Vertcal Category:\n\n\nEntertainment, storytelling\n\n\n## Inspiration\n\n\nThis project was inspired by synesthesia. As artists we wanted to try and blend the senses to experience more of the world we live in; in this case, to see sound. It was also inspired by our team leaders experiences with composition. There are many people who would love to write music, and have a terrific sense of melody, but don't have any way to write it down. This is because music has an incredible amount of syntax, and requires years of training to completely understand. This application was intended to put the power of music in the hands of people who would like to explore a virtual world where it might be a bit easier to experiment with sound. It immerses the user in their work in an interactive and simple atmosphere while also creating incredible musical sculptures. \n\n\n## What it does\n\n\nThe user can compose, listen to, and visualize their own music through a wholly new abstract vision. Basically the user can draw music with their hands and watch it play before their very eyes. The notes light up in the color of their pitch, where pitch varies on the vertical axis. So the user can better see how notes might relate to each other the way colors and relative space does. There are several different instruments to choose and test with, and the user can draw with both hands if they would like. \n\n\n## How we built it\n\n\nThe five of us collaborated on music, design and unity development to produce this experience. It took many hours but in the end were very proud of our work and the abilities this VR project brings forth. \n\n\n## Challenges we ran into\n\n\nFinding an impressive way to visualize our physical music system is difficult. We spent much of the first day prototyping and testing different aesthetics in search of one that fit our vision. As you could imagine rethinking music in a 3D space is very difficult has many different options, we needed something simple, intuitive and fun. \n\n\n## Accomplishments that we're proud of\n\n\nWere proud of the notes lighting up in the correct color when they're hit . It really brings motion to our artistic piece and gives the user a much better sense of timing. We're also very proud of our intuitive UI. We feel it was extremely different traditional music composition and it gave user lots of freedom. We also tested it with composor's outside our group to see their thoughts, and they felt it would greatly enhance understanding of music theory. We're proud we have a new way to experience music and how different it is than the user might expect. \n\n\n## What we learned\n\n\nWe learned that key really matters. #HarmonyIsKey. Zach learned a lot about shaders, I learned a lot about how the Vive's controls. Our group was one of many people with very different backgrounds, and we we're to take input from everyone and blend ideas. We also learned that this project has a lot of potential to go deep into music theory into many different abstract ways. \n\n\n## What's next for Vream Team\n\n\nThere's a lot of ways we intend to take this project. In general we would love to keep exploring the relations of space and visualizing music. Hopefully we could add different effects to the music such as distortion, reverb etc. Better visuals and animations would also be amazing to truly create a performance of art. We also would like to add an editing feature to change the music already written. We'd also like to pause music, change volume of music, give better control on the looping aspect. Vream Team definitely proved a proof of concept that could go very far in terms of VR application.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/d2",
            "title": "D2",
            "blurb": "Depression diagnosis for Diabetes patients, hacked in 48h for #RealityVirtuallyHack",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/a-uA7xsdAKQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Explore Nature, find coins, enjoy the fireworks!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/144/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Konrad Feiler",
                    "about": "Engineer, polyglot programmer, nature enthusiast",
                    "photo": "https://www.gravatar.com/avatar/0fd70f0e37ab418e86035c1ccad58eee?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Rohit Agrawal",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/260/datas/profile.jpg"
                },
                {
                    "name": "Sarfraz Ahmad",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b37ce38af182aec3d4e86d2d23039036?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jeanne Kelly",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/782/594/datas/profile.jpeg"
                },
                {
                    "name": "RikasShen",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/270/datas/profile.jpg"
                }
            ],
            "built_with": [
                "android-wear",
                "gearvr",
                "iot",
                "unity"
            ],
            "content_html": "<div>\n<h2>What it does</h2>\n<p>Our concept is to provide a self administered, entertaining and approachable method of relief from depression to patients with diabetes.\nWe propose a lush tranquil exploratory virtual environment with moments of discovery, gratification and reward. Our experience will allow the patient to freely explore a lush natural setting with moments of magical discovery, distracting the patients attention from their illness and depression - allowing them moments of joy that can actually improve their condition and quality of life.</p>\n<h2>How we built it</h2>\n<p>Through collaborative effort of a multi dimensional team consisting of a Game Developer, Software Designer, Product Manager, Physician and Software engineer.\nWe developed our product using Unity platform, Gear VR (Oculus) and Samsung S6. </p>\n<h2>Challenges we ran into</h2>\n<p>Apart from bug fixes, there were some issues with the linking of navigation with the field of vision. Designing for depressed patients is also difficult and the immersive surrounding will evolve based on the feedback from the users over the period of time.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We got great feedback and confidence in our concept from many experts.\nWe also managed to add the navigation component and discovery of objects in the scene to the immersive experience.\nWe were running tight on time but still were able to add many additional components to the immersive surrounding.</p>\n<h2>What we learned</h2>\n<p>We learned many things about the unity platform and how to work in collaborative atmosphere. Depression in diabetic patients is an under-addressed issue with high costs to individuals and society. We should all probably be spending a lot more time walking in nature.</p>\n<h2>What's next for D2</h2>\n<p>To validate the results of our effort in a clinical study. The results will help us to refine our product and to make it more user engaging.\nOur ultimate aim is to make it cost effective so that it be used by users all over the globe. </p>\n<p>Team Lead: Rohit Agrawal , 8572346739, Floor 6 - Table 8, Human Well-Being - Health</p>\n</div>",
            "content_md": "\n## What it does\n\n\nOur concept is to provide a self administered, entertaining and approachable method of relief from depression to patients with diabetes.\nWe propose a lush tranquil exploratory virtual environment with moments of discovery, gratification and reward. Our experience will allow the patient to freely explore a lush natural setting with moments of magical discovery, distracting the patients attention from their illness and depression - allowing them moments of joy that can actually improve their condition and quality of life.\n\n\n## How we built it\n\n\nThrough collaborative effort of a multi dimensional team consisting of a Game Developer, Software Designer, Product Manager, Physician and Software engineer.\nWe developed our product using Unity platform, Gear VR (Oculus) and Samsung S6. \n\n\n## Challenges we ran into\n\n\nApart from bug fixes, there were some issues with the linking of navigation with the field of vision. Designing for depressed patients is also difficult and the immersive surrounding will evolve based on the feedback from the users over the period of time.\n\n\n## Accomplishments that we're proud of\n\n\nWe got great feedback and confidence in our concept from many experts.\nWe also managed to add the navigation component and discovery of objects in the scene to the immersive experience.\nWe were running tight on time but still were able to add many additional components to the immersive surrounding.\n\n\n## What we learned\n\n\nWe learned many things about the unity platform and how to work in collaborative atmosphere. Depression in diabetic patients is an under-addressed issue with high costs to individuals and society. We should all probably be spending a lot more time walking in nature.\n\n\n## What's next for D2\n\n\nTo validate the results of our effort in a clinical study. The results will help us to refine our product and to make it more user engaging.\nOur ultimate aim is to make it cost effective so that it be used by users all over the globe. \n\n\nTeam Lead: Rohit Agrawal , 8572346739, Floor 6 - Table 8, Human Well-Being - Health\n\n\n"
        },
        {
            "source": "https://devpost.com/software/fizzfilter",
            "title": "Fizz Filter",
            "blurb": "Choose Your Flavor",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/46vKzjChjsU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Yannick Boers",
                    "about": "Worked on the \"logical\" placement of 3D models, finding and refining those models. Branding & Video.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/677/datas/profile.jpg"
                },
                {
                    "name": "Andrew Dupuis",
                    "about": "I worked on the Hololens Spatial Mapping/ Understanding, object placement, and scene management systems. ",
                    "photo": "https://www.gravatar.com/avatar/2d42b9fcd0f3da0b43890a6ef1629c26?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "jinny yan",
                    "about": "I worked on refining 3d models and putting together thematic scenes.\n",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/398/215/datas/profile.jpg"
                },
                {
                    "name": "Yasmeen Roumie",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/423/829/datas/profile.JPG"
                },
                {
                    "name": "Lisa van Acquoij",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/688/datas/profile.png"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Snapchat uses augmented reality to dynamically apply lenses to users' faces. What if you could apply a lens similarly to alter the environment around you? We thought it would be interesting to instantly be able to redesign any room you're in to transport you into a completely different location. What if you wanted to feel the warmth of those nights in the ski lodge, next to the fireplace and looking out of the window and watching as the snow falls down. Or what if you were alone that night, but wanted to be around people, while all of your friends were busy at home? You can redecorate, or reimagine, your current location with <b>Fizz Filter</b>, which brings a spark to your life at any time you want.</p>\n<h2>What it does</h2>\n<p><b>Fizz Filter</b> is an augmented reality application tailored for ease of use and scalability of filters. You enter a simple environment where you are given a choice of how you want your environment to be reimagined. Then, your environment is scanned by the Hololens and soon enough your room is populated with decorations and other indicators that has you feeling the theme that you chose.</p>\n<h2>How we built it</h2>\n<p>We utilized the HoloLens' spatial mapping and spatial understanding capabilities to recognize the features of room the user is in. Then, we used a smart placement algorithm to determine how to distribute the objects across the space based on various input factors. <i>About 15% of our project was built using Microsoft's HoloLens API.</i></p>\n<h2>What's next for Fizz Filter</h2>\n<p>In the future, we would like to partner with companies, as Snapchat does, to sponsor daily scenes. We would also like to allow users to create and submit more environments and ideas for future filters we could create. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSnapchat uses augmented reality to dynamically apply lenses to users' faces. What if you could apply a lens similarly to alter the environment around you? We thought it would be interesting to instantly be able to redesign any room you're in to transport you into a completely different location. What if you wanted to feel the warmth of those nights in the ski lodge, next to the fireplace and looking out of the window and watching as the snow falls down. Or what if you were alone that night, but wanted to be around people, while all of your friends were busy at home? You can redecorate, or reimagine, your current location with **Fizz Filter**, which brings a spark to your life at any time you want.\n\n\n## What it does\n\n\n**Fizz Filter** is an augmented reality application tailored for ease of use and scalability of filters. You enter a simple environment where you are given a choice of how you want your environment to be reimagined. Then, your environment is scanned by the Hololens and soon enough your room is populated with decorations and other indicators that has you feeling the theme that you chose.\n\n\n## How we built it\n\n\nWe utilized the HoloLens' spatial mapping and spatial understanding capabilities to recognize the features of room the user is in. Then, we used a smart placement algorithm to determine how to distribute the objects across the space based on various input factors. *About 15% of our project was built using Microsoft's HoloLens API.*\n\n\n## What's next for Fizz Filter\n\n\nIn the future, we would like to partner with companies, as Snapchat does, to sponsor daily scenes. We would also like to allow users to create and submit more environments and ideas for future filters we could create. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/times-of-crisis",
            "title": "Times of Crisis",
            "blurb": "Understand the challenges faced by people crossing a sea in search of their future",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Jassim Ahmad",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3fb64ff7d6c1c86af50f0e344c02facc?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Roye Avidor",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b8a2f57ebc41b2e4f760cbe54a521474?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The fire hose of digital news has created a false illusion of knowledge. In truth, we are increasingly polarised about the issues that shape the world - incapable of seeing others' views. Since 2015, over <strong>1.3 million people risked their lives</strong> by crossing the Meditteranean to reach Europe. How can VR help us understand these journeys from the perspective of their agents? How can we highlight the <strong>urgency for action</strong>?</p>\n<h2>What it does</h2>\n<p>You start standing on a boat at sea with no coastline in sight - a view rarely seen by anyone but the migrants. An audio narration explains why you are here. A carousel of photographs, each narrated to convey stages of the crossing. Several images are an entry point to video providing further context.</p>\n<h2>How we built it</h2>\n<p>We wanted this important issue to <strong>reach the maximum audience</strong>, so selected the cheapest, most widely accessible VR solution. We used Unity as a development platform to create a VR app for Android devices. Audiovisual news content was sourced from Reuters and edited. We focused on the boat crossing as it presents a dramatic choice for migrants and is a view rarely seen by others. A script was researched, composed and recorded for audio narration.</p>\n<h2>Challenges we ran into</h2>\n<p>Our developer had almost no experience of Unity so had to learn the environment on the fly. We didn't have enough capacity to realise our vision! It was also a challenge to source compelling content and edit it into a cohesive narrative.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of having created our first VR environment for a story that matters!</p>\n<h2>What we learned</h2>\n<p>We both learnt the importance of considering purpose upfront. We used paper to draw and model our VR environment. Roye learnt how to code Unity.</p>\n<h2>What's next for Times of Crisis</h2>\n<p>To add additional spaces to continue the journey from the <strong>sea</strong> to the <strong>land</strong> and on to <strong>home</strong>. We would like to add multiple users to the virtual boat, emphasising the critical issue of overcrowding to maximise profit. We would also like to enable users to take action by sharing content beyond the platform.</p>\n<h2>Team Lead</h2>\n<p>Jassim Ahmad</p>\n<h2>Phone</h2>\n<p>(857) 308-6874</p>\n<h2>Location</h2>\n<p>6th floor, desk 37</p>\n<h2>Vertical</h2>\n<p>Entertainment/Storytelling (Gaming, Film Journalism, Art/Design)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe fire hose of digital news has created a false illusion of knowledge. In truth, we are increasingly polarised about the issues that shape the world - incapable of seeing others' views. Since 2015, over **1.3 million people risked their lives** by crossing the Meditteranean to reach Europe. How can VR help us understand these journeys from the perspective of their agents? How can we highlight the **urgency for action**?\n\n\n## What it does\n\n\nYou start standing on a boat at sea with no coastline in sight - a view rarely seen by anyone but the migrants. An audio narration explains why you are here. A carousel of photographs, each narrated to convey stages of the crossing. Several images are an entry point to video providing further context.\n\n\n## How we built it\n\n\nWe wanted this important issue to **reach the maximum audience**, so selected the cheapest, most widely accessible VR solution. We used Unity as a development platform to create a VR app for Android devices. Audiovisual news content was sourced from Reuters and edited. We focused on the boat crossing as it presents a dramatic choice for migrants and is a view rarely seen by others. A script was researched, composed and recorded for audio narration.\n\n\n## Challenges we ran into\n\n\nOur developer had almost no experience of Unity so had to learn the environment on the fly. We didn't have enough capacity to realise our vision! It was also a challenge to source compelling content and edit it into a cohesive narrative.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of having created our first VR environment for a story that matters!\n\n\n## What we learned\n\n\nWe both learnt the importance of considering purpose upfront. We used paper to draw and model our VR environment. Roye learnt how to code Unity.\n\n\n## What's next for Times of Crisis\n\n\nTo add additional spaces to continue the journey from the **sea** to the **land** and on to **home**. We would like to add multiple users to the virtual boat, emphasising the critical issue of overcrowding to maximise profit. We would also like to enable users to take action by sharing content beyond the platform.\n\n\n## Team Lead\n\n\nJassim Ahmad\n\n\n## Phone\n\n\n(857) 308-6874\n\n\n## Location\n\n\n6th floor, desk 37\n\n\n## Vertical\n\n\nEntertainment/Storytelling (Gaming, Film Journalism, Art/Design)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/poetic-justice",
            "title": "Poetic Justice",
            "blurb": "InteractiveSpoken Word Poetry in Virtual Reality",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/xJXQ-fyjPO8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Kylila Bullard",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/999/datas/profile.jpg"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "iphone-sdk",
                "photoshop",
                "theta",
                "unity",
                "visual-studio",
                "youtube"
            ],
            "content_html": "<div>\n<h2>Inspiration Millenials, housing affordability , and the struggle to reach the American dream.</h2>\n<h2>What it does Involves the consumer in a powerful and interactive narrative that is entertaining , emotional , and empathic.</h2>\n<h2>How we built it VR Video with Theta, Premier, , Drop Box, and H4n Zoom Field recorder and Creative Commons Music</h2>\n<h2>Challenges we ran into: Conversion and time, and people having other obligations</h2>\n<h2>Accomplishments that we're proud of: I submitted something :-)</h2>\n<h2>What we learned: That VR video is fun , and more complicated than I expected</h2>\n<h2>What's next for Poetic Justice: Keep creating and getting better with VR film. Learn how to use Unity to make the film more interactive , powerful, and educational.</h2>\n</div>",
            "content_md": "\n## Inspiration Millenials, housing affordability , and the struggle to reach the American dream.\n\n\n## What it does Involves the consumer in a powerful and interactive narrative that is entertaining , emotional , and empathic.\n\n\n## How we built it VR Video with Theta, Premier, , Drop Box, and H4n Zoom Field recorder and Creative Commons Music\n\n\n## Challenges we ran into: Conversion and time, and people having other obligations\n\n\n## Accomplishments that we're proud of: I submitted something :-)\n\n\n## What we learned: That VR video is fun , and more complicated than I expected\n\n\n## What's next for Poetic Justice: Keep creating and getting better with VR film. Learn how to use Unity to make the film more interactive , powerful, and educational.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/fear-conquer-your-fear-of-public-speaking",
            "title": "+-Fear - Conquer your fear of public speaking",
            "blurb": "Mixed reality application that allows someone to practice speaking in public to hologram \"audiences\" ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Martin Tarr",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/907/datas/profile.jpg"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Team leader has a fear of speaking in public</p>\n<h2>What it does</h2>\n<p>Display holographic images of audiences where the player can practice speaking to audiences of different sizes</p>\n<h2>How I built it</h2>\n<p>Microsoft HoloLens</p>\n<h2>Challenges I ran into</h2>\n<p>No team member had Unity or HoloLens experience</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Diving in and learning new technologies</p>\n<h2>What I learned</h2>\n<p>Intro to Unity and HoloLens</p>\n<h2>What's next for +-Fear - Conquer your fear of public speaking</h2>\n<p>undecided</p>\n<p>Team Leader Shailee Rindani 404-271-5393</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTeam leader has a fear of speaking in public\n\n\n## What it does\n\n\nDisplay holographic images of audiences where the player can practice speaking to audiences of different sizes\n\n\n## How I built it\n\n\nMicrosoft HoloLens\n\n\n## Challenges I ran into\n\n\nNo team member had Unity or HoloLens experience\n\n\n## Accomplishments that I'm proud of\n\n\nDiving in and learning new technologies\n\n\n## What I learned\n\n\nIntro to Unity and HoloLens\n\n\n## What's next for +-Fear - Conquer your fear of public speaking\n\n\nundecided\n\n\nTeam Leader Shailee Rindani 404-271-5393\n\n\n"
        },
        {
            "source": "https://devpost.com/software/time-in-a-bottle",
            "title": "Time in a bottle",
            "blurb": "Memory exploration through sound ques, music, and 3D visuals. ",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Time in a bottle",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/102/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Dulce Baerga",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c6b79be3756c1a4572870f9e4148d9a9?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "sokudo",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3ad627f8de8ea48f537df105609c246e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Cl\u00e9mence Hermann",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/076/861/datas/profile.jpg"
                },
                {
                    "name": "Sherrie Robertson",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/432/581/datas/profile.jpg"
                },
                {
                    "name": "Colin Greenhill",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/85b769fac153e8a3ccf4197b209023fb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "javascript",
                "unity"
            ],
            "content_html": "<div>\n<p>The user goes back to his childhood home after being away for many years. Now, at the age of twenty-eight, he is reeling from a bad break-up. Things are arranged as they were when he was still a teenager, but they are now aged and dusty.</p>\n<p>Amidst the dusty yearbooks, travel photos, and old things he strewn about his room, the user finds a song he has not heard in a long time. As he plays it, he is pulled into a place he has not been for a long, long time. He is pulled through a portal of time to explore key romantic moments at ages six, thirteen, and then nineteen years old.</p>\n</div>",
            "content_md": "\nThe user goes back to his childhood home after being away for many years. Now, at the age of twenty-eight, he is reeling from a bad break-up. Things are arranged as they were when he was still a teenager, but they are now aged and dusty.\n\n\nAmidst the dusty yearbooks, travel photos, and old things he strewn about his room, the user finds a song he has not heard in a long time. As he plays it, he is pulled into a place he has not been for a long, long time. He is pulled through a portal of time to explore key romantic moments at ages six, thirteen, and then nineteen years old.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/unnamed-vr-project",
            "title": "Uncanny",
            "blurb": "Uncanny World",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Marcelo DeCastro",
                    "about": "Unity3D Programming",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/633/316/datas/profile.jpg"
                },
                {
                    "name": "kellian adams",
                    "about": "I was the game designer and storyteller. I worked on ideation, playmaps, wireframes and text. ",
                    "photo": "https://www.gravatar.com/avatar/f81ba121dd2abb1532c1643d9ba8a5ed?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Youssef Barhomi",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ae20325f9765b7ff6b6fa2aa5494dc3d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Monica Bolles",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3fddb8a34a0e53c94c66f6faec23cb45?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kathy Wu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/540dff14656b6402fee29659f082f3ad?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Description:</h2>\n<p>Human Well-Being/Entertainment</p>\n<p>Git:</p>\n<p><a href=\"https://github.com/treeborg/uncanny\" rel=\"nofollow\">https://github.com/treeborg/uncanny</a></p>\n</div>",
            "content_md": "\n## Description:\n\n\nHuman Well-Being/Entertainment\n\n\nGit:\n\n\n<https://github.com/treeborg/uncanny>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/nudge-m0yzp5",
            "title": "Nudge",
            "blurb": "An AR app that helps start conversations",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/IQ9PetAPLa8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Lily Fan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0abe865ca812cccc6e7ffbf2ea3eee9a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Natasha Polozenko",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e0ba0b27b0116653a1a24003f991b8f2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "David Z",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9a6e3b0ae810e5874e7058860ef4c37a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lucas Callado",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/894/datas/profile.jpg"
                },
                {
                    "name": "Arnon Karnkaeng",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/bd9cf533f5fa2ba198008734d3c9a80e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-cognitive-services",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<p>Team Lead: Natasha Polozenko 6176025248\nCategory: Human Connection\nLocation: Table 17 on 6 floor by window/\"balcony\"</p>\n<h2>Inspiration</h2>\n<p>We were inspired by how AR (particularly in Pokemon Go) started organic conversations, brought people together and created communities.</p>\n<h2>What it does</h2>\n<p>It allows users to scan other users and figure out mutual interests to organically start conversations. </p>\n<h2>How we built it</h2>\n<p>We used Unity, CSharp, Microsoft-Cognitive-Services, and Vuforia.</p>\n<h2>Challenges we ran into</h2>\n<p>The act of actually mapping something next to a face proved to be something more difficult than anticipated. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We've managed to accomplish facial tracking on the Tango.</p>\n<h2>What's next for Nudge</h2>\n<p>Ideally, this would eventually recognize individual faces instead just sensing when there's a face in front of the camera. This would allow customization with the different graphics. Nudge could ideally become a more interactive experience with more motion instead of tracking static images. With technological processing advances, the tracking would also ideally be more immediate.</p>\n</div>",
            "content_md": "\nTeam Lead: Natasha Polozenko 6176025248\nCategory: Human Connection\nLocation: Table 17 on 6 floor by window/\"balcony\"\n\n\n## Inspiration\n\n\nWe were inspired by how AR (particularly in Pokemon Go) started organic conversations, brought people together and created communities.\n\n\n## What it does\n\n\nIt allows users to scan other users and figure out mutual interests to organically start conversations. \n\n\n## How we built it\n\n\nWe used Unity, CSharp, Microsoft-Cognitive-Services, and Vuforia.\n\n\n## Challenges we ran into\n\n\nThe act of actually mapping something next to a face proved to be something more difficult than anticipated. \n\n\n## Accomplishments that we're proud of\n\n\nWe've managed to accomplish facial tracking on the Tango.\n\n\n## What's next for Nudge\n\n\nIdeally, this would eventually recognize individual faces instead just sensing when there's a face in front of the camera. This would allow customization with the different graphics. Nudge could ideally become a more interactive experience with more motion instead of tracking static images. With technological processing advances, the tracking would also ideally be more immediate.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/studyvr-ihg451",
            "title": "StudyVR",
            "blurb": "A kinesthetic learning tool that helps grade school kids learn science that aligns with the NGSS curriculum.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/d1JDFn9po3Q?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "StudyVR splash screen",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/218/datas/original.png"
                },
                {
                    "title": "After testing with the help of some middle school students.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/993/datas/original.png"
                },
                {
                    "title": "StudyVR at work",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/018/datas/original.jpeg"
                }
            ],
            "team": [
                {
                    "name": "valfx9",
                    "about": "Project management, UX and educational research, conceptualization and design, asset modeling, texturing, animation and Unity implementation",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/742/datas/profile.jpg"
                },
                {
                    "name": "Sean Bailey",
                    "about": "Ideation, Concept Development, Scripting User Interaction in Unity C#",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/432/756/datas/profile.png"
                },
                {
                    "name": "Tyler C. Roach",
                    "about": "I worked on the user interface design/development as well as the functionality of the actual modules themselves.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/146/301/datas/profile.jpg"
                },
                {
                    "name": "Brendan Luu",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14Gi2PFIynORsfaV_V6HpsyIPhiSBkkanPTCFVf10OA?height=180&width=180"
                },
                {
                    "name": "Fatma Ozen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/4c4348acb572b0a695fac99b98ba7040?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "htc-vive",
                "maya",
                "unity"
            ],
            "content_html": "<div>\n<p>Have you ever studied a concept that you never applied in real life? Most children learn best through doing, and StudyVR is our solution to engage the next generation of kinesthetic learners.</p>\n<p>StudyVR is a learning tool for grade school kids to connect with their learning concepts with the HTC Vive, using kinesthetic hands-on interaction to engage them on a deeper level and bring science to life. </p>\n<p>Even better, our content is designed to work with the NGSS (Next Generation Science Standards) curriculum, an initiative by 26 US states to revitalize STEM teaching with technology and interdisciplinary curriculum. One major goal of NGSS is to demonstrate how scientific concepts can span across different subjects, and our application achieves that with two immersive learning modules. All assets were created by our team of 3D modelers for this Hackathon, making this an experience that can't be found anywhere else. </p>\n<p>We surveyed elementary and middle school teachers to determine what subjects their students struggled to learn with 2D textbook examples. Using the concept of \"pressure\" as a keystone, we developed two modules that integrate a singular mechanic to show real-world examples in the fields of Physics and Life Sciences. Students can help a fish swim around a tank by adjusting the pressure of its swim bladder, control a hot-air balloon while learning about the Ideal Gas Laws, and watch narrated videos that reinforce the science behind the simulations. A study \"room serves as a centralized hub to this experience, allowing the players to travel between lessons through \"portals\" in their textbooks.</p>\n<p>While this experience was developed to take advantage of hands-on Vive interactions, our single-input mechanic was designed to be ported to Google Cardboard in the future. The accessibility of Mobile VR could bring immersive educational experiences to any child with a smartphone, while the Vive version could be used in classroom demonstrations and home study. After demonstrating our proof of concept application to a group of elementary school children, we're confident that our idea has the potential to be expanded upon in the future.</p>\n</div>",
            "content_md": "\nHave you ever studied a concept that you never applied in real life? Most children learn best through doing, and StudyVR is our solution to engage the next generation of kinesthetic learners.\n\n\nStudyVR is a learning tool for grade school kids to connect with their learning concepts with the HTC Vive, using kinesthetic hands-on interaction to engage them on a deeper level and bring science to life. \n\n\nEven better, our content is designed to work with the NGSS (Next Generation Science Standards) curriculum, an initiative by 26 US states to revitalize STEM teaching with technology and interdisciplinary curriculum. One major goal of NGSS is to demonstrate how scientific concepts can span across different subjects, and our application achieves that with two immersive learning modules. All assets were created by our team of 3D modelers for this Hackathon, making this an experience that can't be found anywhere else. \n\n\nWe surveyed elementary and middle school teachers to determine what subjects their students struggled to learn with 2D textbook examples. Using the concept of \"pressure\" as a keystone, we developed two modules that integrate a singular mechanic to show real-world examples in the fields of Physics and Life Sciences. Students can help a fish swim around a tank by adjusting the pressure of its swim bladder, control a hot-air balloon while learning about the Ideal Gas Laws, and watch narrated videos that reinforce the science behind the simulations. A study \"room serves as a centralized hub to this experience, allowing the players to travel between lessons through \"portals\" in their textbooks.\n\n\nWhile this experience was developed to take advantage of hands-on Vive interactions, our single-input mechanic was designed to be ported to Google Cardboard in the future. The accessibility of Mobile VR could bring immersive educational experiences to any child with a smartphone, while the Vive version could be used in classroom demonstrations and home study. After demonstrating our proof of concept application to a group of elementary school children, we're confident that our idea has the potential to be expanded upon in the future.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/reality-plane",
            "title": "Reality Plane",
            "blurb": "A haptic tablet bridging the gap between the virtual and reality, allowing new interactions with a familiar interface",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Tablet Side View",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/002/datas/original.jpg"
                },
                {
                    "title": "Circuit Integration",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/000/datas/original.jpg"
                },
                {
                    "title": "Circuit Integration",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/001/datas/original.jpg"
                },
                {
                    "title": "Laser EtchingTablet Interface",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/003/datas/original.JPG"
                }
            ],
            "team": [
                {
                    "name": "igal nassima",
                    "about": "Design of the concept and hardware design for haptic feedback.\nDeveloped Unity project, integrating Vuforia with HoloLens.\nBuilt Arduino Haptic interface.\nTeam building, and task management.",
                    "photo": "https://www.gravatar.com/avatar/e867a8572cbbea701fc2f0c2b1f5b7da?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Yereem Park",
                    "about": "I mainly worked on the design side of the project - made frame marker, worked on design of the plexiglass part of the tablet and handheld part of the tablet, designed menu icons, banners, and assets for Unity. I also assisted the team with developing concept and demo ideas.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/402/222/datas/profile.jpg"
                },
                {
                    "name": "Michael Peguero",
                    "about": "Concept development. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/421/007/datas/profile.PNG"
                },
                {
                    "name": "Jeff Katz",
                    "about": "* Concept and experience ideation",
                    "photo": "https://www.gravatar.com/avatar/912490a14ffec834f1e86be0ac9ff55f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "ableton",
                "acryllic",
                "arduino",
                "blood-sweat-and-tears",
                "btle",
                "c#",
                "caffine",
                "cinema4d",
                "foam-core",
                "microsoft-hololens",
                "unity",
                "vibration-motor"
            ],
            "content_html": "<div>\n<h2>TEAM INFO</h2>\n<p><strong>Team lead:</strong>  Igal Nassima</p>\n<p><strong>Team lead's telephone number:</strong> 646 696 9286</p>\n<p><strong>We're located:</strong> (Floor 6, table 7, on the corner where the balcony entrance door is)</p>\n<p><strong>Category:</strong> Commerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)</p>\n<h2>Inspiration</h2>\n<p>\"The physical plane, in emanationist metaphysics taught in Neoplatonism, Hermeticism, Hinduism, and Theosophy, refers to the visible reality of space and time, energy and matter\"</p>\n<p>As we subscribe ourselves to worlds created or altered in Virtual or Augmented Reality, we are leaving and redefining our current plane of reality. In this mode of transition, it will become important to explore how our existing bodies simultaneously interact with physical and virtual environments.</p>\n<p>The Reality Plane begins to explore our interaction in Mixed Reality through added sensory cues in a familiar input device, a tablet.\u00a0</p>\n<h2>What it does</h2>\n<p>Reality Plane presents a completely blank surface onto which information can be projected in mixed or virtual reality. It has the potential to support natural interactions (touching, sliding, pinching, et cetera) and provides haptic feedback for interactions. Developers can create their own experiences while users experience a more natural mode of interactions (as compared with air gestures or game controllers)</p>\n<p>The orientation of the Reality Plane allows for different types of information to be displayed. In the demo game we created, when orientated horizontally a game map appears and displays the location of crystals to collect. Vertical orientation allows you to engage with the crystals that are floating in your environment. When you catch the crystals you simultaneously feel a buzz and hear the sound that each crystal emits.\u00a0</p>\n<p>Our demo game and tablet shows how it is possible to seamlessly integrate Arduino with Hololens, Vuforia and Unity to add tactile experience in addition to visual, audio, and motion.\u00a0</p>\n<h2>How we built it</h2>\n<ul>\n<li>Hololens as the primary mixed reality device. </li>\n<li>We created a Unity plugin for communicating with BTLE devices</li>\n<li>We created an Arduino Circuit, and integrated it into a custom fabricated tablet with a vibration motor to create haptic feedback</li>\n<li>We integrated Vuforia AR tracking into Hololens to be able to track the location of the tablet</li>\n<li>We also created an interactive \"game\" displaying some of the potential of the device.</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<p>We built a BLE library for unity from scratch, which took a long time to get working with consistency--it's already a great start but there's more to do</p>\n<p>We had to design haptic tablet in multiple iterations and test it to create a balanced physical UX. The viewport scale of </p>\n<p>Hololens creates a challenge in terms of designing the interaction between tablet, navigation and gestures that change the environment at any time.</p>\n<p>Vuforia performance create a delay in the content movement, we designed in the interface to minimize its use.</p>\n<p>One of our team members, Dale, had to drop out due to sickness. Feel better Dale!</p>\n<h2>Accomplishments that we're proud of</h2>\n<ul>\n<li>We created a new BLE library for Unity Windows 10 UWP.<br/></li>\n<li> Designing a tablet from scratch, fabricating with haptic responsive circuit.</li>\n<li>Integrating Vuforia and Hololens to create a unique game experience.</li>\n<li>Having something to present in 48 hours</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Lots about GATT and the UWP API</li>\n<li>Creating AR experiences in Unity</li>\n<li>It's possible to create and connect devices to Unity</li>\n<li>Learned about creating markers for Vuforia</li>\n<li>3D Software Cinema 4d</li>\n<li>Got much deeper into HoloLens Mixed Reality capabilities</li>\n<li>Design considerations building a physical haptic interface, such as weight, balance, etc.</li>\n</ul>\n<h2>What's next for Reality Plane</h2>\n<ul>\n<li>Adding IR Tracking, IMU, and other sensors and inputs to the tablet to make the experience even more compelling</li>\n<li>Using the rotation of the tablet to trigger menu items</li>\n</ul>\n</div>",
            "content_md": "\n## TEAM INFO\n\n\n**Team lead:** Igal Nassima\n\n\n**Team lead's telephone number:** 646 696 9286\n\n\n**We're located:** (Floor 6, table 7, on the corner where the balcony entrance door is)\n\n\n**Category:** Commerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)\n\n\n## Inspiration\n\n\n\"The physical plane, in emanationist metaphysics taught in Neoplatonism, Hermeticism, Hinduism, and Theosophy, refers to the visible reality of space and time, energy and matter\"\n\n\nAs we subscribe ourselves to worlds created or altered in Virtual or Augmented Reality, we are leaving and redefining our current plane of reality. In this mode of transition, it will become important to explore how our existing bodies simultaneously interact with physical and virtual environments.\n\n\nThe Reality Plane begins to explore our interaction in Mixed Reality through added sensory cues in a familiar input device, a tablet.\u00a0\n\n\n## What it does\n\n\nReality Plane presents a completely blank surface onto which information can be projected in mixed or virtual reality. It has the potential to support natural interactions (touching, sliding, pinching, et cetera) and provides haptic feedback for interactions. Developers can create their own experiences while users experience a more natural mode of interactions (as compared with air gestures or game controllers)\n\n\nThe orientation of the Reality Plane allows for different types of information to be displayed. In the demo game we created, when orientated horizontally a game map appears and displays the location of crystals to collect. Vertical orientation allows you to engage with the crystals that are floating in your environment. When you catch the crystals you simultaneously feel a buzz and hear the sound that each crystal emits.\u00a0\n\n\nOur demo game and tablet shows how it is possible to seamlessly integrate Arduino with Hololens, Vuforia and Unity to add tactile experience in addition to visual, audio, and motion.\u00a0\n\n\n## How we built it\n\n\n* Hololens as the primary mixed reality device.\n* We created a Unity plugin for communicating with BTLE devices\n* We created an Arduino Circuit, and integrated it into a custom fabricated tablet with a vibration motor to create haptic feedback\n* We integrated Vuforia AR tracking into Hololens to be able to track the location of the tablet\n* We also created an interactive \"game\" displaying some of the potential of the device.\n\n\n## Challenges we ran into\n\n\nWe built a BLE library for unity from scratch, which took a long time to get working with consistency--it's already a great start but there's more to do\n\n\nWe had to design haptic tablet in multiple iterations and test it to create a balanced physical UX. The viewport scale of \n\n\nHololens creates a challenge in terms of designing the interaction between tablet, navigation and gestures that change the environment at any time.\n\n\nVuforia performance create a delay in the content movement, we designed in the interface to minimize its use.\n\n\nOne of our team members, Dale, had to drop out due to sickness. Feel better Dale!\n\n\n## Accomplishments that we're proud of\n\n\n* We created a new BLE library for Unity Windows 10 UWP.\n* Designing a tablet from scratch, fabricating with haptic responsive circuit.\n* Integrating Vuforia and Hololens to create a unique game experience.\n* Having something to present in 48 hours\n\n\n## What we learned\n\n\n* Lots about GATT and the UWP API\n* Creating AR experiences in Unity\n* It's possible to create and connect devices to Unity\n* Learned about creating markers for Vuforia\n* 3D Software Cinema 4d\n* Got much deeper into HoloLens Mixed Reality capabilities\n* Design considerations building a physical haptic interface, such as weight, balance, etc.\n\n\n## What's next for Reality Plane\n\n\n* Adding IR Tracking, IMU, and other sensors and inputs to the tablet to make the experience even more compelling\n* Using the rotation of the tablet to trigger menu items\n\n\n"
        },
        {
            "source": "https://devpost.com/software/hashtag360-ygp2vi",
            "title": "hashtag360",
            "blurb": "Instagram but 360",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Jesslyn Tannady",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0d781df703734e1afb7f421e6b2c5bc6?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Seda Kochian",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2dec3d4d8c5e76e348fc677755fb446c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>helllo</p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for hashtag360</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nhelllo\n\n\n## What it does\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for hashtag360\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-reads",
            "title": "VR Reads",
            "blurb": "Enhance your reading experience through VR Reads.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Ira",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/1a206cf6aa29a9635aaedede7ca4927c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Nabib Ahmed",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/157447e130b8a8dbec2ff82f65606c26?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Merve \u00c7elebi",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7e239f2ad94c7c92429adca1ba72c2ec?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "gearvr",
                "javascript",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>My aunt Amanda has always enjoyed a good book, however two years ago, after a devastating car crash, she developed paralysis in most of her body, including her arms and hands, thus losing the ability to ever hold and flip through a book. She continues to follow her love of reading through audio books but sorely misses reading through pages. We were inspired to pursue this project by enabling her to experience books through pages once more. We hope this tool will enable her experience pages as she once did before.</p>\n<h2>What it does</h2>\n<p>We essentially created a VR tool to experience books and enhance the reading experience. Users will see pages of their books in VR and use head motions to flip through pages. In addition, the background will change relative to the content in the story or can be set by the user (thus giving them a pleasant visual environment to enjoy their reading).</p>\n<h2>How we built it</h2>\n<p>We used Unity, C#, and Javascript to develop an virtual environment that can be run through GearVR.</p>\n<h2>Challenges we ran into</h2>\n<p>Being that all team members were novices in VR and Unity programming, we had to overcome the learning curve required to complete our project.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were able to develop to our goal and hope that Amanda will soon be able to reunite with love and passion for reading through the visual pages.</p>\n<h2>What we learned</h2>\n<p>We learn the possibilities contained within the realm of VR along with technical skills using Unity, C#, GearVR, and Javascript</p>\n<h2>What's next for VR Reads</h2>\n<p>We hope to make it more dynamic, expand our library collection, and make the experience for all bookworms, like Amanda, more comfortable.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMy aunt Amanda has always enjoyed a good book, however two years ago, after a devastating car crash, she developed paralysis in most of her body, including her arms and hands, thus losing the ability to ever hold and flip through a book. She continues to follow her love of reading through audio books but sorely misses reading through pages. We were inspired to pursue this project by enabling her to experience books through pages once more. We hope this tool will enable her experience pages as she once did before.\n\n\n## What it does\n\n\nWe essentially created a VR tool to experience books and enhance the reading experience. Users will see pages of their books in VR and use head motions to flip through pages. In addition, the background will change relative to the content in the story or can be set by the user (thus giving them a pleasant visual environment to enjoy their reading).\n\n\n## How we built it\n\n\nWe used Unity, C#, and Javascript to develop an virtual environment that can be run through GearVR.\n\n\n## Challenges we ran into\n\n\nBeing that all team members were novices in VR and Unity programming, we had to overcome the learning curve required to complete our project.\n\n\n## Accomplishments that we're proud of\n\n\nWe were able to develop to our goal and hope that Amanda will soon be able to reunite with love and passion for reading through the visual pages.\n\n\n## What we learned\n\n\nWe learn the possibilities contained within the realm of VR along with technical skills using Unity, C#, GearVR, and Javascript\n\n\n## What's next for VR Reads\n\n\nWe hope to make it more dynamic, expand our library collection, and make the experience for all bookworms, like Amanda, more comfortable.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/microcinema",
            "title": "microCinema",
            "blurb": "VR based Cinema Experience",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Allen Hang",
                    "about": "I am mainly responsible for product management and programming. I learned how to  use the Unity 3D to build VR project and how to use Photon  Realtime and Phton Voice to build multi-player system. ",
                    "photo": "https://www.gravatar.com/avatar/a74f8a3d0f99a660f0956b6cafa8d052?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "android-studio",
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<p>Sometimes it is very difficult for us to get together watching a movie, or take part in an activity because we are so busy or maybe in two different regions far away. But when we have some free time, we want to be with our friends or family.</p>\n<p>We try to use Virtual Reality to solve this problem. Virtual Reality is a great technology and specially useful for bridging the gap between us.</p>\n<p>We use Unity 3D Game engine to build our project and we use Google Daydream Headset to test and load our project. Inside the project, we use Photon Realtime for multiplayer game implementation.</p>\n<p>The biggest problem we have met is that none of us have any past experience with Unity 3D design or C# programming experience. We basically start from zero and we have to learn everything in order to achieve our goal. But in less than 48 hours, we made it work. Honestly, it is beyond our initial expectation.</p>\n<p>The project is a VR-based cinema. Multi-users could use our project to watch the same movie and talk to each other. It uses eye-tracking control system so no extra controller needed.</p>\n<p>The most important thing for us is the valuable experience. After the VR hackathon, every one of us in the team knows how to build a VR project.</p>\n<p>In the future, we plan to extend our project not only to the cinema, but also any situation that could use live stream like drama or concert.  We hope we could connect people by sharing fun experience .</p>\n</div>",
            "content_md": "\nSometimes it is very difficult for us to get together watching a movie, or take part in an activity because we are so busy or maybe in two different regions far away. But when we have some free time, we want to be with our friends or family.\n\n\nWe try to use Virtual Reality to solve this problem. Virtual Reality is a great technology and specially useful for bridging the gap between us.\n\n\nWe use Unity 3D Game engine to build our project and we use Google Daydream Headset to test and load our project. Inside the project, we use Photon Realtime for multiplayer game implementation.\n\n\nThe biggest problem we have met is that none of us have any past experience with Unity 3D design or C# programming experience. We basically start from zero and we have to learn everything in order to achieve our goal. But in less than 48 hours, we made it work. Honestly, it is beyond our initial expectation.\n\n\nThe project is a VR-based cinema. Multi-users could use our project to watch the same movie and talk to each other. It uses eye-tracking control system so no extra controller needed.\n\n\nThe most important thing for us is the valuable experience. After the VR hackathon, every one of us in the team knows how to build a VR project.\n\n\nIn the future, we plan to extend our project not only to the cinema, but also any situation that could use live stream like drama or concert. We hope we could connect people by sharing fun experience .\n\n\n"
        },
        {
            "source": "https://devpost.com/software/augmented-beatz-7fiare",
            "title": "Augmented Beatz",
            "blurb": "Augmented Reality DJing!",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "So Sun Park",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2354742c40fbdd47ce56477769ea449e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tom Strissel",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/dc2e1ce7145d346bef2ade2290773d61?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Randall Spence",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/bff38e48f213cc38b69bc184c9fab3b2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Erik Warringer",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/1873cbb50e3fca1390d64e9424c786dd?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Ever want to get the party started at a completely random time and place? Well now you can! Augmented Beatz brings the party to you! Just put on your headset on a jam!</p>\n<h2>What it does</h2>\n<p>Augmented Beatz provides you with a DJ console and the power to rock out! You provide the moves, we provide the grooves!</p>\n<h2>How we built it</h2>\n<p>Augmented Beatz was built with love, care, Unity 3d, and Microsoft Hololens.</p>\n<h2>Challenges we ran into</h2>\n<p>Having never even used a Microsoft Hololens before yesterday, the largest difficulties we faced mostly involved understanding, testing, and applying the Hololens Gaze and Gesture functions.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're super proud of the UI, applying Hololens hand gestures to start and stop multiple tracks, and we are proud of seeing our idea come to life. It's a surreal experience!</p>\n<h2>What we learned</h2>\n<p>This weekend has been an astonishingly powerful learning experience for us all. As mentioned before, we had no prior experience with the Hololens, so it was incredibly fascinating to dive into this new world of advanced AR.  Having the opportunity to work with a team to build an app using some of the most cutting edge technology in the world is truly inspiring. Essentially, it feels as though we spent a weekend learning about the future.</p>\n<h2>What's next for Augmented Beatz</h2>\n<p>There are  so many additional functions we are planning to add after the hackathon is over. We would like to apply grab/hold functionality to the moving dials to allow the user to make custom adjustments to audio levels, pitch, tone and more within the augmented reality environment.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEver want to get the party started at a completely random time and place? Well now you can! Augmented Beatz brings the party to you! Just put on your headset on a jam!\n\n\n## What it does\n\n\nAugmented Beatz provides you with a DJ console and the power to rock out! You provide the moves, we provide the grooves!\n\n\n## How we built it\n\n\nAugmented Beatz was built with love, care, Unity 3d, and Microsoft Hololens.\n\n\n## Challenges we ran into\n\n\nHaving never even used a Microsoft Hololens before yesterday, the largest difficulties we faced mostly involved understanding, testing, and applying the Hololens Gaze and Gesture functions.\n\n\n## Accomplishments that we're proud of\n\n\nWe're super proud of the UI, applying Hololens hand gestures to start and stop multiple tracks, and we are proud of seeing our idea come to life. It's a surreal experience!\n\n\n## What we learned\n\n\nThis weekend has been an astonishingly powerful learning experience for us all. As mentioned before, we had no prior experience with the Hololens, so it was incredibly fascinating to dive into this new world of advanced AR. Having the opportunity to work with a team to build an app using some of the most cutting edge technology in the world is truly inspiring. Essentially, it feels as though we spent a weekend learning about the future.\n\n\n## What's next for Augmented Beatz\n\n\nThere are so many additional functions we are planning to add after the hackathon is over. We would like to apply grab/hold functionality to the moving dials to allow the user to make custom adjustments to audio levels, pitch, tone and more within the augmented reality environment.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/joycestick",
            "title": "JoyceStick",
            "blurb": "A digital humanities project employing Unity to construct a VR game from Joyce\u2019s Ulysses.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "ryan reede",
                    "about": "dev lead",
                    "photo": "https://www.gravatar.com/avatar/0c8125d5b7748720b98928c555764847?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Drew Hoo",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/11a203f73a9c20d8ae66c0f75ccbbb60?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Evan Otero",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/406/045/datas/profile.jpg"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<ul>\n<li>A Boston College digital humanities project employing Unity (game engine) to construct a number of immersive experiences\nof scenes from Joyce\u2019s Ulysses for viewing on the Oculus Rift and HTC Vive.</li>\n<li>Utilize MVC architecture and script in C#. The game development team also assists the 3D modeling team, where software\nsuch as Blender, Adobe Photoshop, and more are utilized.</li>\n<li>Serve as head of the website development team, where I deployed an extensively customized WordPress site</li>\n</ul>\n</div>",
            "content_md": "\n* A Boston College digital humanities project employing Unity (game engine) to construct a number of immersive experiences\nof scenes from Joyce\u2019s Ulysses for viewing on the Oculus Rift and HTC Vive.\n* Utilize MVC architecture and script in C#. The game development team also assists the 3D modeling team, where software\nsuch as Blender, Adobe Photoshop, and more are utilized.\n* Serve as head of the website development team, where I deployed an extensively customized WordPress site\n\n\n"
        },
        {
            "source": "https://devpost.com/software/lazar",
            "title": "LazAR",
            "blurb": "A mixed reality physical game using Lighthouse position tracking and Tango AR to create a laser maze puzzle",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Geva Patz",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2e63d4a78d3537913c0638bfc3356c23?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Douglas Patz",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/121591c0564092035499cb0056c35fea?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Javair Ratliff",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e953be4f1215746e0718182afb8889d0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "fpga",
                "intel-curie",
                "lighthouse",
                "steamvr",
                "tango",
                "unity"
            ],
            "content_html": "<div>\n<h2>Summary</h2>\n<p>We're all Vive developers who are interested in pushing the great Lighthouse tracking technology in new directions. We've developed hardware which works as a Lighthouse tracked object and were looking for a project to experiment with whole body tracking in a non-VR context. Mobile AR with Tango seemed like an excellent environment to pair this with.</p>\n<h2>Location</h2>\n<p>6th floor, table 39</p>\n<h2>Contact telephone</h2>\n<p>617.319.5217</p>\n<h2>Vertical</h2>\n<p>Entertainment &amp; storytelling (with a side of human connection)</p>\n</div>",
            "content_md": "\n## Summary\n\n\nWe're all Vive developers who are interested in pushing the great Lighthouse tracking technology in new directions. We've developed hardware which works as a Lighthouse tracked object and were looking for a project to experiment with whole body tracking in a non-VR context. Mobile AR with Tango seemed like an excellent environment to pair this with.\n\n\n## Location\n\n\n6th floor, table 39\n\n\n## Contact telephone\n\n\n617.319.5217\n\n\n## Vertical\n\n\nEntertainment & storytelling (with a side of human connection)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/block-e",
            "title": "Block-E",
            "blurb": "Innovate the way we play and the way we learn. Virtual blocks = infinite possibilities",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/TlT6amMaN_w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Aman Jha",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/780/312/datas/profile.jpg"
                },
                {
                    "name": "Jacob Nazarenko",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/849/767/datas/profile.JPG"
                },
                {
                    "name": "Peter Fan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/86af37fc3486d9e4601ff0bbdb7d6d37?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "json",
                "microsoft-hololens",
                "unity",
                "xml"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Limits have been shattered with advancements in augmented reality. These limits have been shattered in every industry, especially entertainment and education. Block-E aims to change the way we play and learn through blocks. </p>\n<p>Problems with current block games like LEGO:</p>\n<ul>\n<li>Easy to lose blocks</li>\n<li>Blocks are very expensive, especially custom blocks for sets</li>\n<li>Blocks are limited, they can only be used once in a creation</li>\n<li>Can be hard to pry apart blocks</li>\n<li>Stepping on them hurts!!!\nBlock-E eliminates all of those problems through visualizing everything. Since all blocks are digitized, they're impossible to lose, they're easy to replicate and the user has access to an infinite amount of blocks. Blocks can be used in any creation, and multiple creations can be saved by one user with ease. The interface is incredibly easy to work with - users can change entire swaths of blocks' color through a couple of clicks, and can separate pieces through deleting them with a single airtap. Best of all - stepping on a holographic block does no damage, unlike regular LEGOs.</li>\n</ul>\n<p>Block-E can change the way we approach education. By using a modular codebase, Block-E can change the way we teach robotics and inspire creativity in the household. Virtual simulations let students create and explore to their hearts content. The limitation is no longer infrastructure, but rather, their creativity.</p>\n<h2>What it does</h2>\n<p>Block-E is a virtual platform for block development. Built on a highly flexible framework, Block-E combines the best of the virtual and real world to make an augmented reality app that transcends real life. Through blocks, the application revolutionizes block building games. By virtualizing everything, Block-E offers more flexibility and features than current systems, at a much cheaper cost. Block-E lets the user select from an array of bricks and colors to sculpt large creations in a virtual environment. </p>\n<h2>How we built it</h2>\n<p>The HoloLens application was entirely built in Unity3D. We first created the application for PC. Using a free movement camera, we tested the application until we finished all the basic functionality. Because deploying to Hololens was heavily delayed due to IP connection issues, most of the code was initially created for PC gameplay. Once we got Hololens working, we moved onto pure Hololens development. Despite all the setbacks, we were able to port the entire framework into Hololens. </p>\n<h2>Challenges we ran into</h2>\n<p>The number one challenge was the use of HoloLens from an application originally designed for PC. We had originally created the application for use in the Unity Editor, including having commands like OnMouseEnter. When we had to shift it to HoloLens, everything was simple in the manner of changing the commands to something like OnGazeEnter. However, for the first 17 hours of the hackathon, we were unable to deploy to HoloLens. </p>\n<h3>Deployment Issues</h3>\n<p>Though one of our computers was able to interface to the HoloLens, the main Unity coder was unable to connect to the HoloLens. Multiple mentors from Microsoft and Unity were perplexed as to the source of the issue - Visual Studio was updated, the device was repaired, the computer and device restarted several times, but to no avail. Everytime the project built and got ready to deploy, Visual Studio would complain and crash. \nThe problem was that the IP service on the laptop - one of the mentors ran a command in command prompt which enabled the computer to interface to the HoloLens - but by then it was midnight and 17 hours were gone. </p>\n<h3>Saving</h3>\n<p>Our second major issue was saving. Unlike the simple saving techniques of a computer, Universal Windows Platform is significantly more complex and requires different storage protocols. The challenge took the majority of the second day, but was a very important problem that needed to be solved in order to finish the product. </p>\n<h2>Accomplishments that we're proud of</h2>\n<h3>Shared Worldview</h3>\n<p>Playing with blocks as a child was mostly a multiplayer experience. As a kid, we visited other friends' houses in order to create together. This point is not lost upon us. Using Hololens's world coordinate sharing system, we let users share their experience with other Hololens users. This adds another dimension in Block-E. \nWe accomplished this through use of a laptop to create a service. We will create a client app in the future to streamline this process. The two Hololens's connect to the same service and through spacial mapping, can sync their worlds. </p>\n<h3>Saving</h3>\n<p>One of our largest problems was finding a proper way to save user creations to be reopened at a later time. Being able to save is an essential part of any creation application. Initially we planned on using XML serialization to save on the Hololens drive similar to saving on a PC, but things weren't so simple. Saving and reading from files on Hololens is in its very early stages, and right away, we know that things were going to get messy. Our goal was to serialize an array of objects, obtain a string, and write it to a json file. Unfortunately, the libraries capable of saving files regularly on a computer were incompatible with the ones under the Universal Windows Platform (UWP) needed to save files on a Hololens device. Furthermore, there is no possibility to open a dialog for saving a file in a certain location. Despite the fact that a temporary location and file name needed to be hard-coded into the code for exporting, we managed to run the app, and to add a voice command for this functionality. </p>\n<h3>Flexible Codebase</h3>\n<p>We're very proud of our highly flexible codebase. The block system is very modular, meaning a database of hundreds of types of blocks can be easily integrated into the game. The flexible system also lets the user incorporate different block types. Though we only have 3 prototype blocks to showcase the application, the user can theoretically link plates, tiles,hinges and other complex blocks. </p>\n<h2>What we learned</h2>\n<p>The entire experience was an excellent way to get familiar with Microsoft HoloLens and AR devices. We learned the differences between developing on Universal Windows Platform compared to regular applications. Specifically, we learned the most through the problems we solved. </p>\n<ul>\n<li>System storage architecture - the best way to serialize and save files on UWP and regular PC's. </li>\n<li>Augmented Reality development - the differences between augmented reality and virtual reality, and the applications</li>\n<li>Design principles - there are ways to create HUD UI and create user-friendly experiences that are intuitive and comfortable - one of the most important things we learned at this hackathon</li>\n<li>Professional design - our team is entirely made of students (two of us are high schoolers). At this hackathon, we were able to watch and learn from professional </li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nLimits have been shattered with advancements in augmented reality. These limits have been shattered in every industry, especially entertainment and education. Block-E aims to change the way we play and learn through blocks. \n\n\nProblems with current block games like LEGO:\n\n\n* Easy to lose blocks\n* Blocks are very expensive, especially custom blocks for sets\n* Blocks are limited, they can only be used once in a creation\n* Can be hard to pry apart blocks\n* Stepping on them hurts!!!\nBlock-E eliminates all of those problems through visualizing everything. Since all blocks are digitized, they're impossible to lose, they're easy to replicate and the user has access to an infinite amount of blocks. Blocks can be used in any creation, and multiple creations can be saved by one user with ease. The interface is incredibly easy to work with - users can change entire swaths of blocks' color through a couple of clicks, and can separate pieces through deleting them with a single airtap. Best of all - stepping on a holographic block does no damage, unlike regular LEGOs.\n\n\nBlock-E can change the way we approach education. By using a modular codebase, Block-E can change the way we teach robotics and inspire creativity in the household. Virtual simulations let students create and explore to their hearts content. The limitation is no longer infrastructure, but rather, their creativity.\n\n\n## What it does\n\n\nBlock-E is a virtual platform for block development. Built on a highly flexible framework, Block-E combines the best of the virtual and real world to make an augmented reality app that transcends real life. Through blocks, the application revolutionizes block building games. By virtualizing everything, Block-E offers more flexibility and features than current systems, at a much cheaper cost. Block-E lets the user select from an array of bricks and colors to sculpt large creations in a virtual environment. \n\n\n## How we built it\n\n\nThe HoloLens application was entirely built in Unity3D. We first created the application for PC. Using a free movement camera, we tested the application until we finished all the basic functionality. Because deploying to Hololens was heavily delayed due to IP connection issues, most of the code was initially created for PC gameplay. Once we got Hololens working, we moved onto pure Hololens development. Despite all the setbacks, we were able to port the entire framework into Hololens. \n\n\n## Challenges we ran into\n\n\nThe number one challenge was the use of HoloLens from an application originally designed for PC. We had originally created the application for use in the Unity Editor, including having commands like OnMouseEnter. When we had to shift it to HoloLens, everything was simple in the manner of changing the commands to something like OnGazeEnter. However, for the first 17 hours of the hackathon, we were unable to deploy to HoloLens. \n\n\n### Deployment Issues\n\n\nThough one of our computers was able to interface to the HoloLens, the main Unity coder was unable to connect to the HoloLens. Multiple mentors from Microsoft and Unity were perplexed as to the source of the issue - Visual Studio was updated, the device was repaired, the computer and device restarted several times, but to no avail. Everytime the project built and got ready to deploy, Visual Studio would complain and crash. \nThe problem was that the IP service on the laptop - one of the mentors ran a command in command prompt which enabled the computer to interface to the HoloLens - but by then it was midnight and 17 hours were gone. \n\n\n### Saving\n\n\nOur second major issue was saving. Unlike the simple saving techniques of a computer, Universal Windows Platform is significantly more complex and requires different storage protocols. The challenge took the majority of the second day, but was a very important problem that needed to be solved in order to finish the product. \n\n\n## Accomplishments that we're proud of\n\n\n### Shared Worldview\n\n\nPlaying with blocks as a child was mostly a multiplayer experience. As a kid, we visited other friends' houses in order to create together. This point is not lost upon us. Using Hololens's world coordinate sharing system, we let users share their experience with other Hololens users. This adds another dimension in Block-E. \nWe accomplished this through use of a laptop to create a service. We will create a client app in the future to streamline this process. The two Hololens's connect to the same service and through spacial mapping, can sync their worlds. \n\n\n### Saving\n\n\nOne of our largest problems was finding a proper way to save user creations to be reopened at a later time. Being able to save is an essential part of any creation application. Initially we planned on using XML serialization to save on the Hololens drive similar to saving on a PC, but things weren't so simple. Saving and reading from files on Hololens is in its very early stages, and right away, we know that things were going to get messy. Our goal was to serialize an array of objects, obtain a string, and write it to a json file. Unfortunately, the libraries capable of saving files regularly on a computer were incompatible with the ones under the Universal Windows Platform (UWP) needed to save files on a Hololens device. Furthermore, there is no possibility to open a dialog for saving a file in a certain location. Despite the fact that a temporary location and file name needed to be hard-coded into the code for exporting, we managed to run the app, and to add a voice command for this functionality. \n\n\n### Flexible Codebase\n\n\nWe're very proud of our highly flexible codebase. The block system is very modular, meaning a database of hundreds of types of blocks can be easily integrated into the game. The flexible system also lets the user incorporate different block types. Though we only have 3 prototype blocks to showcase the application, the user can theoretically link plates, tiles,hinges and other complex blocks. \n\n\n## What we learned\n\n\nThe entire experience was an excellent way to get familiar with Microsoft HoloLens and AR devices. We learned the differences between developing on Universal Windows Platform compared to regular applications. Specifically, we learned the most through the problems we solved. \n\n\n* System storage architecture - the best way to serialize and save files on UWP and regular PC's.\n* Augmented Reality development - the differences between augmented reality and virtual reality, and the applications\n* Design principles - there are ways to create HUD UI and create user-friendly experiences that are intuitive and comfortable - one of the most important things we learned at this hackathon\n* Professional design - our team is entirely made of students (two of us are high schoolers). At this hackathon, we were able to watch and learn from professional\n\n\n"
        },
        {
            "source": "https://devpost.com/software/babytuckoo",
            "title": "BabyTuckoo",
            "blurb": "VR game: Creating a nexus of nodes with your eyes",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Devi Acharya",
                    "about": "3D modeling in blender and particle effects.",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xjj5WPLzkHC3Cf7LxJ29WAvbXWNuCu7LjpdVWA6FRu8ggHZ51RDWFltdoFqt3wmdYO0UQnZfBBIs?height=180&width=180"
                },
                {
                    "name": "Joseph Gillen",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/963/datas/profile.jpg"
                },
                {
                    "name": "Isaiah Mann",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/391/132/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "glsl"
            ],
            "content_html": "<div>\n<p>Link: <a href=\"https://github.com/imann24/BabyTuckoo\" rel=\"nofollow\">https://github.com/imann24/BabyTuckoo</a></p>\n</div>",
            "content_md": "\nLink: <https://github.com/imann24/BabyTuckoo>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/hololegotoy",
            "title": "HoloCity",
            "blurb": "HoloCity is a reconfigurable city simulation tool using Tangible Interface (LEGO/Magnet) and Mixed Reality (HoloLens)",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Poseidon Ho",
                    "about": "\u2022 Developed real-time simulation and data visualization for holographic coordinate system over tangible interfaces (LEGO).\n\u2022 Developed functions for user to interact and overlay digital information with physical models using virtual inputs/feedbacks.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/217/233/datas/profile.jpg"
                },
                {
                    "name": "deleted deleted",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5579f9c2a6e80224d388008f7c4ea431?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Han-Chih Kuo",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d932d47959cc4369deface25e2e16367?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "riaz munshi",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/84ad1e350743cbfc0e670d08602f45a6?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Bolin Zhu",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/6730682?height=180&v=4&width=180"
                },
                {
                    "name": "Ran Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3e6f1722c7fe2935531200bbd05b2bf1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "gausstoys",
                "lego",
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>There is information on cities, housing, and transportation, all freely available to anyone, but seeing numbers and attributes on paper doesn\u2019t quite translate the reality of what someone actually experiences. Inspired by Minecraft, we wanted to create a more personalized experience of city simulation through the use of Lego blocks and Microsoft HoloLens. </p>\n<h2>What it does</h2>\n<p>Our project allows users to experience real-time simulation of housing and transportation through use of a Hololens. Different colors and sizes of the LEGO blocks represent various types of zoning, such as commercial, residential, government, and nature. Flow and density of traffic in an area is visualized through the color and scale of drawn magnetic fields. Other properties such as capacity, walkability between vehicles and buildings, and many more attributes can be easily presented and understood. This has direct applications both for governments in the sense of city planning, communities for envisioning future projects, and for individuals looking to find the perfect neighborhood to call home.</p>\n<h2>How we built it</h2>\n<p>The scope of our project has 3 main parts: \nModels: Physical objects in the form of LEGOs and magnets, and digital information, such as residential populations and job opportunities.\nViews: Mixed reality view from HoloLens.\nControllers: Gaze, gesture, and voice as inputs in HoloLens. Press, Push, and Drag as inputs for the magnetic system.\nUsers interact with the physical models by reconfiguring the LEGO blocks and manipulating the magnetic blocks. These actions trigger corresponding holograms and overlay digital information in the Hololens.</p>\n<h2>Challenges we ran into</h2>\n<p>For collaboration of a shared hologram with multiple HoloLens, we found it is hard to define the global coordinates, as well as letting player see each other\u2019s interaction in real-time. Initially we Ran spent the entire weekend researching networking (web sockets) between two HoloLens and our physical models (LEGOs and magnets), but due to limited time constraints we decided to instead focus on enhancing an individual user\u2019s interactions (gaze, gesture, and voice) with a HoloLens and the models. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Poseidon and Riaz developed the software for Unity with the HoloLens. Han-Chih worked on the magnets and Ran was responsible for the LEGO block integration. Bolin created and maintained the entire design guidelines and user experience between bits and atoms. </p>\n<h2>What we learned</h2>\n<p>In order to trigger digital information on the physical models with virtual inputs using HoloLens, Poseidon and Riaz came up with a grid system overlaying MetaCity, which allows users to combine Gaze and Air Tap to display information and use Voice for searching a specific building. As a whole, we learned a lot about the limitation and application of transforming LEGO blocks and magnet blocks into tangible bits, allowing governments and citizens to do city simulation with an immersive personal experience. </p>\n<h2>What's next for MetaCity</h2>\n<p>First we\u2019re opening all of our source codes, as some of them could be APIs for developers to dive deeper into the applications beyond city simulations. Secondly, we are thinking to make these tools into LEGO and Magnet Developer Kits for kids to create their own cities. The best output of this project might be used by governments, citizens and research labs for collaborative simulation on cities, especially housing and transportation. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThere is information on cities, housing, and transportation, all freely available to anyone, but seeing numbers and attributes on paper doesn\u2019t quite translate the reality of what someone actually experiences. Inspired by Minecraft, we wanted to create a more personalized experience of city simulation through the use of Lego blocks and Microsoft HoloLens. \n\n\n## What it does\n\n\nOur project allows users to experience real-time simulation of housing and transportation through use of a Hololens. Different colors and sizes of the LEGO blocks represent various types of zoning, such as commercial, residential, government, and nature. Flow and density of traffic in an area is visualized through the color and scale of drawn magnetic fields. Other properties such as capacity, walkability between vehicles and buildings, and many more attributes can be easily presented and understood. This has direct applications both for governments in the sense of city planning, communities for envisioning future projects, and for individuals looking to find the perfect neighborhood to call home.\n\n\n## How we built it\n\n\nThe scope of our project has 3 main parts: \nModels: Physical objects in the form of LEGOs and magnets, and digital information, such as residential populations and job opportunities.\nViews: Mixed reality view from HoloLens.\nControllers: Gaze, gesture, and voice as inputs in HoloLens. Press, Push, and Drag as inputs for the magnetic system.\nUsers interact with the physical models by reconfiguring the LEGO blocks and manipulating the magnetic blocks. These actions trigger corresponding holograms and overlay digital information in the Hololens.\n\n\n## Challenges we ran into\n\n\nFor collaboration of a shared hologram with multiple HoloLens, we found it is hard to define the global coordinates, as well as letting player see each other\u2019s interaction in real-time. Initially we Ran spent the entire weekend researching networking (web sockets) between two HoloLens and our physical models (LEGOs and magnets), but due to limited time constraints we decided to instead focus on enhancing an individual user\u2019s interactions (gaze, gesture, and voice) with a HoloLens and the models. \n\n\n## Accomplishments that we're proud of\n\n\nPoseidon and Riaz developed the software for Unity with the HoloLens. Han-Chih worked on the magnets and Ran was responsible for the LEGO block integration. Bolin created and maintained the entire design guidelines and user experience between bits and atoms. \n\n\n## What we learned\n\n\nIn order to trigger digital information on the physical models with virtual inputs using HoloLens, Poseidon and Riaz came up with a grid system overlaying MetaCity, which allows users to combine Gaze and Air Tap to display information and use Voice for searching a specific building. As a whole, we learned a lot about the limitation and application of transforming LEGO blocks and magnet blocks into tangible bits, allowing governments and citizens to do city simulation with an immersive personal experience. \n\n\n## What's next for MetaCity\n\n\nFirst we\u2019re opening all of our source codes, as some of them could be APIs for developers to dive deeper into the applications beyond city simulations. Secondly, we are thinking to make these tools into LEGO and Magnet Developer Kits for kids to create their own cities. The best output of this project might be used by governments, citizens and research labs for collaborative simulation on cities, especially housing and transportation. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/isaac-s-lab",
            "title": "Isaac's Lab",
            "blurb": "Basic Physics Education via Experiments and Demos in Virtual Reality",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Loren Ybarrondo",
                    "about": "I'm new to Unity, but was lucky enough to get on a team with some real experts.  With their guidance, I quickly got up to speed on the basics and was able to apply my background in other applications to our project development.  I worked on sourcing, conversion, and importing some of the models that we used.  I also staged some of the scenes that were then integrated with the overall project.  I was also able to develop some of the scripts used to overlay contextual information (e.g., velocity text).  As a mechanical engineer with a background in instructional design, I helped with storyboarding and the underlying equations of motion we used to govern the VR world.  I brought all kinds of equipment, tools, and tricks that we leveraged to hack together a very cool project.  Lastly, this was an amazing weekend and I hope that my enthusiasm was contagious.  We definitely meshed as a team and worked very well together.  ",
                    "photo": "https://www.gravatar.com/avatar/e4593634dc4d39438c9f1cafa0a04745?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Christopher Hong",
                    "about": "My main role in this project was to develop the scripts needed to do the physics computations for drawing the velocity vectors and free body diagram vectors on the moving objects and projectiles. In addition, I developed the graphs needed to plot the position vs. time, velocity vs. time, and acceleration vs. time.",
                    "photo": "https://www.gravatar.com/avatar/e6c5f7e4c249ebc253c0e633bb81f509?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Nick Peck",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/4b58fe74c326d219c756440123854d11?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Mengjia Luo",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ccd6c17725c58fdc15cc181ef62f1f21?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dan Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/1bd50dba4e6373218b07ced76a2a1315?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "htc-vive",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Bored of textbook physics education? Check out Isaac's Lab! With HTC Vive's virtual reality, the physical concepts which fascinated humans for centuries no longer have to be constrained by ink and paper. Our team hopes to use surreal worlds to give kids a hands-on and \"real\" experience with basic physics, while exposing them to the math and science at play. </p>\n<h2>What it does</h2>\n<p>Issac's Lab simulates classic physics examples such as the ball drop (gravity) and cannon shot (parabolic motion). It takes advantage of the interactive nature of virtual reality for kids to take rare actions or difficult experiments. Seriously, how often does a kid get to shoot a cannon ball for learning velocity? </p>\n<h2>How we built it</h2>\n<p>As for our user interface, we used Rhino and 3D Max for modeling. As for the physics within, we wrote every physical interaction with Unity C# script; for example, arrows denoting the forces at work for the cannon shot are calculated and illustrated each frame with a single C# script.  </p>\n<h2>Challenges we ran into</h2>\n<p>Our biggest challenge was Unity. The majority of our team members did not have experience with the game engine or C#, so it took a while for all of us to get up to speed. The other issue we faced was integration of various components worked by different members. Because Unity does not have a collaboration platform and some members were new to Github, a lot of time was spent on combining scenes. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We all met separately at the team formation. We came together to form a cohesive team build on diverse talents. \nLearning Unity!  </p>\n<h2>What we learned</h2>\n<p>Developing for a virtual environment is rewarding and challenging.  You can do so much more with an immersive medium, but to do it well you must have a compelling story and follow good design practices.</p>\n<h2>What's next for Isaac's Lab</h2>\n<p>We are hoping to add more experiments to Issac's Lab, and to polish off the UI/UX. We also hope to refine the current virtual experiments so that they form a sound instructional course on the equations of motion and then other lesson plans. We will also seek sponsorship to advance our work in support of STEAM education. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nBored of textbook physics education? Check out Isaac's Lab! With HTC Vive's virtual reality, the physical concepts which fascinated humans for centuries no longer have to be constrained by ink and paper. Our team hopes to use surreal worlds to give kids a hands-on and \"real\" experience with basic physics, while exposing them to the math and science at play. \n\n\n## What it does\n\n\nIssac's Lab simulates classic physics examples such as the ball drop (gravity) and cannon shot (parabolic motion). It takes advantage of the interactive nature of virtual reality for kids to take rare actions or difficult experiments. Seriously, how often does a kid get to shoot a cannon ball for learning velocity? \n\n\n## How we built it\n\n\nAs for our user interface, we used Rhino and 3D Max for modeling. As for the physics within, we wrote every physical interaction with Unity C# script; for example, arrows denoting the forces at work for the cannon shot are calculated and illustrated each frame with a single C# script. \n\n\n## Challenges we ran into\n\n\nOur biggest challenge was Unity. The majority of our team members did not have experience with the game engine or C#, so it took a while for all of us to get up to speed. The other issue we faced was integration of various components worked by different members. Because Unity does not have a collaboration platform and some members were new to Github, a lot of time was spent on combining scenes. \n\n\n## Accomplishments that we're proud of\n\n\nWe all met separately at the team formation. We came together to form a cohesive team build on diverse talents. \nLearning Unity! \n\n\n## What we learned\n\n\nDeveloping for a virtual environment is rewarding and challenging. You can do so much more with an immersive medium, but to do it well you must have a compelling story and follow good design practices.\n\n\n## What's next for Isaac's Lab\n\n\nWe are hoping to add more experiments to Issac's Lab, and to polish off the UI/UX. We also hope to refine the current virtual experiments so that they form a sound instructional course on the equations of motion and then other lesson plans. We will also seek sponsorship to advance our work in support of STEAM education. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/working-title-vr-language-acquisiton-application",
            "title": "Into the Mountains: A VR Language Immersion Experience",
            "blurb": "Learning language through immersion and captivating story",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "An example of one of the future levels",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/232/datas/original.jpg"
                },
                {
                    "title": "Intro UI: Customized menus so you can choose your language before entering the experience.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/233/datas/original.jpg"
                },
                {
                    "title": "Intro UI: Customized menus so you can choose your language before entering the experience.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/234/datas/original.jpg"
                },
                {
                    "title": "Intro UI: Customized menus so you can choose your language before entering the experience.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/235/datas/original.jpg"
                },
                {
                    "title": "Intro UI: Customized menus so you can choose your language before entering the experience.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/236/datas/original.jpg"
                },
                {
                    "title": "Intro UI: Customized menus so you can choose your level of difficulty before entering the experience.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/237/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Kiera Johnson",
                    "about": "I created the story and wrote the script that our experience follows. During the process I oversaw story, design, and sound. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/679/datas/profile.JPG"
                },
                {
                    "name": "Matt Maes",
                    "about": "I produced sketches, created the Hard-Surface assets and assisted in overall production flow. ",
                    "photo": "https://www.gravatar.com/avatar/7c1843dc1b10b862dd26b05b174f11e5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Viraj Rai",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/52f6296ba873aa13c9b26f58e8ffa468?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Alfredo Barzola",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/253/datas/profile.JPG"
                },
                {
                    "name": "Katy Hamilton",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/bfda3312fab2ab0f275d91fadbdb22dc?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "audacity",
                "blender",
                "unity"
            ],
            "content_html": "<div>\n<p>We wanted to make language learning immersive and enjoyable for adults and children alike. </p>\n<h2>Inspiration</h2>\n<p>Currently, language learning is often a chore. One has to block off time in a day to go through programs like Rosetta Stone or DuoLingo, and even with gamification aspects it can still be mundane. How fun is it really to simulate buying groceries or paying for a taxi? We wanted to make an immersive language experience that is enjoyable for the user while fully immersive in the chosen language. We wanted an experience that you would want to explore beautiful levels and solve a mystery, even without the added language learning bonus. Language learning should be immersive, interactive, and enjoyable, and make you look forward to learning more.</p>\n<h2>What it does</h2>\n<p>The program is an immersive language learning adventure. Your mysterious orb host will guide you through beautiful, striking levels as you discover who this mysterious child is and how she is connected to you and where to find her. It is fully in the chosen language with no translations, and you will surprise yourself as you continue through levels of the amount of words and phrases you will begin to recognize through full immersion and repetition of phrases and words in levels. </p>\n<h2>How we built it</h2>\n<p>We mostly used Unity and Blender, with assets from Incompetech and TurboSquid for music and 3D models. </p>\n<h2>Challenges we ran into</h2>\n<p>Our developers had programing experience, but did not have experience with Unity and the game development environment. Because of this, design conversations had to be delayed until proof of deliverability. And since there was a learning curve involved, time constraints were especially felt in this whole process.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our developers are very proud of how much Unity they were able to learn in a short span of time. The fact that we built something that works is always cool! We\u2019re proud of our idea, and we believe it has originality and a lot of future potential. Finally, we built a VR app with no prior experience with VR - something our developers didn\u2019t imagine they\u2019d be doing this weekend! </p>\n<h2>What's next for Into the Mountains: A VR language immersion experience</h2>\n<p>We want to expand our world and create more experiences and levels, as well as polish and finish our current level. </p>\n<h2>Location</h2>\n<p>We're on the third floor. Check us out!</p>\n</div>",
            "content_md": "\nWe wanted to make language learning immersive and enjoyable for adults and children alike. \n\n\n## Inspiration\n\n\nCurrently, language learning is often a chore. One has to block off time in a day to go through programs like Rosetta Stone or DuoLingo, and even with gamification aspects it can still be mundane. How fun is it really to simulate buying groceries or paying for a taxi? We wanted to make an immersive language experience that is enjoyable for the user while fully immersive in the chosen language. We wanted an experience that you would want to explore beautiful levels and solve a mystery, even without the added language learning bonus. Language learning should be immersive, interactive, and enjoyable, and make you look forward to learning more.\n\n\n## What it does\n\n\nThe program is an immersive language learning adventure. Your mysterious orb host will guide you through beautiful, striking levels as you discover who this mysterious child is and how she is connected to you and where to find her. It is fully in the chosen language with no translations, and you will surprise yourself as you continue through levels of the amount of words and phrases you will begin to recognize through full immersion and repetition of phrases and words in levels. \n\n\n## How we built it\n\n\nWe mostly used Unity and Blender, with assets from Incompetech and TurboSquid for music and 3D models. \n\n\n## Challenges we ran into\n\n\nOur developers had programing experience, but did not have experience with Unity and the game development environment. Because of this, design conversations had to be delayed until proof of deliverability. And since there was a learning curve involved, time constraints were especially felt in this whole process.\n\n\n## Accomplishments that we're proud of\n\n\nOur developers are very proud of how much Unity they were able to learn in a short span of time. The fact that we built something that works is always cool! We\u2019re proud of our idea, and we believe it has originality and a lot of future potential. Finally, we built a VR app with no prior experience with VR - something our developers didn\u2019t imagine they\u2019d be doing this weekend! \n\n\n## What's next for Into the Mountains: A VR language immersion experience\n\n\nWe want to expand our world and create more experiences and levels, as well as polish and finish our current level. \n\n\n## Location\n\n\nWe're on the third floor. Check us out!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/virtually-a-genome",
            "title": "Virtually A Genome",
            "blurb": "Visualizing the genome in virtual reality",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Two insulated neighborhoods",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/261/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "dborgesr",
                    "about": "I provided the backend JSON files which fully describes and annotates the Insulated Neighborhoods for each Gene which is displayed in the viewer.",
                    "photo": "https://www.gravatar.com/avatar/0ba5664082199d26f8be4e1b2cc000a5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Eileen Hing",
                    "about": "I helped with generating the visual assets, defining the problem, needs and solution from a researcher's perspective.",
                    "photo": "https://www.gravatar.com/avatar/66a0fe260659b0639864543843335ab3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Seth Persigehl",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/6101485?height=180&v=4&width=180"
                },
                {
                    "name": "Christina Kayastha",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/358a56d25c46052f8ec8a2838c0336f0?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "aframe",
                "unreal-engine"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Currently Biologists visualize the genome in one dimension (1D), although we are well aware that the genome is highly structured within the nucleus (~2 meters of DNA folded into a 6 micrometer diameter sphere, perfectly every time). Far away regions, upwards of 1.5 megabases, can loop to and affect the expression of genes. This 1D viewing has led biologists to incorrectly assign disease causing variations leading to decades of wasted research and millions of research dollars </p>\n<h2>What it does</h2>\n<p>Visualizing the genome in VR with relevant annotation, provides an entirely new perspective, allowing for instant assessments and predictions as to what variation causes disease. Using CRISPR we can now edit that variation down to the single basepair, effectively curing the disease permanently. Here we showcase two phenotypes, variation causing obesity and polydactyly (extra fingers)</p>\n<h2>How we built it</h2>\n<p>Using DNA interactions, large scale databases, machine learning, A-Frame, Unreal engine 4, and cardboard all talking through github we are able to bring everything under one umbrella</p>\n<h2>Challenges we ran into</h2>\n<p>A-Frame decided it was no longer going to play nice with HTC-Vive :(</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The novel perspective n the genome this gives will change what hypothesis are generated by every biologist who is studying genetic diseases, biologists are now able to actually walk through and interact with the relevant portions of the genome for their given gene of interest. The implementations within webVR allows for a wide adoption, essentially instantly.</p>\n<h2>What we learned</h2>\n<p>How to bring together an eclectic group of people (from VR developer, to chemist, to computational biologist, to javascripter) and an equally eclectic set of tools and databases, into a functional, useful, and revolutionary view of the genome.</p>\n<h2>What's next for Virtually A Genome</h2>\n<p>Because of its integration with large, ever-updating, databases VirtuallyAGenoome will continuously evolve, with plenty of room to integrate new databases.</p>\n<h4>Diego Borges</h4>\n<h4>617-447-4962</h4>\n<p>Building Floor: 3, at the ping pong table</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nCurrently Biologists visualize the genome in one dimension (1D), although we are well aware that the genome is highly structured within the nucleus (~2 meters of DNA folded into a 6 micrometer diameter sphere, perfectly every time). Far away regions, upwards of 1.5 megabases, can loop to and affect the expression of genes. This 1D viewing has led biologists to incorrectly assign disease causing variations leading to decades of wasted research and millions of research dollars \n\n\n## What it does\n\n\nVisualizing the genome in VR with relevant annotation, provides an entirely new perspective, allowing for instant assessments and predictions as to what variation causes disease. Using CRISPR we can now edit that variation down to the single basepair, effectively curing the disease permanently. Here we showcase two phenotypes, variation causing obesity and polydactyly (extra fingers)\n\n\n## How we built it\n\n\nUsing DNA interactions, large scale databases, machine learning, A-Frame, Unreal engine 4, and cardboard all talking through github we are able to bring everything under one umbrella\n\n\n## Challenges we ran into\n\n\nA-Frame decided it was no longer going to play nice with HTC-Vive :(\n\n\n## Accomplishments that we're proud of\n\n\nThe novel perspective n the genome this gives will change what hypothesis are generated by every biologist who is studying genetic diseases, biologists are now able to actually walk through and interact with the relevant portions of the genome for their given gene of interest. The implementations within webVR allows for a wide adoption, essentially instantly.\n\n\n## What we learned\n\n\nHow to bring together an eclectic group of people (from VR developer, to chemist, to computational biologist, to javascripter) and an equally eclectic set of tools and databases, into a functional, useful, and revolutionary view of the genome.\n\n\n## What's next for Virtually A Genome\n\n\nBecause of its integration with large, ever-updating, databases VirtuallyAGenoome will continuously evolve, with plenty of room to integrate new databases.\n\n\n#### Diego Borges\n\n\n#### 617-447-4962\n\n\nBuilding Floor: 3, at the ping pong table\n\n\n"
        },
        {
            "source": "https://devpost.com/software/a-tech-of-the-blocks-yu71aq",
            "title": "A-tech of the Blocks",
            "blurb": "Modernizing the classic game of Tetris with a touch of alchemy and danger",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Beginnings",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/020/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Aidan McLaughlin",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/77a5e9214cd609ca642ebf4f34dd10b2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Brandon Henriquez",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5695a81b47144755de434ec2dd7e45d1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The inspiration comes from Brandon who wanted to use AR/VR to recreate Tetris using new technology for a more engaging experience.</p>\n<h2>What it does</h2>\n<p>Makes Tetris 3D! We would like to add material properties beyond just looks so that, while still playing by the classic mechanics, the player must also pay attention to interactions between blocks in 3 dimensions (why you see wood, dirt, ice, concrete, etc.)</p>\n<h2>How we built it</h2>\n<p>We built it using Unity and various Vive plugins.</p>\n<h2>Challenges we ran into</h2>\n<p>Learning to use VR input in Unity in a consistent way. Lots of trial and error. Also Matrices. In general.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p><a href=\"https://drive.google.com/file/d/0ByKYBqi5ISu0MHREOXJKUzJwLW8/view?usp=sharing\" rel=\"nofollow\">https://drive.google.com/file/d/0ByKYBqi5ISu0MHREOXJKUzJwLW8/view?usp=sharing</a>\n!!!</p>\n<h2>What we learned</h2>\n<h2>What's next for A-tech of the Blocks</h2>\n<p>Making the elements a significant game mechanic!</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe inspiration comes from Brandon who wanted to use AR/VR to recreate Tetris using new technology for a more engaging experience.\n\n\n## What it does\n\n\nMakes Tetris 3D! We would like to add material properties beyond just looks so that, while still playing by the classic mechanics, the player must also pay attention to interactions between blocks in 3 dimensions (why you see wood, dirt, ice, concrete, etc.)\n\n\n## How we built it\n\n\nWe built it using Unity and various Vive plugins.\n\n\n## Challenges we ran into\n\n\nLearning to use VR input in Unity in a consistent way. Lots of trial and error. Also Matrices. In general.\n\n\n## Accomplishments that we're proud of\n\n\n<https://drive.google.com/file/d/0ByKYBqi5ISu0MHREOXJKUzJwLW8/view?usp=sharing>\n!!!\n\n\n## What we learned\n\n\n## What's next for A-tech of the Blocks\n\n\nMaking the elements a significant game mechanic!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vrv-virtual-reality-virtutually-syk10m",
            "title": "VRV Virtual Reality Virtutually",
            "blurb": "The Virtual Reality Experience of the Virtual Reality Hackathon (in Virtual Reality)",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Lex Dreitser",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/274/391/datas/profile.png"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>We are on the Second floor, please ask for Lex to get a Demo</h2>\n<h2>Inspiration</h2>\n<p>This Project was inspired by this amazing hackathon an the possibilies of exporing spaces and events in VR</p>\n<h2>What it does</h2>\n<p>It allows you to see the Organizers, Mentors Sponsors and participants in 3D in VR</p>\n<h2>How I built it</h2>\n<p>Using Unity and Android VR</p>\n<h2>Challenges I ran into</h2>\n<p>There is never enough time. We wanted to scan everybody into the app, so we did a smaller sample size this time.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>This will document this hackathon. Everybody can experience it for years to come.</p>\n<h2>What I learned</h2>\n<p>Amazing community, I got to meet some great people, and scan. </p>\n<h2>What's next for VRV Virtual Reality Virtutually</h2>\n<p>Publishing on Google Play store tonight.</p>\n</div>",
            "content_md": "\n## We are on the Second floor, please ask for Lex to get a Demo\n\n\n## Inspiration\n\n\nThis Project was inspired by this amazing hackathon an the possibilies of exporing spaces and events in VR\n\n\n## What it does\n\n\nIt allows you to see the Organizers, Mentors Sponsors and participants in 3D in VR\n\n\n## How I built it\n\n\nUsing Unity and Android VR\n\n\n## Challenges I ran into\n\n\nThere is never enough time. We wanted to scan everybody into the app, so we did a smaller sample size this time.\n\n\n## Accomplishments that I'm proud of\n\n\nThis will document this hackathon. Everybody can experience it for years to come.\n\n\n## What I learned\n\n\nAmazing community, I got to meet some great people, and scan. \n\n\n## What's next for VRV Virtual Reality Virtutually\n\n\nPublishing on Google Play store tonight.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/magic-studio",
            "title": "Magic Studio",
            "blurb": "Simplifying the complex task of video editing through intuitive design and collaboration.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "megan Li",
                    "about": "Product and UI design",
                    "photo": "https://www.gravatar.com/avatar/fac171cb152716c44c7d6d0ddef845f3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Xindeling Pan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/971/datas/profile.jpg"
                },
                {
                    "name": "Alyssa Li",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/406/327/datas/profile.JPG"
                },
                {
                    "name": "Jennifer Hurford",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/32f83f94dc2d87eaeafe11a23e01a7be?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Hisham Bedri",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8356b048c3efac6746982ff1e0a6e593?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Wesley Abbey",
                    "about": "",
                    "photo": "https://graph.facebook.com/10209607109827196/picture?height=180&width=180"
                },
                {
                    "name": "Mike D",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e799c13ed007327dff0e30e64dced95b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "adobe-premier",
                "c#",
                "photoshop",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>With so many VR practitioners focused on building immersive experiences, our team wanted to focus on building a tool to make creative professionals more productive. Specifically, we wanted to explore how virtual reality might make the process of video editing more intuitive and collaborative. Magic Studio is a step in that direction and one of the first apps that recognizes VR's ability to complete complex tasks. </p>\n<p>We pay particular attention to the <strong>pain points</strong> of professional video editors. While today\u2019s editors are privileged by a suite of powerful tools, they're also constrained by two-dimensional surfaces and a collaboration process that stifles creativity. As a result, their quality of product and degree of self-expression often suffer. That means decreased productivity and more expensive post-production services for digital media companies, TV networks, film studios.</p>\n<p>Magic Studios seeks to address those issues by building the most intuitive video editing tool possible. Our design is based around the principles of direct manipulation, world-in-miniature perspective, and infinite screen space. We believe that this approach will not only improve the efficiency of professional video editors, but also expand the accessibility of video editing to new consumer markets.</p>\n<h2>What it does</h2>\n<p>Magic Studio is a VR application that allows users to edit video and collaborate with others inside a 3D environment. Users can drag video clips from a pre-organized folder, set in and out points using the controllers, quickly access and edit clips on the storyboard, add and rearrange multiple clips to a timeline, and preview playback of the sequence. To collaborate with others, users can duplicate the existing timeline for others to make modifications to.</p>\n<h2>How we built it</h2>\n<p>We built Magic Studio for the Vive using Unity, C#, Photoshop and Premier.</p>\n<h2>Challenges we ran into</h2>\n<p>We weren\u2019t able to get real time collaboration to work, but we were able to get a version of forking to work. </p>\n<p>There are many UI ideas that we weren\u2019t able to implement in time such as layout, transitions, audio within the clips - these are minor changes but would have made the experience better.</p>\n<p>We lacked of free video codec that was useful so we had to code our own from scratch. We ended up implementing 2 different kinds of video playback methods using image stacks and we ended up using a mix of both of them for the final demo - twitch play and slow play. We did this because it was impossible to have many clips playing at full frame rate loading from the file system and we didn\u2019t have the time to implement an H264 encoder.</p>\n<p>We spent too much of our time benchmarking our video-streaming algorithm to make sure it could handle our project before we built anything.</p>\n<p>We experienced difficulty with 3D UI design - there was so much we were able to do but we didn\u2019t know what we should do because we are not used to thinking beyond 2D interface so we have to spent a lot of time figuring out what would make sense for the user.</p>\n<p>We experienced challenges optimizing our program so that it would run at an excellent frame rate.</p>\n<p>We struggled to show multiple clips at once without losing frame rate, we struggled to find our upper bound with how many clips we can show on the screen at one time.</p>\n<p>We had some issues implementing an efficient fork feature, where we could fork our project into a duplicate. Doing deep copies proved expensive and severely impacted frame rate, while shallow copies lacked some of the full features we wanted at first.</p>\n<p>We experienced issues with grabbing certain objects and there was an issue with overlapping items so we dragged two at the same time.  </p>\n<p>From a design perspective, given that we had professional editors on the team as well as people that have never edited before, we struggled to create an MVP that would be satisfactory to both audiences.</p>\n<p>We experienced challenges around adding traditional assets to 3D a world and learning new 3D design tools using Unity.</p>\n<p>There were many features that we did want to add, but due to the time constraints we had to select the core value added features to present for the demo.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Working collaboratively with a diverse team of coders, designers, and media professionals to complete the prototype in 24 hours of work time in the Media Lab. We are proud of creating something from scratch leveraging our passion in future technologies. While the differing levels of Unity experience could have been a problem on the team, we worked together and learned from each other in an extremely harmonious way by breaking into small teams, dividing up the work and ensuring that everyone had equal voice. It was awesome possum. </p>\n<p>Working together was smooth like butter!</p>\n<h2>What we learned</h2>\n<p>One of designers and one of our developers learned Unity.</p>\n<p>Those not familiar with coding learned about \"forking.\"</p>\n<p>Editing video in a 3D space is a lot more intuitive than in a 2D setting because it is easier to manipulate clips with your hands than with a mouse whilst still maintaining references to familiar user interfaces. </p>\n<p>Design thinking was a useful in helping us learn how to better empathize with user needs and pain points. </p>\n<p>We learned quick sketches were a great way to help us visualize complex perspectives inside a 3D environment. </p>\n<p>Before brainstorming as a group, we learned that individual ideation was important. </p>\n<h2>What's next for Magic Studio</h2>\n<p>World domination. Our immediate next steps are to further interview media practitioners, fine tune the MVP, test and iterate the prototype as we move towards higher fidelity outputs. </p>\n<h2>Registration Details</h2>\n<p>Location: E14-348K</p>\n<p>Vertical: Entertainment/Storytelling (Gaming, Film Journalism, Art/Design)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWith so many VR practitioners focused on building immersive experiences, our team wanted to focus on building a tool to make creative professionals more productive. Specifically, we wanted to explore how virtual reality might make the process of video editing more intuitive and collaborative. Magic Studio is a step in that direction and one of the first apps that recognizes VR's ability to complete complex tasks. \n\n\nWe pay particular attention to the **pain points** of professional video editors. While today\u2019s editors are privileged by a suite of powerful tools, they're also constrained by two-dimensional surfaces and a collaboration process that stifles creativity. As a result, their quality of product and degree of self-expression often suffer. That means decreased productivity and more expensive post-production services for digital media companies, TV networks, film studios.\n\n\nMagic Studios seeks to address those issues by building the most intuitive video editing tool possible. Our design is based around the principles of direct manipulation, world-in-miniature perspective, and infinite screen space. We believe that this approach will not only improve the efficiency of professional video editors, but also expand the accessibility of video editing to new consumer markets.\n\n\n## What it does\n\n\nMagic Studio is a VR application that allows users to edit video and collaborate with others inside a 3D environment. Users can drag video clips from a pre-organized folder, set in and out points using the controllers, quickly access and edit clips on the storyboard, add and rearrange multiple clips to a timeline, and preview playback of the sequence. To collaborate with others, users can duplicate the existing timeline for others to make modifications to.\n\n\n## How we built it\n\n\nWe built Magic Studio for the Vive using Unity, C#, Photoshop and Premier.\n\n\n## Challenges we ran into\n\n\nWe weren\u2019t able to get real time collaboration to work, but we were able to get a version of forking to work. \n\n\nThere are many UI ideas that we weren\u2019t able to implement in time such as layout, transitions, audio within the clips - these are minor changes but would have made the experience better.\n\n\nWe lacked of free video codec that was useful so we had to code our own from scratch. We ended up implementing 2 different kinds of video playback methods using image stacks and we ended up using a mix of both of them for the final demo - twitch play and slow play. We did this because it was impossible to have many clips playing at full frame rate loading from the file system and we didn\u2019t have the time to implement an H264 encoder.\n\n\nWe spent too much of our time benchmarking our video-streaming algorithm to make sure it could handle our project before we built anything.\n\n\nWe experienced difficulty with 3D UI design - there was so much we were able to do but we didn\u2019t know what we should do because we are not used to thinking beyond 2D interface so we have to spent a lot of time figuring out what would make sense for the user.\n\n\nWe experienced challenges optimizing our program so that it would run at an excellent frame rate.\n\n\nWe struggled to show multiple clips at once without losing frame rate, we struggled to find our upper bound with how many clips we can show on the screen at one time.\n\n\nWe had some issues implementing an efficient fork feature, where we could fork our project into a duplicate. Doing deep copies proved expensive and severely impacted frame rate, while shallow copies lacked some of the full features we wanted at first.\n\n\nWe experienced issues with grabbing certain objects and there was an issue with overlapping items so we dragged two at the same time. \n\n\nFrom a design perspective, given that we had professional editors on the team as well as people that have never edited before, we struggled to create an MVP that would be satisfactory to both audiences.\n\n\nWe experienced challenges around adding traditional assets to 3D a world and learning new 3D design tools using Unity.\n\n\nThere were many features that we did want to add, but due to the time constraints we had to select the core value added features to present for the demo.\n\n\n## Accomplishments that we're proud of\n\n\nWorking collaboratively with a diverse team of coders, designers, and media professionals to complete the prototype in 24 hours of work time in the Media Lab. We are proud of creating something from scratch leveraging our passion in future technologies. While the differing levels of Unity experience could have been a problem on the team, we worked together and learned from each other in an extremely harmonious way by breaking into small teams, dividing up the work and ensuring that everyone had equal voice. It was awesome possum. \n\n\nWorking together was smooth like butter!\n\n\n## What we learned\n\n\nOne of designers and one of our developers learned Unity.\n\n\nThose not familiar with coding learned about \"forking.\"\n\n\nEditing video in a 3D space is a lot more intuitive than in a 2D setting because it is easier to manipulate clips with your hands than with a mouse whilst still maintaining references to familiar user interfaces. \n\n\nDesign thinking was a useful in helping us learn how to better empathize with user needs and pain points. \n\n\nWe learned quick sketches were a great way to help us visualize complex perspectives inside a 3D environment. \n\n\nBefore brainstorming as a group, we learned that individual ideation was important. \n\n\n## What's next for Magic Studio\n\n\nWorld domination. Our immediate next steps are to further interview media practitioners, fine tune the MVP, test and iterate the prototype as we move towards higher fidelity outputs. \n\n\n## Registration Details\n\n\nLocation: E14-348K\n\n\nVertical: Entertainment/Storytelling (Gaming, Film Journalism, Art/Design)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/phosphene-uc1ofl",
            "title": "Phosphene",
            "blurb": "1 vs 1 battle with motion and color through a personal narrative",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Bo Scott Pu",
                    "about": "I worked on the concept with the team, worked on the 3D portion, and animation in Maya. ",
                    "photo": "https://www.gravatar.com/avatar/8c63f0f9950f5b18fb502f0b69fe7c08?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Hanyang Jiang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/687/datas/profile.JPG"
                }
            ],
            "built_with": [
                "c#",
                "htcvive",
                "maya",
                "photoshop",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our inspiration came from a series of ideas of hoping to return to the age when we were all kids with imaginations running wild. From there we each explored what we enjoyed, and settle on drawing. The freedom of creation in Tilt Brush reminded us how we are able to draw our imagination, and in our concept, something that we can defend ourselves with, protecting and preserving our memories. </p>\n<h2>What it does</h2>\n<p>It is a versus game in HTC Vive where you draw whatever you want, and defend yourself against an enemy AI, who the is the representation of frustration from your childhood</p>\n<h2>How we built it</h2>\n<p>We used mainly Maya and Unity as the platforms to built this prototype experience. </p>\n<h2>Challenges we ran into</h2>\n<p>Because of our abstract concept and art direction, there was a bold challenge of how to tackle and create and animate the 3D models based on the concepts. Also because we are new to Vive development, it is very difficult to debug for us.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were able to overcome these previously mentioned challenges, as well has nurturing our group into a cohesive team and work for 2 days straight.</p>\n<h2>What we learned</h2>\n<p>We learned many things that we through not possible before, Vive development, new techniques within Unity and writing and understanding shaders. And the new techniques of animation in Maya and its pipeline with Unity</p>\n<h2>What's next for Phosphene</h2>\n<p>We are planning to possibly continue this project into something more fully fledged.</p>\n<h2>Team Lead</h2>\n<p>Hanyang Jiang</p>\n<h2>Phone Number of Team Lead</h2>\n<p>216-2334541</p>\n<h2>Location</h2>\n<p>MIT Media Lab, 6th floor, room E14-674, Table 32</p>\n<h2>Category</h2>\n<p>Entertainment/Storytelling (Gaming)</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur inspiration came from a series of ideas of hoping to return to the age when we were all kids with imaginations running wild. From there we each explored what we enjoyed, and settle on drawing. The freedom of creation in Tilt Brush reminded us how we are able to draw our imagination, and in our concept, something that we can defend ourselves with, protecting and preserving our memories. \n\n\n## What it does\n\n\nIt is a versus game in HTC Vive where you draw whatever you want, and defend yourself against an enemy AI, who the is the representation of frustration from your childhood\n\n\n## How we built it\n\n\nWe used mainly Maya and Unity as the platforms to built this prototype experience. \n\n\n## Challenges we ran into\n\n\nBecause of our abstract concept and art direction, there was a bold challenge of how to tackle and create and animate the 3D models based on the concepts. Also because we are new to Vive development, it is very difficult to debug for us.\n\n\n## Accomplishments that we're proud of\n\n\nWe were able to overcome these previously mentioned challenges, as well has nurturing our group into a cohesive team and work for 2 days straight.\n\n\n## What we learned\n\n\nWe learned many things that we through not possible before, Vive development, new techniques within Unity and writing and understanding shaders. And the new techniques of animation in Maya and its pipeline with Unity\n\n\n## What's next for Phosphene\n\n\nWe are planning to possibly continue this project into something more fully fledged.\n\n\n## Team Lead\n\n\nHanyang Jiang\n\n\n## Phone Number of Team Lead\n\n\n216-2334541\n\n\n## Location\n\n\nMIT Media Lab, 6th floor, room E14-674, Table 32\n\n\n## Category\n\n\nEntertainment/Storytelling (Gaming)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-professional-in-vr",
            "title": "L\u00e9on: The Professional in VR",
            "blurb": "You are put into Leon's shoes to act out one of the most memorable scenes in action cinema!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/AIph6fWqvUg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jasmine Davis",
                    "about": "Designer (Level, Graphics, Textures), Unity Developer, and Communications Manager",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/419/866/datas/profile.jpg"
                },
                {
                    "name": "Whitney Fahnbulleh",
                    "about": "Game Design\n3D Modeling\nLevel Design\nGameplay Testing\n",
                    "photo": "https://www.gravatar.com/avatar/eace00865b4d600e83c3408e1ffce96f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Fabian Patino",
                    "about": "Developer, ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/703/datas/profile.jpg"
                },
                {
                    "name": "Luke Dickerson",
                    "about": "3D modelling in blender and worked in unity as well",
                    "photo": "https://www.gravatar.com/avatar/24effa22e2b44adf98f9298de428920e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lex Dreitser",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/274/391/datas/profile.png"
                },
                {
                    "name": "Freddy Garcia",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/076/976/datas/profile.jpg"
                },
                {
                    "name": "Kari Wu",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/738/628/datas/profile.jpg"
                }
            ],
            "built_with": [
                "adobe-creative-suite",
                "blender",
                "maya",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<h2>L\u00e9on: The Professional in VR</h2>\n<h2>We  are on the second floor in the lab, please come see our demo</h2>\n<h2>What it does:</h2>\n<p>You are put into Leon's shoes to act out one of the most memorable scenes in action cinema!</p>\n<h2>How we built it:</h2>\n<p>Using the Unity engine we have brought together all the elements of the movie into the HTC Vive.  We recreated the environment using custom Maya and Blender objects, textures, and scripts. We've also incorporated sound clips from the movie and replicated the movie's narrative structure and scene layout. </p>\n<h2>Challenges we ran into:</h2>\n<p>The hardware failed during our build, but much like Leon, nothing will stop this from happening!</p>\n<h2>Accomplishments that we're proud of:</h2>\n<p>Even though the PC attached to our Vive failed, our workflow didn't. We focused on what was controllable by the team and successfully completed every one of our main goals for the hackathon. Also, even though we all came with different backgrounds and levels of experience we managed to create a cohesive team that had a lot of fun together and is proud of their work.</p>\n<h2>What's next for L\u00e9on: The Professional in VR</h2>\n<p>In the future, we plan to add additional levels to the VR experience as well as expand on the spawning mechanics and enemy AI. </p>\n</div>",
            "content_md": "\n## L\u00e9on: The Professional in VR\n\n\n## We are on the second floor in the lab, please come see our demo\n\n\n## What it does:\n\n\nYou are put into Leon's shoes to act out one of the most memorable scenes in action cinema!\n\n\n## How we built it:\n\n\nUsing the Unity engine we have brought together all the elements of the movie into the HTC Vive. We recreated the environment using custom Maya and Blender objects, textures, and scripts. We've also incorporated sound clips from the movie and replicated the movie's narrative structure and scene layout. \n\n\n## Challenges we ran into:\n\n\nThe hardware failed during our build, but much like Leon, nothing will stop this from happening!\n\n\n## Accomplishments that we're proud of:\n\n\nEven though the PC attached to our Vive failed, our workflow didn't. We focused on what was controllable by the team and successfully completed every one of our main goals for the hackathon. Also, even though we all came with different backgrounds and levels of experience we managed to create a cohesive team that had a lot of fun together and is proud of their work.\n\n\n## What's next for L\u00e9on: The Professional in VR\n\n\nIn the future, we plan to add additional levels to the VR experience as well as expand on the spawning mechanics and enemy AI. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/kid-city-vr",
            "title": "TreeHouse by KidCity VR",
            "blurb": "TreeHouse is an educational platform that allows parents and kids to play and explore together in virtual reality. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/BFBj2zhA9zk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Anish Dhesikan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/213/datas/profile.jpg"
                },
                {
                    "name": "Kachina Studer",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/231/datas/profile.jpg"
                },
                {
                    "name": "Jacqueline Assar",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/693/datas/profile.jpeg"
                },
                {
                    "name": "Theji Jayaratne",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/207/datas/profile.jpg"
                },
                {
                    "name": "Emily Pascual",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/209/datas/profile.png"
                }
            ],
            "built_with": [
                "ableton",
                "adobe-illustrator",
                "blender",
                "c#",
                "htc-vive",
                "photoshop",
                "steam",
                "unity",
                "vr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our team was inspired by the current lack of interactivity in virtual reality, particularly the inability for children and adults to play together in a virtual world. We were also inspired by the fact that children who are experiencing virtual reality for the first time will benefit from exploring the technology with a trusted adult. Many adults are also new pioneers of this medium themselves. We set out to create an experience where children and adults can can explore virtual reality, together. </p>\n<p>Careers have become global. Service men and women, and countless others currently can video chat with their family and children, but with TreeHouse anyone living abroad can play with their kids in and help them achieve educational milestones through the game. Other inspirations for this game include that we want to offer incarcerated parents can interact with their children in the beautiful virtual environment that is TreeHouse.</p>\n<p>Most virtual reality experiences are one-sided, and currently none exist to facilitate an educational and familial bond between young children and caregivers. Technology has presently allowed text communication and voice, but we want to bring a collaborative and immersive experience to families. Our mission is to ensure that no matter where you are in the world you can connect with your children, because family time should have no boundaries.</p>\n<h2>What it does</h2>\n<p>TreeHouse is an educational platform that allows parents and kids to play and explore together in virtual reality. TreeHouse is an educational platform that lets kids access multiple educational worlds. Children start out in a magical tree house space; this is their homebase. From here the player has access all platform activities. Inside, the child meets Mr. Pouf, an adorable robot companion that acts as a guide throughout all of the experiences. We envision Mr. Pouf as a helper for all the different activities throughout the game and a tool to keep children on task during gameplay.</p>\n<p>For the purposes of the demo, we have created Exploratorium: the introductory experience in TreeHouse which enables parents and kids to get acquainted with all of the base-level functionalities of the game. In Exploratorium, children can learn about logic and shapes through playing with blocks, expand their creative skills with free-form drawing and connect with their parents to complete fun and educational tasks such as drawing their favorite animal and favorite food. </p>\n<p>The parent is able to access the game in the same room as their child or remotely through a mobile phone, laptop or desktop computer.  If they want a multiplayer experience, the parent can use a headset if they would like to (any headset can work including HTC Vive, Samsung Gear, Google Cardboard etc.). </p>\n<p>Portal Mode access is the second option, to enter the game through our unique portal-style video feed. In this view, children can both see and hear their parent in the game, additionally the parent can also manipulate objects in the game space remotely from their phone or computer. This innovative functionality has endless opportunity for education. One example demonstrated in the game is counting and stacking with blocks. In our Exploratorium, we combine virtual-physical space to building math fluency. This innovative way to teach children spatial reasoning, logical reasoning, and tactile learning offers a sensory experience; which is paramount to early-childhood development.</p>\n<p>Parents  can also also \u201cgift\u201d their children objects that are accessible from the TreeHouse App Store. These objects are instantaneity placed in the game for the child to interact with in real time, as the parent can observe, interact, or instruct. Additionally parents can also use TreeHouse to teach their children responsible technology usage practices from an early age.</p>\n<h2>How we built it</h2>\n<p>TreeHouse was built in Unity Game Engine, heavily using the Unity Networking framework and going into low level coding concepts like sending packets of byte arrays. In order to send video streaming data, we had to take the textures produced by the webcam and simplify them down to byte arrays in order to send them over the LAN network. There were many problems, including being restricted on packet size and syncing commands and object transforms through unity and C#. However, we found solutions to our problems by compromising things like resolution and low latency for more reliability.</p>\n<h2>Challenges we ran into</h2>\n<p>As a team, it was very important to us that the functionality of our product is realistic. Initially, we started out with a plan to allow the child and the parent to each use an HTC Vive headset to enter the game and interact with each other. However, we realized that very few households have access to two headsets. Our solution was to allow the parent to access the game (with or without a headset) in the same room as their child or remotely through a mobile phone, laptop or desktop computer.</p>\n<p>We also ran into challenges with the video feed for the interactive portal but we were able to reason our way through it to make it a reality! </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our team is most proud of the technology we developed to enable the interaction between children and adults during the game. Our interactive Portal Mode technology has far-reaching applications for virtual reality far beyond gaming. This technology can be used for learning and training in multiple industries. It can also be used for research since researchers will be able to watch children learning, playing and exploring in real time. </p>\n<p>Our team is very proud of the aesthetics and music of the game. We believe that music, when utilized properly, can help guide the experience of a game and unleash creativity. To accomplish this, we developed an original soundtrack for TreeHouse. The TreeHouse design and infrastructure is custom-made by our team from the beautiful wood panels down to the nails on the floor board. </p>\n<p>TreeHouse is more than just a game or an experience for children of all ages to have fun, it is a particularly fantastic tool for young children to have their first experiences in virtual reality. We believe young children should be able to see and hear their parent with them in immersive games. Unfortunately, in the educational game market, content and interaction between parent and child is few and far between. With TreeHouse, we\u2019re bringing a product to market that has immediate need and endless applications. </p>\n<h2>What we learned</h2>\n<p>Our team learned more than we ever thought would be possible in this time period. We each expanded our capabilities with the hardware and software we used. We also learned how to add a new functionality to VR, with the introduction of our interactive portal-mode technology. </p>\n<h2>What's next</h2>\n<p>TreeHouse is not just one game, it is a platform for games and interactive learning activities. Parents and kids can access the TreeHouse App Store to purchase more \u201cworlds\u201d for their kids to play in. While we did not have the time to build all of the worlds for this hackathon, we would like to build out more games and experiences for TreeHouse. </p>\n<p>Our team would also like to conduct research on how to get the best controller experience for children in TreeHouse. We would also like to conduct further research on education and media literacy in VR so we can optimize our games and experiences in TreeHouse. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur team was inspired by the current lack of interactivity in virtual reality, particularly the inability for children and adults to play together in a virtual world. We were also inspired by the fact that children who are experiencing virtual reality for the first time will benefit from exploring the technology with a trusted adult. Many adults are also new pioneers of this medium themselves. We set out to create an experience where children and adults can can explore virtual reality, together. \n\n\nCareers have become global. Service men and women, and countless others currently can video chat with their family and children, but with TreeHouse anyone living abroad can play with their kids in and help them achieve educational milestones through the game. Other inspirations for this game include that we want to offer incarcerated parents can interact with their children in the beautiful virtual environment that is TreeHouse.\n\n\nMost virtual reality experiences are one-sided, and currently none exist to facilitate an educational and familial bond between young children and caregivers. Technology has presently allowed text communication and voice, but we want to bring a collaborative and immersive experience to families. Our mission is to ensure that no matter where you are in the world you can connect with your children, because family time should have no boundaries.\n\n\n## What it does\n\n\nTreeHouse is an educational platform that allows parents and kids to play and explore together in virtual reality. TreeHouse is an educational platform that lets kids access multiple educational worlds. Children start out in a magical tree house space; this is their homebase. From here the player has access all platform activities. Inside, the child meets Mr. Pouf, an adorable robot companion that acts as a guide throughout all of the experiences. We envision Mr. Pouf as a helper for all the different activities throughout the game and a tool to keep children on task during gameplay.\n\n\nFor the purposes of the demo, we have created Exploratorium: the introductory experience in TreeHouse which enables parents and kids to get acquainted with all of the base-level functionalities of the game. In Exploratorium, children can learn about logic and shapes through playing with blocks, expand their creative skills with free-form drawing and connect with their parents to complete fun and educational tasks such as drawing their favorite animal and favorite food. \n\n\nThe parent is able to access the game in the same room as their child or remotely through a mobile phone, laptop or desktop computer. If they want a multiplayer experience, the parent can use a headset if they would like to (any headset can work including HTC Vive, Samsung Gear, Google Cardboard etc.). \n\n\nPortal Mode access is the second option, to enter the game through our unique portal-style video feed. In this view, children can both see and hear their parent in the game, additionally the parent can also manipulate objects in the game space remotely from their phone or computer. This innovative functionality has endless opportunity for education. One example demonstrated in the game is counting and stacking with blocks. In our Exploratorium, we combine virtual-physical space to building math fluency. This innovative way to teach children spatial reasoning, logical reasoning, and tactile learning offers a sensory experience; which is paramount to early-childhood development.\n\n\nParents can also also \u201cgift\u201d their children objects that are accessible from the TreeHouse App Store. These objects are instantaneity placed in the game for the child to interact with in real time, as the parent can observe, interact, or instruct. Additionally parents can also use TreeHouse to teach their children responsible technology usage practices from an early age.\n\n\n## How we built it\n\n\nTreeHouse was built in Unity Game Engine, heavily using the Unity Networking framework and going into low level coding concepts like sending packets of byte arrays. In order to send video streaming data, we had to take the textures produced by the webcam and simplify them down to byte arrays in order to send them over the LAN network. There were many problems, including being restricted on packet size and syncing commands and object transforms through unity and C#. However, we found solutions to our problems by compromising things like resolution and low latency for more reliability.\n\n\n## Challenges we ran into\n\n\nAs a team, it was very important to us that the functionality of our product is realistic. Initially, we started out with a plan to allow the child and the parent to each use an HTC Vive headset to enter the game and interact with each other. However, we realized that very few households have access to two headsets. Our solution was to allow the parent to access the game (with or without a headset) in the same room as their child or remotely through a mobile phone, laptop or desktop computer.\n\n\nWe also ran into challenges with the video feed for the interactive portal but we were able to reason our way through it to make it a reality! \n\n\n## Accomplishments that we're proud of\n\n\nOur team is most proud of the technology we developed to enable the interaction between children and adults during the game. Our interactive Portal Mode technology has far-reaching applications for virtual reality far beyond gaming. This technology can be used for learning and training in multiple industries. It can also be used for research since researchers will be able to watch children learning, playing and exploring in real time. \n\n\nOur team is very proud of the aesthetics and music of the game. We believe that music, when utilized properly, can help guide the experience of a game and unleash creativity. To accomplish this, we developed an original soundtrack for TreeHouse. The TreeHouse design and infrastructure is custom-made by our team from the beautiful wood panels down to the nails on the floor board. \n\n\nTreeHouse is more than just a game or an experience for children of all ages to have fun, it is a particularly fantastic tool for young children to have their first experiences in virtual reality. We believe young children should be able to see and hear their parent with them in immersive games. Unfortunately, in the educational game market, content and interaction between parent and child is few and far between. With TreeHouse, we\u2019re bringing a product to market that has immediate need and endless applications. \n\n\n## What we learned\n\n\nOur team learned more than we ever thought would be possible in this time period. We each expanded our capabilities with the hardware and software we used. We also learned how to add a new functionality to VR, with the introduction of our interactive portal-mode technology. \n\n\n## What's next\n\n\nTreeHouse is not just one game, it is a platform for games and interactive learning activities. Parents and kids can access the TreeHouse App Store to purchase more \u201cworlds\u201d for their kids to play in. While we did not have the time to build all of the worlds for this hackathon, we would like to build out more games and experiences for TreeHouse. \n\n\nOur team would also like to conduct research on how to get the best controller experience for children in TreeHouse. We would also like to conduct further research on education and media literacy in VR so we can optimize our games and experiences in TreeHouse. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/political-landscape",
            "title": "Political Landscape",
            "blurb": "This VR app visualizes political proliviites as an immersive topographical 'quilt' to encourage real-world dialogue",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Emerging landscape reflecting affective and semantic data",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/314/datas/original.PNG"
                },
                {
                    "title": "Emerging landscape reflecting affective and semantic data",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/317/datas/original.PNG"
                },
                {
                    "title": "Emerging landscape reflecting affective and semantic data",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/316/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Jim Moffet",
                    "about": "Ideation, UI design, assistant Unity/C# developer, lead developer for semantic language analysis",
                    "photo": "https://www.gravatar.com/avatar/31981dee12baf7861c607a59041b6c23?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Barry Dineen",
                    "about": "Ideation, reactive environment design, environment modelling, lighting, unity development",
                    "photo": "https://www.gravatar.com/avatar/73fe09d2b53f3117cfec379366ec4030?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Justin Chin",
                    "about": "Lead Unity/C# Developer",
                    "photo": "https://www.gravatar.com/avatar/2d8eb9f6a84be0e27c0fd8b7eb7f9a8c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tom Gorham",
                    "about": "Ideation, storyboards, and audio work",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/588/datas/profile.JPG"
                },
                {
                    "name": "keith hartwig",
                    "about": "creative director, storyboards, narrative, sketches",
                    "photo": "https://www.gravatar.com/avatar/9df243a2ab6feb0ef1d54c462f7ee792?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "3dsmax",
                "assets",
                "c#",
                "unity",
                "vive",
                "vrtk"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>As a team we are interested in how bias informs people's reactions to questions and real-world events. This project explores the possibility of using VR to visualize information in an immersive and three-dimensional way. </p>\n<p>Since 1994, the Pew Research Center has been tracking the partisanship of American voters. It has uncovered a worrying trend of growing ideological consistency and political polarization. This VR application is an experiment to see if virtual reality can influence individuals\u2019 perceptions of the political proclivities of other people as well as their own. It uses an adapted version of Pew\u2019s Ideological Consistency Scale with an additional element of personal relevance. </p>\n<h2>What it does</h2>\n<p>As individuals explore the environment and answer a series of questions they begin to adapt the atmosphere, textures and three-dimensional qualities of their environment. This emergent landscape is an abstract representation of their political proclivity. </p>\n<p>In-game users begin exploring an environment that is replete of form and atmosphere. As they walk around in this empty space they discover artifacts which, when engaged, ask the user a question about a single political topic. They can select from a range of answers to charge this artifact and imbue it with their sentiment. The artifact can then be tossed into the landscape causing a series of effects to take place. Where the artifact lands the ground changes form, the scenery unfolds and the atmosphere is colored. The changes are a direct reflection of their answer to each question. The process is repeated until the entire virtual environment has changed from a blank void to a dynamic landscape.</p>\n<p>Upon completion of the survey the user zooms out to see their custom landscape relative to all those who have taken the survey before them. The aggregate is a topographical quilt of everyone's political proclivities. </p>\n<h2>How we built it</h2>\n<p>The project was built through Unity and C# for the VIVE. After plenty of iterating and sketches we finally arrived at a prototype we were proud to run with and make work for Sunday's demo! </p>\n<h2>Challenges we ran into</h2>\n<p>Initially we desired a 2 person dialogue and interactive environment, but due to technological limitations we had to simplify to a single person interface. Also discovering best practices for version control for working collaboratively in Unity.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We came together as individuals with no former experience working together and have managed to successfully collaborate on a dynamic and engaging project! (We have become friend to boot!)</p>\n<h2>What we learned</h2>\n<p>Debug on the move; don't just stare at the problems or let them add up! Explain your code to a rubber duck :)</p>\n<h2>What's next for Political Landscape</h2>\n<p>We are interested in how this type of application and virtual experience can be used for conflict management, social science research and community building.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAs a team we are interested in how bias informs people's reactions to questions and real-world events. This project explores the possibility of using VR to visualize information in an immersive and three-dimensional way. \n\n\nSince 1994, the Pew Research Center has been tracking the partisanship of American voters. It has uncovered a worrying trend of growing ideological consistency and political polarization. This VR application is an experiment to see if virtual reality can influence individuals\u2019 perceptions of the political proclivities of other people as well as their own. It uses an adapted version of Pew\u2019s Ideological Consistency Scale with an additional element of personal relevance. \n\n\n## What it does\n\n\nAs individuals explore the environment and answer a series of questions they begin to adapt the atmosphere, textures and three-dimensional qualities of their environment. This emergent landscape is an abstract representation of their political proclivity. \n\n\nIn-game users begin exploring an environment that is replete of form and atmosphere. As they walk around in this empty space they discover artifacts which, when engaged, ask the user a question about a single political topic. They can select from a range of answers to charge this artifact and imbue it with their sentiment. The artifact can then be tossed into the landscape causing a series of effects to take place. Where the artifact lands the ground changes form, the scenery unfolds and the atmosphere is colored. The changes are a direct reflection of their answer to each question. The process is repeated until the entire virtual environment has changed from a blank void to a dynamic landscape.\n\n\nUpon completion of the survey the user zooms out to see their custom landscape relative to all those who have taken the survey before them. The aggregate is a topographical quilt of everyone's political proclivities. \n\n\n## How we built it\n\n\nThe project was built through Unity and C# for the VIVE. After plenty of iterating and sketches we finally arrived at a prototype we were proud to run with and make work for Sunday's demo! \n\n\n## Challenges we ran into\n\n\nInitially we desired a 2 person dialogue and interactive environment, but due to technological limitations we had to simplify to a single person interface. Also discovering best practices for version control for working collaboratively in Unity.\n\n\n## Accomplishments that we're proud of\n\n\nWe came together as individuals with no former experience working together and have managed to successfully collaborate on a dynamic and engaging project! (We have become friend to boot!)\n\n\n## What we learned\n\n\nDebug on the move; don't just stare at the problems or let them add up! Explain your code to a rubber duck :)\n\n\n## What's next for Political Landscape\n\n\nWe are interested in how this type of application and virtual experience can be used for conflict management, social science research and community building.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/hg",
            "title": "HG",
            "blurb": "Two player social dance with HMDs",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/A0TH0oQQQy4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Geyao Zhang",
                    "about": "Hello",
                    "photo": "https://www.gravatar.com/avatar/c73bd33b9cec76faf43312863418dda3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&height=180&s=180&width=180"
                },
                {
                    "name": "Henry Lam",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e342b4727f9b7fcdc2a8cde649008419?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "android",
                "google-vr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Two player interaction that involves close contact</p>\n<h2>What it does</h2>\n<p>It asks two individuals to manipulate their views and contort their bodies to something that resembles a dance</p>\n<h2>How we built it</h2>\n<p>We used Google VR Unity SDK and Unity Networking to create a different multiplayer VR experience</p>\n<h2>Challenges we ran into</h2>\n<p>To let chance create humorous positions or to choreograph the movements</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>First time developing for Android and Google VR</p>\n<h2>What we learned</h2>\n<p>Unity screen attributes and Google VR classes and handlers</p>\n<h2>What's next for HG</h2>\n<p>Sleep</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nTwo player interaction that involves close contact\n\n\n## What it does\n\n\nIt asks two individuals to manipulate their views and contort their bodies to something that resembles a dance\n\n\n## How we built it\n\n\nWe used Google VR Unity SDK and Unity Networking to create a different multiplayer VR experience\n\n\n## Challenges we ran into\n\n\nTo let chance create humorous positions or to choreograph the movements\n\n\n## Accomplishments that we're proud of\n\n\nFirst time developing for Android and Google VR\n\n\n## What we learned\n\n\nUnity screen attributes and Google VR classes and handlers\n\n\n## What's next for HG\n\n\nSleep\n\n\n"
        },
        {
            "source": "https://devpost.com/software/waypointpx",
            "title": "WaypointRx",
            "blurb": "AR fulfillment for pharmacies",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/186220545?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "WaypointRx",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/039/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Umar Arshad",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/84bed654a4dc9153a0c93666846fd495?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Varun Mani",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/419/454/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<p>1.) The name of your team lead\nUMAR ARSHAD</p>\n<p>2.) The team lead's telephone number\n7033079266</p>\n<p>3.) The location where the judges can find your team including the floor number\n5th FLOOR - Conference Room 514b</p>\n<p>4.) The vertical category in which you are competing\nHuman Well-Being (Education/Health/Wellness/Activism)\nCommerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)</p>\n</div>",
            "content_md": "\n1.) The name of your team lead\nUMAR ARSHAD\n\n\n2.) The team lead's telephone number\n7033079266\n\n\n3.) The location where the judges can find your team including the floor number\n5th FLOOR - Conference Room 514b\n\n\n4.) The vertical category in which you are competing\nHuman Well-Being (Education/Health/Wellness/Activism)\nCommerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vrdoc",
            "title": "VRDoc",
            "blurb": "Caught between the war and a hard place",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Jiabao Li",
                    "about": "Project Creator, Unity Developer and Designer, 3D Modeling and Rendering, Co-Story Teller",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/465/221/datas/profile.jpg"
                },
                {
                    "name": "Dan Porter",
                    "about": "Project Manager, Co-Story Teller, Creative Direction ",
                    "photo": "https://www.gravatar.com/avatar/94500f722a64e059a9613062c82b9890?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Roelof Tijdens",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/893/datas/profile.jpg"
                },
                {
                    "name": "Mark Dugas",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fa1a32a7558e70717afaed6a2a7fc800?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "John Benton",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b654a0ce6817ac6e2e848e6ce9fa3794?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&height=180&s=180&width=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<p>Inspo = Documentaries are third person perspective and we want to make them first person</p>\n<p>What it does = allows viewers to partake in difficult experiences first hand</p>\n<p>How we built = unity 3D, 3D models and Avid Media Composer</p>\n<p>Challenges = creating a moving story that invokes emotions</p>\n<p>Accomplishments = the story we are telling and the ability of choice</p>\n<p>What we learned = working creatively as a team is a challenge as we all have views that differ and because of that we must learn to listen and understand each other (also what our experience is trying to show - a choice is not as simple as it seems)</p>\n<p>Whats next = finding other difficult experiences where someone has a drastic choice and creating opportunities for people to better understand the experience and the choice within.</p>\n</div>",
            "content_md": "\nInspo = Documentaries are third person perspective and we want to make them first person\n\n\nWhat it does = allows viewers to partake in difficult experiences first hand\n\n\nHow we built = unity 3D, 3D models and Avid Media Composer\n\n\nChallenges = creating a moving story that invokes emotions\n\n\nAccomplishments = the story we are telling and the ability of choice\n\n\nWhat we learned = working creatively as a team is a challenge as we all have views that differ and because of that we must learn to listen and understand each other (also what our experience is trying to show - a choice is not as simple as it seems)\n\n\nWhats next = finding other difficult experiences where someone has a drastic choice and creating opportunities for people to better understand the experience and the choice within.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/project-grapevine",
            "title": "Project Grapevine",
            "blurb": "For planning and tracking the design process, 2D tools have failed us.  VR allows a new paradigm - the Grapevine.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "This visual inspiration image is human centered, comforting, and beautiful.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/029/datas/original.jpg"
                },
                {
                    "title": "Our &quot;Infinite Corridor&quot;",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/030/datas/original.jpg"
                },
                {
                    "title": "Fun for a girl and a boy.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/032/datas/original.jpg"
                },
                {
                    "title": "The design of our 3D cylindrical corridor is fit to the human body",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/033/datas/original.jpg"
                },
                {
                    "title": "No comment",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/089/datas/original.png"
                },
                {
                    "title": "Sketchup Iteration 2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/421/242/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Glyn Anderson",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/420/683/datas/profile.jpg"
                },
                {
                    "name": "Wilson Lee",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1475944258/sgn1x63iwazbrlchfz9b.jpg?height=180&width=180"
                },
                {
                    "name": "Janeen Anderson",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/94ad7c8306029ab792e993bb5e4317b8?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Michael Kyes",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/533/348/datas/profile.jpg"
                }
            ],
            "built_with": [
                "javascript"
            ],
            "content_html": "<div>\n<h2>Pertinent Team Info:</h2>\n<p>Project Lead: Michael Kyes, 617-308-0454\nTeam location: 6th floor West room\nVertical category: Commerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)</p>\n<h2>Design Challenge:</h2>\n<p>Project planning is difficult, whether designing a high-rise, developing software, or planning a family vacation  Planning for a design process in particular is especially challenging given the non-linear, iterative nature of ideation, testing, and learning within the context of managing multiple objectives.  Navigating and finding control within this \"organized chaos\" gets even more difficult when multiple people are part of the planning and decision making process.  </p>\n<p>Thinking about the possibilities that VR and Mixed Reality can offer, we feel we are ready to move beyond 2D tools such as Gantt charts and flow diagrams and have created a new 4D model to assist project planning and tracking that we call the \"Grapevine\". It is collaborative at its core.  To assure successful adoption, we realize it has to be simple, intuitive, and fun.</p>\n<h2>Design Solution:</h2>\n<p>Our solution addresses this complexity first and foremost by mapping or \"modeling a spatial framework\" for collaborative design to take place within.  This 3D shape is simple enough to be held in your hand and complex enough to walk through an immersive and data-rich environment holding a robust record of key information, team decisions, and interrelated tasks.  Think MS Project meets the Slinky.  </p>\n<p>Virtual Reality and Mixed Reality allows us not only to utilize a third dimension to better visualize these complex relationships, but also to literally steps into the fourth dimension of time to allow us to remember and understand past decisions and plan for future outcomes.    Our 4D model is crafted into the shape of a virtual cylindrical \"corridor\"  Picture walking through a Slinky set in a straight path but with portions of the coils densely packed at times and more stretched out at other times.  This shape can be scaled down to hold in your hand or hover above a conference table in a mixed reality environment or scaled-up to be able to walk through using a collaborative VR cave or with interconnected head-mounted displays.  </p>\n<p>This spiraling curving line defining the corridor, or \"vine\", serves a continuous project timeline.  Attached along this vine are nodes representing ideas that are born at a certain place along this timeline.  Each node has its own unique linear timeline that exists for it's own lifespan.<br/>\nAs one travels forward through time within the project, initial ideas are revisited in an iterative fashion as the curved line of the overall project timeline intersects with straight lines of each idea's individual timeline.  </p>\n<p>To mention what may be obvious from our project's name, these idea nodes can be understood as grapes on the grapevine.  To take the analogy further and to add a sense of play, it may be said that some ideas \"die on the vine\" and we wouldn\u2019t be surprised if one starts imitating Marvin Gaye as they transverse this virtual design space.</p>\n<h2>Key words to guide the design:</h2>\n<pre class=\"language-nolang\"><code>\u2022 Collaborative\n\u2022 Controllable / Interactive\n\u2022 Intuitive\n\u2022 Simple\n\u2022 Fun\n</code></pre>\n<h2>What it does</h2>\n<p>Provide a new tool for planning, tracking multiple types of design in VR and mixed reality</p>\n<h2>How I built it</h2>\n<p>Will explain in person</p>\n<h2>Challenges I ran into</h2>\n<p>Unity not known, tried, and abandoned\nA Frame learned successfully</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Coming together with a basic idea and over the course of programming and thinking about our shared philosophies of designing, we came up with a learning model that just might be an innovative breakthrough ;-)</p>\n<h2>What we learned</h2>\n<p>See above</p>\n<h2>What's next for Project Grapevine:</h2>\n<p>We think this 1.0 minimum viable product is a strong foundation for building additional functionality such as timed task predictions, more intuitive hand gesture, voice command, and gaze controls to support goals of the Design Thinking process, Lean project planning, Agile development, or just planning a family trip to Disney.</p>\n</div>",
            "content_md": "\n## Pertinent Team Info:\n\n\nProject Lead: Michael Kyes, 617-308-0454\nTeam location: 6th floor West room\nVertical category: Commerce/Industry (Architecture/Engineering/Construction/Productivity/Industry/Commerce)\n\n\n## Design Challenge:\n\n\nProject planning is difficult, whether designing a high-rise, developing software, or planning a family vacation Planning for a design process in particular is especially challenging given the non-linear, iterative nature of ideation, testing, and learning within the context of managing multiple objectives. Navigating and finding control within this \"organized chaos\" gets even more difficult when multiple people are part of the planning and decision making process. \n\n\nThinking about the possibilities that VR and Mixed Reality can offer, we feel we are ready to move beyond 2D tools such as Gantt charts and flow diagrams and have created a new 4D model to assist project planning and tracking that we call the \"Grapevine\". It is collaborative at its core. To assure successful adoption, we realize it has to be simple, intuitive, and fun.\n\n\n## Design Solution:\n\n\nOur solution addresses this complexity first and foremost by mapping or \"modeling a spatial framework\" for collaborative design to take place within. This 3D shape is simple enough to be held in your hand and complex enough to walk through an immersive and data-rich environment holding a robust record of key information, team decisions, and interrelated tasks. Think MS Project meets the Slinky. \n\n\nVirtual Reality and Mixed Reality allows us not only to utilize a third dimension to better visualize these complex relationships, but also to literally steps into the fourth dimension of time to allow us to remember and understand past decisions and plan for future outcomes. Our 4D model is crafted into the shape of a virtual cylindrical \"corridor\" Picture walking through a Slinky set in a straight path but with portions of the coils densely packed at times and more stretched out at other times. This shape can be scaled down to hold in your hand or hover above a conference table in a mixed reality environment or scaled-up to be able to walk through using a collaborative VR cave or with interconnected head-mounted displays. \n\n\nThis spiraling curving line defining the corridor, or \"vine\", serves a continuous project timeline. Attached along this vine are nodes representing ideas that are born at a certain place along this timeline. Each node has its own unique linear timeline that exists for it's own lifespan.  \n\nAs one travels forward through time within the project, initial ideas are revisited in an iterative fashion as the curved line of the overall project timeline intersects with straight lines of each idea's individual timeline. \n\n\nTo mention what may be obvious from our project's name, these idea nodes can be understood as grapes on the grapevine. To take the analogy further and to add a sense of play, it may be said that some ideas \"die on the vine\" and we wouldn\u2019t be surprised if one starts imitating Marvin Gaye as they transverse this virtual design space.\n\n\n## Key words to guide the design:\n\n\n\n```\n\u2022 Collaborative\n\u2022 Controllable / Interactive\n\u2022 Intuitive\n\u2022 Simple\n\u2022 Fun\n\n```\n\n## What it does\n\n\nProvide a new tool for planning, tracking multiple types of design in VR and mixed reality\n\n\n## How I built it\n\n\nWill explain in person\n\n\n## Challenges I ran into\n\n\nUnity not known, tried, and abandoned\nA Frame learned successfully\n\n\n## Accomplishments that we're proud of\n\n\nComing together with a basic idea and over the course of programming and thinking about our shared philosophies of designing, we came up with a learning model that just might be an innovative breakthrough ;-)\n\n\n## What we learned\n\n\nSee above\n\n\n## What's next for Project Grapevine:\n\n\nWe think this 1.0 minimum viable product is a strong foundation for building additional functionality such as timed task predictions, more intuitive hand gesture, voice command, and gaze controls to support goals of the Design Thinking process, Lean project planning, Agile development, or just planning a family trip to Disney.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/arge-of-empires",
            "title": "ARge of Empires",
            "blurb": "Medieval Real Time Strategy Game in Augmented Reality on Google Tango",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Michael Norris",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/04545ce49bf4a81e4469aff061451f9a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "project-tango",
                "unity"
            ],
            "content_html": "<div>\n<p>Tower Defense on Google Tango using Augmented Reality.  Place your base and units and defend it for as long as you can!</p>\n<p>I'm on the 3rd floor.</p>\n</div>",
            "content_md": "\nTower Defense on Google Tango using Augmented Reality. Place your base and units and defend it for as long as you can!\n\n\nI'm on the 3rd floor.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/nosebleed-saves-the-world-r9lwxv",
            "title": "Nosebleed Saves The World",
            "blurb": "Lie down, bleed your nose, save the world.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Icon",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/081/datas/original.jpg"
                },
                {
                    "title": "Screenshot_1",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/925/datas/original.png"
                },
                {
                    "title": "Screenshot_2",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/924/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Terry Li",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7f1e5266e954691f8d71c045ba7efe5b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Tony Kao",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/82de5716fcdc1ebb49d211ef38335b7c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We have only one Oculus Rift DK2 for hackathon. This lack of equipment made us wonder what kind of head-based interaction we can create. We like a PSVR game called \"Street Luge\" which requires players to lie down to play. Taking inspiration from \"Street Luge\" we made \"Nosebleed Saves The World\" to let players lie down and use head movement to shoot meteors in the game.</p>\n<h2>What it does</h2>\n<p>A game where you need to lie down and shoot stuff with your nosebleed. Every time you look at your date, you can refill your nosebleed to keep shooting! </p>\n<h2>How we built it</h2>\n<p>Unity3D, Maya</p>\n<h2>Challenges we ran into</h2>\n<p>Adjusting camera position to make players feel they are embodied.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The game is fun and also kinda difficult. </p>\n<h2>What we learned</h2>\n<p>You can design a lot of interesting game mechanics just around head-based interaction in VR.</p>\n<h2>What's next for Nosebleed Saves The World</h2>\n<p>Release it on itch.io, an indie game community, and maybe Steam.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe have only one Oculus Rift DK2 for hackathon. This lack of equipment made us wonder what kind of head-based interaction we can create. We like a PSVR game called \"Street Luge\" which requires players to lie down to play. Taking inspiration from \"Street Luge\" we made \"Nosebleed Saves The World\" to let players lie down and use head movement to shoot meteors in the game.\n\n\n## What it does\n\n\nA game where you need to lie down and shoot stuff with your nosebleed. Every time you look at your date, you can refill your nosebleed to keep shooting! \n\n\n## How we built it\n\n\nUnity3D, Maya\n\n\n## Challenges we ran into\n\n\nAdjusting camera position to make players feel they are embodied.\n\n\n## Accomplishments that we're proud of\n\n\nThe game is fun and also kinda difficult. \n\n\n## What we learned\n\n\nYou can design a lot of interesting game mechanics just around head-based interaction in VR.\n\n\n## What's next for Nosebleed Saves The World\n\n\nRelease it on itch.io, an indie game community, and maybe Steam.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ar-nodal-network-visualization-k8c3fw",
            "title": "AR Nodal Network Visualization for Academic Citations",
            "blurb": "Use AR to visualize academic citation data to identify influencers in a given field, and facilitate comparisons.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Charles Niu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/71dc5af38927ad75e8c99245a980a9ea?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Paulo Jansen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/151ded7e736f005846dadcae8f93d582?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jes\u00fas Armand",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/5ac6f572bf830ab82d354db8a43ed3ca?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ralston Louie",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/50455d6065b1388e3fed4d2fe4929aa3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "elsevier-api",
                "meta2",
                "unity"
            ],
            "content_html": "<div>\n<p>Team Point person: Charles Niu</p>\n<p>Phone Number: 425-260-1098</p>\n<p>Location: 3rd Floor, far in the back, left side (call if cannot find, we are pretty hidden)</p>\n<p>Vertical: Human Well-being (education specifically)</p>\n<h2>Inspiration</h2>\n<p>Prior to becoming an engineer, I studied History in University. When conducting research for my thesis, I became increasingly aware that despite the sea of books/articles on a given topic, they all reference the same core academic sources. In other words, in most fields, there is only a handful of highly influential sources of which all other papers derive from. This kind of information, when parsed in the traditional fashion, requires hours of dedicated research, the majority of it spent on sifting through citation data. I feel that by utilizing AR as a medium for visualizing data, we can display a nodal network of academic citations to more easily view the origins of data, and influencers in any given field. </p>\n<h2>What it does</h2>\n<p>Parse an online JSON repository of data, such as a scholarly archive. Bring up a paper, see its citations displayed behind it through connecting lines. As the network is projected in 3D space, one merely has to walk over to see the other citations.</p>\n<h2>How I built it</h2>\n<p>We used Unity in conjunction with the Elsevier api. We collect a JSON dataset from calls to the Elsevier api, and then project the information onto a nodal network framework we built in Unity.</p>\n<h2>Challenges I ran into</h2>\n<p>Displaying the data in a meaningful and understandable way has been challenging. We've run into numerous UX troubles in figuring out the best way to display the data so that it is not overly cluttered. This is especially challenging for papers with a large pool of sources. We iterated through many different nodal models. By the end of the hackathon, there is still work to be done to improve the UX experience.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Successfully calling the API, organizing the JSON data, and inputting it into the nodal network framework that we built was the meat of the project. And it surprisingly worked almost without any hitches. Great success there!</p>\n<h2>What I learned</h2>\n<p>Our team learned a lot about organizing visual data networks, as well as the best practice methods to display the data in a meaningful way. Lots more to be learned still however!</p>\n<h2>What's next for AR Nodal Network Visualization</h2>\n<p>A couple of our team have spent time organizing data for other topics, including mapping taxonomic relationships, historical photographs from China. We are building the system to be robust enough for all kinds of data visualization, and once we have iterated the kinks out with the UX, we hope this will be a useful open source tool!</p>\n</div>",
            "content_md": "\nTeam Point person: Charles Niu\n\n\nPhone Number: 425-260-1098\n\n\nLocation: 3rd Floor, far in the back, left side (call if cannot find, we are pretty hidden)\n\n\nVertical: Human Well-being (education specifically)\n\n\n## Inspiration\n\n\nPrior to becoming an engineer, I studied History in University. When conducting research for my thesis, I became increasingly aware that despite the sea of books/articles on a given topic, they all reference the same core academic sources. In other words, in most fields, there is only a handful of highly influential sources of which all other papers derive from. This kind of information, when parsed in the traditional fashion, requires hours of dedicated research, the majority of it spent on sifting through citation data. I feel that by utilizing AR as a medium for visualizing data, we can display a nodal network of academic citations to more easily view the origins of data, and influencers in any given field. \n\n\n## What it does\n\n\nParse an online JSON repository of data, such as a scholarly archive. Bring up a paper, see its citations displayed behind it through connecting lines. As the network is projected in 3D space, one merely has to walk over to see the other citations.\n\n\n## How I built it\n\n\nWe used Unity in conjunction with the Elsevier api. We collect a JSON dataset from calls to the Elsevier api, and then project the information onto a nodal network framework we built in Unity.\n\n\n## Challenges I ran into\n\n\nDisplaying the data in a meaningful and understandable way has been challenging. We've run into numerous UX troubles in figuring out the best way to display the data so that it is not overly cluttered. This is especially challenging for papers with a large pool of sources. We iterated through many different nodal models. By the end of the hackathon, there is still work to be done to improve the UX experience.\n\n\n## Accomplishments that I'm proud of\n\n\nSuccessfully calling the API, organizing the JSON data, and inputting it into the nodal network framework that we built was the meat of the project. And it surprisingly worked almost without any hitches. Great success there!\n\n\n## What I learned\n\n\nOur team learned a lot about organizing visual data networks, as well as the best practice methods to display the data in a meaningful way. Lots more to be learned still however!\n\n\n## What's next for AR Nodal Network Visualization\n\n\nA couple of our team have spent time organizing data for other topics, including mapping taxonomic relationships, historical photographs from China. We are building the system to be robust enough for all kinds of data visualization, and once we have iterated the kinks out with the UX, we hope this will be a useful open source tool!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/storybrookear",
            "title": "StoryBrookeAR",
            "blurb": "Give children the ability to create their own story and content.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/S0K0kP8nv1Q?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/760/datas/original.jpg"
                },
                {
                    "title": "Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/761/datas/original.jpg"
                },
                {
                    "title": "Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/762/datas/original.jpg"
                },
                {
                    "title": "Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/763/datas/original.jpg"
                },
                {
                    "title": "Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/764/datas/original.jpg"
                },
                {
                    "title": "Feature Photo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/423/768/datas/original.jpg"
                },
                {
                    "title": "What does it look like?",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/214/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Evan McCall",
                    "about": "I worked on the HoloLens Implementation and wrapping all the features into unity.",
                    "photo": "https://www.gravatar.com/avatar/0e7b87afe993a39e5734b660e3e23f4d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Raghava Manvitha Reddy Ponnapati",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1468610115/fdlg0tmuox95s8vupkht.jpg?height=180&width=180"
                },
                {
                    "name": "Tom Keefe",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2dfd52f72fcac91374d12691ce41d4de?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Peter Blanco",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/9084c6ea50eda65c47f4c72fd8c66924?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "linx",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d04a2a893e3cf74b65043362f8b9e67e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "django",
                "microsoft-hololens",
                "python",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We wanted to give children and users the ability to draw and create the story that they are telling. My inspiration was the 9/11 memorial where a person may record the events of their day and an algorithm lines up people, places and timeline and plays all stories overlaying different aspects of the event and day.</p>\n<h2>What it does</h2>\n<p>It lets children either free-form draw a picture or use connect the dot templates to overlay a picture on a table. it then allows children to take a picture and generate a 3D Mesh that the child can place in a story environment</p>\n<h2>How we built it</h2>\n<p>We built the main interface using Unity3D and Microsoft HoloLens and we built our backend using Python and Django to submit and extract vector2 coordinates that are sent back to unity and a mesh is generated.</p>\n<h2>Challenges we ran into</h2>\n<p>Generating the Mesh\nFormatting the data\nSmooth Gesture Usage</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The entire project was created!!!</p>\n<h2>What we learned</h2>\n<p>A lot about HoloLens Development and usability of apps that allow content to be generated by the user</p>\n<h2>What's next for StoryBrookeAR</h2>\n<p>We want to expand on the app to include fashion industry concept previews and see where else this type of feature may be applicable.</p>\n<h2>Floor On</h2>\n<p>We are on Floor 6</p>\n<h2>What is our Vertical?</h2>\n<p>Education, Entertainment</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe wanted to give children and users the ability to draw and create the story that they are telling. My inspiration was the 9/11 memorial where a person may record the events of their day and an algorithm lines up people, places and timeline and plays all stories overlaying different aspects of the event and day.\n\n\n## What it does\n\n\nIt lets children either free-form draw a picture or use connect the dot templates to overlay a picture on a table. it then allows children to take a picture and generate a 3D Mesh that the child can place in a story environment\n\n\n## How we built it\n\n\nWe built the main interface using Unity3D and Microsoft HoloLens and we built our backend using Python and Django to submit and extract vector2 coordinates that are sent back to unity and a mesh is generated.\n\n\n## Challenges we ran into\n\n\nGenerating the Mesh\nFormatting the data\nSmooth Gesture Usage\n\n\n## Accomplishments that we're proud of\n\n\nThe entire project was created!!!\n\n\n## What we learned\n\n\nA lot about HoloLens Development and usability of apps that allow content to be generated by the user\n\n\n## What's next for StoryBrookeAR\n\n\nWe want to expand on the app to include fashion industry concept previews and see where else this type of feature may be applicable.\n\n\n## Floor On\n\n\nWe are on Floor 6\n\n\n## What is our Vertical?\n\n\nEducation, Entertainment\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rendever",
            "title": "Rendever",
            "blurb": "Team building escape simulation.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Reed Hayes",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8c62e6dfcb57fce6fb89a2ddc931d63f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "tomneumann85",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e6a6cbabad842b3e2b0aed39dc79d3f4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Team building is an extremely important process for groups and companies to function and innovate together. However, many teams have trouble accessing compelling team building exercises or lack the motivation given the sometimes dull nature of said exercises. Virtual reality provides a wonderful platform to not only making very advanced and fun team building simulations but also the capability to collect detailed analytics on performance.</h2>\n<h2>Our system combines the HTC Vive and networks it with a team of Gear VR users. We place users in different rooms while providing innovative ways for them to communicate outside of using voice. The team members must work together to solve the puzzles in the rooms as well as master collaboration and efficiency when no single person has access to all information.</h2>\n<h2>Our team built the simulation in Unity and developed networking solutions and modified existing unity frameworks to develop the experience. We also have built extensive and robust networking code to allow gear VR and Vive users to engage with each other, see what the other group are seeing. In addition, we built tools to allow each other to paint messages to each other. Our team has also built prototype machine learning models that can be used to predict teams success based on quantitaive data collected in the simulation.</h2>\n<h2>Networking code to allow HTC Vive and Gear VR to work together. Minimizing processing done on the Gear VR to reduce lag. Optimizing lag and performance of interaction between rooms and users.</h2>\n<h2>The networking code and screen mirroring (screen within a screen) while minimizing performance. In addition, we are very proud of our teams ability to modify complex and existing frameworks to fit into our simulation as well as leverage our networking solutions (Rock climbing, archery, screen within a screen, networked painting).</h2>\n<h2>We learned that Unity can be very powerful with all of the pre-existing assets especially if a developer is able to modify frameworks to fit new code innovations. We also found out that a significant amount of optimization work must be done to get the Gear VR to handle enviroments. We also learned new ways to solve networking challenges between the Gear VR and the HTC Vive.</h2>\n<h2>Rendever will release the model to the public and finish out the analytics framework to provide actionable insights leverage machine learning models. The machine learning models will provide teams with precise information on what ways they can function better as a team, where they are strong, where the team needs to work on.</h2>\n</div>",
            "content_md": "\n## Team building is an extremely important process for groups and companies to function and innovate together. However, many teams have trouble accessing compelling team building exercises or lack the motivation given the sometimes dull nature of said exercises. Virtual reality provides a wonderful platform to not only making very advanced and fun team building simulations but also the capability to collect detailed analytics on performance.\n\n\n## Our system combines the HTC Vive and networks it with a team of Gear VR users. We place users in different rooms while providing innovative ways for them to communicate outside of using voice. The team members must work together to solve the puzzles in the rooms as well as master collaboration and efficiency when no single person has access to all information.\n\n\n## Our team built the simulation in Unity and developed networking solutions and modified existing unity frameworks to develop the experience. We also have built extensive and robust networking code to allow gear VR and Vive users to engage with each other, see what the other group are seeing. In addition, we built tools to allow each other to paint messages to each other. Our team has also built prototype machine learning models that can be used to predict teams success based on quantitaive data collected in the simulation.\n\n\n## Networking code to allow HTC Vive and Gear VR to work together. Minimizing processing done on the Gear VR to reduce lag. Optimizing lag and performance of interaction between rooms and users.\n\n\n## The networking code and screen mirroring (screen within a screen) while minimizing performance. In addition, we are very proud of our teams ability to modify complex and existing frameworks to fit into our simulation as well as leverage our networking solutions (Rock climbing, archery, screen within a screen, networked painting).\n\n\n## We learned that Unity can be very powerful with all of the pre-existing assets especially if a developer is able to modify frameworks to fit new code innovations. We also found out that a significant amount of optimization work must be done to get the Gear VR to handle enviroments. We also learned new ways to solve networking challenges between the Gear VR and the HTC Vive.\n\n\n## Rendever will release the model to the public and finish out the analytics framework to provide actionable insights leverage machine learning models. The machine learning models will provide teams with precise information on what ways they can function better as a team, where they are strong, where the team needs to work on.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/velocityvr-1bgpz7",
            "title": "VelocityVR",
            "blurb": "Shop furniture Virtually in 3D from the comfort of your home.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "Product Catalog",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/226/datas/original.png"
                },
                {
                    "title": "Animated Product Detail",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/227/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "burc oral",
                    "about": "ideation, wayfair api usage,  rotationetc",
                    "photo": "https://www.gravatar.com/avatar/9ca32bf7b132545bbf6e4f5ad94bb5eb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Govarthanan Ravi",
                    "about": "Worked as a developer in the team",
                    "photo": "https://www.gravatar.com/avatar/19da540635f8a9f20b6079dbfd169f2a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&height=180&s=180&width=180"
                },
                {
                    "name": "Srinivas Patibandla",
                    "about": "Worked as developer in the team",
                    "photo": "https://www.gravatar.com/avatar/1d93c01b7d0280e24f641c70a81e22af?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Pramod Kumar Rapolu",
                    "about": "Worked as a developer",
                    "photo": "https://www.gravatar.com/avatar/d136dac3006d00c04e4ad6ca6d23c86f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "android-studio",
                "c#",
                "google-cardboard",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration: Virtual Shopping.</h2>\n<h2>What it does: Furniture Catalog Browsing with dynamically retrieved 3D models.</h2>\n<h2>How we built it: Used Wayfair API and Unity with Google Cardboard SDK.</h2>\n<h2>Challenges we ran into: Not knowing any Unity and C# at all.</h2>\n<h2>Accomplishments that we're proud of: Learned Unity, C# and Wayfair API and created a working VR app.</h2>\n<h2>What we learned: Learned Unity, C# and Wayfair API.</h2>\n<h2>What's next for VelocityVR: Create a shopping experience with our own 3D models using structure sensing.</h2>\n</div>",
            "content_md": "\n## Inspiration: Virtual Shopping.\n\n\n## What it does: Furniture Catalog Browsing with dynamically retrieved 3D models.\n\n\n## How we built it: Used Wayfair API and Unity with Google Cardboard SDK.\n\n\n## Challenges we ran into: Not knowing any Unity and C# at all.\n\n\n## Accomplishments that we're proud of: Learned Unity, C# and Wayfair API and created a working VR app.\n\n\n## What we learned: Learned Unity, C# and Wayfair API.\n\n\n## What's next for VelocityVR: Create a shopping experience with our own 3D models using structure sensing.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/playdate",
            "title": "PLAYDATE",
            "blurb": "It's a thing-network visualizer. ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Yujie Hong",
                    "about": "I worked on concept development, user interaction design and prototyping and Unity development.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/320/datas/profile.jpg"
                },
                {
                    "name": "Joanne Cheung",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/606/datas/profile.jpg"
                },
                {
                    "name": "Jenny Shen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7af74dd8e5c9a0d18826237e71efe730?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dan Taeyoung",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/61bfd27a1596dab8f465ce423392d930?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Austin Smith",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/48dca61143a85f3a565e7839c94c8515?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "rhino",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Ecology and human-computer interaction. </p>\n<h2>What it does</h2>\n<p>Users can interact with objects in the world (starting with a building), remove parts, and the parts touched will be transformed into its constituent elements. I.e. Log cabin -&gt; Log -&gt; Trees. The singular becomes the multiple.  </p>\n<h2>How we built it</h2>\n<p>We modelled assets and environment in Rhino and interactions in Unity </p>\n<h2>Challenges we ran into</h2>\n<p>Designing for real-life and virtual fields in reciprocity was difficult, especially when interacting with a field of objects. </p>\n<h2>What's next for playdate</h2>\n<p>Build the re-composition interactions. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEcology and human-computer interaction. \n\n\n## What it does\n\n\nUsers can interact with objects in the world (starting with a building), remove parts, and the parts touched will be transformed into its constituent elements. I.e. Log cabin -> Log -> Trees. The singular becomes the multiple. \n\n\n## How we built it\n\n\nWe modelled assets and environment in Rhino and interactions in Unity \n\n\n## Challenges we ran into\n\n\nDesigning for real-life and virtual fields in reciprocity was difficult, especially when interacting with a field of objects. \n\n\n## What's next for playdate\n\n\nBuild the re-composition interactions. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/hashtag360-46edqx",
            "title": "hashtag360",
            "blurb": "instagram but 360 with an option to \"hang out\" in the picture",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Seda Kochian",
                    "about": "teamlead, backend, sockets, a-frame",
                    "photo": "https://www.gravatar.com/avatar/2dec3d4d8c5e76e348fc677755fb446c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "vincent chan",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/23a3ef66b5445dc830833386cd77434a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "a-frame",
                "angular.js",
                "es6",
                "firebase",
                "html5",
                "javascript",
                "node.js",
                "socket.io",
                "webgl"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We wanted to give our friends a fuller exerince of our facebook and instagram pics </p>\n<h2>What it does</h2>\n<h2>How I built it</h2>\n<p>We just did it! </p>\n<h2>Challenges I ran into</h2>\n<p>It took us some time to integrate A-frame with AngularJS and... then with firebase and socket.io</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We overcame the challenges that we listed above </p>\n<h2>What I learned</h2>\n<p>We learnt how to use A-Frame and WebGL</p>\n<h2>What's next for hashtag360</h2>\n<p>commercialize it and build and Pro version for businesses as VR collaboration tool </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe wanted to give our friends a fuller exerince of our facebook and instagram pics \n\n\n## What it does\n\n\n## How I built it\n\n\nWe just did it! \n\n\n## Challenges I ran into\n\n\nIt took us some time to integrate A-frame with AngularJS and... then with firebase and socket.io\n\n\n## Accomplishments that I'm proud of\n\n\nWe overcame the challenges that we listed above \n\n\n## What I learned\n\n\nWe learnt how to use A-Frame and WebGL\n\n\n## What's next for hashtag360\n\n\ncommercialize it and build and Pro version for businesses as VR collaboration tool \n\n\n"
        },
        {
            "source": "https://devpost.com/software/trails-lgo0ie",
            "title": "Trails",
            "blurb": "An experience that lets one walk in someone else's shoes",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/cm0uIrq-bng?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Trails Concept Image",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/424/254/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Lucas Cassiano",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/000a5395cf271408746d8dda9c7deb3d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "cdvm",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/234ee1a9515eaf51bf1cc7093c7e126f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "microsoft-hololens"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We spend most of our time navigating directions and establishing paths - whether it is for running, going on tours, hiking on mountain trails, or simply making sure that we find a safe route to walk back home at night. There are a lot of technologies available today that tell us how to get from point A to point B - giving us the shortest route or the less busier route. But what they don\u2019t give us is the experience - the ease of having a path already defined for you, the joy of sharing beautiful trails with others, or the security in walking back home safely. The inspiration behind our project - Trails, is to provide an augmented reality experience that enhances the routes we take and our time spent navigating them.</p>\n<h2>What it does</h2>\n<p>Trails provides an augmented reality experience to the user, where we don\u2019t have to spend time and energy navigating a path. It trains itself to understand older, safer, established routes and allows users to easily select an existing route and integrate that into the environment by showing footsteps in front of the user. The user can navigate their environment by looking ahead and nowhere else, as the footsteps keep getting added in front of them leading the way and telling them where to go.</p>\n<h2>How I built it</h2>\n<p>We paired Hololens with an Android phone using Bluetooth LE. The phone will send the GPS coordinates to the device along with the bearing. The Hololens does a raycast to the floor at 45 degrees, to determine the elevation at which to place footprints. Then, the footprints are placed on the floor for two use cases. First, a user might decide to follow an observers path in places such as an art installation. Second, the footprints can point towards the location the user is trying to go to (AR Google Maps).</p>\n<h2>Challenges I ran into</h2>\n<p>Given Hololens does not have GPS location nor magnetometer, the outdoor scenario was more complicated to develop than expected. Additionally, the spatial mapping of the Hololens is sometimes not accurate in real time, which makes tracking the floor complicated in several cases.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We were proud of the way the footprints look on the HoloLens. \nWe also really liked the balance of skills and personality we had on our team. We encouraged each other and worked well together. </p>\n<h2>What I learned</h2>\n<p>We learned that GPS can be quite complicated to integrate with the HoloLens. \nWe learned more about VR/AR technologies in general and the applications that are possible. </p>\n<h2>What's next for Trails</h2>\n<p>*Integrating GPS into the app for outdoor location to use for safe trails, running paths, and more. </p>\n<p>*Crowdsourcing best paths in cities for a tourism context</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe spend most of our time navigating directions and establishing paths - whether it is for running, going on tours, hiking on mountain trails, or simply making sure that we find a safe route to walk back home at night. There are a lot of technologies available today that tell us how to get from point A to point B - giving us the shortest route or the less busier route. But what they don\u2019t give us is the experience - the ease of having a path already defined for you, the joy of sharing beautiful trails with others, or the security in walking back home safely. The inspiration behind our project - Trails, is to provide an augmented reality experience that enhances the routes we take and our time spent navigating them.\n\n\n## What it does\n\n\nTrails provides an augmented reality experience to the user, where we don\u2019t have to spend time and energy navigating a path. It trains itself to understand older, safer, established routes and allows users to easily select an existing route and integrate that into the environment by showing footsteps in front of the user. The user can navigate their environment by looking ahead and nowhere else, as the footsteps keep getting added in front of them leading the way and telling them where to go.\n\n\n## How I built it\n\n\nWe paired Hololens with an Android phone using Bluetooth LE. The phone will send the GPS coordinates to the device along with the bearing. The Hololens does a raycast to the floor at 45 degrees, to determine the elevation at which to place footprints. Then, the footprints are placed on the floor for two use cases. First, a user might decide to follow an observers path in places such as an art installation. Second, the footprints can point towards the location the user is trying to go to (AR Google Maps).\n\n\n## Challenges I ran into\n\n\nGiven Hololens does not have GPS location nor magnetometer, the outdoor scenario was more complicated to develop than expected. Additionally, the spatial mapping of the Hololens is sometimes not accurate in real time, which makes tracking the floor complicated in several cases.\n\n\n## Accomplishments that I'm proud of\n\n\nWe were proud of the way the footprints look on the HoloLens. \nWe also really liked the balance of skills and personality we had on our team. We encouraged each other and worked well together. \n\n\n## What I learned\n\n\nWe learned that GPS can be quite complicated to integrate with the HoloLens. \nWe learned more about VR/AR technologies in general and the applications that are possible. \n\n\n## What's next for Trails\n\n\n*Integrating GPS into the app for outdoor location to use for safe trails, running paths, and more. \n\n\n*Crowdsourcing best paths in cities for a tourism context\n\n\n"
        }
    ],
    "rv2019": [
        {
            "source": "https://devpost.com/software/ideatear",
            "title": "Ideate AR",
            "blurb": "A collaboration tool that allows users to physically pull content from devices into a shared holographic workspace",
            "awards": [
                "Best use of Vuforia (PTC)"
            ],
            "videos": [
                "https://www.youtube.com/embed/8NN5uF9-j6Q?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "IDEATE AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/448/datas/original.png"
                },
                {
                    "title": "Ideas without boundaries",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/060/datas/original.png"
                },
                {
                    "title": "Process",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/924/datas/original.JPG"
                },
                {
                    "title": "User scenario",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/047/datas/original.JPG"
                },
                {
                    "title": "Interactions",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/936/datas/original.jpg"
                },
                {
                    "title": "Demo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/288/datas/original.png"
                },
                {
                    "title": "Demo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/292/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Jacob Shepherd",
                    "about": "Director of Product Development with specialization in Disruptive Strategy.  Built the Vuforia Studio integration and the Chrome Extension.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/190/datas/profile.jpg"
                },
                {
                    "name": "Iris Rodriguez",
                    "about": "Interactive and Product Designer passionate about emerging technologies and specialized in MR, AR, and VR experiences. She helped with the design thinking process of the idea, user research and the envisioning of future features.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/206/572/datas/profile.jpg"
                },
                {
                    "name": "Kathy Wang",
                    "about": "UI/UX Designer, Industrial Designer and 3D modeler.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/027/datas/profile.jpeg"
                },
                {
                    "name": "Matt Bell",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4520126?height=180&v=4&width=180"
                },
                {
                    "name": "Eliana Mej\u00eda",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/500/datas/profile.png"
                }
            ],
            "built_with": [
                ".net",
                "autodesk",
                "data-visualization",
                "fidelity",
                "javascript",
                "magic-leap",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Location</h2>\n<p>Ideate AR - Team #75 - Floor 6 of MIT Media Lab Building E14, Room E14-674 (Purple / MPR), Table: 18 (Back left corner)</p>\n<p>Contact: Jacob Shepherd - 913.626.4777</p>\n<h2>Inspiration</h2>\n<p>When in brainstorming sessions, we see how difficult it is to express ideas, share them or have participative and fluid communication. Everyone shares ideas differently, with different skill sets and forms to approaching problems. Some participants might not be as inspired or not even present in the session, impacting productivity of the team. This can make sessions time consuming and overwhelming.</p>\n<p>Team communication and sketching is the primary tool when brainstorming:  We tend to explain our ideas verbally and support them with visual elements (sketches, images, videos, data, etc..).  The creative flow of the session can be disrupted when participants are not present or they can\u2019t share the idea at the moment quickly and fluid.</p>\n<p>In fact, we came across as a team with this challenge during the hackathon, as other teams did: People from different parts of the world, profiles and experiences came together to ideate a new product experience. How can we make the ideation process more productive and intuitive with the little time that we had?</p>\n<h2>What it does</h2>\n<p><em><strong>Ideate AR</strong></em>\n<em>Give freedom to your ideas</em></p>\n<p>Ideate AR provides a productive and collaborative environment where people can join, be creative and free the imagination. Participants with different skill sets, creative processes and backgrounds, makes the brainstorming activity richer.  With Ideate AR we can augment this ideas and process by transcends traditional mediums of communication - such as paper, whiteboards and screens - in an immersive, multi-user environment making the ideation session more creative and productive.</p>\n<p>Whit Ideate AR you can get inspired and work on your ideas on the go:  Brainstorm different, get creative, collaborate with others and get your ideas to the next level:</p>\n<p>Get inspired and don\u2019t let your ideas go away: Create sketches, sticky notes, record audio notes, upload images, video, 3D models, text and data that you can share on the ideation sessions.</p>\n<p>Join and ideation session where you will be able to connect in real time with other members in a collaborative space, even if they are in another part of the world.</p>\n<p>Share your ideas, brainstorm, collaborate and create in teams. Through seamless device integration and an understanding of the surrounding spaces, see how the ideas grow and creativity takes place.</p>\n<p>Have a better decision making for next steps. Share the outcomes and decision of the ideation process and track the results. For managers, this is a great tool for improve the participation of the team, incorporate innovation processes, align teams and take better decisions.</p>\n<p>Real-time recommendations and summarizations powered by machine learning and artificial intelligence. Users can also track the development of the idea and comeback to brainstorm in a different session or as a personal task in a different moment.</p>\n<p>Anyone with an idea can use Ideate AR: Agencies, Company teams, Students groups, or anyone that has an idea and wants to take it to the next level.</p>\n<h2>How we built it</h2>\n<p>We thought about the concept of ideation and brainstorm itself and how people do it. Also, how this dynamics change when participants have different backgrounds, speaks different languages, have different skills, mindsets, access to resources, and also how this process gets affected when teams are working remotely.</p>\n<p>We created Ideate AR  with the objective of break the boundaries of the physical world when ideating, (ie. sketching, sharing visual elements, taking notes in a notepad, analyze data) and take it to a virtual environment by enhancing the process and making it more productive.  This way we can solve problems of communication during the sessions, provide a better understanding of the ideas and find a better outcomes by working in teams.</p>\n<p>We decided that the best way to accomplish this was by building this collaborative space as an augmented and mixed reality experience in Unity for Magic Leap, in which we can use the  advantages of the spatial computing, allowing us to improve communication and synchronization between multiple users and devices in one session in the same time.</p>\n<p>We studied the user cases were this problems of productivity and ideation happens and how it affects teams productivity and ideas. We analyzed common scenarios and designed the experience having in mind the common pains on which  participants come across with more frequently and how can we  solve them through a better user experience by the use of mixed  reality. </p>\n<p>The experience: </p>\n<p>We took advantage of the technical features provided Magic Leap, Autodesk and Vuforia:</p>\n<ul>\n<li><p>We built a screen enrolment mechanism to allow for laptops and mobile devices to share assets with the Magic Leap through a drag and drop gesture.</p></li>\n<li><p>Added ability to grab and move assets in the collaboration space.</p></li>\n<li><p>Added ability for assets to be relative to a shared point in the environment (nucleus)</p></li>\n<li><p>After the generic communication layer was built, we focused on providing a user interface that could add post-it notes to the app by grabbing and pulling them out of a laptop or mobile device and into the collaboration space. Next, we added the ability for participants to scan print material, such as a product design document, and popped a 3D model that could be pulled out of a mobile device and into the collaboration space.</p></li>\n</ul>\n<p>-We then worked on allowing participants the ability to use AutoDesk\u2019s Forge product to take pictures of a physical object, which could be loaded into the collaboration space. We also added branding elements and physics to items that are in the collaboration space.</p>\n<ul>\n<li>After discussing the need for team members to be able to share data visualizations, we added the ability for a spreadsheet with a 2D graph to turn into a 3D data visualization that could then be added to the collaboration space.</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<p>We came across different type of challenges while building IdeateAR.  We as a team, saw ourselves confronted to the concept of \u201cideation\u201d and we thought that there must be a better  way to brainstorm. One of the first challenges was on how to represent and abstract concept  like \u201cbrainstorming\u201d and  \u201cideas\u201d in more intuitive way.</p>\n<p>Other challenges was to came across when working with new technologies and pushing ourselves to create in areas we were not too familiar with, more specifically to design and develop experiences in augmented reality and spatial computing.</p>\n<p>When working for IdeateAR, we saw  the importance of the user testing and the way we can introduce a new user to the new medium, and all the user experience problems that emerge when using a new technology as it is with gestures and interactions.</p>\n<p>On the technical side some of the  challenges we ran to was to how to route data through the socket server, the deployment times through iterations to Magic Leap, controlling the tracking on the device, Issues with system slowing to a crawl and needing to be rebooted and overcome the errors were emerging with AutoDesk Forge Reality Capture API.</p>\n<h2>Accomplishments that we are proud of</h2>\n<p>Approaching the design thinking process to enhance it in a way we are making  it more intuitive, imaginative a productive. We were able to work around the implementation of this new technology in our product by testing and learning for the challenges of the user experience and its technical approach.</p>\n<p>One of biggest take away was to learn how to work with people the different points of views, the different backgrounds and skills. It provided a richer way to approach the idea and in consequence the development of the project. We are proud to work in a team so diverse.</p>\n<h2>What we learned</h2>\n<p>How  to work with a this type of technology, with its own design and development challenges:</p>\n<ul>\n<li>Magic Leap</li>\n<li>Vuforia</li>\n<li>Autodesk</li>\n</ul>\n<h2>What's next for IdeateAR</h2>\n<p>A variety of components can be implemented on future iterations of ideateAR:</p>\n<p>Provide pre design sessions for ideation (Empathy maps, product validation)\nConnect other productivity tools (Dropbox, Asana, Trello)</p>\n</div>",
            "content_md": "\n## Location\n\n\nIdeate AR - Team #75 - Floor 6 of MIT Media Lab Building E14, Room E14-674 (Purple / MPR), Table: 18 (Back left corner)\n\n\nContact: Jacob Shepherd - 913.626.4777\n\n\n## Inspiration\n\n\nWhen in brainstorming sessions, we see how difficult it is to express ideas, share them or have participative and fluid communication. Everyone shares ideas differently, with different skill sets and forms to approaching problems. Some participants might not be as inspired or not even present in the session, impacting productivity of the team. This can make sessions time consuming and overwhelming.\n\n\nTeam communication and sketching is the primary tool when brainstorming: We tend to explain our ideas verbally and support them with visual elements (sketches, images, videos, data, etc..). The creative flow of the session can be disrupted when participants are not present or they can\u2019t share the idea at the moment quickly and fluid.\n\n\nIn fact, we came across as a team with this challenge during the hackathon, as other teams did: People from different parts of the world, profiles and experiences came together to ideate a new product experience. How can we make the ideation process more productive and intuitive with the little time that we had?\n\n\n## What it does\n\n\n***Ideate AR***\n*Give freedom to your ideas*\n\n\nIdeate AR provides a productive and collaborative environment where people can join, be creative and free the imagination. Participants with different skill sets, creative processes and backgrounds, makes the brainstorming activity richer. With Ideate AR we can augment this ideas and process by transcends traditional mediums of communication - such as paper, whiteboards and screens - in an immersive, multi-user environment making the ideation session more creative and productive.\n\n\nWhit Ideate AR you can get inspired and work on your ideas on the go: Brainstorm different, get creative, collaborate with others and get your ideas to the next level:\n\n\nGet inspired and don\u2019t let your ideas go away: Create sketches, sticky notes, record audio notes, upload images, video, 3D models, text and data that you can share on the ideation sessions.\n\n\nJoin and ideation session where you will be able to connect in real time with other members in a collaborative space, even if they are in another part of the world.\n\n\nShare your ideas, brainstorm, collaborate and create in teams. Through seamless device integration and an understanding of the surrounding spaces, see how the ideas grow and creativity takes place.\n\n\nHave a better decision making for next steps. Share the outcomes and decision of the ideation process and track the results. For managers, this is a great tool for improve the participation of the team, incorporate innovation processes, align teams and take better decisions.\n\n\nReal-time recommendations and summarizations powered by machine learning and artificial intelligence. Users can also track the development of the idea and comeback to brainstorm in a different session or as a personal task in a different moment.\n\n\nAnyone with an idea can use Ideate AR: Agencies, Company teams, Students groups, or anyone that has an idea and wants to take it to the next level.\n\n\n## How we built it\n\n\nWe thought about the concept of ideation and brainstorm itself and how people do it. Also, how this dynamics change when participants have different backgrounds, speaks different languages, have different skills, mindsets, access to resources, and also how this process gets affected when teams are working remotely.\n\n\nWe created Ideate AR with the objective of break the boundaries of the physical world when ideating, (ie. sketching, sharing visual elements, taking notes in a notepad, analyze data) and take it to a virtual environment by enhancing the process and making it more productive. This way we can solve problems of communication during the sessions, provide a better understanding of the ideas and find a better outcomes by working in teams.\n\n\nWe decided that the best way to accomplish this was by building this collaborative space as an augmented and mixed reality experience in Unity for Magic Leap, in which we can use the advantages of the spatial computing, allowing us to improve communication and synchronization between multiple users and devices in one session in the same time.\n\n\nWe studied the user cases were this problems of productivity and ideation happens and how it affects teams productivity and ideas. We analyzed common scenarios and designed the experience having in mind the common pains on which participants come across with more frequently and how can we solve them through a better user experience by the use of mixed reality. \n\n\nThe experience: \n\n\nWe took advantage of the technical features provided Magic Leap, Autodesk and Vuforia:\n\n\n* We built a screen enrolment mechanism to allow for laptops and mobile devices to share assets with the Magic Leap through a drag and drop gesture.\n* Added ability to grab and move assets in the collaboration space.\n* Added ability for assets to be relative to a shared point in the environment (nucleus)\n* After the generic communication layer was built, we focused on providing a user interface that could add post-it notes to the app by grabbing and pulling them out of a laptop or mobile device and into the collaboration space. Next, we added the ability for participants to scan print material, such as a product design document, and popped a 3D model that could be pulled out of a mobile device and into the collaboration space.\n\n\n-We then worked on allowing participants the ability to use AutoDesk\u2019s Forge product to take pictures of a physical object, which could be loaded into the collaboration space. We also added branding elements and physics to items that are in the collaboration space.\n\n\n* After discussing the need for team members to be able to share data visualizations, we added the ability for a spreadsheet with a 2D graph to turn into a 3D data visualization that could then be added to the collaboration space.\n\n\n## Challenges we ran into\n\n\nWe came across different type of challenges while building IdeateAR. We as a team, saw ourselves confronted to the concept of \u201cideation\u201d and we thought that there must be a better way to brainstorm. One of the first challenges was on how to represent and abstract concept like \u201cbrainstorming\u201d and \u201cideas\u201d in more intuitive way.\n\n\nOther challenges was to came across when working with new technologies and pushing ourselves to create in areas we were not too familiar with, more specifically to design and develop experiences in augmented reality and spatial computing.\n\n\nWhen working for IdeateAR, we saw the importance of the user testing and the way we can introduce a new user to the new medium, and all the user experience problems that emerge when using a new technology as it is with gestures and interactions.\n\n\nOn the technical side some of the challenges we ran to was to how to route data through the socket server, the deployment times through iterations to Magic Leap, controlling the tracking on the device, Issues with system slowing to a crawl and needing to be rebooted and overcome the errors were emerging with AutoDesk Forge Reality Capture API.\n\n\n## Accomplishments that we are proud of\n\n\nApproaching the design thinking process to enhance it in a way we are making it more intuitive, imaginative a productive. We were able to work around the implementation of this new technology in our product by testing and learning for the challenges of the user experience and its technical approach.\n\n\nOne of biggest take away was to learn how to work with people the different points of views, the different backgrounds and skills. It provided a richer way to approach the idea and in consequence the development of the project. We are proud to work in a team so diverse.\n\n\n## What we learned\n\n\nHow to work with a this type of technology, with its own design and development challenges:\n\n\n* Magic Leap\n* Vuforia\n* Autodesk\n\n\n## What's next for IdeateAR\n\n\nA variety of components can be implemented on future iterations of ideateAR:\n\n\nProvide pre design sessions for ideation (Empathy maps, product validation)\nConnect other productivity tools (Dropbox, Asana, Trello)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/accessibild",
            "title": "accessIbIld",
            "blurb": "An AR solution for assessing ADA compliance in buildings, facilities and other spaces with one tap.",
            "awards": [
                "Best use of AR and IOT in industrial, manufacturing and field service (PTC)"
            ],
            "videos": [
                "https://www.youtube.com/embed/Qd9c4dSo8IE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Main menu",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/017/datas/original.jpg"
                },
                {
                    "title": "Toilet recognition through customvision.ai",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/018/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Alexandria  de Aranzeta",
                    "about": "Idea creator and domain expert in ADA compliance. Team Lead and Product Manager- set all vision, specs (incl. AI), content, and strategy for building the app and MVP feature.  As an example, assessed ADA standards (federal/ local) and converted to MVP feature strategy (60\" turnaround) and test parameters. Recorded in-app audio, designed prototype UI/ user flow, testing, and in-app photographs.       ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/526/datas/profile.jpg"
                },
                {
                    "name": "Chris Papenfu\u00df",
                    "about": "HoloLens develompent, Icon creation, Unity development, ARFoundation development",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/517/947/datas/profile.png"
                },
                {
                    "name": "Ishlok Vashistha",
                    "about": "Statistical data research and Project management. Also helped in collecting data to train the machine learning model.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/511/datas/profile.png"
                },
                {
                    "name": "Amrit Kaur Choudhary",
                    "about": "Trained Artificial Intelligence Model using Custom Vision. Collected and managed various data-sets.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/785/datas/profile.jpg"
                },
                {
                    "name": "Nikolaos Angelos Papastavrou",
                    "about": "AI/ML model training and dataset creation",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/756/923/datas/profile.png"
                }
            ],
            "built_with": [
                "cogito-intelligence",
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Team Name</h2>\n<p>Accessibild (Team #69, Table# 20)</p>\n<h2>Location</h2>\n<p>MIT Media Lab, Floor 6, Multipurpose Room (front table by the entrance)</p>\n<h2>Idea Inspiration</h2>\n<p>Accessibild can allow for faster and more cost efficient access, with people and society in mind. Accessible design and compliance with ADA standards for buildings and physical spaces are critical to achieving barrier-free living and working, and growing equity and inclusion in society. As an issue of increasing importance in the United States, and as universal design is becoming more prevalent worldwide, there is a need to address the gaps in the compliance assessment process, specifically wasted time and cost associated with the redundant site visits from various stakeholders.  This AR app has immediate impact, with no customization needed, for a multiple user segments: US Federal, State and Local government, small to large businesses, contractors, architectural boards and consultants, among others.</p>\n<h2>Use Case</h2>\n<p>Any building/ facilities manager, contractor or consultant wanting to audit an existing bathroom for ADA compliance.</p>\n<h2>Development Tools Used to Build the Project</h2>\n<ul>\n<li>Unity 3D</li>\n<li>Visual Studio</li>\n<li>Paint.net</li>\n<li>Inkscape</li>\n<li>Azure Cognitive Services</li>\n</ul>\n<h2>SDKs Used</h2>\n<ul>\n<li>Universal Windows SDK</li>\n<li>Unity Game Engine</li>\n</ul>\n<h2>APIs Used</h2>\n<ul>\n<li>(HoloLens)Spatial Mapping</li>\n<li>(HoloLens)Spatial Sound</li>\n<li>(HoloLens)Speech recognition</li>\n<li>ARFoundation (not ended up in final product)</li>\n<li>ARCore (not ended up in final product)</li>\n<li>Azure <a href=\"https://customvision.ai\" rel=\"nofollow\">https://customvision.ai</a></li>\n</ul>\n<h2>Assets Used (not created by us)</h2>\n<ul>\n<li>Icons from <a href=\"https://materialdesignicons.com\" rel=\"nofollow\">https://materialdesignicons.com</a></li>\n<li>Sounds from <a href=\"https://freesound.org\" rel=\"nofollow\">https://freesound.org</a></li>\n</ul>\n<h2>Libraries Used</h2>\n<ul>\n<li>Included in Unity &amp; UWP</li>\n<li>MixedRealityToolkit (from Microsoft @Github)</li>\n<li>Json.NET from Newtonsoft</li>\n</ul>\n<h2>Components Not Created at the Hackathon</h2>\n<p>N/A</p>\n</div>",
            "content_md": "\n## Team Name\n\n\nAccessibild (Team #69, Table# 20)\n\n\n## Location\n\n\nMIT Media Lab, Floor 6, Multipurpose Room (front table by the entrance)\n\n\n## Idea Inspiration\n\n\nAccessibild can allow for faster and more cost efficient access, with people and society in mind. Accessible design and compliance with ADA standards for buildings and physical spaces are critical to achieving barrier-free living and working, and growing equity and inclusion in society. As an issue of increasing importance in the United States, and as universal design is becoming more prevalent worldwide, there is a need to address the gaps in the compliance assessment process, specifically wasted time and cost associated with the redundant site visits from various stakeholders. This AR app has immediate impact, with no customization needed, for a multiple user segments: US Federal, State and Local government, small to large businesses, contractors, architectural boards and consultants, among others.\n\n\n## Use Case\n\n\nAny building/ facilities manager, contractor or consultant wanting to audit an existing bathroom for ADA compliance.\n\n\n## Development Tools Used to Build the Project\n\n\n* Unity 3D\n* Visual Studio\n* Paint.net\n* Inkscape\n* Azure Cognitive Services\n\n\n## SDKs Used\n\n\n* Universal Windows SDK\n* Unity Game Engine\n\n\n## APIs Used\n\n\n* (HoloLens)Spatial Mapping\n* (HoloLens)Spatial Sound\n* (HoloLens)Speech recognition\n* ARFoundation (not ended up in final product)\n* ARCore (not ended up in final product)\n* Azure <https://customvision.ai>\n\n\n## Assets Used (not created by us)\n\n\n* Icons from <https://materialdesignicons.com>\n* Sounds from <https://freesound.org>\n\n\n## Libraries Used\n\n\n* Included in Unity & UWP\n* MixedRealityToolkit (from Microsoft @Github)\n* Json.NET from Newtonsoft\n\n\n## Components Not Created at the Hackathon\n\n\nN/A\n\n\n"
        },
        {
            "source": "https://devpost.com/software/accudrive",
            "title": "AccuDrive",
            "blurb": "Bringing robust safety of heads-up display to the distracted masses",
            "awards": [
                "Best use of WayRay's True AR SDK"
            ],
            "videos": [
                "https://www.youtube.com/embed/K1FFypjD7PM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Brendan Nelligan",
                    "about": "I was responsible for developing the code for calculating driver score, and exporting it to a text document. I was also involved in all other aspects of our code development, since it was a very collaborative effort.",
                    "photo": "https://avatars3.githubusercontent.com/u/27979470?height=180&v=4&width=180"
                },
                {
                    "name": "Himanshu Aggarwal",
                    "about": "I was responsible for coding and developing the algorithms for various scenarios that could occur on the road while driving.\nIt was my first time working with True AR SDK and I learned a lot. Worked together with the SDK developers to code and implement new functionalities for our project.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/739/243/datas/profile.jpg"
                },
                {
                    "name": "Vamsi Batchu",
                    "about": "I was responsible for the user experience and to create the use cases associated with the app. Also responsible for brainstorming and ideating sessions. I have also worked on designing the logos and creating 3D models required for the simulation in blender.  ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/542/datas/profile.jpg"
                },
                {
                    "name": "Niveditha Gopalkrishna",
                    "about": "I was involved during the ideating/Brainstorming Phase on the design of the application. Since it was a very collaborative effort, I contributed to the Application Coding and Rendering of 3D models. \n",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQFq5mcO8FE6bg/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=rsrxrgIM-Lh67cy6PoQoP9Ph2M9-h-cdPLMNGqaj0q4&v=beta&width=180"
                },
                {
                    "name": "Rupesh Pamaihgari",
                    "about": "I was responsible for ideating and developing the algorithms that can track behavior of a driver on road. I have designed architecture and workflow that can combine different APIs to achieve the Holographic representations in screen ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/260/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "blender",
                "c++",
                "html5",
                "wayray-sdk"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Every year millions of people die and commit accidents due to unsafe driving behaviors such as overspeeding, irregular lane changing, not following the traffic signals and many more. Right now, there is no platform to monitor driving behaviors and we hope to fill the hole by providing ways to make the road safe for everybody. </p>\n<h2>What it does</h2>\n<p>Using True AR SDK we want to increase awareness of traffic rules and regulations for the drivers by analyzing their driving patterns on the roads and using their historic behavior to create their driving profile and scores. The violations would be penalized and the good behaviors would be rewarded thereby driving them towards good driving patterns.</p>\n<h2>Use Cases</h2>\n<p><strong>Helping People to Drive Better</strong>\nThe primary users for this would be general public who would be benefited the most as good driving behaviors can be embedded via this.</p>\n<p><strong>Automating Driving Tests</strong>\nDriving tests can be automated and the driver\u2019s behavior can be monitored over a period of time to evaluate them.</p>\n<p><strong>Cab Driver performance Monitoring</strong>\nMonitoring Cab drivers to ensure the safety of the passengers and giving them appropriate training based on the scores.</p>\n<p><strong>Insurance companies</strong>\nThese scores can be used by auto insurance companies to evaluate risk and charge monthly premiums accordingly. This could also be used to establish fault for accidents.</p>\n<h2>How we built it</h2>\n<p>It was mainly built using the Android Studio using the TrueAR SDK. We started looking at the documentation of the SDK , listed out all the capabilities which it currently provides and started leveraging those to communicate our ideas. We had 5 main features which utilizes the objects in the simulator and distributed tasks among ourselves. All the programming is done using c++ including features like exporting the simulator data to external text files. The 3D models and assets were built on Sketch and Blender and them imported into the android studio. The Admin dashboard is built using HTML5, CSS.</p>\n<h2>Challenges we ran into</h2>\n<p><strong>SDK</strong>: The biggest challenge for the hackathon was the SDK as it is just 1 month old and still evolving, there is not really much you can accomplish with the features currently available.</p>\n<h2>Accomplishments that we are proud of</h2>\n<ol>\n<li>We are really proud of making the drivers aware of the traffic rules and ensuring that they follow them via our experience.</li>\n<li>We were successfully able to export the driving behaviour scores from the simulator to a text document which is vital.\n3.We were able to detect improper parking and alert the drivers accordingly and there is no technology right now which is capable of doing that.</li>\n</ol>\n<h2>What's next for AccuDrive</h2>\n<ul>\n<li><p>Implementing more complex features such as sharp turns, lane discipline, detection of crashes, one way roads, improper passing to name a few.</p></li>\n<li><p>Leveraging machine learning to detect roads and locations where drivers are most likely to commit an accident based on previous data collected through the SDK.</p></li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEvery year millions of people die and commit accidents due to unsafe driving behaviors such as overspeeding, irregular lane changing, not following the traffic signals and many more. Right now, there is no platform to monitor driving behaviors and we hope to fill the hole by providing ways to make the road safe for everybody. \n\n\n## What it does\n\n\nUsing True AR SDK we want to increase awareness of traffic rules and regulations for the drivers by analyzing their driving patterns on the roads and using their historic behavior to create their driving profile and scores. The violations would be penalized and the good behaviors would be rewarded thereby driving them towards good driving patterns.\n\n\n## Use Cases\n\n\n**Helping People to Drive Better**\nThe primary users for this would be general public who would be benefited the most as good driving behaviors can be embedded via this.\n\n\n**Automating Driving Tests**\nDriving tests can be automated and the driver\u2019s behavior can be monitored over a period of time to evaluate them.\n\n\n**Cab Driver performance Monitoring**\nMonitoring Cab drivers to ensure the safety of the passengers and giving them appropriate training based on the scores.\n\n\n**Insurance companies**\nThese scores can be used by auto insurance companies to evaluate risk and charge monthly premiums accordingly. This could also be used to establish fault for accidents.\n\n\n## How we built it\n\n\nIt was mainly built using the Android Studio using the TrueAR SDK. We started looking at the documentation of the SDK , listed out all the capabilities which it currently provides and started leveraging those to communicate our ideas. We had 5 main features which utilizes the objects in the simulator and distributed tasks among ourselves. All the programming is done using c++ including features like exporting the simulator data to external text files. The 3D models and assets were built on Sketch and Blender and them imported into the android studio. The Admin dashboard is built using HTML5, CSS.\n\n\n## Challenges we ran into\n\n\n**SDK**: The biggest challenge for the hackathon was the SDK as it is just 1 month old and still evolving, there is not really much you can accomplish with the features currently available.\n\n\n## Accomplishments that we are proud of\n\n\n1. We are really proud of making the drivers aware of the traffic rules and ensuring that they follow them via our experience.\n2. We were successfully able to export the driving behaviour scores from the simulator to a text document which is vital.\n3.We were able to detect improper parking and alert the drivers accordingly and there is no technology right now which is capable of doing that.\n\n\n## What's next for AccuDrive\n\n\n* Implementing more complex features such as sharp turns, lane discipline, detection of crashes, one way roads, improper passing to name a few.\n* Leveraging machine learning to detect roads and locations where drivers are most likely to commit an accident based on previous data collected through the SDK.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/clear-sight-zgrb7v",
            "title": "cleAR sight",
            "blurb": "A magic leap AR accessibility application for individuals with low vision",
            "awards": [
                "Best use of Magic Leap",
                "Wayfair's Way-more"
            ],
            "videos": [
                "https://www.youtube.com/embed/jcAOVeejTWc?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Mike Dopsa",
                    "about": "Concept & Technical Designer, User Flow, Sightless Interface Design, Video Producer, Voice Over Artist, BTS Documentation",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/719/datas/profile.jpg"
                },
                {
                    "name": "Keith Bradley",
                    "about": "Low Vision and Accessibility standards, 3D models, Audio Engineering, UX.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/101/datas/profile.jpg"
                },
                {
                    "name": "Media Ridha",
                    "about": "",
                    "photo": "https://graph.facebook.com/10104283282534841/picture?height=180&width=180"
                },
                {
                    "name": "BXB2",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/34118361?height=180&v=4&width=180"
                },
                {
                    "name": "Peter Lu",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/683/122/datas/profile.jpg"
                }
            ],
            "built_with": [
                "magicleap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>According to the National Center for Health Statistics, 26 Million Americans Adults experience significant vision loss, creating challenges with Depth Perception, Low-Light scenarios and proprioception. CleARsight for Magic Leap is a Spatial Computing Accessibility application, to improve the daily lives of individuals with Low Vision. </p>\n<h2>What it does</h2>\n<p>Using Magic Leap\u2019s World Mapping, Haptic Feedback, Spatial Audio, and Holographic Visual Overlay, CleARsight illuminates your environment, outlining object edges with High Contrast colours, and highlighting horizontal planes with a vivid pattern. </p>\n<p>Pulling the trigger of the 6DoF Controller, activates the environmental awareness system \u2018virtual cane.\u2019 Measuring the distance between the Controller, and the spatial mesh generated by the magic leap\u2019s world reconstruction system, Haptic feedback increases in intensity as obstacles get closer. Like the use of echo-location in nature, this allows the CleARsight wearer to gain a spatial understanding of their immediate environment, without relying exclusively on Vision. </p>\n<p>Additionally, a spatial audio file is played from the pointer\u2019s position on the environment mesh. Like an Audible Pedestrian Crossing, this sound allows the CleARsight wearer to hear, in real space, their distance from the obstacle.\nOperating concurrently, this synchronized combination of haptics, spatial audio and High-Visibility Holographic outlines serve to supplement the CleARsight wearer\u2019s reliance on Visual Navigation. </p>\n<p>In the home, and other familiar locations, CleARsight allows the recording of contextual \u2018Placed Audio Memos.\u2019 Spatially affixed to their recording location, CleARsight wearers, or their caregivers can drop a message of their choosing, which will automatically playback when approached. \nToday, CleARsight introduces a novel use of Spatial Computing technology, to amplify the wearer\u2019s senses. Tomorrow, improvements to this technology can be built upon with other digital integrations like Object Recognition, Voice Recognition, and IoT Controls to bolster a robust Sightless User Interface. </p>\n<h2>What's Next For the Project</h2>\n<p>We'd like to continue to extend the feature set of this application and further bolster its usability with additional user testing and integrations to further improve its offering to low vision individuals.</p>\n<p>Some examples are:\n<em>Dynamic Object Recognition</em></p>\n<ul>\n<li>Spatial Mapping data persistently analyzed, matching and expanding 3D Object Database for on-demand contextual information and passive Machine Learning Training.</li>\n</ul>\n<p><em>Voice Recognition</em></p>\n<ul>\n<li>Custom voice-triggered commands, Action events (confirm, cancel, etc.), Speech-To-Text parsing and Personal Assistant integration. </li>\n</ul>\n<p><em>IoT, Home &amp; Ecosystem Integration</em></p>\n<ul>\n<li>Open-Source API &amp; SDK for developers to implement in products and services. </li>\n</ul>\n<p><em>Geospatial Positional Synchronization</em></p>\n<ul>\n<li>Landmarks, known paths, obstacles and other context-sensitive annotations. </li>\n</ul>\n<p><em>High-Speed &amp; Hazard Warnings</em> </p>\n<ul>\n<li>Detect high-speed movement alarm for vehicles, cliffed overhangs and environmental obstructions. </li>\n</ul>\n<p>Slide Deck: <a href=\"https://www.slideshare.net/keithbradley1/magic-leap-accessibility-app-for-low-vision\" rel=\"nofollow\">link</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAccording to the National Center for Health Statistics, 26 Million Americans Adults experience significant vision loss, creating challenges with Depth Perception, Low-Light scenarios and proprioception. CleARsight for Magic Leap is a Spatial Computing Accessibility application, to improve the daily lives of individuals with Low Vision. \n\n\n## What it does\n\n\nUsing Magic Leap\u2019s World Mapping, Haptic Feedback, Spatial Audio, and Holographic Visual Overlay, CleARsight illuminates your environment, outlining object edges with High Contrast colours, and highlighting horizontal planes with a vivid pattern. \n\n\nPulling the trigger of the 6DoF Controller, activates the environmental awareness system \u2018virtual cane.\u2019 Measuring the distance between the Controller, and the spatial mesh generated by the magic leap\u2019s world reconstruction system, Haptic feedback increases in intensity as obstacles get closer. Like the use of echo-location in nature, this allows the CleARsight wearer to gain a spatial understanding of their immediate environment, without relying exclusively on Vision. \n\n\nAdditionally, a spatial audio file is played from the pointer\u2019s position on the environment mesh. Like an Audible Pedestrian Crossing, this sound allows the CleARsight wearer to hear, in real space, their distance from the obstacle.\nOperating concurrently, this synchronized combination of haptics, spatial audio and High-Visibility Holographic outlines serve to supplement the CleARsight wearer\u2019s reliance on Visual Navigation. \n\n\nIn the home, and other familiar locations, CleARsight allows the recording of contextual \u2018Placed Audio Memos.\u2019 Spatially affixed to their recording location, CleARsight wearers, or their caregivers can drop a message of their choosing, which will automatically playback when approached. \nToday, CleARsight introduces a novel use of Spatial Computing technology, to amplify the wearer\u2019s senses. Tomorrow, improvements to this technology can be built upon with other digital integrations like Object Recognition, Voice Recognition, and IoT Controls to bolster a robust Sightless User Interface. \n\n\n## What's Next For the Project\n\n\nWe'd like to continue to extend the feature set of this application and further bolster its usability with additional user testing and integrations to further improve its offering to low vision individuals.\n\n\nSome examples are:\n*Dynamic Object Recognition*\n\n\n* Spatial Mapping data persistently analyzed, matching and expanding 3D Object Database for on-demand contextual information and passive Machine Learning Training.\n\n\n*Voice Recognition*\n\n\n* Custom voice-triggered commands, Action events (confirm, cancel, etc.), Speech-To-Text parsing and Personal Assistant integration.\n\n\n*IoT, Home & Ecosystem Integration*\n\n\n* Open-Source API & SDK for developers to implement in products and services.\n\n\n*Geospatial Positional Synchronization*\n\n\n* Landmarks, known paths, obstacles and other context-sensitive annotations.\n\n\n*High-Speed & Hazard Warnings* \n\n\n* Detect high-speed movement alarm for vehicles, cliffed overhangs and environmental obstructions.\n\n\nSlide Deck: [link](https://www.slideshare.net/keithbradley1/magic-leap-accessibility-app-for-low-vision)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/accessiblelocomotionwebxr",
            "title": "AccessibleLocomotionWebXR",
            "blurb": "A-Frame component that enables quadriplegic users to navigate webvr spaces with binary input controls",
            "awards": [
                "Wayfair's Way-more",
                "Best application for accessibility"
            ],
            "videos": [
                "https://www.youtube.com/embed/9wxEeiWeGMs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Roland Dubois",
                    "about": "Concept & Idea, team lead, A-Frame design, code, and implementation, UX and mechanics",
                    "photo": "https://avatars.githubusercontent.com/u/347570?height=180&v=3&width=180"
                },
                {
                    "name": "Selena de Leon",
                    "about": "I worked on the background information, cross-sectional research, and storytelling/presentation of our project. I was unfamiliar with the ways assistive technology is used to operate software and hardware for people with different abilities. I also drafted the DevPost write up.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/515/datas/profile.jpg"
                },
                {
                    "name": "Jan Kalfus",
                    "about": "I developed conceptional sketches, infographics and animated diagrams to illustrate the mechanics and user experience, created original 3D assets. I helped the team to gain a broader understanding on designing accessible spaces based on real life work experience in architecture.",
                    "photo": "https://www.gravatar.com/avatar/866b03076c1b005493046b030a0e107b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Pilar Aranda",
                    "about": "Designed the case study environment for this demo, while learning how to retrieve the Waifair model library and optimize the design and polycount for Web XR. During this hackathon I created a pipeline to quickly and efficiently deploy custom environments to gltf format.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/764/datas/profile.jpg"
                }
            ],
            "built_with": [
                "a-frame",
                "google-drive",
                "javascript"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our project was inspired by the false assumption that there are immersive experiences available for all people, while they are actually only accessible to able-bodied people. The underlying assumption, within the world of immersive technology, is that everyone has a free range of motion. This leads to a lack of integration for assistive technology for individuals with different levels of mobility. </p>\n<p>Controllers demand the use of limbs, hands, and fingers, which is not available to people who are affected by quadriplegia. Our goal is to establish a new standard in the field, which used binary control so that people with different abilities have access to WebXR.</p>\n<h2>What it does</h2>\n<p>Our project uses a binary input function from the analysis of sip-and-puff mechanics -- technology created in the 1960s for individuals who are unable to use their limbs, and therefore, use their mouths to \u201csip\u201d or \u201cpuff\u201d air into a device (usually a straw or tube) that is linked to hardware or software that enables them to carry out day-to-day functions. <a href=\"https://www.youtube.com/watch?v=Bhj5vs9P5cw\" rel=\"nofollow\">https://www.youtube.com/watch?v=Bhj5vs9P5cw</a></p>\n<h2>How we built it</h2>\n<p>We used JavaScript &amp; A-frame to create the VR experience, as well as Maya for reducing poly-count and editing models, Rhino for building free-models to scale, and Google Drive for writing notes and a slide presentation. All models are integrated into A-Frame.</p>\n<h2>Challenges we ran into</h2>\n<p>We were being challenged to learn more efficient ways of executing our tasks and methods of input that consider breathing as the baseline to trigger interaction. We were also learning how to integrate assistive technology independent mechanics of binary input functions in VR. Converting files has been a common theme of challenges we have faced so far. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We explored the possibilities of creating access to VR for people with different physical abilities and introducing a change within the field of VR for the greater good of humanity.\nWe built a functioning first version of an A-Frame component \"binary-controls\" that will be released to the A-Frame registry to coexist with other more common input controls like \"gamepad, keyboard or hand\" controllers\nWe defined three basic timed binary input sequences as sufficient mechanics to navigate and interact within VR: \nClick Event to dispatch (click): on - [0ms - 500ms] - off\nSkip/Tab Event to change focus (long-click): on - [500ms - 1000ms] - off\nToggle Event to change input mode interaction&lt;&gt;locomotion (click&amp;hold): on - [3000ms - 4000ms] - off\n<a href=\"https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing\" rel=\"nofollow\">https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing</a></p>\n<h2>What we learned</h2>\n<p>Navigating in the embodiment of a quadriplegic user gives you a new definition of the time and determination it takes to execute simple tasks that able-bodied users are likely to overlook. The demo makes you more patient and empathetic but also opens up opportunities for social inclusion for quadriplegic users in VR. </p>\n<h2>What's next for AccessibleLocomotionWebXR</h2>\n<p>After a few performance and animation updates, we will be publishing the component on the A-Frame registry. We will try the component with the quadriplegic users in collaboration with EqualEntry hosts of A11YNYC.\nA projection for Accessible Locomotion XR is the AR component\u2026 We are currently creating a model for VR experience. Eventually, we hope that the methods we introduce here are taken into a broad range of XR experiences and creates an inclusive environment for locomotion.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur project was inspired by the false assumption that there are immersive experiences available for all people, while they are actually only accessible to able-bodied people. The underlying assumption, within the world of immersive technology, is that everyone has a free range of motion. This leads to a lack of integration for assistive technology for individuals with different levels of mobility. \n\n\nControllers demand the use of limbs, hands, and fingers, which is not available to people who are affected by quadriplegia. Our goal is to establish a new standard in the field, which used binary control so that people with different abilities have access to WebXR.\n\n\n## What it does\n\n\nOur project uses a binary input function from the analysis of sip-and-puff mechanics -- technology created in the 1960s for individuals who are unable to use their limbs, and therefore, use their mouths to \u201csip\u201d or \u201cpuff\u201d air into a device (usually a straw or tube) that is linked to hardware or software that enables them to carry out day-to-day functions. <https://www.youtube.com/watch?v=Bhj5vs9P5cw>\n\n\n## How we built it\n\n\nWe used JavaScript & A-frame to create the VR experience, as well as Maya for reducing poly-count and editing models, Rhino for building free-models to scale, and Google Drive for writing notes and a slide presentation. All models are integrated into A-Frame.\n\n\n## Challenges we ran into\n\n\nWe were being challenged to learn more efficient ways of executing our tasks and methods of input that consider breathing as the baseline to trigger interaction. We were also learning how to integrate assistive technology independent mechanics of binary input functions in VR. Converting files has been a common theme of challenges we have faced so far. \n\n\n## Accomplishments that we're proud of\n\n\nWe explored the possibilities of creating access to VR for people with different physical abilities and introducing a change within the field of VR for the greater good of humanity.\nWe built a functioning first version of an A-Frame component \"binary-controls\" that will be released to the A-Frame registry to coexist with other more common input controls like \"gamepad, keyboard or hand\" controllers\nWe defined three basic timed binary input sequences as sufficient mechanics to navigate and interact within VR: \nClick Event to dispatch (click): on - [0ms - 500ms] - off\nSkip/Tab Event to change focus (long-click): on - [500ms - 1000ms] - off\nToggle Event to change input mode interaction<>locomotion (click&hold): on - [3000ms - 4000ms] - off\n<https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing>\n\n\n## What we learned\n\n\nNavigating in the embodiment of a quadriplegic user gives you a new definition of the time and determination it takes to execute simple tasks that able-bodied users are likely to overlook. The demo makes you more patient and empathetic but also opens up opportunities for social inclusion for quadriplegic users in VR. \n\n\n## What's next for AccessibleLocomotionWebXR\n\n\nAfter a few performance and animation updates, we will be publishing the component on the A-Frame registry. We will try the component with the quadriplegic users in collaboration with EqualEntry hosts of A11YNYC.\nA projection for Accessible Locomotion XR is the AR component\u2026 We are currently creating a model for VR experience. Eventually, we hope that the methods we introduce here are taken into a broad range of XR experiences and creates an inclusive environment for locomotion.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/sound-space",
            "title": "Sound Space",
            "blurb": "Acoustic Simulation and Visualization - VR Architectural Design to understand acoustic influence on design",
            "awards": [
                "Best use of Autodesk forge cloud platform",
                "Best in Industrial & Commercial"
            ],
            "videos": [
                "https://www.youtube.com/embed/3bQfJdd1CYg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Group Working",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/541/datas/original.jpg"
                },
                {
                    "title": "Group Photo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/687/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Adam Chernick",
                    "about": "I worked on the Forge Web Application. Getting our Revit Model and Data through to Unity/VR. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/500/datas/profile.jpg"
                },
                {
                    "name": "Zeyu Ren",
                    "about": "I worked on the visual effects, animation, and visual development with After Effects and Unity.",
                    "photo": "https://www.gravatar.com/avatar/635a800df15eb8cf17a68baa1d7ebdfb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sabrina Naumovski",
                    "about": "I worked on the user interface in Unity, the 3D model in Revit, and the general graphic design, specifically of the UI.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/332/datas/profile.JPG"
                },
                {
                    "name": "Christopher Morse",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/866/datas/profile.jpg"
                },
                {
                    "name": "Luke Gehron",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4754292?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "forge",
                "unity"
            ],
            "content_html": "<div>\n<h2>OUR IDEA</h2>\n<p>Acoustic Simulation and Visualization - VR Architectural Design to understand acoustic influence on design</p>\n<h2>Inspiration</h2>\n<p>XR technologies enable us to visualize things that are intangible, and we want to take advantage of this opportunity to enhance the process of designing for design elements that are inherently non-visual. These tools can influence design in a new way.</p>\n<h2>What Sound Space Does</h2>\n<p>Provides architects with a visual tool to be more sensitive and aware of the acoustic impacts of their different design options.</p>\n<h2>OUR TEAM</h2>\n<p>TEAM MEMBER 1: Luke Gehron works at Payette (architecture) in Boston where he has previous research work in creating acoustic simulations/visualizations for architecture. For this project he developed the C# script in Unity to control the particle simulation. He has a BArch from  Auburn University.</p>\n<p>TEAM MEMBER 2: Christopher Morse works at SHoP Architects in NYC. Has extensive experience in VR, specifically related to how it integrates into the architectural realm. Key previous VR project includes the Jenny Sabin Studio MoMA PS1 entry. He has a Masters Degree in Architecture from Cornell.</p>\n<p>TEAM MEMBER 3: Sabrina Naumovski is a Senior Research/Design Fellow at TerreformONE in Brooklyn, NY. She received her graduate degree from the Institute for Advanced Architecture of Catalonia (IAAC), and her B.Arch from NJIT.</p>\n<p>TEAM MEMBER 4:  Adam Chernick works at SHoP where he researches and develops for the Architecture, Engineering, and Construction industry and previously at HOK. He received his M.Arch from Pratt, and his bachelors degree from University of Colorado.</p>\n<p>TEAM MEMBER 5: Zeyu Ren is currently a graduate Student at RISD in the Digital Arts program.</p>\n</div>",
            "content_md": "\n## OUR IDEA\n\n\nAcoustic Simulation and Visualization - VR Architectural Design to understand acoustic influence on design\n\n\n## Inspiration\n\n\nXR technologies enable us to visualize things that are intangible, and we want to take advantage of this opportunity to enhance the process of designing for design elements that are inherently non-visual. These tools can influence design in a new way.\n\n\n## What Sound Space Does\n\n\nProvides architects with a visual tool to be more sensitive and aware of the acoustic impacts of their different design options.\n\n\n## OUR TEAM\n\n\nTEAM MEMBER 1: Luke Gehron works at Payette (architecture) in Boston where he has previous research work in creating acoustic simulations/visualizations for architecture. For this project he developed the C# script in Unity to control the particle simulation. He has a BArch from Auburn University.\n\n\nTEAM MEMBER 2: Christopher Morse works at SHoP Architects in NYC. Has extensive experience in VR, specifically related to how it integrates into the architectural realm. Key previous VR project includes the Jenny Sabin Studio MoMA PS1 entry. He has a Masters Degree in Architecture from Cornell.\n\n\nTEAM MEMBER 3: Sabrina Naumovski is a Senior Research/Design Fellow at TerreformONE in Brooklyn, NY. She received her graduate degree from the Institute for Advanced Architecture of Catalonia (IAAC), and her B.Arch from NJIT.\n\n\nTEAM MEMBER 4: Adam Chernick works at SHoP where he researches and develops for the Architecture, Engineering, and Construction industry and previously at HOK. He received his M.Arch from Pratt, and his bachelors degree from University of Colorado.\n\n\nTEAM MEMBER 5: Zeyu Ren is currently a graduate Student at RISD in the Digital Arts program.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/highar-ground",
            "title": "HighAR Ground",
            "blurb": "Augmented Reality disaster relief planning and guidance. ",
            "awards": [
                "Best use of ESRI",
                "Best use of AR in the public Realm",
                "Best in Social Good"
            ],
            "videos": [
                "https://www.youtube.com/embed/mmkc2knNbh0?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Inspiration for the AR component of the mobile app came from Weather Channel broadcasting.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/449/datas/original.PNG"
                },
                {
                    "title": "ESRI powered map of Boston&#39;s flood-prone areas",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/736/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Susan Ip-Jewell MD",
                    "about": "As physician-scientist and exponential technology innovator, and entrepreneur in medicine, healthcare, biomedical fields.As SME offered specialized expertise in fields of disaster relief, search and rescue and medical triage.protocols.and business perspectives",
                    "photo": "https://graph.facebook.com/10156361956751443/picture?height=180&width=180"
                },
                {
                    "name": "Jesse Paterson",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGFZh12LSwPnQ/profile-displayphoto-shrink_100_100/0?e=1554336000&height=180&t=D8PF7oihaq0gmcDEvnCJqfUkpeBoLGyEzRmTE-pOA9g&v=beta&width=180"
                },
                {
                    "name": "oscarec79",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/37260020?height=180&v=4&width=180"
                },
                {
                    "name": "Haiden McGill",
                    "about": "",
                    "photo": "https://graph.facebook.com/v3.3/10208958771884851/picture?height=180&width=180"
                },
                {
                    "name": "Deborah Pereira",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/306/datas/profile.jpg"
                }
            ],
            "built_with": [
                "android",
                "arcore",
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Do you have a personal evacuation plan in place in the case of a natural disaster? If you're with 75% of Americans, or over 90% of this hackathon's participants, chances are you're under-prepared or have no plan in place. </p>\n<p>As we studied data sets and notification systems from a range of government agencies including NOAA, FEMA, and local municipalities, it became clear that the general public is largely apathetic or slow to action when warned of an impending natural disaster. </p>\n<p>Our goal was to counteract this trend by providing a dual-facing AR application system allowing the general public to educate and interact with an evacuation system determined for their area by emergency personnel. In the event of a disaster, this app would act as a real time alert system and guide towards shelter or a secure location. It would also give emergency personnel an easy way to visualize their city and receive accurate location-tagged SOS beacons if a citizen became incapacitated during an event. </p>\n<h2>What it does</h2>\n<p>HighAR Ground is split into two applications: an android mobile app geared towards the general population, and a Hololens application for Emergency Personnel. </p>\n<p>The mobile app utilizes a user's location and sends emergency alerts when a natural disaster event is approaching. It then provides navigation towards a predetermined government shelter or secure location, providing the option to send emergency contacts plain-text messages with a timestamp and coordinate pair. There is also an SOS option that will immediately notify local law enforcement/emergency personnel with your location should you become incapacitated during the evacuation. </p>\n<p>The Hololens app utilizes an ESRI map of the city of Boston, pulling geographic coordinates and topographical readings to determine where storm surge warnings have been deployed. Once an SOS beacon is received, it illuminates on the map at its specific geographic coordinate, including a three-level triage system that allows emergency personnel to rate the severity of the case (green, yellow, red).</p>\n<h2>How we built it</h2>\n<p>The team brought in ESRI data of the nearby Boston harbor, showing buildings and the bay. Using Unity and the Hololens SDK, we created an AR experience to be used by emergency response personnel. The mobile app was wireframed using Sketch. Iconography pulled from icons8.com and Sketch App resources.</p>\n<h2>Challenges we ran into</h2>\n<p>No development occurred on day 1, as we struggled to get past Hololens + unity connectivity issues.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We had a powerhouse team. Incredibly proud of the time our team spent on day 1 to define our problems, solutions, and scope, from a user-centered design perspective. Team member Debby led the discussion, providing space for all team members to voice their opinions, iterate on ideas, as well as user experience and visual design of mobile app. Our developers worked around the clock and prioritized swiftly. We were extremely lucky to have Susan provide a strong industry perspective and direction around search, rescue, and triage. Helena provided invaluable sound engineering support, as well as a strong collaborator around UX.</p>\n<h2>What we learned</h2>\n<p>Hololens development.</p>\n<h2>What's next for HighAR Ground</h2>\n<ol>\n<li><p>Ability to conduct rapid prototyping and R&amp;D of \u201cHighAR Ground\u201d technology with Mars Academy USA Analog Astronaut Simulation Training Program (monthly missions). This allows for iterations, optimization and future expansion of more Added Value to increase a wider target market sectors, eg, general public, commercial, government, medical, academic.</p></li>\n<li><p>Conduct a validation study of the technology in a longitudinal project </p></li>\n<li><p>Develop a user case study to analyze challenges, pros/cons, benefits, disadvantage/advantages for integrating into the current markets, ie, overcome adoption issues, education training, costs/pricing, etc\n    a. Conduct in-depth user research and usability studies for both apps</p></li>\n<li><p>Entrepreneurial Plans:\n-Formation of a startup LLC company \n-Pitch to investors</p></li>\n</ol>\n</div>",
            "content_md": "\n## Inspiration\n\n\nDo you have a personal evacuation plan in place in the case of a natural disaster? If you're with 75% of Americans, or over 90% of this hackathon's participants, chances are you're under-prepared or have no plan in place. \n\n\nAs we studied data sets and notification systems from a range of government agencies including NOAA, FEMA, and local municipalities, it became clear that the general public is largely apathetic or slow to action when warned of an impending natural disaster. \n\n\nOur goal was to counteract this trend by providing a dual-facing AR application system allowing the general public to educate and interact with an evacuation system determined for their area by emergency personnel. In the event of a disaster, this app would act as a real time alert system and guide towards shelter or a secure location. It would also give emergency personnel an easy way to visualize their city and receive accurate location-tagged SOS beacons if a citizen became incapacitated during an event. \n\n\n## What it does\n\n\nHighAR Ground is split into two applications: an android mobile app geared towards the general population, and a Hololens application for Emergency Personnel. \n\n\nThe mobile app utilizes a user's location and sends emergency alerts when a natural disaster event is approaching. It then provides navigation towards a predetermined government shelter or secure location, providing the option to send emergency contacts plain-text messages with a timestamp and coordinate pair. There is also an SOS option that will immediately notify local law enforcement/emergency personnel with your location should you become incapacitated during the evacuation. \n\n\nThe Hololens app utilizes an ESRI map of the city of Boston, pulling geographic coordinates and topographical readings to determine where storm surge warnings have been deployed. Once an SOS beacon is received, it illuminates on the map at its specific geographic coordinate, including a three-level triage system that allows emergency personnel to rate the severity of the case (green, yellow, red).\n\n\n## How we built it\n\n\nThe team brought in ESRI data of the nearby Boston harbor, showing buildings and the bay. Using Unity and the Hololens SDK, we created an AR experience to be used by emergency response personnel. The mobile app was wireframed using Sketch. Iconography pulled from icons8.com and Sketch App resources.\n\n\n## Challenges we ran into\n\n\nNo development occurred on day 1, as we struggled to get past Hololens + unity connectivity issues.\n\n\n## Accomplishments that we're proud of\n\n\nWe had a powerhouse team. Incredibly proud of the time our team spent on day 1 to define our problems, solutions, and scope, from a user-centered design perspective. Team member Debby led the discussion, providing space for all team members to voice their opinions, iterate on ideas, as well as user experience and visual design of mobile app. Our developers worked around the clock and prioritized swiftly. We were extremely lucky to have Susan provide a strong industry perspective and direction around search, rescue, and triage. Helena provided invaluable sound engineering support, as well as a strong collaborator around UX.\n\n\n## What we learned\n\n\nHololens development.\n\n\n## What's next for HighAR Ground\n\n\n1. Ability to conduct rapid prototyping and R&D of \u201cHighAR Ground\u201d technology with Mars Academy USA Analog Astronaut Simulation Training Program (monthly missions). This allows for iterations, optimization and future expansion of more Added Value to increase a wider target market sectors, eg, general public, commercial, government, medical, academic.\n2. Conduct a validation study of the technology in a longitudinal project\n3. Develop a user case study to analyze challenges, pros/cons, benefits, disadvantage/advantages for integrating into the current markets, ie, overcome adoption issues, education training, costs/pricing, etc\n a. Conduct in-depth user research and usability studies for both apps\n4. Entrepreneurial Plans:\n-Formation of a startup LLC company \n-Pitch to investors\n\n\n"
        },
        {
            "source": "https://devpost.com/software/continuum-dy1av0",
            "title": "Continuum",
            "blurb": "Stories you move through",
            "awards": [
                "Best use of ESRI"
            ],
            "videos": [
                "https://player.vimeo.com/video/312304542?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "ADRIANA GUIMAN",
                    "about": "logo animation, presentation video, worked collaboratively on the UI/UX design",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/683/957/datas/profile.jpg"
                },
                {
                    "name": "James Gong",
                    "about": "I worked on the backend technology that utilized the ESRI geolocation API, and integrated ESRI's AR camera view with ARKit's.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/552/337/datas/profile.jpg"
                },
                {
                    "name": "Tyler Angert",
                    "about": "I built the AR frontend and helped with UI,UX, and interaction design. I also conceived the project concept.",
                    "photo": "https://avatars3.githubusercontent.com/u/11527126?height=180&v=4&width=180"
                },
                {
                    "name": "Mira Sachdeva",
                    "about": "I worked on branding, user flows, user testing, and UI/UX. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/606/datas/profile.jpg"
                },
                {
                    "name": "Jenny Goldstick",
                    "about": "Branding + logo development. User flows, user testing, UI/UX",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/519/datas/profile.jpg"
                }
            ],
            "built_with": [
                "arkit",
                "esri",
                "ios"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our group kept revisiting the idea of memories. We thought about how we create, interact and re-experience them; our app explores this idea. Generally, science fiction that involves time travel was the main source of inspiration, like Donnie Darko. Also shoutout to Zach Lieberman for his amazing work on audio visualizations in AR which inspired this project.</p>\n<h2>What it does</h2>\n<p>Users explore and create stop-motion \"videos\" in the place they were taken. Users can walk forward and backward through these moments in AR.</p>\n<h2>How we built it</h2>\n<p>We started with the idea of stories in space, which gave way to capturing stop-motion photo sequences. We collaboratively worked on the UI/UX design and development in tandem. Specifically, we used paper-prototypes to test out the idea of content floating in space, InVision for low-fidelity prototyping, and Xcode and Swift for the implementation of the mobile app. Development wise, everything was built with Apple's native ARKit modules.</p>\n<h2>Challenges we ran into</h2>\n<p>One of our main struggles was balancing the story and technical aspects of this app. Mentor feedback led us to decide the value of our project was in its technology. The challenge then became how to integrate story while letting technology lead the way. We also had trouble figuring out which features to prioritize and whether to focus purely on one audience.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are especially proud of pulling together an initially amorphous idea into a cohesive experience. We were able to successfully collaborate in a team with three designers / two engineers in a very integrated way where every team member's opinions were included in the final project.</p>\n<h2>What we learned</h2>\n<p>We learned how to find value in creative tools / novel experiences. We thought we would have had to tailor the project to some commercial purpose, but embraced going all in with creating an artistic tool. We also learned how important early user testing is (or how horrible late user testing is!), especially when you start to assume that your project is easy/obvious to use.</p>\n<h2>What's next for Continuum</h2>\n<p>We want to let you save your projects in space (location persistence) so you can access older or other users' stop motion photo sequences in any space. Also adding sound syncing to each photo path and editing (photo/sound) to each path is a top priority.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur group kept revisiting the idea of memories. We thought about how we create, interact and re-experience them; our app explores this idea. Generally, science fiction that involves time travel was the main source of inspiration, like Donnie Darko. Also shoutout to Zach Lieberman for his amazing work on audio visualizations in AR which inspired this project.\n\n\n## What it does\n\n\nUsers explore and create stop-motion \"videos\" in the place they were taken. Users can walk forward and backward through these moments in AR.\n\n\n## How we built it\n\n\nWe started with the idea of stories in space, which gave way to capturing stop-motion photo sequences. We collaboratively worked on the UI/UX design and development in tandem. Specifically, we used paper-prototypes to test out the idea of content floating in space, InVision for low-fidelity prototyping, and Xcode and Swift for the implementation of the mobile app. Development wise, everything was built with Apple's native ARKit modules.\n\n\n## Challenges we ran into\n\n\nOne of our main struggles was balancing the story and technical aspects of this app. Mentor feedback led us to decide the value of our project was in its technology. The challenge then became how to integrate story while letting technology lead the way. We also had trouble figuring out which features to prioritize and whether to focus purely on one audience.\n\n\n## Accomplishments that we're proud of\n\n\nWe are especially proud of pulling together an initially amorphous idea into a cohesive experience. We were able to successfully collaborate in a team with three designers / two engineers in a very integrated way where every team member's opinions were included in the final project.\n\n\n## What we learned\n\n\nWe learned how to find value in creative tools / novel experiences. We thought we would have had to tailor the project to some commercial purpose, but embraced going all in with creating an artistic tool. We also learned how important early user testing is (or how horrible late user testing is!), especially when you start to assume that your project is easy/obvious to use.\n\n\n## What's next for Continuum\n\n\nWe want to let you save your projects in space (location persistence) so you can access older or other users' stop motion photo sequences in any space. Also adding sound syncing to each photo path and editing (photo/sound) to each path is a top priority.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/cosmovr",
            "title": "CosmosVR",
            "blurb": "Explore our cosmos in VR exploiting movement and scale to develop an intuitive understanding of vast structures",
            "awards": [
                "Data Visualization Challenge (Fidelity)"
            ],
            "videos": [
                "https://www.youtube.com/embed/bbMWvdTdhs4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "introduction slide",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/570/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Beste Aydin",
                    "about": "I am an undergraduate student at the University of Michigan who is studying Computer Engineering. I worked on the Unity development and the overall design. While working on the project, I learned a lot more about Unity and developing for VR. ",
                    "photo": "https://www.gravatar.com/avatar/90786c1d3b7fe339a9714bd96e9689e5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "dborgesr",
                    "about": "I'm a computational biologist with deep experience processing and visualizing large biologically related datasets. I was able to aid in identifying the best way to process and flip the data into the right format in order to load into Unity and begin manipulating it. I definitely intend to extend and leverage what we have created to understand the data that I manipulate daily.",
                    "photo": "https://www.gravatar.com/avatar/0ba5664082199d26f8be4e1b2cc000a5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "milchada",
                    "about": "I'm a graduate student in computational cosmology, so I run and analyse simulations like this one for a living. I love communicating science to a general audience, and write for Yale Scientific, Science in the News and AstroBites. ",
                    "photo": "https://avatars3.githubusercontent.com/u/8669608?height=180&v=4&width=180"
                },
                {
                    "name": "paulrus",
                    "about": "I'm a software developer in the self-driving automotive space, I was really happy to work on a completely different type of project than my day to day work! In this project I wrote most of the C# scripting for Unity. I also led the User Story development to drive product definition. I learned a ton about rendering in Unity and hope to take some of that knowledge back to my work to render point-cloud data for Radar and Lidar sensors!",
                    "photo": "https://graph.facebook.com/10157029303841263/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "hdf5",
                "python",
                "steamvr",
                "unity"
            ],
            "content_html": "<div>\n<p><img alt=\"Introduction slide\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/cosmos_intro.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--Na6qHAjN--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/cosmos_intro.png\"/></p>\n<h2>Inspiration</h2>\n<p>When you think big, you might think of our Solar System, or maybe even our galaxy - but our cosmos is far, far more vast than that. Our home galaxy, the Milky Way, hosts hundreds of billions of stars, enough gas to make billions more,   and five times more mass in something mysterious, called Dark Matter. At least trillions - that is millions of millions - of galaxies fill up the visible Universe. Together, they form the structure you see now, called the Cosmic Web.</p>\n<p>Some galaxies, like our Milky Way, live in groups with a few dozen partners. The Milky Way is on a collision course with our nearest neighbour, Andromeda. Don't worry, the collision isn't due for another 4 billion years! But when it happens, it will be spectacular. Groups live in long chains called cosmic filaments, which eventually feed the largest groups, called galaxy clusters. </p>\n<h2>What it does</h2>\n<p>Look around you. Navigate your way through filaments, groups, and individual galaxies. When you find an interesting object, give it a little attention. If you look long enough, you'll get to zoom in and see it in much more detail, and learn more about that type of object. What do you see? How do the stars, gas and dark matter live in relation to each other?</p>\n<h2>The Data</h2>\n<p>Every point you see in this experience is a real data point from the Illustris cosmological simulation. This simulation started with a Universe merely 51 million years old and runs it all the way to the present day, at age 13.7 <em>billion</em> years. Astronomers know that in the early days, the Universe was largely very smooth, just one big hot soup of protons, electrons, light particles, and dark matter. If one part of this soup is a little denser, it attracts other parts to it by gravity and grows even denser over time; regions that were less dense in the beginning grow emptier and emptier. As gas gets denser, some of it forms clumps that eventually form stars, and the Universe starts to light up. Finally, by the present day - which we show - the large scale structure looks like a giant web, called the Cosmic Web. The zoom-in at the end takes you to high-resolution data on a single galaxy.</p>\n<p>This simulation thus shows our best understanding to date of how the Universe evolves on the largest scales. Urmila published her first academic paper on measuring dark matter distribution in large groups of galaxies (galaxy clusters) by looking at the light from the stars in the cluster galaxies, using this very simulation! Dozens of papers have used this simulation to understand what exactly are the predictions of our current model of dark matter and dark energy, and how these predictions compare to observations. Now, anyone can experience this.</p>\n<h2>User Experience</h2>\n<p>We tried to design our experience around the user. We created personas that we wanted to target the experience towards.</p>\n<p><img alt=\"Users\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.003.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--N_cCW0bi--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.003.png\"/></p>\n<p>Then for each user, we mapped out there \"User Journey\" through our application. We tried to imagine what would be important at each stage for each user. </p>\n<p><img alt=\"BobUserStory\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.004.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--aTcicRrk--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.004.png\"/>\n<img alt=\"JessicaUserStory\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.005.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--WOoWCq4A--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.005.png\"/></p>\n<p><img alt=\"AlexUserStory\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.006.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--o3fDxG7b--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.006.png\"/></p>\n<p>Our conclusion from this study is that we wanted to primarily target an audience like \"Bob\" - someone who doesn't have a lot of technical knowledge already but is interested and wants to have an exciting experience! We also considered expanding this in the future to target researchers like Alex as a secondary audience who might use this for further data analysis that isn't possible in traditional mediums.</p>\n<h1>Design</h1>\n<p>Based on our user story conclusion we tried to design an experience that would fit our primary audience. Part of this was designing a fun and \"cool\" heads-up-display that would make the user feel like they're in an astronaut's helmet. Part of this design choice was also to counteract the possibility of a nauseating experience caused by flying in space. We also wanted to implement a system on changing helmets to see different types of information such as gas and stars. These designs were first sketched on paper, which is shown below.</p>\n<p><img alt=\"Mockup\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/Mockups.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--EN-_tQlf--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/Mockups.jpg\"/></p>\n<p>Next, the heads-up-display design was implemented using Inkscape and imported to Unity.</p>\n<p><img alt=\"hudImage\" data-canonical-url=\"https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/hudImage.png\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--GjBmm2-p--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/hudImage.png\"/></p>\n<h2>Implementation</h2>\n<p>We downloaded snapshots from the openly accessible Illustris simulation (<a href=\"http://www.illustris-project.org/data\" rel=\"nofollow\">www.illustris-project.org/data</a>). Like most astrophysical simulations, this was in HDF5 format. We used Python to select the information we wanted to visualize - density of dark matter and gas, the colors of star particles, and positions for everything - and convert it into .xyz files for Meshlab (<a href=\"http://www.meshlab.net/\" rel=\"nofollow\">http://www.meshlab.net/</a>). Meshlab converted these into PLY files, and we found an open-source library to load these into Unity. This gave us our first view of the Universe in 3D!</p>\n<p>We then used the Vive SDK to allow users to fly through the simulation using the controllers. </p>\n<h2>Challenges we ran into</h2>\n<p>Loading the particle data into Unity was a huge challenge! We first considered reading in JSON files and making a ParticleSystem, but couldn't figure out how to do this. The Meshlab workaround really saved the day. </p>\n<p>Rendering the particles was also a challenge. The dataset was so large and we were worried about performance and frame-rate. Luckily we found and utilised a public project that had a custom mesh renderer to visualise a large point cloud (<a href=\"https://blog.sketchfab.com/tutorial-processing-point-cloud-data-unity/\" rel=\"nofollow\">https://blog.sketchfab.com/tutorial-processing-point-cloud-data-unity/</a>). </p>\n<p>We seem to be having a Unity sound driver issue, so that we can't hear any sounds playing from Unity. We have been working with a mentor to debug this, but unfortunately ran out of time. We would like to expand the interactions to support grab-and-move style locomotion, more advanced selection of objects (galaxy groups, clusters, etc.) using both hand and eye controllers, and an in-depth HUD. For this project we developed a simple \"fly-around\" movement system using the Vive hand controller triggers. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We thought a lot about the user experience and pedagogical content. We are really proud that we could show an accurate data-set without compromising performance. </p>\n<p>We are also really proud that our team (all strangers before this weekend) came together and utilised our diverse skillsets to create a unique and engaging experience! We come from a variety of backgrounds - Astronomy, Biology,  UI Research, Design, and Software. </p>\n<h2>What we learned</h2>\n<p>Beste, Diego and Paul learned about extragalactic astronomy (including the term extragalactic astronomy), the large scale structure of the Universe, and how astronomers simulate and observe the various components of the Universe. Urmila learned to work with HDF5 files instead of relying on other people's highly specialized wrappers. We all got a really deep dive into Unity and the Vive SDK, including how to load in and render large datasets, how to move around the space and various ways to trigger actions. We quickly gelled as team members and worked like a well-oiled machine! </p>\n<h2>What's next for CosmoVR</h2>\n<p>We want to add a lot more sound and information to the app to make it a powerful educational tool for consumers at various levels, ranging from curious school kids to scientific researchers. We also want to support more zoom levels and the ability to dynamically change scale without loading new scenes. Since a typical dataset in this space has different time slices we'd also like to visualise how these particles move and change over the course of billions of years. We'd also like to continue implementing our interaction points. </p>\n<p>One future work will be to try and port this experience to a standalone headset and prove out the feasibility on a reduced performance hardware platform (like the Oculus Go or GearVR). We believe even a simplified port to a standalone headset could increase adoption in museums and schools who don't want to spend excess resources to drive an HTC Vive.</p>\n<h2>Resources Used</h2>\n<p><strong>Unity Assets</strong></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/environments/sci-fi/sci-fi-platforms-3d-models-83091\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/environments/sci-fi/sci-fi-platforms-3d-models-83091</a> (not used in final project)\n<a href=\"https://assetstore.unity.com/packages/2d/textures-materials/sky/free-hdr-sky-61217\" rel=\"nofollow\">https://assetstore.unity.com/packages/2d/textures-materials/sky/free-hdr-sky-61217</a> (not used in final project)\n<a href=\"https://assetstore.unity.com/packages/2d/textures-materials/sky/urban-night-sky-134468\" rel=\"nofollow\">https://assetstore.unity.com/packages/2d/textures-materials/sky/urban-night-sky-134468</a>\n<a href=\"https://nasa3d.arc.nasa.gov/detail/helmet\" rel=\"nofollow\">https://nasa3d.arc.nasa.gov/detail/helmet</a></p>\n<p><strong>Unity Libraries and Projects</strong></p>\n<p><a href=\"https://github.com/leon196/PointCloudExporter\" rel=\"nofollow\">https://github.com/leon196/PointCloudExporter</a></p>\n<p><a href=\"http://wiki.unity3d.com/index.php/SimpleJSON\" rel=\"nofollow\">http://wiki.unity3d.com/index.php/SimpleJSON</a> (not used in final project)</p>\n<p><strong>Other Tools</strong></p>\n<p>Meshlab: <a href=\"http://www.meshlab.net/\" rel=\"nofollow\">http://www.meshlab.net/</a></p>\n<p>Illustris: <a href=\"http://www.illustris-project.org/data\" rel=\"nofollow\">www.illustris-project.org/data</a></p>\n<p>Sketch: <a href=\"https://www.sketchapp.com/\" rel=\"nofollow\">https://www.sketchapp.com/</a></p>\n<p>Keynote: <a href=\"https://www.apple.com/keynote/\" rel=\"nofollow\">https://www.apple.com/keynote/</a></p>\n<p>Inkscape: <a href=\"https://inkscape.org/\" rel=\"nofollow\">https://inkscape.org/</a></p>\n</div>",
            "content_md": "\n![Introduction slide](https://res.cloudinary.com/devpost/image/fetch/s--Na6qHAjN--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/cosmos_intro.png)\n\n\n## Inspiration\n\n\nWhen you think big, you might think of our Solar System, or maybe even our galaxy - but our cosmos is far, far more vast than that. Our home galaxy, the Milky Way, hosts hundreds of billions of stars, enough gas to make billions more, and five times more mass in something mysterious, called Dark Matter. At least trillions - that is millions of millions - of galaxies fill up the visible Universe. Together, they form the structure you see now, called the Cosmic Web.\n\n\nSome galaxies, like our Milky Way, live in groups with a few dozen partners. The Milky Way is on a collision course with our nearest neighbour, Andromeda. Don't worry, the collision isn't due for another 4 billion years! But when it happens, it will be spectacular. Groups live in long chains called cosmic filaments, which eventually feed the largest groups, called galaxy clusters. \n\n\n## What it does\n\n\nLook around you. Navigate your way through filaments, groups, and individual galaxies. When you find an interesting object, give it a little attention. If you look long enough, you'll get to zoom in and see it in much more detail, and learn more about that type of object. What do you see? How do the stars, gas and dark matter live in relation to each other?\n\n\n## The Data\n\n\nEvery point you see in this experience is a real data point from the Illustris cosmological simulation. This simulation started with a Universe merely 51 million years old and runs it all the way to the present day, at age 13.7 *billion* years. Astronomers know that in the early days, the Universe was largely very smooth, just one big hot soup of protons, electrons, light particles, and dark matter. If one part of this soup is a little denser, it attracts other parts to it by gravity and grows even denser over time; regions that were less dense in the beginning grow emptier and emptier. As gas gets denser, some of it forms clumps that eventually form stars, and the Universe starts to light up. Finally, by the present day - which we show - the large scale structure looks like a giant web, called the Cosmic Web. The zoom-in at the end takes you to high-resolution data on a single galaxy.\n\n\nThis simulation thus shows our best understanding to date of how the Universe evolves on the largest scales. Urmila published her first academic paper on measuring dark matter distribution in large groups of galaxies (galaxy clusters) by looking at the light from the stars in the cluster galaxies, using this very simulation! Dozens of papers have used this simulation to understand what exactly are the predictions of our current model of dark matter and dark energy, and how these predictions compare to observations. Now, anyone can experience this.\n\n\n## User Experience\n\n\nWe tried to design our experience around the user. We created personas that we wanted to target the experience towards.\n\n\n![Users](https://res.cloudinary.com/devpost/image/fetch/s--N_cCW0bi--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.003.png)\n\n\nThen for each user, we mapped out there \"User Journey\" through our application. We tried to imagine what would be important at each stage for each user. \n\n\n![BobUserStory](https://res.cloudinary.com/devpost/image/fetch/s--aTcicRrk--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.004.png)\n![JessicaUserStory](https://res.cloudinary.com/devpost/image/fetch/s--WOoWCq4A--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.005.png)\n\n\n![AlexUserStory](https://res.cloudinary.com/devpost/image/fetch/s--o3fDxG7b--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/UserJourneys/UserJourneys.006.png)\n\n\nOur conclusion from this study is that we wanted to primarily target an audience like \"Bob\" - someone who doesn't have a lot of technical knowledge already but is interested and wants to have an exciting experience! We also considered expanding this in the future to target researchers like Alex as a secondary audience who might use this for further data analysis that isn't possible in traditional mediums.\n\n\n# Design\n\n\nBased on our user story conclusion we tried to design an experience that would fit our primary audience. Part of this was designing a fun and \"cool\" heads-up-display that would make the user feel like they're in an astronaut's helmet. Part of this design choice was also to counteract the possibility of a nauseating experience caused by flying in space. We also wanted to implement a system on changing helmets to see different types of information such as gas and stars. These designs were first sketched on paper, which is shown below.\n\n\n![Mockup](https://res.cloudinary.com/devpost/image/fetch/s--EN-_tQlf--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/Mockups.jpg)\n\n\nNext, the heads-up-display design was implemented using Inkscape and imported to Unity.\n\n\n![hudImage](https://res.cloudinary.com/devpost/image/fetch/s--GjBmm2-p--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://raw.githubusercontent.com/RealityVirtually2019/CosmosVR/master/Mockups/hudImage.png)\n\n\n## Implementation\n\n\nWe downloaded snapshots from the openly accessible Illustris simulation ([www.illustris-project.org/data](http://www.illustris-project.org/data)). Like most astrophysical simulations, this was in HDF5 format. We used Python to select the information we wanted to visualize - density of dark matter and gas, the colors of star particles, and positions for everything - and convert it into .xyz files for Meshlab (<http://www.meshlab.net/>). Meshlab converted these into PLY files, and we found an open-source library to load these into Unity. This gave us our first view of the Universe in 3D!\n\n\nWe then used the Vive SDK to allow users to fly through the simulation using the controllers. \n\n\n## Challenges we ran into\n\n\nLoading the particle data into Unity was a huge challenge! We first considered reading in JSON files and making a ParticleSystem, but couldn't figure out how to do this. The Meshlab workaround really saved the day. \n\n\nRendering the particles was also a challenge. The dataset was so large and we were worried about performance and frame-rate. Luckily we found and utilised a public project that had a custom mesh renderer to visualise a large point cloud (<https://blog.sketchfab.com/tutorial-processing-point-cloud-data-unity/>). \n\n\nWe seem to be having a Unity sound driver issue, so that we can't hear any sounds playing from Unity. We have been working with a mentor to debug this, but unfortunately ran out of time. We would like to expand the interactions to support grab-and-move style locomotion, more advanced selection of objects (galaxy groups, clusters, etc.) using both hand and eye controllers, and an in-depth HUD. For this project we developed a simple \"fly-around\" movement system using the Vive hand controller triggers. \n\n\n## Accomplishments that we're proud of\n\n\nWe thought a lot about the user experience and pedagogical content. We are really proud that we could show an accurate data-set without compromising performance. \n\n\nWe are also really proud that our team (all strangers before this weekend) came together and utilised our diverse skillsets to create a unique and engaging experience! We come from a variety of backgrounds - Astronomy, Biology, UI Research, Design, and Software. \n\n\n## What we learned\n\n\nBeste, Diego and Paul learned about extragalactic astronomy (including the term extragalactic astronomy), the large scale structure of the Universe, and how astronomers simulate and observe the various components of the Universe. Urmila learned to work with HDF5 files instead of relying on other people's highly specialized wrappers. We all got a really deep dive into Unity and the Vive SDK, including how to load in and render large datasets, how to move around the space and various ways to trigger actions. We quickly gelled as team members and worked like a well-oiled machine! \n\n\n## What's next for CosmoVR\n\n\nWe want to add a lot more sound and information to the app to make it a powerful educational tool for consumers at various levels, ranging from curious school kids to scientific researchers. We also want to support more zoom levels and the ability to dynamically change scale without loading new scenes. Since a typical dataset in this space has different time slices we'd also like to visualise how these particles move and change over the course of billions of years. We'd also like to continue implementing our interaction points. \n\n\nOne future work will be to try and port this experience to a standalone headset and prove out the feasibility on a reduced performance hardware platform (like the Oculus Go or GearVR). We believe even a simplified port to a standalone headset could increase adoption in museums and schools who don't want to spend excess resources to drive an HTC Vive.\n\n\n## Resources Used\n\n\n**Unity Assets**\n\n\n<https://assetstore.unity.com/packages/3d/environments/sci-fi/sci-fi-platforms-3d-models-83091> (not used in final project)\n<https://assetstore.unity.com/packages/2d/textures-materials/sky/free-hdr-sky-61217> (not used in final project)\n<https://assetstore.unity.com/packages/2d/textures-materials/sky/urban-night-sky-134468>\n<https://nasa3d.arc.nasa.gov/detail/helmet>\n\n\n**Unity Libraries and Projects**\n\n\n<https://github.com/leon196/PointCloudExporter>\n\n\n<http://wiki.unity3d.com/index.php/SimpleJSON> (not used in final project)\n\n\n**Other Tools**\n\n\nMeshlab: <http://www.meshlab.net/>\n\n\nIllustris: [www.illustris-project.org/data](http://www.illustris-project.org/data)\n\n\nSketch: <https://www.sketchapp.com/>\n\n\nKeynote: <https://www.apple.com/keynote/>\n\n\nInkscape: <https://inkscape.org/>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/hear",
            "title": "heAR",
            "blurb": "heAR: An Augmented Reality Visualizer for the Hearing Impaired",
            "awards": [
                "Best in Mobility & Communications"
            ],
            "videos": [
                "https://www.youtube.com/embed/sfkNcNDVAXs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Joanna Liu",
                    "about": "Worked on Magic Leap + Unity set up and integration, wireframing, product/user flow definition, visual assets, presentation and video. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/936/177/datas/profile.jpeg"
                },
                {
                    "name": "Richard Gao",
                    "about": "Worked on syncing Magic Leap controls with the user experience, as well as building the raycast system to interact with the environment. Handled speech bubble placement at end-point of raycast, and populating the bubbles with text.",
                    "photo": "https://avatars1.githubusercontent.com/u/26847285?height=180&v=4&width=180"
                },
                {
                    "name": "Mustafa Eyceoz",
                    "about": "Worked on basic Magic Leap set up, integration, and development, as well as various event triggers, actions, scripts, and objects for the project through Unity. Had basic game dev experience with Unity before and decently familiar with C#, but had to start from scratch with AR/VR development.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/234/datas/profile.png"
                },
                {
                    "name": "Ziyan  Ma",
                    "about": "",
                    "photo": "https://res.cloudinary.com/devpost/image/upload/b_transparent,c_pad,g_center,h_150,w_150/v1484894676/rkwvqwamskl6szbscnp0.png?height=180&width=180"
                },
                {
                    "name": "Devanshi Udeshi",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/46818550?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "flask",
                "gcloud",
                "ibm-watson",
                "keras",
                "magic-leap",
                "photoshop",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration:</h2>\n<p>heAR was born out of our team's desire to create an AR application that would positively impact the lives of hearing impaired individuals. </p>\n<h2>What it does:</h2>\n<p>heAR facilitates conversation between a hearing impaired individual and individuals with normal hearing using speech to text translation and American Sign Language to text translation. The hearing impaired individual will wear the Magic Leap 1 headset and point their controller towards the person who is speaking and push down the trigger button on the controller for the device to begin translating and displaying text. A speech bubble with the text will appear next to the person who is speaking, so that the hearing impaired individual may participate in a verbal conversation with others. In addition, messages will be recorded under a message history option.</p>\n<h2>How we built it:</h2>\n<p>We use Magic Leap SDK for Unity and also Lumin SDK to build up the AR experience, including space scanning, object overlaying for our speech bubbles and ray-casting for human recognition. </p>\n<p>We used IBM Watson API and Watson's Unity SDK for speech to text recognition, and a custom CNN for gesture / sign language recognition.</p>\n<p>We use keras and dataset from kaggle to train a VGG network and host it on Google Cloud Services using Flask, which provides Restful API endpoint to communicate with magic leap. </p>\n<p>We use AfterEffect, Illustrator, C4D, and Blender to build custom assets and animation. </p>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>For most of us, it was our first time working on an VR/AR project.</li>\n<li>Cycle of learning then implementing throughout the 2.5 days.</li>\n<li>Networking in Unity is very painful and challenging.</li>\n<li>Magic Leap SDK setup and installation.</li>\n<li>Magic Leap device integration.</li>\n<li>Integrating custom AI solution in AR project.</li>\n<li>Having to adapt to new technology that we had never seen/used before.</li>\n</ul>\n<h2>Accomplishments that we're proud of:</h2>\n<ul>\n<li>We were able to produce a fully functional prototype in two and a half days and integrate it with the Magic Leap system. </li>\n<li>We had a great team dynamic and worked well to balance out our strengths and weaknesses. </li>\n<li>Most importantly, we had the opportunity to learn from each other and the mentors to take away some valuable skills from the Reality Virtually Hackathon. </li>\n</ul>\n<h2>What we learned:</h2>\n<ul>\n<li>For most of us, it was our first time working on an VR/AR project, so we worked together to build a working MVP.</li>\n<li>A lot of technical skills including Unity, Magic Leap, IBM Watson, GCP and Neural Network. </li>\n</ul>\n<h2>What's next for heAR:</h2>\n<ul>\n<li>We aim to implement facial recognition into the system to detect when a human face appears in the frame.</li>\n<li>Figure out and implement audio location automatically (auto-place bubbles).</li>\n<li>Make great improvements to the UI/aesthetic for the most pleasant/non-intrusive experience possible.</li>\n<li>Continue working to build our MVP into a product.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration:\n\n\nheAR was born out of our team's desire to create an AR application that would positively impact the lives of hearing impaired individuals. \n\n\n## What it does:\n\n\nheAR facilitates conversation between a hearing impaired individual and individuals with normal hearing using speech to text translation and American Sign Language to text translation. The hearing impaired individual will wear the Magic Leap 1 headset and point their controller towards the person who is speaking and push down the trigger button on the controller for the device to begin translating and displaying text. A speech bubble with the text will appear next to the person who is speaking, so that the hearing impaired individual may participate in a verbal conversation with others. In addition, messages will be recorded under a message history option.\n\n\n## How we built it:\n\n\nWe use Magic Leap SDK for Unity and also Lumin SDK to build up the AR experience, including space scanning, object overlaying for our speech bubbles and ray-casting for human recognition. \n\n\nWe used IBM Watson API and Watson's Unity SDK for speech to text recognition, and a custom CNN for gesture / sign language recognition.\n\n\nWe use keras and dataset from kaggle to train a VGG network and host it on Google Cloud Services using Flask, which provides Restful API endpoint to communicate with magic leap. \n\n\nWe use AfterEffect, Illustrator, C4D, and Blender to build custom assets and animation. \n\n\n## Challenges we ran into\n\n\n* For most of us, it was our first time working on an VR/AR project.\n* Cycle of learning then implementing throughout the 2.5 days.\n* Networking in Unity is very painful and challenging.\n* Magic Leap SDK setup and installation.\n* Magic Leap device integration.\n* Integrating custom AI solution in AR project.\n* Having to adapt to new technology that we had never seen/used before.\n\n\n## Accomplishments that we're proud of:\n\n\n* We were able to produce a fully functional prototype in two and a half days and integrate it with the Magic Leap system.\n* We had a great team dynamic and worked well to balance out our strengths and weaknesses.\n* Most importantly, we had the opportunity to learn from each other and the mentors to take away some valuable skills from the Reality Virtually Hackathon.\n\n\n## What we learned:\n\n\n* For most of us, it was our first time working on an VR/AR project, so we worked together to build a working MVP.\n* A lot of technical skills including Unity, Magic Leap, IBM Watson, GCP and Neural Network.\n\n\n## What's next for heAR:\n\n\n* We aim to implement facial recognition into the system to detect when a human face appears in the frame.\n* Figure out and implement audio location automatically (auto-place bubbles).\n* Make great improvements to the UI/aesthetic for the most pleasant/non-intrusive experience possible.\n* Continue working to build our MVP into a product.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bright",
            "title": "Bright",
            "blurb": "An augmented reality solution to aid the elderly with visual impairment.",
            "awards": [
                "Best in Health & Wellness",
                "Best AR"
            ],
            "videos": [
                "https://www.youtube.com/embed/nH7JEJCwsdw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ashish Bakshi",
                    "about": "I originated the idea, recruited team members, and led the team in the development of the prototype and pitch presentation.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/741/datas/profile.jpg"
                },
                {
                    "name": "Charlene Yu",
                    "about": "I worked on the logo design and branding, as well as  video editing and sound editing. I collaborated with others on problems solving to create the product.",
                    "photo": "https://graph.facebook.com/1115764118575572/picture?height=180&width=180"
                },
                {
                    "name": "Andreas Dias",
                    "about": "I helped develop the zoom and facial recognition software.  I also woked together to problem solve a lot of the features and creation of the product.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/771/datas/profile.jpg"
                },
                {
                    "name": "Cesar de Castro",
                    "about": "I facilitated the problem-solving process, provided brand strategy, creative direction, hands-on ux/ui and communication design to contribute shaping our award-winning AR project, Bright.  ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/635/datas/profile.jpg"
                },
                {
                    "name": "Jan Simson",
                    "about": "I worked on general Unity troubleshooting, text recognition (OCR), zoom and the emergency call feature. In collaboration and dialog with the others I helped finding and solving problems during the design and creation of Bright.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/752/datas/profile.jpg"
                }
            ],
            "built_with": [
                "azure",
                "c#",
                "microsoft-hololens",
                "unity",
                "uwp"
            ],
            "content_html": "<div>\n<h2>Description</h2>\n<p>According to the WHO, over 250 million people around the world suffer from moderate to severe vision loss, 81% of whom are above the age of 50. Many face significant challenges in daily life. Some of leading solutions for a variety of vision impairments are extremely expensive headsets that essentially provide a zoomed-in view of the world.</p>\n<p>Bright is a seed to make faces familiar again and everyday challenges manageable.</p>\n<p>Bright runs as an app on Microsoft HoloLens and allows users to zoom in to a customizable level, hear written text (e.g. books, newspapers, documents, TV) as spoken speech, recognize other people nearby (giving names for stored people, and age/gender/emotion estimates for others), and contact others in the case of an emergency.</p>\n<p>We built our solution in Unity, implementing various services from Microsoft\u2019s Azure Cognitive Services and Twilio for phone calls.</p>\n<h2>Team Members</h2>\n<p>Ashish Bakshi, Cesar de Castro, Andreas Dias, Jan Simson, Charlene Yu</p>\n<h2>Location</h2>\n<p>We worked on the 6th floor of the MIT Media Lab (Building E14), in the Multipurpose Room, Table 30.</p>\n<h2>Development</h2>\n<p>We used:</p>\n<p>Platform: Microsoft Hololens / Universal Windows Platform\nDevelopment Tools: Microsoft Visual Studio 2017, Unity, Github\nSDKs: Windows SDK\nAPIs: Microsoft Azure Cognitive Services, Twilio\nComponents used not created at hackathon: Mixed Reality Toolkit, Wontonst \nTwilio SMS on Unity (<a href=\"https://github.com/wontonst/twilio-sms-unity\" rel=\"nofollow\">https://github.com/wontonst/twilio-sms-unity</a>).\nAssets: None</p>\n</div>",
            "content_md": "\n## Description\n\n\nAccording to the WHO, over 250 million people around the world suffer from moderate to severe vision loss, 81% of whom are above the age of 50. Many face significant challenges in daily life. Some of leading solutions for a variety of vision impairments are extremely expensive headsets that essentially provide a zoomed-in view of the world.\n\n\nBright is a seed to make faces familiar again and everyday challenges manageable.\n\n\nBright runs as an app on Microsoft HoloLens and allows users to zoom in to a customizable level, hear written text (e.g. books, newspapers, documents, TV) as spoken speech, recognize other people nearby (giving names for stored people, and age/gender/emotion estimates for others), and contact others in the case of an emergency.\n\n\nWe built our solution in Unity, implementing various services from Microsoft\u2019s Azure Cognitive Services and Twilio for phone calls.\n\n\n## Team Members\n\n\nAshish Bakshi, Cesar de Castro, Andreas Dias, Jan Simson, Charlene Yu\n\n\n## Location\n\n\nWe worked on the 6th floor of the MIT Media Lab (Building E14), in the Multipurpose Room, Table 30.\n\n\n## Development\n\n\nWe used:\n\n\nPlatform: Microsoft Hololens / Universal Windows Platform\nDevelopment Tools: Microsoft Visual Studio 2017, Unity, Github\nSDKs: Windows SDK\nAPIs: Microsoft Azure Cognitive Services, Twilio\nComponents used not created at hackathon: Mixed Reality Toolkit, Wontonst \nTwilio SMS on Unity (<https://github.com/wontonst/twilio-sms-unity>).\nAssets: None\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bird-box",
            "title": "Bird Box",
            "blurb": "A multiparticipant AR-VR painting experience in which the work is experienced differently from each point of view.",
            "awards": [
                "Best in Art, Media & Entertainment"
            ],
            "videos": [
                "https://www.youtube.com/embed/zYRyFQH7IDk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "VR Soundcage",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/261/datas/original.png"
                },
                {
                    "title": "Cross-Dimensional Experience",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/262/datas/original.png"
                },
                {
                    "title": "Sending signals from the real world.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/304/datas/original.jpg"
                },
                {
                    "title": "VR mirroring AR brush strokes",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/300/datas/original.jpg"
                },
                {
                    "title": "Enter the Bird Box",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/298/datas/original.jpg"
                },
                {
                    "title": "Two participants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/299/datas/original.jpg"
                },
                {
                    "title": "Sending messages from another dimension",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/302/datas/original.jpg"
                },
                {
                    "title": "VR world real-time scene",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/303/datas/original.jpg"
                },
                {
                    "title": "AR window into the VR world",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/301/datas/original.jpg"
                },
                {
                    "title": "VR world paint strokes",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/305/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Emily Shoemaker",
                    "about": "I worked primarily on the experience design and text work for this project. I also helped create, find, and organize assets. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/343/datas/profile.PNG"
                },
                {
                    "name": "David Tames",
                    "about": "I contributed to the design and worked primarily on the VR implementation using Unity, VIU, and Photon (learning along the way). ",
                    "photo": "https://avatars3.githubusercontent.com/u/24768857?height=180&v=4&width=180"
                },
                {
                    "name": "Alessio Grancini",
                    "about": "I worked on UI interface and sound effects - collider sound-strategy [sound cage]. \nI contributed in setting up the Unity environment for the Photon based multi-player experience using the Microsoft Mixed Reality headset. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/615/072/datas/profile.jpg"
                },
                {
                    "name": "Kelon Cen",
                    "about": "I worked on the concept design and 3D scene design. I also helped with the narrative aspect of the project.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/938/datas/profile.png"
                },
                {
                    "name": "Runze Zhang",
                    "about": "My main hacking in this project is to infrastructure the network platform of bring AR and VR together. I customize a Photon Unity Network v2(PUN2) template for both BirdBox AR and VR Unity Scenes. My strategy is simply to see if AR and VR can share the same spatial visualization through separate project build while sharing the same PUN2 ID, and it works. So, I find out we can now create multiplayer projects crossing both AR VR platforms, specifically in our projects we utilize Android Phones and Mixed Reality Headset.",
                    "photo": "https://avatars1.githubusercontent.com/u/37249382?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "aftereffects",
                "arcore",
                "arfoundation",
                "googlepixel2",
                "googlepixel3",
                "logicpro",
                "maya",
                "microsoftmixedreality",
                "openvr",
                "photon",
                "photoshop",
                "rhino",
                "unity",
                "viu"
            ],
            "content_html": "<div>\n<h2>Location, Floor, and Room</h2>\n<p>Building E15, Floor 2, Room 283</p>\n<h2>Project Summary</h2>\n<p><strong>Bird Box is a multiparticipant AR/VR experience in which one person paints in a VR world, the other person paints in AR-overlaid reality, and both people see transformed versions of each other's art appearing real-time in their own respective spaces.</strong></p>\n<p>As humans, we each inhabit our own unique reality. We can never experience the rich synaptic context of thoughts, memories, and emotions that define another person's perception of the world. Therefore, even though we often feel or assume that we are communicating our ideas to others with complete clarity, this is almost never true. There will always be transformations and noise in the signal.</p>\n<p>Bird Box is a metaphor for all the layers of richness that are lost in the communication process. The title is a nod to the homonymous Netflix film, in which \u2013 upon viewing the same unknown entity \u2013 some people experience enlightenment while others encounter their worst fears. </p>\n<p>Participants in Bird Box share an audiovisual space in which their experience of each other's actions is largely similar, yet irreconcilably different. The participant in VR is enclosed in a soundcage, creating music with their motions as they paint in the canvas of a surreal virtual world. The participant in AR, who can digitally paint in the real world, can hear the VR participant's music but can never share the act of composing. The paint strokes created by each participant in their respective realities appear as traces in the other participant's world.</p>\n<p>In the course of design, we pivoted the original concept many times. At first, the project involved transforming a room into a spatial instrument in which two people \u2013 one in VR, blind to the environment, and one in AR, unable to trigger the notes \u2013 must work collaboratively in order to create beautiful sounds. We found ourselves repeatedly drawn toward the delicate tension that is created between two people who cannot see eye to eye. Thus, we shifted to focus around the imperfect act of communication as a visually creative, interactive experience.</p>\n<p>The score and concept art were originally composed for this project during this hackathon.</p>\n<h2>Assets Made by Others</h2>\n<p>We pulled a few free 3D models from Google Poly and TurboSquid. A few sound effects for the demo video were pulled from freesound.org. </p>\n<h2>Components Not Created at the Hackathon</h2>\n<p>Other than freemium assets pulled in from the web, every part of this project was created during this hackathon.</p>\n<h2>Contact the Creators</h2>\n<p><strong>Alessio Grancini</strong>\n<a href=\"mailto:alessiograncini@gmail.com\" rel=\"nofollow\">alessiograncini@gmail.com</a>\n@alessiograncini\nalessiograncini.com\ndoorwi.com</p>\n<p><strong>Runze Zhang</strong> \nzr-z.com\n@runze____\n<a href=\"mailto:archzrz@gmail.com\" rel=\"nofollow\">archzrz@gmail.com</a>\ndoorwi.com</p>\n<p><strong>Emily Shoemaker</strong>\n<a href=\"mailto:emily_shoemaker@gse.harvard.edu\" rel=\"nofollow\">emily_shoemaker@gse.harvard.edu</a></p>\n<p><strong>Kelon Cen</strong>\n<a href=\"mailto:keloncen@alum.calarts.edu\" rel=\"nofollow\">keloncen@alum.calarts.edu</a>\ninstagram @kelonc\nvimeo.com/kelon</p>\n<p><strong>David Tames</strong>\n@kinoscopia (Instagram) \n@cinemakinoeye (Twitter)\n<a href=\"mailto:dtames@northeastern.edu\" rel=\"nofollow\">dtames@northeastern.edu</a> (email)</p>\n</div>",
            "content_md": "\n## Location, Floor, and Room\n\n\nBuilding E15, Floor 2, Room 283\n\n\n## Project Summary\n\n\n**Bird Box is a multiparticipant AR/VR experience in which one person paints in a VR world, the other person paints in AR-overlaid reality, and both people see transformed versions of each other's art appearing real-time in their own respective spaces.**\n\n\nAs humans, we each inhabit our own unique reality. We can never experience the rich synaptic context of thoughts, memories, and emotions that define another person's perception of the world. Therefore, even though we often feel or assume that we are communicating our ideas to others with complete clarity, this is almost never true. There will always be transformations and noise in the signal.\n\n\nBird Box is a metaphor for all the layers of richness that are lost in the communication process. The title is a nod to the homonymous Netflix film, in which \u2013 upon viewing the same unknown entity \u2013 some people experience enlightenment while others encounter their worst fears. \n\n\nParticipants in Bird Box share an audiovisual space in which their experience of each other's actions is largely similar, yet irreconcilably different. The participant in VR is enclosed in a soundcage, creating music with their motions as they paint in the canvas of a surreal virtual world. The participant in AR, who can digitally paint in the real world, can hear the VR participant's music but can never share the act of composing. The paint strokes created by each participant in their respective realities appear as traces in the other participant's world.\n\n\nIn the course of design, we pivoted the original concept many times. At first, the project involved transforming a room into a spatial instrument in which two people \u2013 one in VR, blind to the environment, and one in AR, unable to trigger the notes \u2013 must work collaboratively in order to create beautiful sounds. We found ourselves repeatedly drawn toward the delicate tension that is created between two people who cannot see eye to eye. Thus, we shifted to focus around the imperfect act of communication as a visually creative, interactive experience.\n\n\nThe score and concept art were originally composed for this project during this hackathon.\n\n\n## Assets Made by Others\n\n\nWe pulled a few free 3D models from Google Poly and TurboSquid. A few sound effects for the demo video were pulled from freesound.org. \n\n\n## Components Not Created at the Hackathon\n\n\nOther than freemium assets pulled in from the web, every part of this project was created during this hackathon.\n\n\n## Contact the Creators\n\n\n**Alessio Grancini**\n[alessiograncini@gmail.com](mailto:alessiograncini@gmail.com)\n@alessiograncini\nalessiograncini.com\ndoorwi.com\n\n\n**Runze Zhang** \nzr-z.com\n@runze\\_\\_\\_\\_\n[archzrz@gmail.com](mailto:archzrz@gmail.com)\ndoorwi.com\n\n\n**Emily Shoemaker**\n[emily\\_shoemaker@gse.harvard.edu](mailto:emily_shoemaker@gse.harvard.edu)\n\n\n**Kelon Cen**\n[keloncen@alum.calarts.edu](mailto:keloncen@alum.calarts.edu)\ninstagram @kelonc\nvimeo.com/kelon\n\n\n**David Tames**\n@kinoscopia (Instagram) \n@cinemakinoeye (Twitter)\n[dtames@northeastern.edu](mailto:dtames@northeastern.edu) (email)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/together-gi26w9",
            "title": "Together",
            "blurb": "Co-op game between HTC Vive and up to three mobile devices. Solve puzzles Together, and change the world around you.",
            "awards": [
                "Best in Games & Learning"
            ],
            "videos": [
                "https://www.youtube.com/embed/rIk59CEwm0M?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Colin Everton",
                    "about": "Blocked out the main scene, created animations, modeled the shell of the room and other small components, Pre-fabbed almost everything. Github Guru.",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQERlA-vo6TxKg/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=X8jG9obUViQRQ8Gp_PMAAJFi9e6cJdgcGfpXDcQ7yJE&v=beta&width=180"
                },
                {
                    "name": "Jason Glenn",
                    "about": "I was one of the main programmers working on lighting objects opun being hit by a raycast. The objects would start off invisible and while the light was on it, the object slowly came into the scene and now became dynamic. These objects were then used to snap into place as the missing piece of the puzzle that needed to be solved. Being a asymmetrical experience the mobile player was the only one that could trigger the effect while the VR player could then and only then interact with that object.",
                    "photo": "https://www.gravatar.com/avatar/46e735b2e6bdc2794e54b824af04de03?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Anna Shabayev",
                    "about": "I worked on programming the game puzzles. I also worked on design, concept art, and 3D modeling of the drone and room frame.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/353/377/datas/profile.jpg"
                },
                {
                    "name": "Josh Widdicombe",
                    "about": "Worked on Design and level creation. Decorated the interior with collected assets as well as created the landscape and exterior skybox environment. Implemented AR core into project as well as collaborated with general ideation and puzzle design with team.",
                    "photo": "https://www.gravatar.com/avatar/9230b3fb059967ce65a8b1a0ffb50164?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Eric Chan",
                    "about": "Worked on the game mechanic design, I worked on the cross platform multiplayer networking on the application in unity. This networking allows for both mobile and VR users to share the same space at the same time. ",
                    "photo": "https://avatars2.githubusercontent.com/u/8290772?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "ar-foundation",
                "blender",
                "maya",
                "photon",
                "unity",
                "vui"
            ],
            "content_html": "<div>\n<h2>2nd Video</h2>\n<p><a href=\"https://youtu.be/UYBDKsvzO8Q\" rel=\"nofollow\">https://youtu.be/UYBDKsvzO8Q</a></p>\n<h2>Summary</h2>\n<p>Together is 2-4 player co-op game where one player uses a Vive headset and three other's use mobile devices to solve puzzles. The three mobile players assist with identifying puzzles, setting waypoints and finding objects while the Vive player completes the puzzles. Each mobile player emits a certain color light where they are looking at, this light shows objects that only they can see. Once the player identifies these objects, the Vive player is able to interact with them. Asymmetrical puzzle game.</p>\n<h2>Inspiration</h2>\n<p>From our experience, VR tends to be an isolated experience for the user. Not only that, but it tends to be expensive, often requiring consumers to buy a gaming PC as well. When approaching this, we wanted people to still be able to be part of the experience even if they don't have a VR headset, so instead they can join in on their phone.</p>\n<h2>What it does</h2>\n<p>Together is a cooperative asymmetrical cross platform escape the room puzzle game. One player is in room scale VR while three other people are playing on their phones to see the same environment in AR. The VR player is the only one able to complete the puzzles, but the AR players are the only ones that can see the puzzles. So without using speech, they will need to communicate with each other through body language to solve the puzzles. </p>\n<h2>Challenges we ran into</h2>\n<p>None of us have ever worked with cross platform multiplayer before. None of us had ever built games or worked with AR for mobile devices.</p>\n<h2>What's next for Together</h2>\n<p>We are going to continue working on it so that we can launch it!</p>\n</div>",
            "content_md": "\n## 2nd Video\n\n\n<https://youtu.be/UYBDKsvzO8Q>\n\n\n## Summary\n\n\nTogether is 2-4 player co-op game where one player uses a Vive headset and three other's use mobile devices to solve puzzles. The three mobile players assist with identifying puzzles, setting waypoints and finding objects while the Vive player completes the puzzles. Each mobile player emits a certain color light where they are looking at, this light shows objects that only they can see. Once the player identifies these objects, the Vive player is able to interact with them. Asymmetrical puzzle game.\n\n\n## Inspiration\n\n\nFrom our experience, VR tends to be an isolated experience for the user. Not only that, but it tends to be expensive, often requiring consumers to buy a gaming PC as well. When approaching this, we wanted people to still be able to be part of the experience even if they don't have a VR headset, so instead they can join in on their phone.\n\n\n## What it does\n\n\nTogether is a cooperative asymmetrical cross platform escape the room puzzle game. One player is in room scale VR while three other people are playing on their phones to see the same environment in AR. The VR player is the only one able to complete the puzzles, but the AR players are the only ones that can see the puzzles. So without using speech, they will need to communicate with each other through body language to solve the puzzles. \n\n\n## Challenges we ran into\n\n\nNone of us have ever worked with cross platform multiplayer before. None of us had ever built games or worked with AR for mobile devices.\n\n\n## What's next for Together\n\n\nWe are going to continue working on it so that we can launch it!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/iron-man",
            "title": "PresentXR",
            "blurb": "Immersive Presentations",
            "awards": [
                "Best in Productivity"
            ],
            "videos": [
                "https://www.youtube.com/embed/RIuSX3dxuDM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Bob Levy",
                    "about": "team formation, product & use case definition, videos, prototyping",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/744/900/datas/profile.jpg"
                },
                {
                    "name": "Lewis Gardner",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/578/datas/profile.png"
                },
                {
                    "name": "awbriones",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/22118264?height=180&v=4&width=180"
                },
                {
                    "name": "Aaron Moffatt",
                    "about": "",
                    "photo": "https://graph.facebook.com/10218496942434372/picture?height=180&width=180"
                },
                {
                    "name": "Elena Chong",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/2405664?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "leap-motion",
                "microsoft-mr",
                "oculus",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<p>PresentXR is an immersive solution where regular business users create &amp; deliver captivating presentations.  Unlike traditional presenting software, PresentXR boosts audience attention to your message instead of being distracted by their smartphones (which are now occluded from view).</p>\n<h2>Inspiration</h2>\n<ul>\n<li>Storytelling power of immersive media.</li>\n<li>Need for immersive content creation tools useable by business stakeholders.</li>\n<li>Pain at industry conference booth audiences paying more attention to their cellphone (which, in a headset, becomes impossible)</li>\n<li>Movies like Iron Man, Minority Report, Avatar, where spatial media has been used for user interaction surpassing what's possible on flat pieces of plastic</li>\n</ul>\n<h2>What it does</h2>\n<p>1) Composition</p>\n<ul>\n<li>Scenes-as-slides</li>\n<li>Add &amp; manipulate text, image, video, 3D models, sound, map, dataviz .. using Iron Man-style interface about your hand</li>\n</ul>\n<p>2) Presentation</p>\n<ul>\n<li>Paging through immersive scenes-as-slides</li>\n<li>Sample presentation that tells the story of global warming, which the user is asked to use our tool to complete</li>\n</ul>\n<h2>How we built it</h2>\n<ul>\n<li>Discussing pain points</li>\n<li>Exploring interaction models &amp; available resources</li>\n<li>Storyboarding workflows</li>\n<li>Decomposing work breakdown structure &amp; assignments</li>\n<li>Hourly \"stand up\"'s</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li>Incompatible packages</li>\n<li>Learning new SDKs &amp; hardware in finite time</li>\n<li>Collisions between static rigid bodies &amp; kinematics (poorly documented SDK use case)</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<ul>\n<li>Interact in VR using hands</li>\n<li>Easy to use physical interaction with spatial content </li>\n<li>Document model for immersive \"PowerPoint\"</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>Leap Motion SDK</li>\n</ul>\n<h2>What's next for PresentXR?</h2>\n<ul>\n<li>Import existing content e.g. PowerPoint slides, as a starting point for immersive slides one builds around (i.e. meet users where they already have content &amp; ease the transition)</li>\n<li>UI toolkit for uniquely spatial computing (as a basis for this &amp; other projects)</li>\n<li>Speaker training using AR (Method of Loci for memorizing your talk)</li>\n</ul>\n<h2>Team Bios</h2>\n<p><a href=\"http://aaronmoffatt.com/\" rel=\"nofollow\">Aaron Moffatt</a> has worked at the intersection of film, music, and technology for a decade and a half \u2013 as a bioinformatics software engineer, a VR/emerging technology startup founder, and a symphony and solo violinist.</p>\n<p>Lewis Gardner is a full-stack XR software engineer fluent in 15+ languages.</p>\n<p>Elena is a graduate research assistant at the MIT Media Lab. Her current research involves exploring the use of multi-sensory media to create new interactions that blend the digital realm with the physical.</p>\n<p>Aaron Briones is an Experience Designer with a formal education in Human Computer Interaction. He is passionate about creating experiences that people love using.</p>\n<p><a href=\"https://www.linkedin.com/in/boblevy/\" rel=\"nofollow\">Bob Levy</a> is CEO of <a href=\"https://www.immersionanalytics.com\" rel=\"nofollow\">Immersion Analytics</a>. He speaks regularly on XR with talks including MIT Technology Review's EmTech Caribbean, O\u2019Reilly Media\u2019s Strata Data Conference, DataWorld, StartupFest, QuantCon &amp; <a href=\"https://www.youtube.com/watch?v=s6mjACuna4U&amp;t=11s\" rel=\"nofollow\">AR in Action / MIT Media Lab</a>.</p>\n</div>",
            "content_md": "\nPresentXR is an immersive solution where regular business users create & deliver captivating presentations. Unlike traditional presenting software, PresentXR boosts audience attention to your message instead of being distracted by their smartphones (which are now occluded from view).\n\n\n## Inspiration\n\n\n* Storytelling power of immersive media.\n* Need for immersive content creation tools useable by business stakeholders.\n* Pain at industry conference booth audiences paying more attention to their cellphone (which, in a headset, becomes impossible)\n* Movies like Iron Man, Minority Report, Avatar, where spatial media has been used for user interaction surpassing what's possible on flat pieces of plastic\n\n\n## What it does\n\n\n1) Composition\n\n\n* Scenes-as-slides\n* Add & manipulate text, image, video, 3D models, sound, map, dataviz .. using Iron Man-style interface about your hand\n\n\n2) Presentation\n\n\n* Paging through immersive scenes-as-slides\n* Sample presentation that tells the story of global warming, which the user is asked to use our tool to complete\n\n\n## How we built it\n\n\n* Discussing pain points\n* Exploring interaction models & available resources\n* Storyboarding workflows\n* Decomposing work breakdown structure & assignments\n* Hourly \"stand up\"'s\n\n\n## Challenges we ran into\n\n\n* Incompatible packages\n* Learning new SDKs & hardware in finite time\n* Collisions between static rigid bodies & kinematics (poorly documented SDK use case)\n\n\n## Accomplishments that we're proud of\n\n\n* Interact in VR using hands\n* Easy to use physical interaction with spatial content\n* Document model for immersive \"PowerPoint\"\n\n\n## What we learned\n\n\n* Leap Motion SDK\n\n\n## What's next for PresentXR?\n\n\n* Import existing content e.g. PowerPoint slides, as a starting point for immersive slides one builds around (i.e. meet users where they already have content & ease the transition)\n* UI toolkit for uniquely spatial computing (as a basis for this & other projects)\n* Speaker training using AR (Method of Loci for memorizing your talk)\n\n\n## Team Bios\n\n\n[Aaron Moffatt](http://aaronmoffatt.com/) has worked at the intersection of film, music, and technology for a decade and a half \u2013 as a bioinformatics software engineer, a VR/emerging technology startup founder, and a symphony and solo violinist.\n\n\nLewis Gardner is a full-stack XR software engineer fluent in 15+ languages.\n\n\nElena is a graduate research assistant at the MIT Media Lab. Her current research involves exploring the use of multi-sensory media to create new interactions that blend the digital realm with the physical.\n\n\nAaron Briones is an Experience Designer with a formal education in Human Computer Interaction. He is passionate about creating experiences that people love using.\n\n\n[Bob Levy](https://www.linkedin.com/in/boblevy/) is CEO of [Immersion Analytics](https://www.immersionanalytics.com). He speaks regularly on XR with talks including MIT Technology Review's EmTech Caribbean, O\u2019Reilly Media\u2019s Strata Data Conference, DataWorld, StartupFest, QuantCon & [AR in Action / MIT Media Lab](https://www.youtube.com/watch?v=s6mjACuna4U&t=11s).\n\n\n"
        },
        {
            "source": "https://devpost.com/software/garden-cards",
            "title": "gARden Cards",
            "blurb": "Send collaboratively made AR/VR get well soon cards friends",
            "awards": [
                "Best MR"
            ],
            "videos": [
                "https://www.youtube.com/embed/S0gnBWntSqI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Mo Kakwan",
                    "about": "I wrote the model handling and rendering allowing for the particle effects as well as the Augmented Reality portion. Graphics were done in A-frame and Vue.js with augmented reality work done in AR.js.",
                    "photo": "https://avatars0.githubusercontent.com/u/315474?height=180&v=4&width=180"
                },
                {
                    "name": "Jason Morris",
                    "about": "I wrote most of the WebVR code in Vue and A-Frame, which before this hackathon I did not give nearly enough credit. Oh, and I'd never used Vue until Thursday either. It was a slightly more productive weekend than usual. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/548/datas/profile.JPG"
                },
                {
                    "name": "Lydia Jessup",
                    "about": "I helped with the ideation, design, wireframing, storyboarding and made the 3D flower assets using Cinema 4D (my first time doing 3D modeling!).",
                    "photo": "https://avatars3.githubusercontent.com/u/26204298?height=180&v=4&width=180"
                },
                {
                    "name": "Victoria Grant",
                    "about": "I helped with the design and created the print assets for the physical Get-Well Card.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/854/699/datas/profile.jpg"
                },
                {
                    "name": "ooookai",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/9296143?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "a-frame",
                "blender",
                "cinema4d",
                "firebase",
                "javascript",
                "microsoft-mixed-reality",
                "three.js",
                "vue",
                "webar",
                "webvr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>One of our teammates got an email from his boss that a colleague from their office had fallen sick and would be out of the office for awhile in the hospital. People in the office struggled with coordinating to send flowers and cards and general well wishes. We wanted to create a tool that allowed us to make a heart felt message of well wishes and connect us with the recipient through technology.</p>\n<h2>What it does</h2>\n<p>gARden Cards allows users to collaboratively build a VR/AR experience for someone else. You are given an easy to use collaborative web interface that allows you and your friends to plant flowers on a garden planet and write and/or record good wishes for your recipient. The system then presents you with a PDF that you can use to print out a physical card. The card has a simple message and a AR Marker inside of it. When the recipient views the card from a phone or VR headset they will see the custom garden with voice and written messages (activated by gaze) spill open on a virtual landscape. The recipient is encouraged to come back to the garden over a period of time to see the garden grow and bloom in a celebration of their health and wellness.</p>\n<h2>How we built it</h2>\n<p>We used A-Frame in combination with Vue.js to build a platform agnostic VR/AR experience.</p>\n<h2>Challenges we ran into</h2>\n<p>We were super excited to use our Microsoft Mixed Reality but we only had MacBook Pro's with barely any graphical power to use them with. To make sure that we could all develop and be productive we leveraged webVR to ease our development and iteration cycle.</p>\n<h2>Accomplishments that we're proud of</h2>\n<ul>\n<li>We made new friends!</li>\n<li>Our solution is made to help connect people while using cutting edge technology. In a world with an increasing amount of devices that monitor us, it's still a real problem that we don't have emotional awareness of our relationships.</li>\n<li>None of us had used A-Frame before this weekend</li>\n<li>None of us had designed 3D graphic assests before this weekend</li>\n<li>We combined these technologies in a unique way to allow a community or group of friends to make a custom VR/AR experience for someone else</li>\n<li>VR is usually an isolating solo experience and this is an experience that is social and emotional</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li>How to use A-Frame</li>\n<li>How to use Cinema 4D</li>\n<li>How to quickly take a big idea and fit it into a weekend</li>\n<li>How to build a platform agnostic VR/AR experience</li>\n<li>How to do VR/AR development on macbook pros</li>\n</ul>\n<h2>What's next for gARden Cards</h2>\n<ul>\n<li>To make it accessible to all. We made this to share.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOne of our teammates got an email from his boss that a colleague from their office had fallen sick and would be out of the office for awhile in the hospital. People in the office struggled with coordinating to send flowers and cards and general well wishes. We wanted to create a tool that allowed us to make a heart felt message of well wishes and connect us with the recipient through technology.\n\n\n## What it does\n\n\ngARden Cards allows users to collaboratively build a VR/AR experience for someone else. You are given an easy to use collaborative web interface that allows you and your friends to plant flowers on a garden planet and write and/or record good wishes for your recipient. The system then presents you with a PDF that you can use to print out a physical card. The card has a simple message and a AR Marker inside of it. When the recipient views the card from a phone or VR headset they will see the custom garden with voice and written messages (activated by gaze) spill open on a virtual landscape. The recipient is encouraged to come back to the garden over a period of time to see the garden grow and bloom in a celebration of their health and wellness.\n\n\n## How we built it\n\n\nWe used A-Frame in combination with Vue.js to build a platform agnostic VR/AR experience.\n\n\n## Challenges we ran into\n\n\nWe were super excited to use our Microsoft Mixed Reality but we only had MacBook Pro's with barely any graphical power to use them with. To make sure that we could all develop and be productive we leveraged webVR to ease our development and iteration cycle.\n\n\n## Accomplishments that we're proud of\n\n\n* We made new friends!\n* Our solution is made to help connect people while using cutting edge technology. In a world with an increasing amount of devices that monitor us, it's still a real problem that we don't have emotional awareness of our relationships.\n* None of us had used A-Frame before this weekend\n* None of us had designed 3D graphic assests before this weekend\n* We combined these technologies in a unique way to allow a community or group of friends to make a custom VR/AR experience for someone else\n* VR is usually an isolating solo experience and this is an experience that is social and emotional\n\n\n## What we learned\n\n\n* How to use A-Frame\n* How to use Cinema 4D\n* How to quickly take a big idea and fit it into a weekend\n* How to build a platform agnostic VR/AR experience\n* How to do VR/AR development on macbook pros\n\n\n## What's next for gARden Cards\n\n\n* To make it accessible to all. We made this to share.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/move2improve",
            "title": "Move2Improve",
            "blurb": "Allows physical therapists to quantify progress and admin PT to patients simultaneously, w VR reaching app",
            "awards": [
                "Best VR"
            ],
            "videos": [
                "https://www.youtube.com/embed/Unmh7PgRWdQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Roghayeh (Leila) Barmaki",
                    "about": "In Move2Improve, I worked on ideation, design, project presentation, animation, and code development.\nI also had a role of subject matter expert in physical therapy game design.",
                    "photo": "https://avatars3.githubusercontent.com/u/46650664?height=180&v=4&width=180"
                },
                {
                    "name": "Betsy Skrip",
                    "about": "I thought of the name Move2Improve, worked on ideation and design, drew the line drawing of the fish, and in Maya created and animated the 3D model of the fish.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/496/datas/profile.jpeg"
                },
                {
                    "name": "Brian Cohn",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/614/463/datas/profile.png"
                },
                {
                    "name": "Cameron Peirce",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/34655079?height=180&v=4&width=180"
                },
                {
                    "name": "Chris Berry",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/98202cecc2d397e09a7268d48a56f048?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "maya",
                "unity",
                "vive"
            ],
            "content_html": "<div>\n<p>Want to partner with us or get more information? Reach out to us:\n<a href=\"mailto:brian.cohn@usc.edu\" rel=\"nofollow\">brian.cohn@usc.edu</a>, and\n<a href=\"mailto:rlb@udel.edu\" rel=\"nofollow\">rlb@udel.edu</a></p>\n</div>",
            "content_md": "\nWant to partner with us or get more information? Reach out to us:\n[brian.cohn@usc.edu](mailto:brian.cohn@usc.edu), and\n[rlb@udel.edu](mailto:rlb@udel.edu)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/brain-rap",
            "title": "Brain Rap ",
            "blurb": "Brain Rap is Neuro Powered Music - Connect with your audience, Reach your mental flow",
            "awards": [
                "Peoples' Choice"
            ],
            "videos": [
                "https://www.youtube.com/embed/mxHDIGcVyNU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jazzy Harvey",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xZkvOmL6kSnLpDPX4WZte9bLbxwQp7gexoZcwKWhXxb6p_YIpWMnmvN_LSwiS_jHMjMPYdz_9W6_OYj3H1zVdvvhTW6iOY7k21zVsNvhrWE_OYKiY64AH8ZoBtVf7HO_oHdsy7XWTOBogVxkh4pIad?height=180&width=180"
                },
                {
                    "name": "Shivam Patel",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/19332955?height=180&v=4&width=180"
                },
                {
                    "name": "Centiment",
                    "about": "",
                    "photo": "https://graph.facebook.com/1458330554288092/picture?height=180&width=180"
                },
                {
                    "name": "Adam Sauer",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/544/762/datas/profile.jpg"
                },
                {
                    "name": "Jordan Clark",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32147999?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "emotiv",
                "htc-vive",
                "neurable",
                "neurodata",
                "node.js",
                "php",
                "python",
                "unity"
            ],
            "content_html": "<div>\n<h2>Location: Media Lab</h2>\n<h2>Floor: 6th floor</h2>\n<h2>Room: Purple Room</h2>\n<h2>Development Tools: Unity, Python, PHP, Node.js</h2>\n<h2>SDKs:Unity, Neurable</h2>\n<h2>APIs: Centiment API</h2>\n<h2>outsourced assets: n/A</h2>\n<h2>libraries used: numpy, NMe, scikit learm, R, Unity</h2>\n<h2>Pre-existing components: Centiment API</h2>\n<p>library(dplyr) #Data manipulation (also included in the tidyverse package)\nlibrary(tidytext) #Text mining\nlibrary(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)\nlibrary(widyr) #Use for pairwise correlation</p>\n<h1>Visualizations!</h1>\n<p>library(ggplot2) #Visualizations (also included in the tidyverse package)\nlibrary(ggrepel) #<code>geom_label_repel</code>\nlibrary(gridExtra) #<code>grid.arrange()</code> for multi-graphs\nlibrary(knitr) #Create nicely formatted output tables\nlibrary(kableExtra) #Create nicely formatted output tables\nlibrary(formattable) #For the color_tile function\nlibrary(circlize) #Visualizations - chord diagram\nrequire(memery) #memes - images with plots\nlibrary(magick) #memes - images with plots (image_read)\nlibrary(yarrr)  #Pirate plot\nlibrary(radarchart) #Visualizations\nlibrary(igraph) #ngram network diagrams\nlibrary(ggraph) #ngram network diagrams library(sentimentr)\nlibrary(DataCombine)</p>\n<hr/>\n<h2>Inspiration</h2>\n<p>Brain Rap is inspired by the mis-characterization of Black music artists and interactive musical change.</p>\n<h2>What it does</h2>\n<p>Brain Rap uses EEG, emotion, and live performance to sharpen efficacy and performance skills for musicians. Not only does it enhance the creativity of artists through emotional response, but also the awareness of self on stage. By analyzing data captured with Brain Rap, musicians are able to tailor their performance. Brain rap takes out the guess work of what is working and what is not, leaving more time for the artists to focus on lyrics and performance. The only NAI+ (neuro artificial intelligence) marketing artist development tool on the market.</p>\n<h2>How we built it</h2>\n<p>Brain Rap is built using Emotiv SDK, Unity SDK, Touch Designer SDK, Neurable SDK </p>\n<h2>Challenges we ran into</h2>\n<p>Emotiv SDK didn't work properly, switched to neurable SDK</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Getting Neurable to work, connecting Centiment API</p>\n<h2>What we learned</h2>\n<p>That the team is strong.</p>\n<h2>What's next for Brain Rap</h2>\n<p>Make this into a commercial product</p>\n</div>",
            "content_md": "\n## Location: Media Lab\n\n\n## Floor: 6th floor\n\n\n## Room: Purple Room\n\n\n## Development Tools: Unity, Python, PHP, Node.js\n\n\n## SDKs:Unity, Neurable\n\n\n## APIs: Centiment API\n\n\n## outsourced assets: n/A\n\n\n## libraries used: numpy, NMe, scikit learm, R, Unity\n\n\n## Pre-existing components: Centiment API\n\n\nlibrary(dplyr) #Data manipulation (also included in the tidyverse package)\nlibrary(tidytext) #Text mining\nlibrary(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)\nlibrary(widyr) #Use for pairwise correlation\n\n\n# Visualizations!\n\n\nlibrary(ggplot2) #Visualizations (also included in the tidyverse package)\nlibrary(ggrepel) #`geom_label_repel`\nlibrary(gridExtra) #`grid.arrange()` for multi-graphs\nlibrary(knitr) #Create nicely formatted output tables\nlibrary(kableExtra) #Create nicely formatted output tables\nlibrary(formattable) #For the color\\_tile function\nlibrary(circlize) #Visualizations - chord diagram\nrequire(memery) #memes - images with plots\nlibrary(magick) #memes - images with plots (image\\_read)\nlibrary(yarrr) #Pirate plot\nlibrary(radarchart) #Visualizations\nlibrary(igraph) #ngram network diagrams\nlibrary(ggraph) #ngram network diagrams library(sentimentr)\nlibrary(DataCombine)\n\n\n\n\n---\n\n\n## Inspiration\n\n\nBrain Rap is inspired by the mis-characterization of Black music artists and interactive musical change.\n\n\n## What it does\n\n\nBrain Rap uses EEG, emotion, and live performance to sharpen efficacy and performance skills for musicians. Not only does it enhance the creativity of artists through emotional response, but also the awareness of self on stage. By analyzing data captured with Brain Rap, musicians are able to tailor their performance. Brain rap takes out the guess work of what is working and what is not, leaving more time for the artists to focus on lyrics and performance. The only NAI+ (neuro artificial intelligence) marketing artist development tool on the market.\n\n\n## How we built it\n\n\nBrain Rap is built using Emotiv SDK, Unity SDK, Touch Designer SDK, Neurable SDK \n\n\n## Challenges we ran into\n\n\nEmotiv SDK didn't work properly, switched to neurable SDK\n\n\n## Accomplishments that we're proud of\n\n\nGetting Neurable to work, connecting Centiment API\n\n\n## What we learned\n\n\nThat the team is strong.\n\n\n## What's next for Brain Rap\n\n\nMake this into a commercial product\n\n\n"
        },
        {
            "source": "https://devpost.com/software/green-tech-for-all",
            "title": "For the Love of Green Tech",
            "blurb": "Where does your mind go when stimulated by these bigger then life pairings on the theme of Green Tech. Take a look.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "tamar-hi",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/46778644?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "sdk",
                "unity"
            ],
            "content_html": "<div>\n<p>Am intrigued by the difference between being moved and being impressed and see both as having advantages. But in the realm of getting across information, one is underutilized. </p>\n<p>My work is always minimal and I feel one get get across so much with subtlety. Through a series of transitions of paired images that have a resemblance yet are worlds apart, I tell a story of green tech. </p>\n<p>My assets are always original. I think there is a place for getting across information with more warmth and there are certainly many stories that need to be told. </p>\n<p>I worked with Blender in preparation for the files to go to Unity and to Vuforia. My teammate was not available to meet and go over the experience together in the short time since Saturday when we formed our team. The idea of the theme of machines started with him.  </p>\n<p>I'm that much closer to realizing my own design and animation ideas thanks to this experience! \nI'm more sensitive to my strengths and weaknesses and the excitement to morph years of design thinking, animation, and other projects into XR couldn't be higher. </p>\n<p>One area that has long been a part of my work: I'd like to help companies get their messages across with XR - particularly ones with messages that provide services or products that are moving to the public. </p>\n</div>",
            "content_md": "\nAm intrigued by the difference between being moved and being impressed and see both as having advantages. But in the realm of getting across information, one is underutilized. \n\n\nMy work is always minimal and I feel one get get across so much with subtlety. Through a series of transitions of paired images that have a resemblance yet are worlds apart, I tell a story of green tech. \n\n\nMy assets are always original. I think there is a place for getting across information with more warmth and there are certainly many stories that need to be told. \n\n\nI worked with Blender in preparation for the files to go to Unity and to Vuforia. My teammate was not available to meet and go over the experience together in the short time since Saturday when we formed our team. The idea of the theme of machines started with him. \n\n\nI'm that much closer to realizing my own design and animation ideas thanks to this experience! \nI'm more sensitive to my strengths and weaknesses and the excitement to morph years of design thinking, animation, and other projects into XR couldn't be higher. \n\n\nOne area that has long been a part of my work: I'd like to help companies get their messages across with XR - particularly ones with messages that provide services or products that are moving to the public. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/dare2drive",
            "title": "Dare2Drive",
            "blurb": "A VR game simulating the experience of driving under influences to educate users not to do it in real life.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/2CxLnBtgyJA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Zhuohui Li",
                    "about": "I proposed the concept and did all the user research. Key contributor to the ideation process. Created the 3D environment and some game elements in unity. ",
                    "photo": "https://www.gravatar.com/avatar/19d536f234d334c9f3c5ece511c98b61?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Geyao Zhang",
                    "about": "I was the game designer and programmer. I contributed to the game and level design and also in charge of game mechanic programming and 3D modelling. Also helped organizing and time management.",
                    "photo": "https://www.gravatar.com/avatar/c73bd33b9cec76faf43312863418dda3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&height=180&s=180&width=180"
                },
                {
                    "name": "Gregory Osborne",
                    "about": "I did all the sound for this project! This included the underscore, the implementation, and all the sound effects.",
                    "photo": "https://media.licdn.com/dms/image/C5603AQHwk1WSXIOmOg/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=NdT2C-XpHagGJPFsjXUN4H-L3qzFv5K3ck4CYP3wynE&v=beta&width=180"
                },
                {
                    "name": "LINGYU LI",
                    "about": "I am a VR developer and I worked on all the VR interaction and part of the game logic for this project! This includes all the related scripts and assets. My job is to  ensure the quality of the interaction and the game flow.\n",
                    "photo": "https://media.licdn.com/dms/image/C5603AQHnw8CxWL3Igw/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=tNwTi_aTkhhGAO3zTzx_utunMUhqCE9JLqXcYbA7bvk&v=beta&width=180"
                },
                {
                    "name": "Qian Pan",
                    "about": "I am UX/UI designer and I worked on the UI interface elements designing and contributed 3D modeling for level design. I am a contributor to the concept development, user flow study;target user/personal establishment/ overall& detail user experience design",
                    "photo": "https://avatars0.githubusercontent.com/u/45648716?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Driving under the influences of alcohol is one of the main causes of fatal traffic accidents. People are often overconfident about their ability to drive after drinking. They don\u2019t realize how dangerous to drive when their central nervous systems impaired.</p>\n<h2>Statement</h2>\n<p>We wanted to have users virtually experience what it\u2019s like when you drive under the influences of alcohol, marijuana, tiredness, and using mobile devices, in order to show them why they shouldn\u2019t do this in their real lives.</p>\n<p>All of these driving situation are 100% preventable by the drivers. We hope that people will make the appropriate decisions for the safety of themselves and all the others around them on the road before they make the deadly mistake of driving impaired.</p>\n<h2>What it does</h2>\n<p>We designed the whole game as a journey of a senior college student.</p>\n<p>The player is a recently hired employee at a Silicon Valley company, and must get there soon for the first day of work because they lied about their address on their application. However, on the way they stop at their friends house for one last party. Unfortunately our protagonist didn't plan properly, and now they're forced to drive impaired on their way to San Francisco.</p>\n<p>Our driving simulator not only simulates hazards on your way, but adds distractions and simulates the various effects of tiredness, drunkenness, and being high while driving, interfering with your ability to make decisions. Your tiredness requires you to clear your head regularly else your eyes close while you're in the middle of the road. Being drunk removes all the warning sound effects and makes the controls fight you more as you struggle to keep your body and mind responsive. Being high introduces a myriad of distractions that pop up in front of your view, wild thoughts scrolling past your windshield , texts and popups that you must respond to because they block your vision, and a changed color scheme that makes non-relevant objects brighter and shinier while potential hazards are colored grey and harder to notice.</p>\n<p>Unlike some racing games, you are punished for the harm you cause on the environment, ranging from amassed tickets blocking your windshield, a crime meter that knows when you've blown through red lights and run over beloved pets, and the potential to end your life in a crash.</p>\n<p>Most importantly, pulling over for a break is always an option, encouraged to help clear the effects of the impairment, at sacrifice to time but to the benefit of everbody's safety.</p>\n<h2>How we built it</h2>\n<p>Using Unity we first designed a level to drive through, came up with environments, and began working on controls for driving sober, the default experience we would add to for each version of inebriation. We invested time into making the steering wheel animation for your hands seem natural by having your hands snap to the wheel even if you weren't perfectly rotating your hands in a circle. </p>\n<h2>Challenges we ran into</h2>\n<p>In terms of Tech Challenges:</p>\n<p>One issue we had was that not everybody started working in the same version of Unity. 2018.2 and 2018.3 had updates to the prefab functions, which meant that we had to spend a solid chunk of time switching half of our team to an older version of Unity after they'd done a lot of work in the newer ones.</p>\n<p>Our team's technical challenge is mostly about how to make the interactions between the user and the steering wheel feel more natural and intuitive. Since in this hackathon, we are not allowed to use proprietary hardware to support the haptic feedback. What we could exploit is the potential of the Oculus Rift Integration package from the unity asset store to simulate the driver's experience. However, the normal oculus package only shows the basic interaction like picking up objects and throwing them off. Interactions like rotating wheels are off the charts. What's more, there are very limited resources about oculus interactions online. The only useful idea is linked here: (<a href=\"https://forum.unity.com/threads/vr-touch-keeping-hands-rendered-to-a-position-while-rotating-an-object.514530/\" rel=\"nofollow\">https://forum.unity.com/threads/vr-touch-keeping-hands-rendered-to-a-position-while-rotating-an-object.514530/</a>), and those related codes are not open-source. So our main hack is to figure out the way to rotate the steering wheel as we do in real life. After thoroughly scanning and reorganizing the codes of the Oculus package,  I write a series of adapted scripts to implement the single-hand-grip interaction successfully. For now, we are trying to make the two-hands-grip interaction happen and make it by the deadline.</p>\n<p>In terms of Design Challenges:</p>\n<p>We ended up running out of time to complete all the versions of driving impairments, so we chose to focus on one impairment, which was sleepiness, because that involved the least amount of work to get the point across reliably.</p>\n<p>We had to find a delicate balance between the humor of the game, funny music and low poly artstyle with the seriousness of the topic, which involved dangerous behaviours that we didn't want to encourage by making them too fun or rewarding. We wanted to give the option to do the right thing at any time, hence the ability to pull over and take a break to try to wait out the impairments, but we wanted the experience to still involve embodying the poor decision making that would have led to this experience.</p>\n<p>Nevertheless, the experience couldn't be just exhausting, and there had to be a reason to make it to the end in a reasonable amount of time, hence the story that we added to give a little more emotional context to the game and introduce characters you care about protecting.</p>\n<p>We wanted to add a tutorial to introduce those unused to VR to basic concepts before throwing them into a driving simulator, but didn't have the time. Instead we start with narration over sober driving.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are very proud of your ideas. We successfully keep the game both engaging and sensitive to the topic. All of our interactions to indicate the driving impairment are very creative!</p>\n<p>We are also very proud of our steering system, which feels intuitive to use for those who have driven before.</p>\n<h2>What we learned</h2>\n<p>We learned about the importance of starting everybody off using the same versions of their respective programs.</p>\n<p>We learned a lot about designing interactions in VR.</p>\n<p>We learned more about how to balance sound and mixes in VR.</p>\n<h2>What's next for Dare2Drive</h2>\n<p>We hope to finish the other two states of driving impairment to fully complete the original conception of the game. We'd also like to add a tutorial to the beginning to make the experience more accessible to less experienced users of VR. Possibly we could add a story to connect all the different kinds of impairment levels and a conclusion at the end of the game that would take your performance into account more than just numbers of hazards hit.\nWe'd also like to explore the possibility of helping with motion sickness, to experiment and try to find best practices to avoid it, especially in the context of driving in VR.</p>\n<p>If we hit the tone correctly, we could bring this experience to schools as part of the lessons on reducing impaired driving!</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nDriving under the influences of alcohol is one of the main causes of fatal traffic accidents. People are often overconfident about their ability to drive after drinking. They don\u2019t realize how dangerous to drive when their central nervous systems impaired.\n\n\n## Statement\n\n\nWe wanted to have users virtually experience what it\u2019s like when you drive under the influences of alcohol, marijuana, tiredness, and using mobile devices, in order to show them why they shouldn\u2019t do this in their real lives.\n\n\nAll of these driving situation are 100% preventable by the drivers. We hope that people will make the appropriate decisions for the safety of themselves and all the others around them on the road before they make the deadly mistake of driving impaired.\n\n\n## What it does\n\n\nWe designed the whole game as a journey of a senior college student.\n\n\nThe player is a recently hired employee at a Silicon Valley company, and must get there soon for the first day of work because they lied about their address on their application. However, on the way they stop at their friends house for one last party. Unfortunately our protagonist didn't plan properly, and now they're forced to drive impaired on their way to San Francisco.\n\n\nOur driving simulator not only simulates hazards on your way, but adds distractions and simulates the various effects of tiredness, drunkenness, and being high while driving, interfering with your ability to make decisions. Your tiredness requires you to clear your head regularly else your eyes close while you're in the middle of the road. Being drunk removes all the warning sound effects and makes the controls fight you more as you struggle to keep your body and mind responsive. Being high introduces a myriad of distractions that pop up in front of your view, wild thoughts scrolling past your windshield , texts and popups that you must respond to because they block your vision, and a changed color scheme that makes non-relevant objects brighter and shinier while potential hazards are colored grey and harder to notice.\n\n\nUnlike some racing games, you are punished for the harm you cause on the environment, ranging from amassed tickets blocking your windshield, a crime meter that knows when you've blown through red lights and run over beloved pets, and the potential to end your life in a crash.\n\n\nMost importantly, pulling over for a break is always an option, encouraged to help clear the effects of the impairment, at sacrifice to time but to the benefit of everbody's safety.\n\n\n## How we built it\n\n\nUsing Unity we first designed a level to drive through, came up with environments, and began working on controls for driving sober, the default experience we would add to for each version of inebriation. We invested time into making the steering wheel animation for your hands seem natural by having your hands snap to the wheel even if you weren't perfectly rotating your hands in a circle. \n\n\n## Challenges we ran into\n\n\nIn terms of Tech Challenges:\n\n\nOne issue we had was that not everybody started working in the same version of Unity. 2018.2 and 2018.3 had updates to the prefab functions, which meant that we had to spend a solid chunk of time switching half of our team to an older version of Unity after they'd done a lot of work in the newer ones.\n\n\nOur team's technical challenge is mostly about how to make the interactions between the user and the steering wheel feel more natural and intuitive. Since in this hackathon, we are not allowed to use proprietary hardware to support the haptic feedback. What we could exploit is the potential of the Oculus Rift Integration package from the unity asset store to simulate the driver's experience. However, the normal oculus package only shows the basic interaction like picking up objects and throwing them off. Interactions like rotating wheels are off the charts. What's more, there are very limited resources about oculus interactions online. The only useful idea is linked here: (<https://forum.unity.com/threads/vr-touch-keeping-hands-rendered-to-a-position-while-rotating-an-object.514530/>), and those related codes are not open-source. So our main hack is to figure out the way to rotate the steering wheel as we do in real life. After thoroughly scanning and reorganizing the codes of the Oculus package, I write a series of adapted scripts to implement the single-hand-grip interaction successfully. For now, we are trying to make the two-hands-grip interaction happen and make it by the deadline.\n\n\nIn terms of Design Challenges:\n\n\nWe ended up running out of time to complete all the versions of driving impairments, so we chose to focus on one impairment, which was sleepiness, because that involved the least amount of work to get the point across reliably.\n\n\nWe had to find a delicate balance between the humor of the game, funny music and low poly artstyle with the seriousness of the topic, which involved dangerous behaviours that we didn't want to encourage by making them too fun or rewarding. We wanted to give the option to do the right thing at any time, hence the ability to pull over and take a break to try to wait out the impairments, but we wanted the experience to still involve embodying the poor decision making that would have led to this experience.\n\n\nNevertheless, the experience couldn't be just exhausting, and there had to be a reason to make it to the end in a reasonable amount of time, hence the story that we added to give a little more emotional context to the game and introduce characters you care about protecting.\n\n\nWe wanted to add a tutorial to introduce those unused to VR to basic concepts before throwing them into a driving simulator, but didn't have the time. Instead we start with narration over sober driving.\n\n\n## Accomplishments that we're proud of\n\n\nWe are very proud of your ideas. We successfully keep the game both engaging and sensitive to the topic. All of our interactions to indicate the driving impairment are very creative!\n\n\nWe are also very proud of our steering system, which feels intuitive to use for those who have driven before.\n\n\n## What we learned\n\n\nWe learned about the importance of starting everybody off using the same versions of their respective programs.\n\n\nWe learned a lot about designing interactions in VR.\n\n\nWe learned more about how to balance sound and mixes in VR.\n\n\n## What's next for Dare2Drive\n\n\nWe hope to finish the other two states of driving impairment to fully complete the original conception of the game. We'd also like to add a tutorial to the beginning to make the experience more accessible to less experienced users of VR. Possibly we could add a story to connect all the different kinds of impairment levels and a conclusion at the end of the game that would take your performance into account more than just numbers of hazards hit.\nWe'd also like to explore the possibility of helping with motion sickness, to experiment and try to find best practices to avoid it, especially in the context of driving in VR.\n\n\nIf we hit the tone correctly, we could bring this experience to schools as part of the lessons on reducing impaired driving!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/xr-fractals",
            "title": "XR Fractals",
            "blurb": "Experience the natural mathematical beauty of fractals: now in 3D space!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/KaFup61hNMQ?enablejsapi=1&hl=en_US&rel=0&start=6&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Fractal in AR Space ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/504/datas/original.png"
                },
                {
                    "title": "Opening Portal",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/535/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "IAN MACKENZIE",
                    "about": "I thought of the idea for creating interactive fractals in 3D space for XR devices after being inspired by 2D mobile app Fraksl.",
                    "photo": "https://avatars0.githubusercontent.com/u/32595467?height=180&v=4&width=180"
                },
                {
                    "name": "Sukhraj Johal",
                    "about": "I worked on creating the UI and the overall user experience within the app. As well as designing the interactive features that the user can perform in AR space!",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/948/027/datas/profile.png"
                },
                {
                    "name": "Eozin Che",
                    "about": "I worked on creating AR portal and interactive functions. ",
                    "photo": "https://avatars3.githubusercontent.com/u/5435638?height=180&v=4&width=180"
                },
                {
                    "name": "Michelle Yakubek",
                    "about": "I worked on researching different methods for generation and experimented with devices and portal placement.",
                    "photo": "https://www.gravatar.com/avatar/4f18f8c52e645fd0de0fcc92e1b1da72?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Matt Ross",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/4544594?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "arkit",
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>First: what <em>are</em> fractals? Fractals are complex structures based on a simple core shape or pattern, and can be found in many places, both mathematical and natural.</p>\n<p><img alt=\"Examples of fractals\" data-canonical-url=\"https://bstillman.weebly.com/uploads/9/4/3/8/9438325/3119540_orig.jpg\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--sgdWQTdN--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://bstillman.weebly.com/uploads/9/4/3/8/9438325/3119540_orig.jpg\"/></p>\n<p>Inspired by the aesthetics of two-dimensional fractals, our team set out to explore the possibility of fractal generation in three-dimensional space.</p>\n<h2>What it does</h2>\n<p>Our iOS mobile app enables users to fiddle with fractals anywhere and anytime! Users interact with an immersive \"fractal portal\" through which they are able to experiment with their own geometric designs. With reactive sound and an intuitive interface, XR Fractals is an intriguing, entertaining mobile experience.</p>\n<h2>How we built it</h2>\n<p>XR Fractals uses Unity and ARKit.</p>\n<h2>SDKs, APIs, Libraries and Assets Used</h2>\n<p>ARKit was used to build the project. </p>\n<p>Spiral Icon in project (under public domain) - <a href=\"https://thenounproject.com/search/?q=spiral&amp;i=11045\" rel=\"nofollow\">https://thenounproject.com/search/?q=spiral&amp;i=11045</a></p>\n<h2>Challenges we ran into</h2>\n<p>Before we even started our project, one of our first challenges was deciding which technologies XR Fractals would use. We experimented with both VR and AR devices, but ultimately settled on a simple mobile AR app. With this platform, our app enables users to explore aesthetically interesting fractals on a whim.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We ran through many possible ideas for ways to generate our 3D fractals, as well as ways for users to interact with their creations. After playing around with shaders and mirrors, we decided to code our own recursive geometry for users to build fractal shapes with. We developed reactive audio that reflects the intricacy of our fractals. And to present our core experience, we devised and successfully implemented an immersive portal and an engaging UI for our users.</p>\n<h2>What we learned</h2>\n<p>Iteration is key: although our team has faced setback after setback, adaptability and willingness to start moving in a new direction have enabled us to successfully develop an entertaining app. Just like a fractal, we've expanded upon our core idea to create an amazing structure!</p>\n<h2>What's next for XR Fractals</h2>\n<p>Our fractals still have more space to grow! While XR Fractals is currently focused in augmented reality, as our team name implies, our app could easily extend to new devices and new technologies. Future expansion into VR or even a continuation of AR with wearable devices would introduce an entirely new level of immersion.</p>\n<h2>About the Team</h2>\n<p>Sukhraj Johal - A third year student at Sheridan College studying Game Design. Encompasses skills in game development, visual art, and UI/UX. </p>\n<p>Michelle Yakubek - Developer</p>\n<p>Eozin Che - Developer </p>\n<p>Ian Mackenzie - C# Developer recently graduated from the New Hampshire Technical Institute. AR/VR Developer and Enthusiast</p>\n<p>Matt Ross - Sound Designer </p>\n<h2>Hackathon Theme and Category</h2>\n<p><strong>Hackathon Theme</strong> \nArts, Media, Entertainment</p>\n<p><strong>Category</strong> \nVisual Art</p>\n<h2>Contact and Location Details</h2>\n<p><strong>Team location</strong>\n6th floor, Main Presentation Room, 3 tables from the door</p>\n<p><strong>Contact</strong>\nIan Mackenzie:  603-209-5658</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nFirst: what *are* fractals? Fractals are complex structures based on a simple core shape or pattern, and can be found in many places, both mathematical and natural.\n\n\n![Examples of fractals](https://res.cloudinary.com/devpost/image/fetch/s--sgdWQTdN--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://bstillman.weebly.com/uploads/9/4/3/8/9438325/3119540_orig.jpg)\n\n\nInspired by the aesthetics of two-dimensional fractals, our team set out to explore the possibility of fractal generation in three-dimensional space.\n\n\n## What it does\n\n\nOur iOS mobile app enables users to fiddle with fractals anywhere and anytime! Users interact with an immersive \"fractal portal\" through which they are able to experiment with their own geometric designs. With reactive sound and an intuitive interface, XR Fractals is an intriguing, entertaining mobile experience.\n\n\n## How we built it\n\n\nXR Fractals uses Unity and ARKit.\n\n\n## SDKs, APIs, Libraries and Assets Used\n\n\nARKit was used to build the project. \n\n\nSpiral Icon in project (under public domain) - <https://thenounproject.com/search/?q=spiral&i=11045>\n\n\n## Challenges we ran into\n\n\nBefore we even started our project, one of our first challenges was deciding which technologies XR Fractals would use. We experimented with both VR and AR devices, but ultimately settled on a simple mobile AR app. With this platform, our app enables users to explore aesthetically interesting fractals on a whim.\n\n\n## Accomplishments that we're proud of\n\n\nWe ran through many possible ideas for ways to generate our 3D fractals, as well as ways for users to interact with their creations. After playing around with shaders and mirrors, we decided to code our own recursive geometry for users to build fractal shapes with. We developed reactive audio that reflects the intricacy of our fractals. And to present our core experience, we devised and successfully implemented an immersive portal and an engaging UI for our users.\n\n\n## What we learned\n\n\nIteration is key: although our team has faced setback after setback, adaptability and willingness to start moving in a new direction have enabled us to successfully develop an entertaining app. Just like a fractal, we've expanded upon our core idea to create an amazing structure!\n\n\n## What's next for XR Fractals\n\n\nOur fractals still have more space to grow! While XR Fractals is currently focused in augmented reality, as our team name implies, our app could easily extend to new devices and new technologies. Future expansion into VR or even a continuation of AR with wearable devices would introduce an entirely new level of immersion.\n\n\n## About the Team\n\n\nSukhraj Johal - A third year student at Sheridan College studying Game Design. Encompasses skills in game development, visual art, and UI/UX. \n\n\nMichelle Yakubek - Developer\n\n\nEozin Che - Developer \n\n\nIan Mackenzie - C# Developer recently graduated from the New Hampshire Technical Institute. AR/VR Developer and Enthusiast\n\n\nMatt Ross - Sound Designer \n\n\n## Hackathon Theme and Category\n\n\n**Hackathon Theme** \nArts, Media, Entertainment\n\n\n**Category** \nVisual Art\n\n\n## Contact and Location Details\n\n\n**Team location**\n6th floor, Main Presentation Room, 3 tables from the door\n\n\n**Contact**\nIan Mackenzie: 603-209-5658\n\n\n"
        },
        {
            "source": "https://devpost.com/software/demystified",
            "title": "Demistified for Magic Leap",
            "blurb": "A 4th-dimensional art experience concept using the Magic Leap and digital museum archives ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/KBGZkb5M3to?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "testing the image recognition",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/483/datas/original.jpg"
                },
                {
                    "title": "Brainstorming!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/485/datas/original.jpg"
                },
                {
                    "title": "storyboarding the interaction",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/484/datas/original.jpg"
                },
                {
                    "title": "getting to work",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/482/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Robyn Janz",
                    "about": "I worked on 2D procurement, photo / video editing as well as sound effect procurement and admin parcels.  This is my first time developing content for Magic Leap.",
                    "photo": "https://avatars0.githubusercontent.com/u/42664959?height=180&v=4&width=180"
                },
                {
                    "name": "cieraej",
                    "about": "I worked on Unity, sequencing and technical  things.",
                    "photo": "https://avatars1.githubusercontent.com/u/6461347?height=180&v=4&width=180"
                },
                {
                    "name": "IrisPan1991",
                    "about": "I worked on the overall interaction design, assets building, particle effects and animation in Unity, and XR user testing.",
                    "photo": "https://avatars2.githubusercontent.com/u/38545715?height=180&v=4&width=180"
                },
                {
                    "name": "Lauren Slowik",
                    "about": "I worked on 3D assets, asset integration into Unity, Ideation for project and some visual effects. This was my first time developing for XR.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/733/datas/profile.jpg"
                },
                {
                    "name": "Annelie Koller",
                    "about": "I worked on design, narrative experience design, UX UI, 3D, conceptual development and project managed.  This was my first time developing for XR. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/587/datas/profile.png"
                }
            ],
            "built_with": [
                "acer",
                "adobe-illustrator",
                "alienware",
                "mac",
                "magic-leap",
                "meshlab",
                "meshmixer",
                "oculus",
                "photoshop",
                "rhino",
                "sketchfab-api-unity",
                "tinkercad",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The 2019 Reality Virtually Hackathon corresponded with the second anniversary of the Women's March. The five of us came to the hack ready to engage with a community of which we love being a part. We didn't <em>intend</em> to form an all-woman team but as we discussed the fourth-dimensional affordances of the Magic Leap headset an exciting idea began to form and we had to pursue it together.</p>\n<h2>What it does</h2>\n<p>The average museum goer spends 17 seconds viewing an art piece* and rarely has the chance to dive into the mists of the history of a work. Demistified will plunge a viewer into the world of an art work and the context of its creation within the gallery viewing experience, hopefully creating a longer, educational engagement with works of art. This format will give those extra viewing seconds to women and historical figures depicted in art who have been marginalized or overlooked. Demistified immerses the viewer in the contextual history of the piece. It is not simply animating the painting, instead the interaction gives life to a world of related archival materials, rarely viewed by the public such as fragile historical garments, ephemera and other difficult-to-display pieces.</p>\n<p>*<a href=\"https://journals.sagepub.com/doi/10.2190/5MQM-59JH-X21R-JN5J\" rel=\"nofollow\">Art viewing study at the Metropolitan Museum of Art, NYC</a></p>\n<h2>How we built it</h2>\n<p>For the hardware and interaction there was no question that the Magic Leap\u2019s hands-free, immersive augmented reality capabilities was the main mechanic we wanted to use. We story-boarded the elements of research we were able to gather about our chosen art piece. From there we began searching for cc licensed assets such as 3D scans of garments and objects, archival photographs and collecting audio effects to complete the immersive experience.</p>\n<h2>Challenges we ran into</h2>\n<p>The image tracking wasn\u2019t as high fidelity as we were hoping but our developer thinks that we can combine the spatial computing room mesh generation with image tracking so that the staging of the augmentation is consistent and delightful in the future.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Proud that we have a strong proof of concept that was merely constrained by time. It\u2019s going to be really special when we can spend even more time making the experience rich. Really happy that we got to focus on fine arts in the context of technology. We\u2019re pretty proud to be an all-women team.</p>\n<h2>What we learned</h2>\n<p>We all learned how to do a VR/AR/XR. Two of us on the team learned how to do a hackathon! But we also learned that Magic Leap\u2019s AR needs a narrow range of dark colors for assets to be viewed satisfactorily. The constrained field of view is something that needs to be kept in mind for both Unity setup and interaction planning for the Magic Leap.</p>\n<h2>What's next</h2>\n<p>An exhibition to test the full extent of the possible experiences, ideally at the Boston MFA where our demo painting is currently on view. Should application in a real museum setting prove effective we would then pursue creating multi-contributor AR platform. This would make it possible for museum curators to create these experiences and users to do research and unearth Creative Commons archives. We would also like to take advantage of the Magic Leap web platform as outlet for art lovers who do not live near art institutions.</p>\n<h2>Logistics</h2>\n<p>6th Floor, MPR, NW Corner</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe 2019 Reality Virtually Hackathon corresponded with the second anniversary of the Women's March. The five of us came to the hack ready to engage with a community of which we love being a part. We didn't *intend* to form an all-woman team but as we discussed the fourth-dimensional affordances of the Magic Leap headset an exciting idea began to form and we had to pursue it together.\n\n\n## What it does\n\n\nThe average museum goer spends 17 seconds viewing an art piece* and rarely has the chance to dive into the mists of the history of a work. Demistified will plunge a viewer into the world of an art work and the context of its creation within the gallery viewing experience, hopefully creating a longer, educational engagement with works of art. This format will give those extra viewing seconds to women and historical figures depicted in art who have been marginalized or overlooked. Demistified immerses the viewer in the contextual history of the piece. It is not simply animating the painting, instead the interaction gives life to a world of related archival materials, rarely viewed by the public such as fragile historical garments, ephemera and other difficult-to-display pieces.\n\n\n*[Art viewing study at the Metropolitan Museum of Art, NYC](https://journals.sagepub.com/doi/10.2190/5MQM-59JH-X21R-JN5J)\n\n\n## How we built it\n\n\nFor the hardware and interaction there was no question that the Magic Leap\u2019s hands-free, immersive augmented reality capabilities was the main mechanic we wanted to use. We story-boarded the elements of research we were able to gather about our chosen art piece. From there we began searching for cc licensed assets such as 3D scans of garments and objects, archival photographs and collecting audio effects to complete the immersive experience.\n\n\n## Challenges we ran into\n\n\nThe image tracking wasn\u2019t as high fidelity as we were hoping but our developer thinks that we can combine the spatial computing room mesh generation with image tracking so that the staging of the augmentation is consistent and delightful in the future.\n\n\n## Accomplishments that we're proud of\n\n\nProud that we have a strong proof of concept that was merely constrained by time. It\u2019s going to be really special when we can spend even more time making the experience rich. Really happy that we got to focus on fine arts in the context of technology. We\u2019re pretty proud to be an all-women team.\n\n\n## What we learned\n\n\nWe all learned how to do a VR/AR/XR. Two of us on the team learned how to do a hackathon! But we also learned that Magic Leap\u2019s AR needs a narrow range of dark colors for assets to be viewed satisfactorily. The constrained field of view is something that needs to be kept in mind for both Unity setup and interaction planning for the Magic Leap.\n\n\n## What's next\n\n\nAn exhibition to test the full extent of the possible experiences, ideally at the Boston MFA where our demo painting is currently on view. Should application in a real museum setting prove effective we would then pursue creating multi-contributor AR platform. This would make it possible for museum curators to create these experiences and users to do research and unearth Creative Commons archives. We would also like to take advantage of the Magic Leap web platform as outlet for art lovers who do not live near art institutions.\n\n\n## Logistics\n\n\n6th Floor, MPR, NW Corner\n\n\n"
        },
        {
            "source": "https://devpost.com/software/sound-garden",
            "title": "Sound Garden",
            "blurb": "Explore your emotions in an AR garden filled with musical plants themed to fit your mood.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/f1DyIdmpV5w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "scene with acorns and plants",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/056/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "shylo shepherd",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/5191443?height=180&v=4&width=180"
                },
                {
                    "name": "Daniel Hart",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/1215698?height=180&v=4&width=180"
                },
                {
                    "name": "Xinye Lin",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/991/datas/profile.jpg"
                }
            ],
            "built_with": [
                "google-poly",
                "logic",
                "photoshop",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration:</h2>\n<p>We wanted to allow individuals to have an augmented environment to experience their emotions, good or bad, through music.</p>\n<h2>The Problem:</h2>\n<p>How can we express our emotions? How do we feel today? Where are we looking to go? </p>\n<h2>Description:</h2>\n<p>We use augmented reality in Magic Leap to create a natural space for you to experience a sound scape. We chose to start with the selection of seeds as a metaphor for growth. Flowers appear when the seeds are planted, and they can be nurtured for further development. The potential of the seeds is illustrated in the diversity of the flowers. Each flower now represents one portion, one possible realization of the sensation. While we suggest the emotions of happiness, melancholy and neutrality, the interpretation is up to the individual. You can linger on one sound, survey them all, or listen for the juxtapositions to match your own mood. In all this we want you to stay grounded, so that you can connect your emotions to your life directly. </p>\n<h2>What it does:</h2>\n<p>We created a garden of sound using plants as instruments. Players choose a mood from the acorns on the tree and can play music through brushing the plants. </p>\n<h2>How we built it:</h2>\n<p>We built the project in Unity. Music was created on Logic and brought into Unity. 3D assets were obtained for free from Google Poly, and Free Unity Assets. Textures were adjusted in Photoshop. </p>\n<h2>Challenges we ran into:</h2>\n<p>We tried many types of interactions in the Magic Leap One headset. Hand gestures were a challenge, so in the end, we opted to use the controller. </p>\n<h2>Accomplishments that we're proud of:</h2>\n<p>We learned a lot about developing on the Magic Leap One, which was very exciting for us. We hope to continue learning using the emulator.</p>\n<h2>What we learned:</h2>\n<p>We now have the ability to start developing further on the Magic Leap One, we learned new skills in Unity, we learned tips and tricks for making objects look better in AR. </p>\n<h2>What's next for Sound Garden:</h2>\n<p>We all have interest in learning more about developing on the Magic Leap One, and continuing to develop in Unity and Logic. </p>\n</div>",
            "content_md": "\n## Inspiration:\n\n\nWe wanted to allow individuals to have an augmented environment to experience their emotions, good or bad, through music.\n\n\n## The Problem:\n\n\nHow can we express our emotions? How do we feel today? Where are we looking to go? \n\n\n## Description:\n\n\nWe use augmented reality in Magic Leap to create a natural space for you to experience a sound scape. We chose to start with the selection of seeds as a metaphor for growth. Flowers appear when the seeds are planted, and they can be nurtured for further development. The potential of the seeds is illustrated in the diversity of the flowers. Each flower now represents one portion, one possible realization of the sensation. While we suggest the emotions of happiness, melancholy and neutrality, the interpretation is up to the individual. You can linger on one sound, survey them all, or listen for the juxtapositions to match your own mood. In all this we want you to stay grounded, so that you can connect your emotions to your life directly. \n\n\n## What it does:\n\n\nWe created a garden of sound using plants as instruments. Players choose a mood from the acorns on the tree and can play music through brushing the plants. \n\n\n## How we built it:\n\n\nWe built the project in Unity. Music was created on Logic and brought into Unity. 3D assets were obtained for free from Google Poly, and Free Unity Assets. Textures were adjusted in Photoshop. \n\n\n## Challenges we ran into:\n\n\nWe tried many types of interactions in the Magic Leap One headset. Hand gestures were a challenge, so in the end, we opted to use the controller. \n\n\n## Accomplishments that we're proud of:\n\n\nWe learned a lot about developing on the Magic Leap One, which was very exciting for us. We hope to continue learning using the emulator.\n\n\n## What we learned:\n\n\nWe now have the ability to start developing further on the Magic Leap One, we learned new skills in Unity, we learned tips and tricks for making objects look better in AR. \n\n\n## What's next for Sound Garden:\n\n\nWe all have interest in learning more about developing on the Magic Leap One, and continuing to develop in Unity and Logic. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/stay-alive-7bsrq2",
            "title": "Stay Alive",
            "blurb": "Interactive, immersive, historical story telling experience",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312404827?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "title screen",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/193/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Shan Shan",
                    "about": "I'm the sound artist. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/741/978/datas/profile.jpg"
                },
                {
                    "name": "bassholio Bassler",
                    "about": "I assisted the team with UX/UI, Story flow, Interactions, layout, environments, creative direction, storyboarding, user flows, etc, etc, etc.",
                    "photo": "https://avatars1.githubusercontent.com/u/22763359?height=180&v=4&width=180"
                },
                {
                    "name": "Viveka CD",
                    "about": "I assisted the team with 3D modelling, props, 2D images, retouching, 3D environments, creative direction, story-boarding, user design, etc.",
                    "photo": "https://avatars0.githubusercontent.com/u/37417994?height=180&v=4&width=180"
                },
                {
                    "name": "vbousis Bousis, MBA, JD",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/46832233?height=180&v=4&width=180"
                },
                {
                    "name": "connerkward",
                    "about": "",
                    "photo": "https://graph.facebook.com/10215994332364810/picture?height=180&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The experience is based on countless survival stories about the genocide I heard while visiting Cambodia. The sad and complex history I discovered while visiting the country and meeting the people was gut-wrenching. It was, however, also inspiring, as so many people managed to courageously survive due to their willingness to live. </p>\n<h2>What it does</h2>\n<p>The interactive experience uses the insprirational narrative of several stories as dramatic pretext to outline the choices the user must make at each scene. Each decision will either result in their survival, imprisonment, or death. With each choice, the stakes heighten. The user stands to lose everything, including their family. Finally, at the end of the experience, the gravitas behind each decision will complete a puzzle about the user and their specific life story. </p>\n<h2>How I built it</h2>\n<p>We took a lot of time in the begining to storyboard the narrative and the interactivity. We also spent time organizing our workflow so we could pick up speed later. We build it using various software programs including unity, Photoshop, logic, audition, 3Ds-Max, QuickTime, iMovie, GitHub, Oculus STK's. We created most of our 3D and 2D assets, but used some free ones from the Unity store.  </p>\n<h2>Challenges I ran into</h2>\n<p>The challenges were making choices that were raising the stakes at each turn get maintaining a strong visual AND emotional narrative. We were also a team with only one new developer so it was challenging with so many scene changes and material to execute. The tradeoff was some of the interactive elements of the experience - it became limited to allowed time to build scenes, driving the narrative. Not to mention, our computer broke down in the middle of the hackathon which caused us a delay in our work. We lost time to replace the computer and then to rebuild some of the lost scenes. Adding the element of time constraint and the importance of this story with each passing hour, our stress increased exponentially. The hackathon became more than a project, but a personal endeavor to protect and represent the stories of so many people respectfully. </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>I am really proud of my team and our accomplishment. They exemplified the upmost commitment to the project and a deep passion for the story from the beginning. We brought this story to life as a collaborative team with shared values. We all worked diligently and extensively, extending our comfort zone to make this project a reality. </p>\n<h2>What I learned</h2>\n<p>I learned that passion comes first. It helps you endure the long nights, challenging moments, and difficult decisions. Second, the difference between a good experience and a great experience is story, story, and story. It all stems from there, then the artistry and technology follow. And thirdly, there is nothing better than pre-production planning and time vested in storyboarding. It helped align the vision for the entire team, whilst also providing direction, task management, and execution.</p>\n<h2>What's next for Stay Alive</h2>\n<p>I would like to continue to work on this experience with this team, while adding additional team members so we can really expand, perfect, and personalize the experience. Eventually, I would love to submit this into a film festival in hopes of bringing attention to these stories and helping people put the pieces together to their past.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe experience is based on countless survival stories about the genocide I heard while visiting Cambodia. The sad and complex history I discovered while visiting the country and meeting the people was gut-wrenching. It was, however, also inspiring, as so many people managed to courageously survive due to their willingness to live. \n\n\n## What it does\n\n\nThe interactive experience uses the insprirational narrative of several stories as dramatic pretext to outline the choices the user must make at each scene. Each decision will either result in their survival, imprisonment, or death. With each choice, the stakes heighten. The user stands to lose everything, including their family. Finally, at the end of the experience, the gravitas behind each decision will complete a puzzle about the user and their specific life story. \n\n\n## How I built it\n\n\nWe took a lot of time in the begining to storyboard the narrative and the interactivity. We also spent time organizing our workflow so we could pick up speed later. We build it using various software programs including unity, Photoshop, logic, audition, 3Ds-Max, QuickTime, iMovie, GitHub, Oculus STK's. We created most of our 3D and 2D assets, but used some free ones from the Unity store. \n\n\n## Challenges I ran into\n\n\nThe challenges were making choices that were raising the stakes at each turn get maintaining a strong visual AND emotional narrative. We were also a team with only one new developer so it was challenging with so many scene changes and material to execute. The tradeoff was some of the interactive elements of the experience - it became limited to allowed time to build scenes, driving the narrative. Not to mention, our computer broke down in the middle of the hackathon which caused us a delay in our work. We lost time to replace the computer and then to rebuild some of the lost scenes. Adding the element of time constraint and the importance of this story with each passing hour, our stress increased exponentially. The hackathon became more than a project, but a personal endeavor to protect and represent the stories of so many people respectfully. \n\n\n## Accomplishments that I'm proud of\n\n\nI am really proud of my team and our accomplishment. They exemplified the upmost commitment to the project and a deep passion for the story from the beginning. We brought this story to life as a collaborative team with shared values. We all worked diligently and extensively, extending our comfort zone to make this project a reality. \n\n\n## What I learned\n\n\nI learned that passion comes first. It helps you endure the long nights, challenging moments, and difficult decisions. Second, the difference between a good experience and a great experience is story, story, and story. It all stems from there, then the artistry and technology follow. And thirdly, there is nothing better than pre-production planning and time vested in storyboarding. It helped align the vision for the entire team, whilst also providing direction, task management, and execution.\n\n\n## What's next for Stay Alive\n\n\nI would like to continue to work on this experience with this team, while adding additional team members so we can really expand, perfect, and personalize the experience. Eventually, I would love to submit this into a film festival in hopes of bringing attention to these stories and helping people put the pieces together to their past.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/stretchxr-a-virtualized-pain-management-therapy",
            "title": "StretchXR - A Virtualized Pain Management Therapy",
            "blurb": "StretchXR offers clinicians a modern, safe, non-invasive approach to chronic pain therapy for patients -- drug Free!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/VCpv0iEljNU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "An Immersive VR-based Embodiment Therapy for Chronic Back Pain",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/433/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Brandon Birckhead",
                    "about": "Came up with the idea for the program based on medical literature and experience. Led story boarding of the intervention.",
                    "photo": "https://avatars0.githubusercontent.com/u/24922385?height=180&v=4&width=180"
                },
                {
                    "name": "Michelle Ju\u00e1rez",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/219/datas/profile.jpg"
                },
                {
                    "name": "Erika Gangware",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/41128488?height=180&v=4&width=180"
                },
                {
                    "name": "annamufa Mu\u00f1oz Farr\u00e9",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/38862917?height=180&v=4&width=180"
                },
                {
                    "name": "deborahnavarro",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/15136688?height=180&v=4&width=180"
                },
                {
                    "name": "masternader",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/41163985?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "leap-motion",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are a team of developers, researchers, and entrepreneurs with a common interest in health and wellness. We were moved by the potential of fully immersive therapies through VR for the relief of chronic physical pain. To begin this journey, we decided to target patients who suffer from chronic lower back pain, as it accounts for over 10% of the population. It is also the most diagnosed chronic physical pain and the most common source of long-term opiate prescription use. Our project pairs physical therapy with visual therapy to form unique treatments for patients. The near-term goal is to characterize the effectiveness of this technique, extending further research into VR-based pain alleviation therapies. We aim to direct the evolution of novel applications of VR/AR technology in medicine and meditation, with the ultimate goal of the deployment of accessible, cost-effective, therapeutic immersive experiences for the reduction of drug dependency among affected populations, improving the overall long-term quality of life among chronic pain sufferers.</p>\n<h2>Background</h2>\n<p>The objective of this project is to explore the therapeutic viability of embodiment experiences in a VR setting with a virtualized avatar body, as applied to alleviate chronic lower back pain. We instruct the subject to stretch their arms outward, triggering an elongation of the lower torso. </p>\n<p>Prior to entering the virtual environment, subjects are fitted with the Woojer strap to provide synchronized haptic feedback with calming, ambient music for the duration of the virtual experience. Once the user enters the virtual environment, the experience opens with the subject finding themselves on a patch of grass with a clear, open sky and a virtualized mirror. The subject is able to visualize their surrogate self in the virtualized mirror and is made aware of their surrogate hands. Once sufficient time has passed to allow for the subject to acclimate to the avatar (~10 seconds), the subject is instructed to perform gentle, flowing arm movements that affect therapeutic reduction of pain during the experience. In this case, the actions trigger modifications to the morphology of the subject\u2019s torso through the elongation of the avatar\u2019s spine, which is observable to the subject in the virtual mirror. The elongation of the torso gradually increases with every successive movement. Techniques that enhance the efficacy or suggestibility of the embodiment experience have been explored, but a significant degree of immersion is accomplished through the subject\u2019s interaction with their surrogate reflection. </p>\n<p>Recent pilot studies (n=19) have reported pain scores reduced by up to 50% in cases of compartmental regional pain syndrome (CRPS type I and II), peripheral neuropathy injury (PNI), and an array of other chronic and acute cases of pain experience.</p>\n<h2>Abstract</h2>\n<p>Over 84% of adults in the US have suffered from lower back at some point in their lives (Carey et al., 2009). Pain that continues for greater than 12 weeks is considered chronic pain. Millions of individuals live with chronic lower back pain, and inappropriately-prescribed opioids long term, a class of medications that may provide effective analgesia but can lead to opioid use disorder, opioid-related overdoses and serious adverse events, including death. It is vital to identify opioid-free treatments to assist in the management of chronic pain. This intervention was developed to give patients the illusion of body ownership with a virtual avatar. The patient will then undergo therapeutic maneuvers and virtual torso elongation to create an analgesic effect based on previous research. </p>\n<h2>Value Proposition:</h2>\n<ul>\n<li>Pain reduction through guided physical therapeutic movement </li>\n<li>Pain reduction through elongation and expansion visualization</li>\n<li>Pain reduction through immersive virtual environment</li>\n<li>Pain reduction through sound therapy</li>\n</ul>\n<h2>Targeted Market</h2>\n<ul>\n<li>Medical professionals </li>\n<li>Hospitals </li>\n<li>Research Institutions </li>\n</ul>\n<h2>Targeted Users</h2>\n<p>We are targeting patients with chronic lower back pain who are in the 30-50 year old range and have mobility. Isolating a target demographic will allow us to collect comparable data. </p>\n<h2>Goal:</h2>\n<p>We intend to run clinical trials using the VR-CORE guidelines. This would include a VR1 study which includes further development using patient centered design via focus groups. This will be followed by  a VR2 study which is used to assess the acceptability from the patient and feasibility of using the device within a clinical setting. Finalized a VR3 study will be done to assess the efficacious of the intervention vs an active control. Previous research has shown approximately 20-25% reduction in pain using a VR intervention in a variety of situations. Further studies will explore the percent reduction within this specific patient population. . </p>\n<h2>What\u2019s Next?</h2>\n<p>We aim to add an array of capabilities we envisioned but were not able to include in our MVP (demo) due to the short-term time-frame of the hackathon. Once our experience is complete, a researcher and M.D. on our team, who presented the idea for the project, aims to use it in clinical trials to treat chronic pain patients in outpatient pain clinics. Pain medicine practices are growing as the opiate epidemic continues to rise. This serves as one way to help alleviate one source of the problem. This mechanism for VR therapy could also be used for preventing patients from going on opioids, by adding this to acute pain regiments.  </p>\n<h2>How we built it</h2>\n<p>The team logo was designed in Adobe Illustrator. To provide haptic feedback during the experience, users are fitted with a Woojer strap. This strap is synchronized to ambient music that plays within the virtual environment. The background music was taken from a royalty-free music site <a href=\"http://jamen.do/t/365392\" rel=\"nofollow\">http://jamen.do/t/365392</a> - \u201cStellar\u201d by Fortadelis. The first asset that was placed in the scene was the mirror. Then the avatar was created using Rhino3D. The main animation that was created was the elongation of the torso of the avatar. The animation was made within Unity3D.  The key action within the program is reflection of the virtual body within the mirror. The shader of the mirror was modified to allow correct visualization of the avatar. We then created an audio script using online-voice-recorder.com, which initiates about 10 seconds into the experience, to allow time for the subject to place the HMD on their head and acclimate to the environment. This audio script instructs the participant to touch a button with their hands to start the exercise. The core of the experience was built in Unity using HTC Vive Pro and Leap Motion hand trackers. Leap Motion was used to provide humanoid upper extremities with accurate tracking of hands. The scene was taken from the unity asset store. </p>\n<h2>Challenges we ran into</h2>\n<p>We initially attempted to use Intel RealSense depth cameras to perform real-time body-tracking; however, we eventually discovered that we needed to use a paid version of OpenCV plugin for Unity to implement use of the sensor.</p>\n<p>Though virtual embodiment is most effect with a humanoid avatar with full body tracking, we could not incorporate leg tracking (due to the body-tracking issue previously-mentioned). As a result, we decided to design the experience as a stationary exercise through a sequence of repetitive arm movements. Additionally, we encountered issues with adding arms to the avatar. The mirror also has issues reflecting the virtual body with the current shader that it came with. It required further modifications.</p>\n<p>A considerable amount of time was spent correcting issues with Git LFS when making commits to the repository (to ensure that all team members were able to successfully commit to it).</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We came together around a passion to help patients based by medical VR research. During our first day we focused on the most important feature of the intervention. This was also a great learning experience for all of us. </p>\n<h2>Literature behind the intervention</h2>\n<p>Several papers were instrumental in the development of the intervention. Rosink et al. demonstrated that a projector-based VR experience could be used to help understand body perception in those with chronic lower back pain (Roosink et al., 2015). We then incorporated elongation based on inspiration from a paper from Dr. Mel Slater, which demonstrated a sense of presence with a limb three times its original size(Kilteni, Normand, Sanchez-Vives, &amp; Slater, 2012).  Dr. Maria V. Sanchez-Vives recently published a brilliant pilot study using virtual embodiment and modifications of both size and opacification of upper extremity to treat chronic pain(Matamala-Gomez, Gonzalez, Slater, &amp; Sanchez-Vives, 2018). Virtual embodiment is the process of causing the illusion of body ownership to a virtual avatar using visuomotor synchrony and a virtual mirror. Recently a randomized trial published in the journal Neurology demonstrating mild analgesia using virtual embodiment for neuropathic pain from a spinal injury(Pozeg et al., 2017). We also incorporated a back exercise based on several papers demonstrated benefit with VR based physical therapy(Thomas, France, Applegate, Leitkam, &amp; Walkowski, 2016)(Trost et al., 2015).</p>\n<h2>Team</h2>\n<p><strong>Brandon Birckhead MD, Principal Investigator</strong>: I am the Co-Chair of the Virtual Medicine Conference at Cedars-Sinai Medical Center and currently running a large randomized clinical trial for chronic pain. I have designed two VR programs one for dental anxiety and one for breast cancer education. I have also designed several clinical studies: randomized VR pain study using the cold pressor test, randomized breast cancer patient education, and taken part in the creation of guidelines for immersive therapeutic clinical trials along with a cohort of international therapeutic VR experts, called the VR-CORE (Virtual Reality \u2013 Clinical Outcomes Research Experts). I had advised several physicians and medical students in the development process of VR clinical trials. </p>\n<p><strong>Erika Gangware, 3D Designer and Unity Developer</strong>. I am a 3D Product Designer and a Unity Developer working primarily on mobile and vive platforms. As a 3D designer I have made biomedical renderings in the oncology space and use 3D software to render anatomical models. As a Unity Developer I have worked on biomedical VR files that aim to improve provider workflow, and plan to design and develop files that improve patient outcomes.</p>\n<p><strong>Deborah Navarro M.S., Visionary &amp; Strategist</strong>: I am a Product Manager and Hyperloop Business Leader. I founded a research group/startup consisting of engineers from UT and MIT.  Together, we took home the Innovation award at SpaceX\u2019s Hyperloop Pod Comp II for our breakthroughs in air-levitation. I have a background in biological sciences, technology commercialization, and engineering. I aim to augment the relationship between tech, sustainability, and artificial intelligence by building products that are mindful of their position in the world and implement them in a way that allows them to connect people. I am exploring how AR/VR can help achieve this. </p>\n<p><strong>Anna Mu\u00f1oz-Farr\u00e9 Biomedical Engineer:</strong> I am a Research Affiliate at Massachusetts Institute of Technology, where I developed my MS thesis in the area of cardiology. I focus in product development, and I been in research and collaborated with different hospitals, both in Boston and Barcelona, Spain. I am passionate about how technology will change the way we treat illnesses. </p>\n<p><strong>Michelle Ju\u00e1rez, Principal Systems Engineer - Applied Research:</strong> I\u2019m a Systems Engineer in the aerospace industry, focusing on applied research and development, modeling and simulation, system design, and radar analysis. I have a background in research in algorithm development and medical image analysis of fMRI brain data, and am currently collaborating on a side project for the development of a novel, holographic heads-up display for vehicles. I am passionate about physics/astronomy/data analytics/biotech/disruptive technologies, and am eager to explore innovating my career in AR/VR/XR development.</p>\n<p><strong>Nader Shokair Research Scientist</strong>, B.A./Pre-Med PostBacc - Research Scientist: I am a premed student with an expertise in physical sciences; namely, optical systems, holography, and analog circuit applications of low-level circuit logic, along with material fabrication and engineering. I enjoy a multifarious set of academic and personal ambitions. My undergraduate degree is in philosophy with an emphasis on cognitive science and ethics. I am passionate about technology, particularly machine learning/CNNs and computer vision and believe the future of medicine will be built upon AR/VR/XR. My passions include a range of motorsports from motoX to wheel-to-wheel road racing and have been working on my own race car project for over 10 years. Using this as a platform I hope to explore the technical problems surrounding the design of HMD-free holography in the  controlled space of a vehicle cockpit, to then expand into the clinical settings I wish to see develop into full XR patient/doctor experiences.\n<strong>Erika Gangware Unity Developer and 3D Designer:</strong><br/></p>\n<h2>What we learned</h2>\n<p>Leap motion\nAnimation\nShaders for reflective mirrors</p>\n<h2>What's next for StretchXR?</h2>\n<p>We aim to add an array of capabilities we envisioned but were not able to include in our MVP (demo) due to the limited time-frame of the hackathon. Once our experience is complete, a researcher and M.D. on our team, who presented the idea for the project, aims to use it in clinical trials to treat chronic pain patients in outpatient pain clinics. Pain medicine practices are growing as the opiate epidemic continues to rise. This serves as one way to help alleviate one source of the problem. This mechanism for VR therapy could also be used for preventing patients from going on opiods, by adding this to acute pain regiments</p>\n<h2>References</h2>\n<p>Carey, T. S., Freburger, J. K., Holmes, G. M., Castel, L., Darter, J., Agans, R., \u2026 Jackman, A. (2009). A long way to go: practice patterns and evidence in chronic low back pain care. Spine, 34(7), 718\u2013724. <a href=\"https://doi.org/10.1097/BRS.0b013e31819792b0\" rel=\"nofollow\">https://doi.org/10.1097/BRS.0b013e31819792b0</a>\nKilteni, K., Normand, J.-M., Sanchez-Vives, M. V., &amp; Slater, M. (2012). Extending Body Space in Immersive Virtual Reality: A Very Long Arm Illusion. PLoS ONE, 7(7), e40867. <a href=\"https://doi.org/10.1371/journal.pone.0040867\" rel=\"nofollow\">https://doi.org/10.1371/journal.pone.0040867</a>\nMatamala-Gomez, M., Gonzalez, A. M. D., Slater, M., &amp; Sanchez-Vives, M. V. (2018). Decreasing pain ratings in chronic arm pain through changing a virtual body: different strategies for different pain types. The Journal of Pain\u202f: Official Journal of the American Pain Society, 0(0). <a href=\"https://doi.org/10.1016/j.jpain.2018.12.001\" rel=\"nofollow\">https://doi.org/10.1016/j.jpain.2018.12.001</a>\nPozeg, P., Palluel, E., Ronchi, R., Solc\u00e0, M., Al-Khodairy, A.-W., Jordan, X., \u2026 Blanke, O. (2017). Virtual reality improves embodiment and neuropathic pain caused by spinal cord injury. Neurology, 89(18), 1894\u20131903. <a href=\"https://doi.org/10.1212/WNL.0000000000004585\" rel=\"nofollow\">https://doi.org/10.1212/WNL.0000000000004585</a>\nRoosink, M., McFadyen, B. J., H\u00e9bert, L. J., Jackson, P. L., Bouyer, L. J., &amp; Mercier, C. (2015). Assessing the Perception of Trunk Movements in Military Personnel with Chronic Non-Specific Low Back Pain Using a Virtual Mirror. PLOS ONE, 10(3), e0120251. <a href=\"https://doi.org/10.1371/journal.pone.0120251\" rel=\"nofollow\">https://doi.org/10.1371/journal.pone.0120251</a>\nThomas, J. S., France, C. R., Applegate, M. E., Leitkam, S. T., &amp; Walkowski, S. (2016). Feasibility and Safety of a Virtual Reality Dodgeball Intervention for Chronic Low Back Pain: A Randomized Clinical Trial. The Journal of Pain, 17(12), 1302\u20131317. <a href=\"https://doi.org/10.1016/j.jpain.2016.08.011\" rel=\"nofollow\">https://doi.org/10.1016/j.jpain.2016.08.011</a>\nTrost, Z., Zielke, M., Guck, A., Nowlin, L., Zakhidov, D., France, C. R., &amp; Keefe, F. (2015). The promise and challenge of virtual gaming technologies for chronic pain: the case of graded exposure for low back pain. Pain Management, 5(3), 197\u2013206. <a href=\"https://doi.org/10.2217/pmt.15.6\" rel=\"nofollow\">https://doi.org/10.2217/pmt.15.6</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are a team of developers, researchers, and entrepreneurs with a common interest in health and wellness. We were moved by the potential of fully immersive therapies through VR for the relief of chronic physical pain. To begin this journey, we decided to target patients who suffer from chronic lower back pain, as it accounts for over 10% of the population. It is also the most diagnosed chronic physical pain and the most common source of long-term opiate prescription use. Our project pairs physical therapy with visual therapy to form unique treatments for patients. The near-term goal is to characterize the effectiveness of this technique, extending further research into VR-based pain alleviation therapies. We aim to direct the evolution of novel applications of VR/AR technology in medicine and meditation, with the ultimate goal of the deployment of accessible, cost-effective, therapeutic immersive experiences for the reduction of drug dependency among affected populations, improving the overall long-term quality of life among chronic pain sufferers.\n\n\n## Background\n\n\nThe objective of this project is to explore the therapeutic viability of embodiment experiences in a VR setting with a virtualized avatar body, as applied to alleviate chronic lower back pain. We instruct the subject to stretch their arms outward, triggering an elongation of the lower torso. \n\n\nPrior to entering the virtual environment, subjects are fitted with the Woojer strap to provide synchronized haptic feedback with calming, ambient music for the duration of the virtual experience. Once the user enters the virtual environment, the experience opens with the subject finding themselves on a patch of grass with a clear, open sky and a virtualized mirror. The subject is able to visualize their surrogate self in the virtualized mirror and is made aware of their surrogate hands. Once sufficient time has passed to allow for the subject to acclimate to the avatar (~10 seconds), the subject is instructed to perform gentle, flowing arm movements that affect therapeutic reduction of pain during the experience. In this case, the actions trigger modifications to the morphology of the subject\u2019s torso through the elongation of the avatar\u2019s spine, which is observable to the subject in the virtual mirror. The elongation of the torso gradually increases with every successive movement. Techniques that enhance the efficacy or suggestibility of the embodiment experience have been explored, but a significant degree of immersion is accomplished through the subject\u2019s interaction with their surrogate reflection. \n\n\nRecent pilot studies (n=19) have reported pain scores reduced by up to 50% in cases of compartmental regional pain syndrome (CRPS type I and II), peripheral neuropathy injury (PNI), and an array of other chronic and acute cases of pain experience.\n\n\n## Abstract\n\n\nOver 84% of adults in the US have suffered from lower back at some point in their lives (Carey et al., 2009). Pain that continues for greater than 12 weeks is considered chronic pain. Millions of individuals live with chronic lower back pain, and inappropriately-prescribed opioids long term, a class of medications that may provide effective analgesia but can lead to opioid use disorder, opioid-related overdoses and serious adverse events, including death. It is vital to identify opioid-free treatments to assist in the management of chronic pain. This intervention was developed to give patients the illusion of body ownership with a virtual avatar. The patient will then undergo therapeutic maneuvers and virtual torso elongation to create an analgesic effect based on previous research. \n\n\n## Value Proposition:\n\n\n* Pain reduction through guided physical therapeutic movement\n* Pain reduction through elongation and expansion visualization\n* Pain reduction through immersive virtual environment\n* Pain reduction through sound therapy\n\n\n## Targeted Market\n\n\n* Medical professionals\n* Hospitals\n* Research Institutions\n\n\n## Targeted Users\n\n\nWe are targeting patients with chronic lower back pain who are in the 30-50 year old range and have mobility. Isolating a target demographic will allow us to collect comparable data. \n\n\n## Goal:\n\n\nWe intend to run clinical trials using the VR-CORE guidelines. This would include a VR1 study which includes further development using patient centered design via focus groups. This will be followed by a VR2 study which is used to assess the acceptability from the patient and feasibility of using the device within a clinical setting. Finalized a VR3 study will be done to assess the efficacious of the intervention vs an active control. Previous research has shown approximately 20-25% reduction in pain using a VR intervention in a variety of situations. Further studies will explore the percent reduction within this specific patient population. . \n\n\n## What\u2019s Next?\n\n\nWe aim to add an array of capabilities we envisioned but were not able to include in our MVP (demo) due to the short-term time-frame of the hackathon. Once our experience is complete, a researcher and M.D. on our team, who presented the idea for the project, aims to use it in clinical trials to treat chronic pain patients in outpatient pain clinics. Pain medicine practices are growing as the opiate epidemic continues to rise. This serves as one way to help alleviate one source of the problem. This mechanism for VR therapy could also be used for preventing patients from going on opioids, by adding this to acute pain regiments. \n\n\n## How we built it\n\n\nThe team logo was designed in Adobe Illustrator. To provide haptic feedback during the experience, users are fitted with a Woojer strap. This strap is synchronized to ambient music that plays within the virtual environment. The background music was taken from a royalty-free music site <http://jamen.do/t/365392> - \u201cStellar\u201d by Fortadelis. The first asset that was placed in the scene was the mirror. Then the avatar was created using Rhino3D. The main animation that was created was the elongation of the torso of the avatar. The animation was made within Unity3D. The key action within the program is reflection of the virtual body within the mirror. The shader of the mirror was modified to allow correct visualization of the avatar. We then created an audio script using online-voice-recorder.com, which initiates about 10 seconds into the experience, to allow time for the subject to place the HMD on their head and acclimate to the environment. This audio script instructs the participant to touch a button with their hands to start the exercise. The core of the experience was built in Unity using HTC Vive Pro and Leap Motion hand trackers. Leap Motion was used to provide humanoid upper extremities with accurate tracking of hands. The scene was taken from the unity asset store. \n\n\n## Challenges we ran into\n\n\nWe initially attempted to use Intel RealSense depth cameras to perform real-time body-tracking; however, we eventually discovered that we needed to use a paid version of OpenCV plugin for Unity to implement use of the sensor.\n\n\nThough virtual embodiment is most effect with a humanoid avatar with full body tracking, we could not incorporate leg tracking (due to the body-tracking issue previously-mentioned). As a result, we decided to design the experience as a stationary exercise through a sequence of repetitive arm movements. Additionally, we encountered issues with adding arms to the avatar. The mirror also has issues reflecting the virtual body with the current shader that it came with. It required further modifications.\n\n\nA considerable amount of time was spent correcting issues with Git LFS when making commits to the repository (to ensure that all team members were able to successfully commit to it).\n\n\n## Accomplishments that we're proud of\n\n\nWe came together around a passion to help patients based by medical VR research. During our first day we focused on the most important feature of the intervention. This was also a great learning experience for all of us. \n\n\n## Literature behind the intervention\n\n\nSeveral papers were instrumental in the development of the intervention. Rosink et al. demonstrated that a projector-based VR experience could be used to help understand body perception in those with chronic lower back pain (Roosink et al., 2015). We then incorporated elongation based on inspiration from a paper from Dr. Mel Slater, which demonstrated a sense of presence with a limb three times its original size(Kilteni, Normand, Sanchez-Vives, & Slater, 2012). Dr. Maria V. Sanchez-Vives recently published a brilliant pilot study using virtual embodiment and modifications of both size and opacification of upper extremity to treat chronic pain(Matamala-Gomez, Gonzalez, Slater, & Sanchez-Vives, 2018). Virtual embodiment is the process of causing the illusion of body ownership to a virtual avatar using visuomotor synchrony and a virtual mirror. Recently a randomized trial published in the journal Neurology demonstrating mild analgesia using virtual embodiment for neuropathic pain from a spinal injury(Pozeg et al., 2017). We also incorporated a back exercise based on several papers demonstrated benefit with VR based physical therapy(Thomas, France, Applegate, Leitkam, & Walkowski, 2016)(Trost et al., 2015).\n\n\n## Team\n\n\n**Brandon Birckhead MD, Principal Investigator**: I am the Co-Chair of the Virtual Medicine Conference at Cedars-Sinai Medical Center and currently running a large randomized clinical trial for chronic pain. I have designed two VR programs one for dental anxiety and one for breast cancer education. I have also designed several clinical studies: randomized VR pain study using the cold pressor test, randomized breast cancer patient education, and taken part in the creation of guidelines for immersive therapeutic clinical trials along with a cohort of international therapeutic VR experts, called the VR-CORE (Virtual Reality \u2013 Clinical Outcomes Research Experts). I had advised several physicians and medical students in the development process of VR clinical trials. \n\n\n**Erika Gangware, 3D Designer and Unity Developer**. I am a 3D Product Designer and a Unity Developer working primarily on mobile and vive platforms. As a 3D designer I have made biomedical renderings in the oncology space and use 3D software to render anatomical models. As a Unity Developer I have worked on biomedical VR files that aim to improve provider workflow, and plan to design and develop files that improve patient outcomes.\n\n\n**Deborah Navarro M.S., Visionary & Strategist**: I am a Product Manager and Hyperloop Business Leader. I founded a research group/startup consisting of engineers from UT and MIT. Together, we took home the Innovation award at SpaceX\u2019s Hyperloop Pod Comp II for our breakthroughs in air-levitation. I have a background in biological sciences, technology commercialization, and engineering. I aim to augment the relationship between tech, sustainability, and artificial intelligence by building products that are mindful of their position in the world and implement them in a way that allows them to connect people. I am exploring how AR/VR can help achieve this. \n\n\n**Anna Mu\u00f1oz-Farr\u00e9 Biomedical Engineer:** I am a Research Affiliate at Massachusetts Institute of Technology, where I developed my MS thesis in the area of cardiology. I focus in product development, and I been in research and collaborated with different hospitals, both in Boston and Barcelona, Spain. I am passionate about how technology will change the way we treat illnesses. \n\n\n**Michelle Ju\u00e1rez, Principal Systems Engineer - Applied Research:** I\u2019m a Systems Engineer in the aerospace industry, focusing on applied research and development, modeling and simulation, system design, and radar analysis. I have a background in research in algorithm development and medical image analysis of fMRI brain data, and am currently collaborating on a side project for the development of a novel, holographic heads-up display for vehicles. I am passionate about physics/astronomy/data analytics/biotech/disruptive technologies, and am eager to explore innovating my career in AR/VR/XR development.\n\n\n**Nader Shokair Research Scientist**, B.A./Pre-Med PostBacc - Research Scientist: I am a premed student with an expertise in physical sciences; namely, optical systems, holography, and analog circuit applications of low-level circuit logic, along with material fabrication and engineering. I enjoy a multifarious set of academic and personal ambitions. My undergraduate degree is in philosophy with an emphasis on cognitive science and ethics. I am passionate about technology, particularly machine learning/CNNs and computer vision and believe the future of medicine will be built upon AR/VR/XR. My passions include a range of motorsports from motoX to wheel-to-wheel road racing and have been working on my own race car project for over 10 years. Using this as a platform I hope to explore the technical problems surrounding the design of HMD-free holography in the controlled space of a vehicle cockpit, to then expand into the clinical settings I wish to see develop into full XR patient/doctor experiences.\n**Erika Gangware Unity Developer and 3D Designer:**  \n\n\n\n## What we learned\n\n\nLeap motion\nAnimation\nShaders for reflective mirrors\n\n\n## What's next for StretchXR?\n\n\nWe aim to add an array of capabilities we envisioned but were not able to include in our MVP (demo) due to the limited time-frame of the hackathon. Once our experience is complete, a researcher and M.D. on our team, who presented the idea for the project, aims to use it in clinical trials to treat chronic pain patients in outpatient pain clinics. Pain medicine practices are growing as the opiate epidemic continues to rise. This serves as one way to help alleviate one source of the problem. This mechanism for VR therapy could also be used for preventing patients from going on opiods, by adding this to acute pain regiments\n\n\n## References\n\n\nCarey, T. S., Freburger, J. K., Holmes, G. M., Castel, L., Darter, J., Agans, R., \u2026 Jackman, A. (2009). A long way to go: practice patterns and evidence in chronic low back pain care. Spine, 34(7), 718\u2013724. <https://doi.org/10.1097/BRS.0b013e31819792b0>\nKilteni, K., Normand, J.-M., Sanchez-Vives, M. V., & Slater, M. (2012). Extending Body Space in Immersive Virtual Reality: A Very Long Arm Illusion. PLoS ONE, 7(7), e40867. <https://doi.org/10.1371/journal.pone.0040867>\nMatamala-Gomez, M., Gonzalez, A. M. D., Slater, M., & Sanchez-Vives, M. V. (2018). Decreasing pain ratings in chronic arm pain through changing a virtual body: different strategies for different pain types. The Journal of Pain\u202f: Official Journal of the American Pain Society, 0(0). <https://doi.org/10.1016/j.jpain.2018.12.001>\nPozeg, P., Palluel, E., Ronchi, R., Solc\u00e0, M., Al-Khodairy, A.-W., Jordan, X., \u2026 Blanke, O. (2017). Virtual reality improves embodiment and neuropathic pain caused by spinal cord injury. Neurology, 89(18), 1894\u20131903. <https://doi.org/10.1212/WNL.0000000000004585>\nRoosink, M., McFadyen, B. J., H\u00e9bert, L. J., Jackson, P. L., Bouyer, L. J., & Mercier, C. (2015). Assessing the Perception of Trunk Movements in Military Personnel with Chronic Non-Specific Low Back Pain Using a Virtual Mirror. PLOS ONE, 10(3), e0120251. <https://doi.org/10.1371/journal.pone.0120251>\nThomas, J. S., France, C. R., Applegate, M. E., Leitkam, S. T., & Walkowski, S. (2016). Feasibility and Safety of a Virtual Reality Dodgeball Intervention for Chronic Low Back Pain: A Randomized Clinical Trial. The Journal of Pain, 17(12), 1302\u20131317. <https://doi.org/10.1016/j.jpain.2016.08.011>\nTrost, Z., Zielke, M., Guck, A., Nowlin, L., Zakhidov, D., France, C. R., & Keefe, F. (2015). The promise and challenge of virtual gaming technologies for chronic pain: the case of graded exposure for low back pain. Pain Management, 5(3), 197\u2013206. <https://doi.org/10.2217/pmt.15.6>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/concordia-module-1-pgmicy",
            "title": "Concordia Module #1",
            "blurb": "Playing solar systems as musical instruments to celebrate the work of Johannes Kepler",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/303232652?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "mobomusic",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/9666835?height=180&v=4&width=180"
                },
                {
                    "name": "Scott Black",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/35820390?height=180&v=4&width=180"
                },
                {
                    "name": "Andrew Luck",
                    "about": "",
                    "photo": "https://graph.facebook.com/10155356325047873/picture?height=180&width=180"
                },
                {
                    "name": "Kelly Snook",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/16545039?height=180&v=4&width=180"
                },
                {
                    "name": "Nick Fox-Gieg",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/771175?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "chunity",
                "lightning-artist-toolkit",
                "lumen-sdk",
                "max-msp",
                "websocket-#"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<h2>What it does</h2>\n<p>Playing solar systems as musical instruments</p>\n<h2>How I built it</h2>\n<h2>Challenges I ran into</h2>\n<h2>Accomplishments that I'm proud of</h2>\n<h2>What I learned</h2>\n<h2>What's next for Concordia Module #1</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n## What it does\n\n\nPlaying solar systems as musical instruments\n\n\n## How I built it\n\n\n## Challenges I ran into\n\n\n## Accomplishments that I'm proud of\n\n\n## What I learned\n\n\n## What's next for Concordia Module #1\n\n\n"
        },
        {
            "source": "https://devpost.com/software/cadence-gs2bcw",
            "title": "Cadence",
            "blurb": "We create volumetric avatars in AR that improve second language learning and onboarding for international travelers.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/C19B-jHC3PU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Mike H",
                    "about": "I pitched the idea, built the team according to the product needs, wrote the copy, and managed the product from idea to MVP.",
                    "photo": "https://media.licdn.com/dms/image/C5603AQHifx7-vsnaNw/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=_-6DKeEkR2RjL4czmRhoR4iIRchImpdpIggOBi7dseE&v=beta&width=180"
                },
                {
                    "name": "Becky Lane",
                    "about": "I created volumetric video using DepthKit and the Microsoft Xbox Kinnect. ",
                    "photo": "https://graph.facebook.com/10154715855227471/picture?height=180&width=180"
                },
                {
                    "name": "ShuningWang0411",
                    "about": "",
                    "photo": "https://graph.facebook.com/1111878235640682/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "android",
                "c#",
                "depthkit",
                "ios",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We are inspired by the history of narrative based language immersion education and pioneering enhanced reality ESL programs like MondlyVR and immerse.me.</p>\n<h2>What it does</h2>\n<p>We create AR experiences that use empirical principles of learning design to enhance learning outcomes for second language acquisition. Our technology has many potential use cases, but for this project we have chosen the familiar situation of participants traveling to an international hackathon.</p>\n<p>Before arriving to the Hackathon, our user can utilize our technology to practice asking for directions in their native language. Upon arriving, our user can scan the MIT Reality Virtually logo using Vuforia to see a volumetric avatar which will tell them where the Hackathon is located, and how to find it. </p>\n<p>Our AR solution can be layered over any conference logo. Entire FAQs could be integrated into our structure to streamline the on-boarding experience for international attendees, as well as help users practice foreign language skills in preparation for international events.</p>\n<h2>How we built it</h2>\n<p>We built the AR component using the Vuforia augmented reality SDK and Unity. We recorded the volumetric video using Depthkit and a Kinect. Special thanks to the Bok Center at Harvard for giving us professional studio access, and to the small nook on the 5th floor of the Media Lab for giving us impromptu studio access. </p>\n<h2>Challenges we ran into</h2>\n<p>Originally we envisioned an entire mixed reality curriculum targeting immersive language learning with use cases taken from everyday life. Based on the Gradual Release of Responsibility model (GRR) fr 2nd language learning, our would have three phases. First, a period of modeled instruction, in which users would watch first person 360 videos in VR of common travel situations, such as asking for directions. Next would be a section of guided instruction, where they would be put in the scene again, and it would only progress after they accurately accomplished the learning objective with a prompt. Lessons would then be reinforced through an independent learning model, where users could project avatars from the situations they watched into AR using Vuforia. Even without a headset, users could practice the situations with realistic avatars on any flat plane.  Stretch goals included a peer learning aspect with VR video-conferencing, where multiple non-native speakers could replay each situation with a partner, as well as a full immersion based chat room. </p>\n<p>Unfortunately, we ran into technical challenges from the beginning. No one on our team is a professional coder. Our only trained engineer left the team originally, came back, and then was sick again on the final day. Because of this we were not able to integrate 360 video into Unity or incorporate any of the voice recognition or branching conversational paths as we had planned.</p>\n<p>We pivoted to focus on the AR component using Vuforia, and were unable to get it to work on Android phones. All hope seemed lost as nothing was working the night before submission. </p>\n<h2>What we learned</h2>\n<p>Without any prior experience with volumetric video or AR, we recorded over fifty interactions with six different avatars in six out of the 35 languages represented here today. We learned about volumetric capture, AR development, and deployment on mobile phones.</p>\n<h2>What's next for Cadence</h2>\n<p>We're going to build our original vision!</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe are inspired by the history of narrative based language immersion education and pioneering enhanced reality ESL programs like MondlyVR and immerse.me.\n\n\n## What it does\n\n\nWe create AR experiences that use empirical principles of learning design to enhance learning outcomes for second language acquisition. Our technology has many potential use cases, but for this project we have chosen the familiar situation of participants traveling to an international hackathon.\n\n\nBefore arriving to the Hackathon, our user can utilize our technology to practice asking for directions in their native language. Upon arriving, our user can scan the MIT Reality Virtually logo using Vuforia to see a volumetric avatar which will tell them where the Hackathon is located, and how to find it. \n\n\nOur AR solution can be layered over any conference logo. Entire FAQs could be integrated into our structure to streamline the on-boarding experience for international attendees, as well as help users practice foreign language skills in preparation for international events.\n\n\n## How we built it\n\n\nWe built the AR component using the Vuforia augmented reality SDK and Unity. We recorded the volumetric video using Depthkit and a Kinect. Special thanks to the Bok Center at Harvard for giving us professional studio access, and to the small nook on the 5th floor of the Media Lab for giving us impromptu studio access. \n\n\n## Challenges we ran into\n\n\nOriginally we envisioned an entire mixed reality curriculum targeting immersive language learning with use cases taken from everyday life. Based on the Gradual Release of Responsibility model (GRR) fr 2nd language learning, our would have three phases. First, a period of modeled instruction, in which users would watch first person 360 videos in VR of common travel situations, such as asking for directions. Next would be a section of guided instruction, where they would be put in the scene again, and it would only progress after they accurately accomplished the learning objective with a prompt. Lessons would then be reinforced through an independent learning model, where users could project avatars from the situations they watched into AR using Vuforia. Even without a headset, users could practice the situations with realistic avatars on any flat plane. Stretch goals included a peer learning aspect with VR video-conferencing, where multiple non-native speakers could replay each situation with a partner, as well as a full immersion based chat room. \n\n\nUnfortunately, we ran into technical challenges from the beginning. No one on our team is a professional coder. Our only trained engineer left the team originally, came back, and then was sick again on the final day. Because of this we were not able to integrate 360 video into Unity or incorporate any of the voice recognition or branching conversational paths as we had planned.\n\n\nWe pivoted to focus on the AR component using Vuforia, and were unable to get it to work on Android phones. All hope seemed lost as nothing was working the night before submission. \n\n\n## What we learned\n\n\nWithout any prior experience with volumetric video or AR, we recorded over fifty interactions with six different avatars in six out of the 35 languages represented here today. We learned about volumetric capture, AR development, and deployment on mobile phones.\n\n\n## What's next for Cadence\n\n\nWe're going to build our original vision!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/worldwhispers",
            "title": "WorldWhispers",
            "blurb": "Introducing one of the World's First Augmented Audio Reality Apps for Bose\u2122 AR Frames",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/mcYI1439JcE?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "AR Cloud will include spatial audio",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/362/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Peter Iordanov",
                    "about": "I worked on building the Unity project and using the Bose AR SDK to interpret a series of head gestures and finger taps to navigate through our story.",
                    "photo": "https://avatars.githubusercontent.com/u/5401480?height=180&v=3&width=180"
                },
                {
                    "name": "Adriana Vecchioli",
                    "about": "I worked on the front-end, UX/UI, name, story & assets.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/723/datas/profile.png"
                },
                {
                    "name": "Marlon Fuentes",
                    "about": "",
                    "photo": "https://graph.facebook.com/10155750215229904/picture?height=180&width=180"
                },
                {
                    "name": "Sunish Gupta",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3a4a0b424eb49465263ff3af0f859471?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Dorothee Schmidt",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b51bceffdee3ece5f4e25c35672fd334?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "bose",
                "ios",
                "unity",
                "xcode"
            ],
            "content_html": "<div>\n<p><strong>Augmenting the real world with contextual audio.</strong> \nLocation: MIT Media Lab, 6th Floor main hall. </p>\n<h1>Design Challenge</h1>\n<ul>\n<li>How might we make immersive experiences accessible (abilities, price) ?</li>\n<li>How might we reduce injuries from mobile phone distraction? </li>\n<li>How might we encourage people to walk? </li>\n<li>How might we give local residents an opportunity to tell their story?</li>\n</ul>\n<h3>Inspiration</h3>\n<p>Cities are \"smart\" and getting smarter, we're told. As information processing becomes embedded within and distributed throughout ever broader regions of urban space, we, humans, arrive at an opportune moment to take back the streets from cars and introduce a new opportunity for optimal pedestrian experiences using augmented reality. There is no higher fidelity than the real world. By augmenting our surroundings with audio, we can preserve a connection with reality while contextualizing it with human stories, critical information, and new paradigms in user experiences and interfaces. </p>\n<h2>What it does</h2>\n<p>World Whisper allows users to take the scenic route and enjoy the sights and sounds of reality with an additional layer of audio augmentation that tells you stories, and provides contextual information about your surroundings. </p>\n<h2>How we built it</h2>\n<p>Using Bose AR frames with projected audio and positional tracking, our team created scenes, menus, and audio pins in Unity for IOS using the Bose SDK. </p>\n<h2>Challenges we ran into</h2>\n<p>With new hardware that was only released to the public weeks before the hackathon, our team ventured into the unknown both on the software side as well as by having to explore new UX paradigms with audio only interaction. </p>\n<p>Technical:</p>\n<ul>\n<li>Bose AR SDK is early in development </li>\n<li>Had to implement our own gesture controls </li>\n<li>Calibrating device to keep center point of view</li>\n<li>Location services implementation </li>\n<li>Emerging paradigms in audio UX / UI</li>\n</ul>\n<h2>Accomplishments that we're proud of</h2>\n<p>Coding the head gestures ourselves was quite a challenge. Also, designing audio AR experience for comfort and creating intuitive interactions led us to dig deep into the human experience for queues that would start a conversation between the technology and the user. </p>\n<h2>What we learned</h2>\n<p>Immersive technology should not be exclusive to visual media. By exploring audio AR our team has learned that it is possible to create menus, icons, and conversations with users without a graphic visualization. </p>\n<h2>What's next for World Whispers</h2>\n<p>After further testing of our build, our team is eager to improve our ability to start a conversation with users and engage them in optimal experiences leveraging their surroundings. We will also collect a survey of our UX for Audio AR challenges and how we solved them. </p>\n</div>",
            "content_md": "\n**Augmenting the real world with contextual audio.** \nLocation: MIT Media Lab, 6th Floor main hall. \n\n\n# Design Challenge\n\n\n* How might we make immersive experiences accessible (abilities, price) ?\n* How might we reduce injuries from mobile phone distraction?\n* How might we encourage people to walk?\n* How might we give local residents an opportunity to tell their story?\n\n\n### Inspiration\n\n\nCities are \"smart\" and getting smarter, we're told. As information processing becomes embedded within and distributed throughout ever broader regions of urban space, we, humans, arrive at an opportune moment to take back the streets from cars and introduce a new opportunity for optimal pedestrian experiences using augmented reality. There is no higher fidelity than the real world. By augmenting our surroundings with audio, we can preserve a connection with reality while contextualizing it with human stories, critical information, and new paradigms in user experiences and interfaces. \n\n\n## What it does\n\n\nWorld Whisper allows users to take the scenic route and enjoy the sights and sounds of reality with an additional layer of audio augmentation that tells you stories, and provides contextual information about your surroundings. \n\n\n## How we built it\n\n\nUsing Bose AR frames with projected audio and positional tracking, our team created scenes, menus, and audio pins in Unity for IOS using the Bose SDK. \n\n\n## Challenges we ran into\n\n\nWith new hardware that was only released to the public weeks before the hackathon, our team ventured into the unknown both on the software side as well as by having to explore new UX paradigms with audio only interaction. \n\n\nTechnical:\n\n\n* Bose AR SDK is early in development\n* Had to implement our own gesture controls\n* Calibrating device to keep center point of view\n* Location services implementation\n* Emerging paradigms in audio UX / UI\n\n\n## Accomplishments that we're proud of\n\n\nCoding the head gestures ourselves was quite a challenge. Also, designing audio AR experience for comfort and creating intuitive interactions led us to dig deep into the human experience for queues that would start a conversation between the technology and the user. \n\n\n## What we learned\n\n\nImmersive technology should not be exclusive to visual media. By exploring audio AR our team has learned that it is possible to create menus, icons, and conversations with users without a graphic visualization. \n\n\n## What's next for World Whispers\n\n\nAfter further testing of our build, our team is eager to improve our ability to start a conversation with users and engage them in optimal experiences leveraging their surroundings. We will also collect a survey of our UX for Audio AR challenges and how we solved them. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/sculpt-trace",
            "title": "Sculpt Trace",
            "blurb": "Imagine yourself learning and creating a perfect sculptures by tracing and manipulating 3D holograms in real time!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/GNAUCu9EobQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Nhan Tran",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/616/datas/profile.JPG"
                },
                {
                    "name": "Rika Nakayama",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/04b893efe3738c15c599ec085ed4b242?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Junxi (Jarvis) Xie",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/754/datas/profile.jpg"
                },
                {
                    "name": "Stephanie Ng",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/7038806?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "ar",
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>The BIG picture (1 minute short animation):</h2>\n<p><a href=\"https://www.youtube.com/watch?v=GNAUCu9EobQ\" rel=\"nofollow\">https://www.youtube.com/watch?v=GNAUCu9EobQ</a></p>\n<p>Demo: <a href=\"https://youtu.be/GNAUCu9EobQ?t=60\" rel=\"nofollow\">https://youtu.be/GNAUCu9EobQ?t=60</a></p>\n<h2>Inspiration</h2>\n<p>Humans have traced images on paper for centuries. It has been practiced since medieval times, and it is essential to artists, architects, animators, and many others who practice visual art. One member of our team is an artist and aspiring sculptor. She frequently uses 2D tracing device (light table, tracing paper, etc) in making illustrations. She and many of her classmates wished to have a 3D version of tracing device to assist sculpting. Realizing this pain point, we brainstormed together and got excited that modern mixed-reality technology could be the key to the problem!</p>\n<h2>What it does</h2>\n<p>Now, it's time to move history to the next step. Imagine a light table or tracing paper for sculptures. With mixed reality, you can create a perfect sculpture by tracing a 3D hologram. This HoloLens application projects a 3D model onto your table for you to trace, and guides you through the sculpting process step by step starting from a basic model. You can increase the level of detail progressively. We believe this tool will be very useful in education, art practices, and for anyone creating a personal gift. You can sculpt your favorite characters or anything your passion guides you! Plus, utilizing hand gestures and voice commands, you don't have to touch your computers/book/model references with dirty hands!</p>\n<h2>How we built it</h2>\n<p>Our team consists of 3 software students and a 2D artist/aspiring sculptor. In the first 5 hours, we focused mainly on the problem space, brainstormed the project blueprint template that we learned from the PTC workshop, and designed our user journey + UX storyboard. After having fun talking about multiple solutions and doing UX roleplaying, we prioritized and narrowed our list of features! We ranked the features based on 1. Impact, 2. Technical Challenge, 3. Art Vibe, and 4. Innovation. Then, we assigned each other tasks. Our youngest team member took the initiative to learn about 3D modeling. Our designer guided us in the UX journey and designed various components. Our two developers learned Microsoft HoloLens development on the go and turned our idea into fruition!</p>\n<p>SDKs, APIs, free assets:</p>\n<ul>\n<li>Microsoft HoloLens &amp; Unity 2017.4+</li>\n<li>HoloToolkit 2017.4.3.0</li>\n<li>Mesh simplifier: <a href=\"https://github.com/Whinarn/UnityMeshSimplifier\" rel=\"nofollow\">https://github.com/Whinarn/UnityMeshSimplifier</a></li>\n<li>MKToon Shader: <a href=\"https://assetstore.unity.com/packages/vfx/shaders/mk-toon-90213\" rel=\"nofollow\">https://assetstore.unity.com/packages/vfx/shaders/mk-toon-90213</a></li>\n<li>Blender 2.79b</li>\n<li>Mountain lion: <a href=\"https://free3d.com/3d-model/mountainlion-v2--107153.html\" rel=\"nofollow\">https://free3d.com/3d-model/mountainlion-v2--107153.html</a></li>\n<li>Santa Claus: <a href=\"https://www.turbosquid.com/3d-models/3d-model-santa-claus-1233978?fbclid=IwAR1V7jdDIHHGUUhtdPCf0rvjoQGOXkRcZLu7Pf43wEhnnGCcnswBDKF-J50\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/3d-model-santa-claus-1233978?fbclid=IwAR1V7jdDIHHGUUhtdPCf0rvjoQGOXkRcZLu7Pf43wEhnnGCcnswBDKF-J50</a></li>\n<li>Rocket: - <a href=\"https://free3d.com/3d-model/rocket-781772.html\" rel=\"nofollow\">https://free3d.com/3d-model/rocket-781772.html</a></li>\n</ul>\n<h2>Challenges we ran into</h2>\n<p>We ran into various challenges. The most frequent one is version mismatch and \"breaking\" changes in our application's dependencies (e.g., some require C# 4.6 experimental vs 3.5 stable, some APIs got deprecated in Unity 2018, and many more). Mixed reality development is a rapidly growing field and every new update seems to significantly alter the development process. We also ran into problems with the HoloToolkit and vuforia.</p>\n<h2>Accomplishments</h2>\n<p>Although we ran out of time to implement all of our desired features, we were so excited to create a minimum viable product that can guide aspiring sculptors through the process and reduce their sculpting time significantly. This MVP allowed us to ask other participants to test and quickly iterate based on their feedback. </p>\n<h2>Future Work:</h2>\n<ol>\n<li><p>In the future, we hope to successfully integrate computer vision technology to this application so that it can provide continuous feedback on the user's performance. While sculpting, you can get scored on how accurately you are following the model and can receive constructive feedback+hints to improve your skills! Sculpting is a time-consuming process that requires lots of patience and high attention to details. We believe our mixed reality application can educate a wide range of aspiring artists and spark interest in the future generation of creative sculptors!</p></li>\n<li><p>Sculptors have preferences of additive process (adding material to form shape) and subtractive process (remove/carve material to form shape). Also, some materials such as wood are subtractive process, and some others are additive process. To address these user needs, we can add feature that makes object to become larger in additive process and become smaller in subtractive process.</p></li>\n<li><p>We also aim to make this tool more accessible in professional art practices and education. We will investigate how to 3D scan the process of professional sculptors in addition to using computer-generated step by step instructions.</p></li>\n</ol>\n</div>",
            "content_md": "\n## The BIG picture (1 minute short animation):\n\n\n<https://www.youtube.com/watch?v=GNAUCu9EobQ>\n\n\nDemo: <https://youtu.be/GNAUCu9EobQ?t=60>\n\n\n## Inspiration\n\n\nHumans have traced images on paper for centuries. It has been practiced since medieval times, and it is essential to artists, architects, animators, and many others who practice visual art. One member of our team is an artist and aspiring sculptor. She frequently uses 2D tracing device (light table, tracing paper, etc) in making illustrations. She and many of her classmates wished to have a 3D version of tracing device to assist sculpting. Realizing this pain point, we brainstormed together and got excited that modern mixed-reality technology could be the key to the problem!\n\n\n## What it does\n\n\nNow, it's time to move history to the next step. Imagine a light table or tracing paper for sculptures. With mixed reality, you can create a perfect sculpture by tracing a 3D hologram. This HoloLens application projects a 3D model onto your table for you to trace, and guides you through the sculpting process step by step starting from a basic model. You can increase the level of detail progressively. We believe this tool will be very useful in education, art practices, and for anyone creating a personal gift. You can sculpt your favorite characters or anything your passion guides you! Plus, utilizing hand gestures and voice commands, you don't have to touch your computers/book/model references with dirty hands!\n\n\n## How we built it\n\n\nOur team consists of 3 software students and a 2D artist/aspiring sculptor. In the first 5 hours, we focused mainly on the problem space, brainstormed the project blueprint template that we learned from the PTC workshop, and designed our user journey + UX storyboard. After having fun talking about multiple solutions and doing UX roleplaying, we prioritized and narrowed our list of features! We ranked the features based on 1. Impact, 2. Technical Challenge, 3. Art Vibe, and 4. Innovation. Then, we assigned each other tasks. Our youngest team member took the initiative to learn about 3D modeling. Our designer guided us in the UX journey and designed various components. Our two developers learned Microsoft HoloLens development on the go and turned our idea into fruition!\n\n\nSDKs, APIs, free assets:\n\n\n* Microsoft HoloLens & Unity 2017.4+\n* HoloToolkit 2017.4.3.0\n* Mesh simplifier: <https://github.com/Whinarn/UnityMeshSimplifier>\n* MKToon Shader: <https://assetstore.unity.com/packages/vfx/shaders/mk-toon-90213>\n* Blender 2.79b\n* Mountain lion: <https://free3d.com/3d-model/mountainlion-v2--107153.html>\n* Santa Claus: <https://www.turbosquid.com/3d-models/3d-model-santa-claus-1233978?fbclid=IwAR1V7jdDIHHGUUhtdPCf0rvjoQGOXkRcZLu7Pf43wEhnnGCcnswBDKF-J50>\n* Rocket: - <https://free3d.com/3d-model/rocket-781772.html>\n\n\n## Challenges we ran into\n\n\nWe ran into various challenges. The most frequent one is version mismatch and \"breaking\" changes in our application's dependencies (e.g., some require C# 4.6 experimental vs 3.5 stable, some APIs got deprecated in Unity 2018, and many more). Mixed reality development is a rapidly growing field and every new update seems to significantly alter the development process. We also ran into problems with the HoloToolkit and vuforia.\n\n\n## Accomplishments\n\n\nAlthough we ran out of time to implement all of our desired features, we were so excited to create a minimum viable product that can guide aspiring sculptors through the process and reduce their sculpting time significantly. This MVP allowed us to ask other participants to test and quickly iterate based on their feedback. \n\n\n## Future Work:\n\n\n1. In the future, we hope to successfully integrate computer vision technology to this application so that it can provide continuous feedback on the user's performance. While sculpting, you can get scored on how accurately you are following the model and can receive constructive feedback+hints to improve your skills! Sculpting is a time-consuming process that requires lots of patience and high attention to details. We believe our mixed reality application can educate a wide range of aspiring artists and spark interest in the future generation of creative sculptors!\n2. Sculptors have preferences of additive process (adding material to form shape) and subtractive process (remove/carve material to form shape). Also, some materials such as wood are subtractive process, and some others are additive process. To address these user needs, we can add feature that makes object to become larger in additive process and become smaller in subtractive process.\n3. We also aim to make this tool more accessible in professional art practices and education. We will investigate how to 3D scan the process of professional sculptors in addition to using computer-generated step by step instructions.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/who-knows-wfhsue",
            "title": "Who Knows",
            "blurb": "AR Diverse Appliance Control",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Private user",
                    "about": "",
                    "photo": "https://devpost-challengepost.netdna-ssl.com/assets/defaults/no-avatar-180-caa7628ae0aae09831858639d32ace2a.png"
                }
            ],
            "built_with": [
                "esp32",
                "google-home",
                "iot",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>-</p>\n<h2>What it does</h2>\n<h2>How I built it</h2>\n<h2>Challenges I ran into</h2>\n<h2>Accomplishments that I'm proud of</h2>\n<h2>What I learned</h2>\n<h2>What's next for Who Knows</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n-\n\n\n## What it does\n\n\n## How I built it\n\n\n## Challenges I ran into\n\n\n## Accomplishments that I'm proud of\n\n\n## What I learned\n\n\n## What's next for Who Knows\n\n\n"
        },
        {
            "source": "https://devpost.com/software/twittervrse",
            "title": "TwitterVRse",
            "blurb": "A new way to interact with tweets",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/vNwWgMvNE1I?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "View today&#39;s trending topics",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/350/datas/original.png"
                },
                {
                    "title": "Welcome to the Twitter Universe!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/351/datas/original.png"
                },
                {
                    "title": "Four twitter birds are reading four trending tweets in the topic you just selected to you",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/352/datas/original.png"
                },
                {
                    "title": "Point your hand at this block and pull the trigger to start speaking about your opinion!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/353/datas/original.png"
                },
                {
                    "title": "Your message has being recognized and shown in front of you!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/345/datas/original.png"
                },
                {
                    "title": "Your message/tweet is published on your Twitter account in real time!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/347/datas/original.PNG"
                },
                {
                    "title": "Tweets published by visitors at the hackathon Demo day (screenshotted 9P.M. Jan 21, 2019)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/348/datas/original.PNG"
                },
                {
                    "title": "The team (Austin Edelman, Lucy Yunxing Liao, Dana Elkis, Simon Zirui Guo, Rebecca Skurnik)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/747/361/datas/original.jpg"
                },
                {
                    "title": "Project logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/706/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Simon Guo",
                    "about": "I worked on creating interactions between users and unity objects, constructing classes and prefabs, and putting all components together.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/437/datas/profile.jpg"
                },
                {
                    "name": "danaelkis",
                    "about": "",
                    "photo": "https://graph.facebook.com/10161214888000696/picture?height=180&width=180"
                },
                {
                    "name": "Yunxing Liao",
                    "about": "",
                    "photo": "https://graph.facebook.com/1190682754413226/picture?height=180&width=180"
                },
                {
                    "name": "REBECCA SKURNIK",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2c80d053042f74f865ca1457367a8334?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Austin Edelman",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b159eb8bd72b7a1488b8699999604ad5?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "c#",
                "cinema4d",
                "cortana",
                "maya",
                "oculus",
                "oculus-integration",
                "twitter",
                "twity",
                "unity"
            ],
            "content_html": "<div>\n<h2>Team info</h2>\n<p>This project was created from Jan 18-20, 2019 at the <a href=\"https://rv2019.devpost.com/\" rel=\"nofollow\">Reality Virtually Hackathon at the MIT Media Lab</a>.</p>\n<ul>\n<li><strong>Simon (Zirui) Guo</strong>: I'm a high school senior currently studying in Toronto, Canada and I love attending hackathons to explore new technologies and build cool projects. Particularly, I want to explore better interactions between humans and machines.</li>\n<li><strong>Rebecca Skurnik</strong>: Rebecca is master's student at the Interactive Telecommunications Program at NYU. She is passionate about creative technology and working in the intersection of the physical and digital world.</li>\n<li><strong>Lucy Liao</strong>: I'm a sophomore majoring in computer science at MIT interested in learning more about virtual reality.</li>\n<li><strong>Austin Edelman</strong>: A sophomore at MIT interested in creating amazing virtual worlds that people can live in. Consequently, I decided to train and become a biomedical engineer, majoring in computer science and electrical engineering and choosing a pre-medical track.</li>\n<li><strong>Dana Elkis</strong>: Dana Elkis is a Graphic Designer / Visual Artist from Tel Aviv (Israel), currently living in New York City. Master student at Interactive Telecommunications Program, NYU, New York.</li>\n</ul>\n<h2>Judging information</h2>\n<ul>\n<li>Location: Table 22, 6th-floor Vive Lounge, Media Lab E14.</li>\n<li>Category: Art, Media and Entertainment.</li>\n</ul>\n<h2>Inspiration</h2>\n<p>We believe immersive technologies have the potential to upgrade our current experiences and we would like to experience it with social media. Social media posting has been mechanical and the same for every platform. We want to bring the experience to the next level by immersing the user in a Twitter universe and make them feel that they are apart of the Twitter community.</p>\n<h2>What it does</h2>\n<p>TwitterVRse is a Virtual Reality experience that seeks to transform the way that we interact with the social media platform Twitter. We have created a VR interface that allows users to access tweets using virtual birds, interact with tweets like physical media, and post your own tweets through voice input.</p>\n<h2>User experience</h2>\n<p>The video above demonstrates a complete session.</p>\n<ol>\n<li>The user puts on an Oculus Rift and uses controllers to open TwitterVRse.</li>\n<li>The top 5 trending topics will be listed, in descending order of popularities. This will correspond with the number of birds in a flock. Users point at \"continue\" to enter the next scene.</li>\n<li>The Twitter universe will be presented with 5 flocks of birds scattered around in the environment. Those flocks have different numbers of birds, which corresponds to the 5 trending topics listed before (e.g. flocks with 5 birds is the most trending topic). Users point at the flock that he/she wants to investigate by pressing down a trigger.</li>\n<li>The birds in the flock will come to the user and, one by one reads out tweets by converting text to speech. Once users finished listening to the tweets, he/she can press the trigger to enter the next scene.</li>\n<li>In this scene, the user can record his/her own opinion by starting the record button and holding down the trigger while speaking. The user's voice will be recognized as text and posted on his/her Twitter account immediately.</li>\n</ol>\n<h2>How we built it</h2>\n<ul>\n<li>Hardware: Oculus Rift system (headset, Touch controllers, trackers)</li>\n<li>Engine: Unity</li>\n<li>SDKs: Oculus Integration</li>\n<li>APIs: Twitter API (Twity), Microsoft Cortana (Windows Speech Recognition)</li>\n<li>Assets: Oculus Integrations, Boxophobic</li>\n<li>Modeling and animations: Cinema 4D, Autodesk Maya, Adobe Illustrator</li>\n</ul>\n<h2>Challenges we ran into</h2>\n<ul>\n<li><strong>Deciding on the platform to use</strong>: The hackathon provides too many options for us to experiment. We were not sure if we want to make this a mobile, VR, AR, or MR experience. We then settled on VR. Originally, we chose Oculus Go, yet the controller SDK had issues due to Unity versioning and the prototyping cycle was too lengthy. We then turn to the more stable option of using an Oculus Rift.<br/></li>\n<li><strong>Getting used to Unity and VR development</strong>: Most of the team has little prior experience with VR development. We had to learn many Unity-specific concepts, such as ray casting, setting user camera position, switching between scene, spawning prefabs with properties.</li>\n<li><strong>Pulling and Posting data with Twitter and incorporating voice engine (STT &amp; TTS)</strong>: We spent a lot of time on using Twitter API in Unity. The Twitter Unity integration is specific for Mobile. Luckily, we found <a href=\"https://github.com/toofusan/Twity\" rel=\"nofollow\">Twity</a>, a C# Twitter API client. We were also stuck on finding a speech-to-text engine, and Google cloud API and IBM Watson does not work well with Unity. We found a workaround using Windows Cortana since the headset is connected to a Windows machine. Due to the limitation of using only free assets, the only text-to-speech engine we found requires a different .NET version. We had to give up coding that features and instead we played pre-recorded audio files.</li>\n<li><strong>Bird animations</strong>: We found a free asset of birds with flapping wings, yet the algorithm was too complex to be included in our application. We then tried to animate our bird to flap its wing in Maya and import as an animation. However, animation import did not work.</li>\n</ul>\n<h2>Accomplishments that we are proud of</h2>\n<ul>\n<li><strong>Getting every component working together</strong>: In the end, we put models, environment, interactions, and backend into each scene and connected them seamlessly. We completed most of the features that we proposed.</li>\n<li><strong>Design and construct complicated multi-scene</strong>: We also needed to carry data between scenes, and we did that by carrying data in instances of the prefab models.</li>\n</ul>\n<h2>What we learned</h2>\n<ul>\n<li><strong>Learning Unity and all spectrum of the VR development process</strong>: For most of us, this is our first VR application. Thanks to the workshops and mentors, we overcame lots of issues with versioning, importing, and structuring components.</li>\n<li><strong>Learning how to work as a team with diverse skillsets</strong>: We had clear roles. Austin on audio, Lucy on Twitter APIs, Simon on interactions and putting Unity project together, Dana on designing, and Rebecca on animations. We cooperated well and held hourly update, and everything worked together in the end.</li>\n<li><strong>Exploring the best practices of developing projects</strong>: Before starting to construct the project, we played with various available tools to test their technical viability. A feature tree and UX diagram are drawn to layout the development road map. Then we started to build features one by one in components for easier debugging. We had the core unity environment testing on one machine and many feature branches to avoid Git issues.</li>\n</ul>\n<h2>What's next for TwitterVRse</h2>\n<ul>\n<li><strong>Better UI and UX</strong>: We would like to get the flapping wing animation working and let the birds move across the scene. Many of the interactions are developed from simple prototypes and their design can be improved. Instruction will also be useful to guide the user through the experience.</li>\n<li><strong>Platform Supports</strong>: We would like to add support for Oculus Go as it is designed as a new platform to interact with social media, and soon the most popular VR device. A mobile AR version will also make the experience more accessible.</li>\n<li><strong>Location-based visualizations</strong>: We also had the idea to see birds migrate based on the locations of the tweets, as a geo-based visualization of trending discussions.</li>\n</ul>\n<h2>Credit and Acknowledgement</h2>\n<p>We would like to particularly thank John La from Oculus, Wiley Corning from the MIT Media Lab, Louis DeScioli from Google, Madison Hight from Microsoft, and Jasmine Roberts from Unity for their mentorship during the hackathon.</p>\n</div>",
            "content_md": "\n## Team info\n\n\nThis project was created from Jan 18-20, 2019 at the [Reality Virtually Hackathon at the MIT Media Lab](https://rv2019.devpost.com/).\n\n\n* **Simon (Zirui) Guo**: I'm a high school senior currently studying in Toronto, Canada and I love attending hackathons to explore new technologies and build cool projects. Particularly, I want to explore better interactions between humans and machines.\n* **Rebecca Skurnik**: Rebecca is master's student at the Interactive Telecommunications Program at NYU. She is passionate about creative technology and working in the intersection of the physical and digital world.\n* **Lucy Liao**: I'm a sophomore majoring in computer science at MIT interested in learning more about virtual reality.\n* **Austin Edelman**: A sophomore at MIT interested in creating amazing virtual worlds that people can live in. Consequently, I decided to train and become a biomedical engineer, majoring in computer science and electrical engineering and choosing a pre-medical track.\n* **Dana Elkis**: Dana Elkis is a Graphic Designer / Visual Artist from Tel Aviv (Israel), currently living in New York City. Master student at Interactive Telecommunications Program, NYU, New York.\n\n\n## Judging information\n\n\n* Location: Table 22, 6th-floor Vive Lounge, Media Lab E14.\n* Category: Art, Media and Entertainment.\n\n\n## Inspiration\n\n\nWe believe immersive technologies have the potential to upgrade our current experiences and we would like to experience it with social media. Social media posting has been mechanical and the same for every platform. We want to bring the experience to the next level by immersing the user in a Twitter universe and make them feel that they are apart of the Twitter community.\n\n\n## What it does\n\n\nTwitterVRse is a Virtual Reality experience that seeks to transform the way that we interact with the social media platform Twitter. We have created a VR interface that allows users to access tweets using virtual birds, interact with tweets like physical media, and post your own tweets through voice input.\n\n\n## User experience\n\n\nThe video above demonstrates a complete session.\n\n\n1. The user puts on an Oculus Rift and uses controllers to open TwitterVRse.\n2. The top 5 trending topics will be listed, in descending order of popularities. This will correspond with the number of birds in a flock. Users point at \"continue\" to enter the next scene.\n3. The Twitter universe will be presented with 5 flocks of birds scattered around in the environment. Those flocks have different numbers of birds, which corresponds to the 5 trending topics listed before (e.g. flocks with 5 birds is the most trending topic). Users point at the flock that he/she wants to investigate by pressing down a trigger.\n4. The birds in the flock will come to the user and, one by one reads out tweets by converting text to speech. Once users finished listening to the tweets, he/she can press the trigger to enter the next scene.\n5. In this scene, the user can record his/her own opinion by starting the record button and holding down the trigger while speaking. The user's voice will be recognized as text and posted on his/her Twitter account immediately.\n\n\n## How we built it\n\n\n* Hardware: Oculus Rift system (headset, Touch controllers, trackers)\n* Engine: Unity\n* SDKs: Oculus Integration\n* APIs: Twitter API (Twity), Microsoft Cortana (Windows Speech Recognition)\n* Assets: Oculus Integrations, Boxophobic\n* Modeling and animations: Cinema 4D, Autodesk Maya, Adobe Illustrator\n\n\n## Challenges we ran into\n\n\n* **Deciding on the platform to use**: The hackathon provides too many options for us to experiment. We were not sure if we want to make this a mobile, VR, AR, or MR experience. We then settled on VR. Originally, we chose Oculus Go, yet the controller SDK had issues due to Unity versioning and the prototyping cycle was too lengthy. We then turn to the more stable option of using an Oculus Rift.\n* **Getting used to Unity and VR development**: Most of the team has little prior experience with VR development. We had to learn many Unity-specific concepts, such as ray casting, setting user camera position, switching between scene, spawning prefabs with properties.\n* **Pulling and Posting data with Twitter and incorporating voice engine (STT & TTS)**: We spent a lot of time on using Twitter API in Unity. The Twitter Unity integration is specific for Mobile. Luckily, we found [Twity](https://github.com/toofusan/Twity), a C# Twitter API client. We were also stuck on finding a speech-to-text engine, and Google cloud API and IBM Watson does not work well with Unity. We found a workaround using Windows Cortana since the headset is connected to a Windows machine. Due to the limitation of using only free assets, the only text-to-speech engine we found requires a different .NET version. We had to give up coding that features and instead we played pre-recorded audio files.\n* **Bird animations**: We found a free asset of birds with flapping wings, yet the algorithm was too complex to be included in our application. We then tried to animate our bird to flap its wing in Maya and import as an animation. However, animation import did not work.\n\n\n## Accomplishments that we are proud of\n\n\n* **Getting every component working together**: In the end, we put models, environment, interactions, and backend into each scene and connected them seamlessly. We completed most of the features that we proposed.\n* **Design and construct complicated multi-scene**: We also needed to carry data between scenes, and we did that by carrying data in instances of the prefab models.\n\n\n## What we learned\n\n\n* **Learning Unity and all spectrum of the VR development process**: For most of us, this is our first VR application. Thanks to the workshops and mentors, we overcame lots of issues with versioning, importing, and structuring components.\n* **Learning how to work as a team with diverse skillsets**: We had clear roles. Austin on audio, Lucy on Twitter APIs, Simon on interactions and putting Unity project together, Dana on designing, and Rebecca on animations. We cooperated well and held hourly update, and everything worked together in the end.\n* **Exploring the best practices of developing projects**: Before starting to construct the project, we played with various available tools to test their technical viability. A feature tree and UX diagram are drawn to layout the development road map. Then we started to build features one by one in components for easier debugging. We had the core unity environment testing on one machine and many feature branches to avoid Git issues.\n\n\n## What's next for TwitterVRse\n\n\n* **Better UI and UX**: We would like to get the flapping wing animation working and let the birds move across the scene. Many of the interactions are developed from simple prototypes and their design can be improved. Instruction will also be useful to guide the user through the experience.\n* **Platform Supports**: We would like to add support for Oculus Go as it is designed as a new platform to interact with social media, and soon the most popular VR device. A mobile AR version will also make the experience more accessible.\n* **Location-based visualizations**: We also had the idea to see birds migrate based on the locations of the tweets, as a geo-based visualization of trending discussions.\n\n\n## Credit and Acknowledgement\n\n\nWe would like to particularly thank John La from Oculus, Wiley Corning from the MIT Media Lab, Louis DeScioli from Google, Madison Hight from Microsoft, and Jasmine Roberts from Unity for their mentorship during the hackathon.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/remi-m1468q",
            "title": "SparkTime",
            "blurb": "Magic Leap kitchen timers for your safety and convenience",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/e3CARyVKmcs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ben Hyl\u00e1k",
                    "about": "everything.",
                    "photo": "https://graph.facebook.com/100000991697895/picture?height=180&width=180"
                },
                {
                    "name": "Santiago Sada",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_1AB4wBtpTOailZPqzzi9564j84CiqMi508inDGzjLU1CqWtJzz7n_h4p_MpYco_JMASn__UydxxGBgsNsCtX__sKuxx_Bg9zJCtReC7gbYEfgwDWv1XvH9Tu2o5POgTXK39qfKvoReo?height=180&width=180"
                }
            ],
            "built_with": [
                "magic-leap"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>-</p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for SparkTime</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n-\n\n\n## What it does\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for SparkTime\n\n\n"
        },
        {
            "source": "https://devpost.com/software/beethoven-s-love-letter",
            "title": "Beethoven's Love Letter",
            "blurb": "Romantic & Immersive VR musical experience through a love letter from Beethoven",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/hUElezrhCUQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Moonlight",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/082/datas/original.PNG"
                },
                {
                    "title": "Open the letter, follow the heart",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/297/datas/original.png"
                },
                {
                    "title": "Follow the heart",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/083/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Xindeling Pan",
                    "about": "Concept & UX",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/546/971/datas/profile.jpg"
                },
                {
                    "name": "Scottie113 Murrell",
                    "about": "Developer",
                    "photo": "https://avatars2.githubusercontent.com/u/35502207?height=180&v=4&width=180"
                },
                {
                    "name": "ShirlySpikes",
                    "about": "Sound design and music",
                    "photo": "https://graph.facebook.com/10212867947467710/picture?height=180&width=180"
                },
                {
                    "name": "Sam De Lara",
                    "about": "3D Artist & Animator",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/919/240/datas/profile.png"
                }
            ],
            "built_with": [
                "cubase",
                "maya",
                "unity"
            ],
            "content_html": "<div>\n<p>Beethoven wrote the moonlight sonata to describe his blind lover how he feels when the moon rises at night. Inspired by this love story, we created an immersive musical experience that lets you not only see and hear, but also feel and interact with the music, putting the user inside a classical masterpiece.</p>\n<p>We built an immersive music VR experience that is accessible for everyone including people with hearing impaired by leveraging the accurate sensors from the new Woojer ryg haptic vest. Transducers on the vest enables people with hearing impaired to feel the music through vibrations corresponding to low frequencies in the music.</p>\n</div>",
            "content_md": "\nBeethoven wrote the moonlight sonata to describe his blind lover how he feels when the moon rises at night. Inspired by this love story, we created an immersive musical experience that lets you not only see and hear, but also feel and interact with the music, putting the user inside a classical masterpiece.\n\n\nWe built an immersive music VR experience that is accessible for everyone including people with hearing impaired by leveraging the accurate sensors from the new Woojer ryg haptic vest. Transducers on the vest enables people with hearing impaired to feel the music through vibrations corresponding to low frequencies in the music.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/magic-leap-martin-luther-king-jr-ar-mr-experience",
            "title": "Magic Leap Martin Luther King Jr. AR MR Experience",
            "blurb": "I have a dream to Keep MLK Jr's Message Alive for Future Generations in Augmented Reality",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312386490?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "MIT HACKATHON MAGIC LEAP MLK JR. TRIBUTE in AR MR XR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/744/192/datas/original.jpg"
                },
                {
                    "title": "MIT HACKATHON MAGIC LEAP MLK JR. TRIBUTE in AR MR XR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/058/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Cliff Baldridge",
                    "about": "I just did a Tribute to Martin Luther King Jr in Magic Leap Augmented Mixed Reality Glasses. When you play the Magic Leap App in The Augmented Reality Glasses, a large holographic Video Media Player plays a video MLK Jr. Speech I am streaming from Cloundinary Video Cloud Hosting. You control the Media Player with the Controller and AR Laser Pointer feature. I added his photo also as an Image UI Element.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/688/416/datas/profile.jpg"
                }
            ],
            "built_with": [
                "c",
                "c++",
                "cloud",
                "cloudinary",
                "lumin",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>I have a dream to Keep MLK Jr's Message Alive for Future Generations in Augmented Reality and Mixed Reality.</p>\n<h2>What it does</h2>\n<p>I just did a Tribute to Martin Luther King Jr in Magic Leap Augmented Mixed Reality Glasses. When you play the Magic Leap App in The Augmented Reality Glasses, a large holographic Video Media Player plays a video MLK Jr. Speech I am streaming from Cloundinary Video Cloud Hosting. You control the Media Player with the Controller and AR Laser Pointer feature. I added his photo also as an Image UI Element.</p>\n<h2>How I built it</h2>\n<p>Magic Leap Lumin OS, Unity, C, C++, C#</p>\n<h2>Challenges I ran into</h2>\n<p>UI Alignment.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Streaming media from Cloudinary Video Streaming Storage Platform. Creating an experience in Magic Leap for MLK Jr. Day and Social Good Media. Doing projects in Magic Leap AR MR Glasses and having that capability to create and develop now and see the result in the Mixed Reality Spatial Computing Realspace in Realtime. </p>\n<h2>What I learned</h2>\n<p>More Magic Leap Dev understanding.</p>\n<h2>What's next for Magic Leap Martin Luther King Jr. AR MR Experience</h2>\n<p>To Get funding to build this project out more with Animated 3D Objects and more Interaction and visualized realtime data. Integrate Watson Speech to Text and Translate in 7 languages to text on Magic Leap AR Glasses Screen. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nI have a dream to Keep MLK Jr's Message Alive for Future Generations in Augmented Reality and Mixed Reality.\n\n\n## What it does\n\n\nI just did a Tribute to Martin Luther King Jr in Magic Leap Augmented Mixed Reality Glasses. When you play the Magic Leap App in The Augmented Reality Glasses, a large holographic Video Media Player plays a video MLK Jr. Speech I am streaming from Cloundinary Video Cloud Hosting. You control the Media Player with the Controller and AR Laser Pointer feature. I added his photo also as an Image UI Element.\n\n\n## How I built it\n\n\nMagic Leap Lumin OS, Unity, C, C++, C#\n\n\n## Challenges I ran into\n\n\nUI Alignment.\n\n\n## Accomplishments that I'm proud of\n\n\nStreaming media from Cloudinary Video Streaming Storage Platform. Creating an experience in Magic Leap for MLK Jr. Day and Social Good Media. Doing projects in Magic Leap AR MR Glasses and having that capability to create and develop now and see the result in the Mixed Reality Spatial Computing Realspace in Realtime. \n\n\n## What I learned\n\n\nMore Magic Leap Dev understanding.\n\n\n## What's next for Magic Leap Martin Luther King Jr. AR MR Experience\n\n\nTo Get funding to build this project out more with Animated 3D Objects and more Interaction and visualized realtime data. Integrate Watson Speech to Text and Translate in 7 languages to text on Magic Leap AR Glasses Screen. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/terraformar",
            "title": "TerraformAR",
            "blurb": "Visualize ecological data and climate change effect on the browser",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/bqVVCs5OsXQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Faye Li",
                    "about": "I worked on the AR overlay with Mapbox API and A-frame. I also helped produce the ecological footprint visualisation.",
                    "photo": "https://avatars2.githubusercontent.com/u/2405697?height=180&v=4&width=180"
                },
                {
                    "name": "Liam Broza",
                    "about": "I worked on the AR overlay using three.js and AFrame. I also worked on the data collection and visualization using ESRI.",
                    "photo": "https://avatars3.githubusercontent.com/u/5104160?height=180&v=3&width=180"
                },
                {
                    "name": "Thomas William Mohr",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/11515396?height=180&v=4&width=180"
                },
                {
                    "name": "Kat Vlasova",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/947aedf1cfe9d08f5839c2df329e090a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "aframe",
                "blender",
                "esri",
                "node.js",
                "three.js",
                "unity",
                "vue",
                "webgl",
                "webrtc",
                "webvr",
                "webxr"
            ],
            "content_html": "<div>\n<h2>Location, floor, and room:</h2>\n<p>E15 LL Floor Atrium</p>\n<h2>The development tools used to build the project:</h2>\n<p>Unity, Maya, Logic. Microsoft HP Developer Mixed Reality Headset </p>\n<h2>SDKs used in the project:</h2>\n<p>Three.js, A-frame, AR.js</p>\n<h2>APIs used in the project:</h2>\n<p>Mapbox</p>\n<h2>Any assets used in the project that you did not create:</h2>\n<p>Unity Asset Store:\nStandard Assets : <a href=\"https://assetstore.unity.com/packages/essentials/asset-packs/standard-assets-32351\" rel=\"nofollow\">https://assetstore.unity.com/packages/essentials/asset-packs/standard-assets-32351</a>\nWebVR Assets: \n<a href=\"https://assetstore.unity.com/packages/templates/systems/webvr-assets-109152\" rel=\"nofollow\">https://assetstore.unity.com/packages/templates/systems/webvr-assets-109152</a>\nSkyBox Series Free:\n<a href=\"https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-series-free-103633\" rel=\"nofollow\">https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-series-free-103633</a></p>\n<p>Outside Free Assets:\nRuin Building 3D Model: <a href=\"https://free3d.com/3d-model/ruin-building-3d-model-25865.html\" rel=\"nofollow\">https://free3d.com/3d-model/ruin-building-3d-model-25865.html</a>\nBuilding: <a href=\"https://free3d.com/3d-model/building-54495.html\" rel=\"nofollow\">https://free3d.com/3d-model/building-54495.html</a>\nOld house: <a href=\"https://free3d.com/3d-model/0ld-house--82527.html\" rel=\"nofollow\">https://free3d.com/3d-model/0ld-house--82527.html</a>\nBuildings in the city: <a href=\"https://free3d.com/3d-model/buildings-in-the-city-471489.html\" rel=\"nofollow\">https://free3d.com/3d-model/buildings-in-the-city-471489.html</a>\nBrick Building: <a href=\"https://free3d.com/3d-model/brick-building-51863.html\" rel=\"nofollow\">https://free3d.com/3d-model/brick-building-51863.html</a>\nNYC Brownstone: <a href=\"https://free3d.com/3d-model/new-york-city-brownstone-building-v1--277076.html\" rel=\"nofollow\">https://free3d.com/3d-model/new-york-city-brownstone-building-v1--277076.html</a>\nBackground Building: <a href=\"https://free3d.com/3d-model/backgroundbuilding-40839.html\" rel=\"nofollow\">https://free3d.com/3d-model/backgroundbuilding-40839.html</a>\nBuilding 3D model: <a href=\"https://free3d.com/3d-model/building-5851.html\" rel=\"nofollow\">https://free3d.com/3d-model/building-5851.html</a>\nBuilding NY: <a href=\"https://free3d.com/3d-model/building-ny-41684.html\" rel=\"nofollow\">https://free3d.com/3d-model/building-ny-41684.html</a>\nNewspaper stand: <a href=\"https://free3d.com/3d-model/-newspaperstand-v2--547433.html\" rel=\"nofollow\">https://free3d.com/3d-model/-newspaperstand-v2--547433.html</a>\nFire Hydrant : <a href=\"https://free3d.com/3d-model/nyc-fire-hydrant-v2--645772.html\" rel=\"nofollow\">https://free3d.com/3d-model/nyc-fire-hydrant-v2--645772.html</a>\nBench: <a href=\"https://free3d.com/3d-model/bench-84662.html\" rel=\"nofollow\">https://free3d.com/3d-model/bench-84662.html</a>\nStop Sign: <a href=\"https://free3d.com/3d-model/stop-sign-58062.html\" rel=\"nofollow\">https://free3d.com/3d-model/stop-sign-58062.html</a>\nTraffic Cone: <a href=\"https://free3d.com/3d-model/traffic-cones-10923.html\" rel=\"nofollow\">https://free3d.com/3d-model/traffic-cones-10923.html</a>\nStreet Lamp: <a href=\"https://free3d.com/3d-model/lamp-post-2-80323.html\" rel=\"nofollow\">https://free3d.com/3d-model/lamp-post-2-80323.html</a>\nRat: <a href=\"https://free3d.com/3d-model/rat-v2--794593.html\" rel=\"nofollow\">https://free3d.com/3d-model/rat-v2--794593.html</a></p>\n<p>3D Icons for AR overlay:\nHealth of Flora+Fauna: <a href=\"https://poly.google.com/view/3tyh15Fbmsx\" rel=\"nofollow\">https://poly.google.com/view/3tyh15Fbmsx</a>\nAir Quality: <a href=\"https://poly.google.com/view/8Tke6WIyZtg\" rel=\"nofollow\">https://poly.google.com/view/8Tke6WIyZtg</a>\nLand Dev: <a href=\"https://poly.google.com/view/a9-g0Ylwft7\" rel=\"nofollow\">https://poly.google.com/view/a9-g0Ylwft7</a>\nPollution: <a href=\"https://poly.google.com/view/70kkg-FJIoi\" rel=\"nofollow\">https://poly.google.com/view/70kkg-FJIoi</a>\nWater Quality: <a href=\"https://poly.google.com/view/eoQ3UQphRi0\" rel=\"nofollow\">https://poly.google.com/view/eoQ3UQphRi0</a>\nAnnual Climate Change: <a href=\"https://poly.google.com/view/58PjkXNdpPb\" rel=\"nofollow\">https://poly.google.com/view/58PjkXNdpPb</a></p>\n<p>Background Music:\n<a href=\"https://freesound.org/people/Speedenza/sounds/206114/\" rel=\"nofollow\">https://freesound.org/people/Speedenza/sounds/206114/</a></p>\n<h2>Any libraries used in the project:</h2>\n<p>Geometric, Mozilla Unity WebVR Export, Exokit</p>\n<p>Any components not created at the hackathon:</p>\n<p>A link to a video of a screen capture of the application on Youtube</p>\n<h2>Inspiration</h2>\n<p>The UN has stated that we only have 12 years to limit the scope of an inevitable climate change catastrophe.</p>\n<p>Sparked by unchecked industrial waste and exploitation of resources, climate change is happening faster than our planet can adapt to it. Is this the future you want?</p>\n<h2>What it does</h2>\n<p>TerraformAR lets you experience this disastrous future we are headed towards and then offers an AR tool to make this problem visible today so disaster isn't our destiny.</p>\n<p>TerraformAR gives form to hidden layers of our ecology as an AR overlay. See the state of the environment around you; including air quality, annual climate change, pollution, land development, water quality, and health of flora and fauna.</p>\n<h2>How we built it</h2>\n<p>There are two parts to TerraformAR, the first is a 60 second WebVR intro experience that shows future downtown NYC. This VR experience was built in Unity and exported to the web. Kat used a mixture of her own modeling and premade assets to build and animate this scene. The second part of TerraformAR is a WebAR overlay powered by custom JavaScript code on top of AFrame and Three.js. This custom code is designed to create an WebAR overlay compatible with any smartphone made in the last 5 years. (I.E. not needing WebXR support to display AR in a mobile browser).</p>\n<h2>Challenges we ran into</h2>\n<p>To make an easy to use AR overlay available to everyone, we had to make a new type of WebAR framework. This took some serious experimenting to get right. </p>\n<p>We also had to source mapping, data and assets that all fit into our concept.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We built a new type of WebAR overlay!</p>\n<p>The future NYC experience looks like rad dreamscape and evokes tons of emotion.</p>\n<h2>What we learned</h2>\n<p>Because of all of our limited experience in WebXR tech, we all had to learn how to mesh our skills and learn as we built.</p>\n<h2>What's next for TerraformAR</h2>\n<p>We hope to mature the AR and VR portions of the experience and to publish the WebAR overlay as an AFrame component.</p>\n<p>Open on a mobile device (Needs GPS, Compass, and Gyro to work):\n<a href=\"https://realityvirtually2019.github.io/TerraformAR/\" rel=\"nofollow\">https://realityvirtually2019.github.io/TerraformAR/</a></p>\n</div>",
            "content_md": "\n## Location, floor, and room:\n\n\nE15 LL Floor Atrium\n\n\n## The development tools used to build the project:\n\n\nUnity, Maya, Logic. Microsoft HP Developer Mixed Reality Headset \n\n\n## SDKs used in the project:\n\n\nThree.js, A-frame, AR.js\n\n\n## APIs used in the project:\n\n\nMapbox\n\n\n## Any assets used in the project that you did not create:\n\n\nUnity Asset Store:\nStandard Assets : <https://assetstore.unity.com/packages/essentials/asset-packs/standard-assets-32351>\nWebVR Assets: \n<https://assetstore.unity.com/packages/templates/systems/webvr-assets-109152>\nSkyBox Series Free:\n<https://assetstore.unity.com/packages/2d/textures-materials/sky/skybox-series-free-103633>\n\n\nOutside Free Assets:\nRuin Building 3D Model: <https://free3d.com/3d-model/ruin-building-3d-model-25865.html>\nBuilding: <https://free3d.com/3d-model/building-54495.html>\nOld house: <https://free3d.com/3d-model/0ld-house--82527.html>\nBuildings in the city: <https://free3d.com/3d-model/buildings-in-the-city-471489.html>\nBrick Building: <https://free3d.com/3d-model/brick-building-51863.html>\nNYC Brownstone: <https://free3d.com/3d-model/new-york-city-brownstone-building-v1--277076.html>\nBackground Building: <https://free3d.com/3d-model/backgroundbuilding-40839.html>\nBuilding 3D model: <https://free3d.com/3d-model/building-5851.html>\nBuilding NY: <https://free3d.com/3d-model/building-ny-41684.html>\nNewspaper stand: <https://free3d.com/3d-model/-newspaperstand-v2--547433.html>\nFire Hydrant : <https://free3d.com/3d-model/nyc-fire-hydrant-v2--645772.html>\nBench: <https://free3d.com/3d-model/bench-84662.html>\nStop Sign: <https://free3d.com/3d-model/stop-sign-58062.html>\nTraffic Cone: <https://free3d.com/3d-model/traffic-cones-10923.html>\nStreet Lamp: <https://free3d.com/3d-model/lamp-post-2-80323.html>\nRat: <https://free3d.com/3d-model/rat-v2--794593.html>\n\n\n3D Icons for AR overlay:\nHealth of Flora+Fauna: <https://poly.google.com/view/3tyh15Fbmsx>\nAir Quality: <https://poly.google.com/view/8Tke6WIyZtg>\nLand Dev: <https://poly.google.com/view/a9-g0Ylwft7>\nPollution: <https://poly.google.com/view/70kkg-FJIoi>\nWater Quality: <https://poly.google.com/view/eoQ3UQphRi0>\nAnnual Climate Change: <https://poly.google.com/view/58PjkXNdpPb>\n\n\nBackground Music:\n<https://freesound.org/people/Speedenza/sounds/206114/>\n\n\n## Any libraries used in the project:\n\n\nGeometric, Mozilla Unity WebVR Export, Exokit\n\n\nAny components not created at the hackathon:\n\n\nA link to a video of a screen capture of the application on Youtube\n\n\n## Inspiration\n\n\nThe UN has stated that we only have 12 years to limit the scope of an inevitable climate change catastrophe.\n\n\nSparked by unchecked industrial waste and exploitation of resources, climate change is happening faster than our planet can adapt to it. Is this the future you want?\n\n\n## What it does\n\n\nTerraformAR lets you experience this disastrous future we are headed towards and then offers an AR tool to make this problem visible today so disaster isn't our destiny.\n\n\nTerraformAR gives form to hidden layers of our ecology as an AR overlay. See the state of the environment around you; including air quality, annual climate change, pollution, land development, water quality, and health of flora and fauna.\n\n\n## How we built it\n\n\nThere are two parts to TerraformAR, the first is a 60 second WebVR intro experience that shows future downtown NYC. This VR experience was built in Unity and exported to the web. Kat used a mixture of her own modeling and premade assets to build and animate this scene. The second part of TerraformAR is a WebAR overlay powered by custom JavaScript code on top of AFrame and Three.js. This custom code is designed to create an WebAR overlay compatible with any smartphone made in the last 5 years. (I.E. not needing WebXR support to display AR in a mobile browser).\n\n\n## Challenges we ran into\n\n\nTo make an easy to use AR overlay available to everyone, we had to make a new type of WebAR framework. This took some serious experimenting to get right. \n\n\nWe also had to source mapping, data and assets that all fit into our concept.\n\n\n## Accomplishments that we're proud of\n\n\nWe built a new type of WebAR overlay!\n\n\nThe future NYC experience looks like rad dreamscape and evokes tons of emotion.\n\n\n## What we learned\n\n\nBecause of all of our limited experience in WebXR tech, we all had to learn how to mesh our skills and learn as we built.\n\n\n## What's next for TerraformAR\n\n\nWe hope to mature the AR and VR portions of the experience and to publish the WebAR overlay as an AFrame component.\n\n\nOpen on a mobile device (Needs GPS, Compass, and Gyro to work):\n<https://realityvirtually2019.github.io/TerraformAR/>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/tarnia-a-leap-language-learning-experience",
            "title": "Tarnia",
            "blurb": "Creating an AR environment where the user can learn a language by exploring the world around with Tarnia",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/UNikLK6pxOI?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Josue V",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQFpqbcyfQYFTA/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=TEqiOf-7AbJ-9o3Qk3T-0Hg3ms14Vd-vsv7_eqaE7u4&v=beta&width=180"
                },
                {
                    "name": "Fin Ambsdorf",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/32569726?height=180&v=4&width=180"
                },
                {
                    "name": "Ruiming Wang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/748/datas/profile.jpg"
                },
                {
                    "name": "Seyitan Oke",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/412/928/datas/profile.jpg"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "google-cloud",
                "ibm-watson",
                "json",
                "magic-leap",
                "mixamo",
                "unity"
            ],
            "content_html": "<div>\n<h2>What inspired us</h2>\n<p>Our whole team is made of individuals where everyone had to learn a second language. We realized that everyone had their own different learning styles that allowed them to learn the language. There are some visual learners, spacial learners, audio learners, and kinetic learners. Classic curriculums have always catered to the paper based memorizing style of learning. We saw a potential for the Magic Leap to allow us to create new language learning experiences that cater to specific learning styles. By using the Magic Leap spatial computing capabilities, with the power of Google's Cloud API we thought we could create a learning experience catered to visual-spatial learners.</p>\n<h2>What it does</h2>\n<p>The Tarnia application uses a task-based learning approached backed by research to set the user on a quest to find items in their space. When they find the items, Tarnia will teach them the German word for the object in the room that they are pointing at. The Magic Leap will keep the translated label of the word in the 3d space and the user can revisit if necessary.</p>\n<h2>How we built it</h2>\n<p>We used the Magic Leap SDK for Unity as a Starting Point to capture a 2D Image with the Magic Leap. Then we worked on a Way to use the Google Vision API, Text to Speech and Translate  with Unity. After that was done, we were able to create a MVP that captures a 2D Image, upload it to Google, analyze it and then tells the User the what he is seeing in his chosen Language.</p>\n<h2>Challenges we ran into</h2>\n<p>We had some Problems with Github as well as with the Magic Leap SDK as it was a completely new toolset for us.</p>\n<h2>Accomplishments that we are proud of</h2>\n<p>The Integration and process flow to identify and image from the magic leap in live view is our greatest accomplishments. We believe this will be the backbone for creating a free flowing task base learning experience.</p>\n<h2>What's next for Tarnia</h2>\n<p>Next we want to continue to develop more modes for learning, more games, and more stories. We think that the magic leap will enable easier language learning experience for different learning styles. For the foreseeable future, we will be designing task based games (proven by research), visual spatial (enables spatial learners), and story based(bringing in human narratives to language learning) experiences.</p>\n</div>",
            "content_md": "\n## What inspired us\n\n\nOur whole team is made of individuals where everyone had to learn a second language. We realized that everyone had their own different learning styles that allowed them to learn the language. There are some visual learners, spacial learners, audio learners, and kinetic learners. Classic curriculums have always catered to the paper based memorizing style of learning. We saw a potential for the Magic Leap to allow us to create new language learning experiences that cater to specific learning styles. By using the Magic Leap spatial computing capabilities, with the power of Google's Cloud API we thought we could create a learning experience catered to visual-spatial learners.\n\n\n## What it does\n\n\nThe Tarnia application uses a task-based learning approached backed by research to set the user on a quest to find items in their space. When they find the items, Tarnia will teach them the German word for the object in the room that they are pointing at. The Magic Leap will keep the translated label of the word in the 3d space and the user can revisit if necessary.\n\n\n## How we built it\n\n\nWe used the Magic Leap SDK for Unity as a Starting Point to capture a 2D Image with the Magic Leap. Then we worked on a Way to use the Google Vision API, Text to Speech and Translate with Unity. After that was done, we were able to create a MVP that captures a 2D Image, upload it to Google, analyze it and then tells the User the what he is seeing in his chosen Language.\n\n\n## Challenges we ran into\n\n\nWe had some Problems with Github as well as with the Magic Leap SDK as it was a completely new toolset for us.\n\n\n## Accomplishments that we are proud of\n\n\nThe Integration and process flow to identify and image from the magic leap in live view is our greatest accomplishments. We believe this will be the backbone for creating a free flowing task base learning experience.\n\n\n## What's next for Tarnia\n\n\nNext we want to continue to develop more modes for learning, more games, and more stories. We think that the magic leap will enable easier language learning experience for different learning styles. For the foreseeable future, we will be designing task based games (proven by research), visual spatial (enables spatial learners), and story based(bringing in human narratives to language learning) experiences.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/arrival-g7qnkd",
            "title": "ARrival",
            "blurb": "A Narrative Experience in which a user establishes a non-verbal connection with an alien using hand gestures. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/bzZceScAHRs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "DreVinciBot",
                    "about": "I worked on the animations for the alien gestures. I used Blender originally, but I learned Autodesk Maya for this Hackathon. Great learning experience!",
                    "photo": "https://avatars3.githubusercontent.com/u/35714752?height=180&v=4&width=180"
                },
                {
                    "name": "alik5",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/7890536?height=180&v=4&width=180"
                },
                {
                    "name": "suibiweng",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/35569607?height=180&v=4&width=180"
                },
                {
                    "name": "Hannah Schilsky",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5103AQG01PH5WnN8Tg/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=Z4KjylMRIAIHKjSjSMxpDwsRF3M38RxiCw7LpTftYwI&v=beta&width=180"
                }
            ],
            "built_with": [
                "blender",
                "magic-leap",
                "maya",
                "photoshop",
                "sculptris",
                "substance-painter",
                "unity",
                "wayfair-model-api"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The Inspiration was Magic Leap's ability to track three fingers which leaves developers able to use hand signals as inputs. This ability offers a new way and less mechanical way to simulate interaction with computers/machines/characters. We decided to take that idea further and create a situation in which the user interacts with an alien as a way of using the technology to show how interaction with a man made character can be achieved and reflects the human nature in our ability to interact with people who may not speak our language when you make an effort to connect.  </p>\n<h2>What it does</h2>\n<p>The app launches the user into an experience with an alien. He shows you how to interact with him and he has something he wants from you. </p>\n<h2>How we built it</h2>\n<p>We built it by creating an alien character and animating him from scratch and using Magic Leap's Unity docs and sdk to create a functioning app. </p>\n<h2>Challenges we ran into</h2>\n<p>Challenges included figuring out how to create a compelling and comfortable experience on Magic Leap and how to best use hand gestures and simulate interaction with a character. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud to have created a unique and endearing character and a functioning narrative experience that will hopefully leave users interested, entertained and surprised. </p>\n<h2>What we learned</h2>\n<p>We learned how to program and design to best suit our vision and also the specs of the Magic Leap One</p>\n<h2>What's next for ARrival</h2>\n<p>More stories! New interaction based exploration. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe Inspiration was Magic Leap's ability to track three fingers which leaves developers able to use hand signals as inputs. This ability offers a new way and less mechanical way to simulate interaction with computers/machines/characters. We decided to take that idea further and create a situation in which the user interacts with an alien as a way of using the technology to show how interaction with a man made character can be achieved and reflects the human nature in our ability to interact with people who may not speak our language when you make an effort to connect. \n\n\n## What it does\n\n\nThe app launches the user into an experience with an alien. He shows you how to interact with him and he has something he wants from you. \n\n\n## How we built it\n\n\nWe built it by creating an alien character and animating him from scratch and using Magic Leap's Unity docs and sdk to create a functioning app. \n\n\n## Challenges we ran into\n\n\nChallenges included figuring out how to create a compelling and comfortable experience on Magic Leap and how to best use hand gestures and simulate interaction with a character. \n\n\n## Accomplishments that we're proud of\n\n\nWe are proud to have created a unique and endearing character and a functioning narrative experience that will hopefully leave users interested, entertained and surprised. \n\n\n## What we learned\n\n\nWe learned how to program and design to best suit our vision and also the specs of the Magic Leap One\n\n\n## What's next for ARrival\n\n\nMore stories! New interaction based exploration. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/major-key",
            "title": "Major Key",
            "blurb": "An application on Magic Leap that teaches you how to read sheet music and play piano. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/FuhKtq8BS18?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Ideation for home page of Major Key",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/489/datas/original.png"
                },
                {
                    "title": "Major Key",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/487/datas/original.png"
                },
                {
                    "title": "Persona",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/484/datas/original.png"
                },
                {
                    "title": "Journey map for Evan",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/483/datas/original.png"
                },
                {
                    "title": "Finding the MVP",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/488/datas/original.png"
                },
                {
                    "title": "Report card presented after each practice",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/748/486/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Surbhi S Gupta",
                    "about": "I helped build the story. I created the interview screener and questionnaire and helped synthesize the research. I helped conduct research. Created Persona and Journey map. Helped with realizing the MVP and ideation with the team.",
                    "photo": "https://media.licdn.com/dms/image/C5603AQEaxSlCrnED7A/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=WzuBzmtNn0liBm3RDduG08VHbPlu5Qa8MIqU3zHRuzk&v=beta&width=180"
                },
                {
                    "name": "jordan gutt",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/8a72f3ab60724a9b70af1e4bc4a9783d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kalila Shapiro",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQGsERTrHvlP2g/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=XIxbd0tldOs9kz7ur8bGalFmjUpeC5P7hk_mgAOYDtM&v=beta&width=180"
                },
                {
                    "name": "Iain Smith",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/39886f2aebf66c964e4191fb17c6b399?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Marcel .",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/959/177/datas/profile.png"
                }
            ],
            "built_with": [
                "abelton-live",
                "ableton-live",
                "blender",
                "finale",
                "magic-leap",
                "photoshop",
                "sketch",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration -- Kalila's brother wanted to teach himself piano, however, most of the resources he found online were video tutorials on YouTube.  We wanted to create a more immersive way to learn sheet music and play piano.</h2>\n<h2>What it does -- The Major Key app provides the user an immersive learning experience focused on teaching students how to read sheet music and play piano.</h2>\n<h2>How I built it -- We sent out surveys to conduct preliminary user research, created an affinity board, a user journey, wireframes, and a working prototype on Magic Leap. We included songs and exercises, an AR keyboard piano model, AR light up keys, and scrolling sheet music implemented with Blender and Unity.</h2>\n<h2>Challenges we ran into -- While understanding the capabilities of the Magic Leap hardware we realized that the microphone was best suited for personal input and was not great at taking input from our surroundings.  Magic Leap can currently only track three fingers while playing piano requires all 5 making finger tracking not feasible.  Proper piano playing requires good wrist position which would also block view of fingers from sensors. This would require a work around using inverse kinematics. Our work around was to use a midi keyboard for the hack.  Magic Leap does not accept serial input making plugging a midi keyboard into it impossible.  We tried to implement a server using node.js on a computer that would send the midi data to the Magic Leap over wifi.  This ended up being very problematic and we ended up just running the project within unity editor where both the magic Leap and the keyboard could work together.  All pianos are also different sizes accounting for that would be difficult (we planned on adding tracking markers onto certain keys with stickers).   After learning this we adapted our vision to best fit to the hardware within the time.</h2>\n<h2>Accomplishments that I'm proud of -- Using various software development tools and network protocols in order take our vision on to Magic Leap's cutting edge device.  Brain storming, wire framing, and prioritizing features to come up with a simple and intuitive user flow and Mixed Reality conventions to make accessing songs and practice usable.</h2>\n<h2>What's next for Major Key -- Lesson structures and fun exercises, being able to use other instruments, gamification to make playing music more fun and appealing, social features including leaderboard concept to increase indulgence in playing music, partnership with local music initiatives and schools or tutors to promote more music learning, bring easy music learning to every home</h2>\n</div>",
            "content_md": "\n## Inspiration -- Kalila's brother wanted to teach himself piano, however, most of the resources he found online were video tutorials on YouTube. We wanted to create a more immersive way to learn sheet music and play piano.\n\n\n## What it does -- The Major Key app provides the user an immersive learning experience focused on teaching students how to read sheet music and play piano.\n\n\n## How I built it -- We sent out surveys to conduct preliminary user research, created an affinity board, a user journey, wireframes, and a working prototype on Magic Leap. We included songs and exercises, an AR keyboard piano model, AR light up keys, and scrolling sheet music implemented with Blender and Unity.\n\n\n## Challenges we ran into -- While understanding the capabilities of the Magic Leap hardware we realized that the microphone was best suited for personal input and was not great at taking input from our surroundings. Magic Leap can currently only track three fingers while playing piano requires all 5 making finger tracking not feasible. Proper piano playing requires good wrist position which would also block view of fingers from sensors. This would require a work around using inverse kinematics. Our work around was to use a midi keyboard for the hack. Magic Leap does not accept serial input making plugging a midi keyboard into it impossible. We tried to implement a server using node.js on a computer that would send the midi data to the Magic Leap over wifi. This ended up being very problematic and we ended up just running the project within unity editor where both the magic Leap and the keyboard could work together. All pianos are also different sizes accounting for that would be difficult (we planned on adding tracking markers onto certain keys with stickers). After learning this we adapted our vision to best fit to the hardware within the time.\n\n\n## Accomplishments that I'm proud of -- Using various software development tools and network protocols in order take our vision on to Magic Leap's cutting edge device. Brain storming, wire framing, and prioritizing features to come up with a simple and intuitive user flow and Mixed Reality conventions to make accessing songs and practice usable.\n\n\n## What's next for Major Key -- Lesson structures and fun exercises, being able to use other instruments, gamification to make playing music more fun and appealing, social features including leaderboard concept to increase indulgence in playing music, partnership with local music initiatives and schools or tutors to promote more music learning, bring easy music learning to every home\n\n\n"
        },
        {
            "source": "https://devpost.com/software/behind-the-product",
            "title": "Insider",
            "blurb": "Augmented Reality to reveal hidden story of people's daily products.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/d8AWnYEVnHQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Yujie Jiang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/474/datas/profile.JPG"
                },
                {
                    "name": "bohanchen Chen",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/19295984?height=180&v=4&width=180"
                },
                {
                    "name": "JunguGuo",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/30543186?height=180&v=4&width=180"
                },
                {
                    "name": "Ta-Kai Shueh",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/250/datas/profile.jpg"
                },
                {
                    "name": "Yi(Yuna) Lu",
                    "about": "",
                    "photo": "https://graph.facebook.com/1597262743714795/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe-xd",
                "blender",
                "maya",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<p>Hey! We are team #84. </p>\n<h2>Location, floor, and room</h2>\n<p>Table 94, MIT E15 Building Ground Floor.</p>\n<h2>The development tools used to build the project:</h2>\n<p>Unity, Blender,  Adobe Photoshop, Maya</p>\n<h2>Inspiration</h2>\n<p>Water bottles are around us daily. Have you ever think about why is so cheap when it might take ages to produce a bottle of water and we might drink up in 1 minute? How much resource and manpower involved in the production? So we start to brainstorming about the story behind it. </p>\n<h2>Purpose</h2>\n<p>AR provides opportunities for us to not only augment reality but also unveil the hidden side of the reality.  </p>\n<p>Technological products have significantly improved our daily life. We enjoy the convenience they provide but most of us rarely think about the issues behind these products. Through the project, we want to promote an awareness of these issues, discussing the negative influences which these high technology products bring out. Our team believes that all products have their own story and are valuable enough to be shared, so we could learn and cherish the hidden efforts and impact. </p>\n<p>Team 84 is here to reveal them with our AR Product.</p>\n<h2>What it does</h2>\n<p>It is an AR app that people can use it to screen any electronic device, i.e., phone, laptop, watches, etc, and you will find out every detail and valuable stories that you had never paid enough attention to.</p>\n<h2>How we built it</h2>\n<p>Prototype: iPhones. \nAssign a unique identifier for each type of iPhone. -----&gt; AR transparent view inside iPhone. -----&gt; Animation for each electronic little components in order to show its own story. </p>\n<h2>Challenges we ran into</h2>\n<p>Non-members have done either AR/VR related projects or animation. Learn from 0 for 24 hours, then debug &amp; improve. </p>\n<p>E15 far from E14 6th floor where they got snacks. </p>\n<h2>What's next for Insider</h2>\n<p>Link to demo video: \nWe are looking forward to expanding our database and revealing more behind product stories.</p>\n</div>",
            "content_md": "\nHey! We are team #84. \n\n\n## Location, floor, and room\n\n\nTable 94, MIT E15 Building Ground Floor.\n\n\n## The development tools used to build the project:\n\n\nUnity, Blender, Adobe Photoshop, Maya\n\n\n## Inspiration\n\n\nWater bottles are around us daily. Have you ever think about why is so cheap when it might take ages to produce a bottle of water and we might drink up in 1 minute? How much resource and manpower involved in the production? So we start to brainstorming about the story behind it. \n\n\n## Purpose\n\n\nAR provides opportunities for us to not only augment reality but also unveil the hidden side of the reality. \n\n\nTechnological products have significantly improved our daily life. We enjoy the convenience they provide but most of us rarely think about the issues behind these products. Through the project, we want to promote an awareness of these issues, discussing the negative influences which these high technology products bring out. Our team believes that all products have their own story and are valuable enough to be shared, so we could learn and cherish the hidden efforts and impact. \n\n\nTeam 84 is here to reveal them with our AR Product.\n\n\n## What it does\n\n\nIt is an AR app that people can use it to screen any electronic device, i.e., phone, laptop, watches, etc, and you will find out every detail and valuable stories that you had never paid enough attention to.\n\n\n## How we built it\n\n\nPrototype: iPhones. \nAssign a unique identifier for each type of iPhone. -----> AR transparent view inside iPhone. -----> Animation for each electronic little components in order to show its own story. \n\n\n## Challenges we ran into\n\n\nNon-members have done either AR/VR related projects or animation. Learn from 0 for 24 hours, then debug & improve. \n\n\nE15 far from E14 6th floor where they got snacks. \n\n\n## What's next for Insider\n\n\nLink to demo video: \nWe are looking forward to expanding our database and revealing more behind product stories.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rv-hack-2019-nature-s-wrath",
            "title": "RV Hack 2019 - Nature's Wrath",
            "blurb": "An immersive VR experience where you play as a giant polar bear seeking to cleanse an abandoned toxic city.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Y6MGPSyKwiQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Gary Nguyen",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/35552077?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "htc-vive-pro",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We wanted to have fun, building something cool, learn as much as we can, and smash things. The idea was to have a polar bear become mutated by radiation, grow to a gigantic size, and seek to destroy a city polluted by waste to make way for nature to revive.</p>\n<h2>What it does</h2>\n<p>You get to smash buildings.</p>\n<h2>How I built it</h2>\n<p>Using Unity, Unity Asset Store, HTC Vive Pro, and Blender</p>\n<h2>Challenges I ran into</h2>\n<p>Making buildings appear to fracture, learning curves for beginners on the team, and destructive paw mechanics.</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Getting to code successful scripts for some animations. Being able to grab and throw objects.</p>\n<h2>What I learned</h2>\n<p>Learned basics of Blender and Unity in a few days' time! And C# scripting for game objects.</p>\n<h2>What's next for RV Hack 2019 - Nature's Wrath</h2>\n<p>To develop a scoring system and to create an antagonist (trash golem maybe).</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe wanted to have fun, building something cool, learn as much as we can, and smash things. The idea was to have a polar bear become mutated by radiation, grow to a gigantic size, and seek to destroy a city polluted by waste to make way for nature to revive.\n\n\n## What it does\n\n\nYou get to smash buildings.\n\n\n## How I built it\n\n\nUsing Unity, Unity Asset Store, HTC Vive Pro, and Blender\n\n\n## Challenges I ran into\n\n\nMaking buildings appear to fracture, learning curves for beginners on the team, and destructive paw mechanics.\n\n\n## Accomplishments that I'm proud of\n\n\nGetting to code successful scripts for some animations. Being able to grab and throw objects.\n\n\n## What I learned\n\n\nLearned basics of Blender and Unity in a few days' time! And C# scripting for game objects.\n\n\n## What's next for RV Hack 2019 - Nature's Wrath\n\n\nTo develop a scoring system and to create an antagonist (trash golem maybe).\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dora",
            "title": "Dora",
            "blurb": "Augmented Human",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/upload?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Dora",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/001/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Pankhuri Agarwal",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7d904e14885f35e4b8b0d1d426393905?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "anshul mittal",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d304cd19452301fb5386bac0ae87e451?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "api-gateway",
                "ar-kit",
                "aws-sumerian",
                "javascript",
                "lambda",
                "polylex",
                "s3",
                "sketchfab",
                "twilio"
            ],
            "content_html": "<div>\n<p>Most of our communication these days is via text. sms, email, watsapp, slack etc. with families being separated by large distances combined by time difference, sometimes text based communication is the only choice.</p>\n<p>Dora brings life to the text. She is an AR host, who understands the importance of eye contact and gestures. It is as though speaking to a live person. She can even show images and videos which so often form a part of our texts.</p>\n<p>Right now Dora is connected with text messages, we would like to expand this connection to include emails and other communication channels like watsapp and slack.</p>\n</div>",
            "content_md": "\nMost of our communication these days is via text. sms, email, watsapp, slack etc. with families being separated by large distances combined by time difference, sometimes text based communication is the only choice.\n\n\nDora brings life to the text. She is an AR host, who understands the importance of eye contact and gestures. It is as though speaking to a live person. She can even show images and videos which so often form a part of our texts.\n\n\nRight now Dora is connected with text messages, we would like to expand this connection to include emails and other communication channels like watsapp and slack.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/n-1",
            "title": "N+1",
            "blurb": "A Magic Leap storytelling experience. Immersive theatre meets AR in this tale of escaping a boring job.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/dQw4w9WgXcQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "The book with image target and AI-generated poem",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/675/datas/original.JPG"
                },
                {
                    "title": "&quot;Very important&quot; file to alphabetize",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/679/datas/original.png"
                },
                {
                    "title": "Initial Idea",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/689/datas/original.JPG"
                },
                {
                    "title": "Fakeout app logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/693/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Casey Krub",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/680/595/datas/profile.png"
                },
                {
                    "name": "E P",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/89e33029000fc2cdb78dcbd5b78361bb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Samuel Lepoil",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/610/datas/profile.jpg"
                },
                {
                    "name": "Christopher Dahm",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fae25bbfd35e8fc4ddfa6f54e159a9cf?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "ListheDis",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/17203774?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "lumen-sdk",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Location</h2>\n<p>E14, 3rd Floor, Conference Room E15-359</p>\n<h2>Dev Tools</h2>\n<p>Unity</p>\n<h2>SDKs</h2>\n<p>Lumen</p>\n<h2>Libraries</h2>\n<p>Standard Unity and Magic Leap libraries</p>\n<h2>Assets in project we did not create</h2>\n<p>Photo marker: Flickr <a href=\"https://www.flickr.com/photos/usgeologicalsurvey/16927843961/\" rel=\"nofollow\">U.S. Geological Survey\n\"View on the Blue River near Mount Powell, looking up. Summit County, Colorado. 1874.\"</a><br/>\nFor The Bristlecone Snag by a computer (AI poetry bot by Zack Scholl) p. 32 <a href=\"https://issuu.com/dukeupb/docs/thearchive_fall2011\" rel=\"nofollow\">The Archive Fall 2011</a><br/>\n<a href=\"https://www.turbosquid.com/3d-models/3d-button-emergency-1326689\" rel=\"nofollow\">Red Button</a><br/>\n<a href=\"https://freesound.org/people/Nightwatcher98/sounds/407292/\" rel=\"nofollow\">Office Ambience</a></p>\n<h2>Inspiration</h2>\n<p>Inspired by the video games Fa\u00e7ade, Keep Talking and Nobody Explodes, Virtual Virtual Reality and immersive experiences like Draw Me Close which blend real actors within virtual experiences. </p>\n<h2>What it does</h2>\n<p>An AR (Magic Leap) storytelling experience with an actor. You are in a boring office environment, you put the headset on and a mysterious narrator helps you to escape. </p>\n</div>",
            "content_md": "\n## Location\n\n\nE14, 3rd Floor, Conference Room E15-359\n\n\n## Dev Tools\n\n\nUnity\n\n\n## SDKs\n\n\nLumen\n\n\n## Libraries\n\n\nStandard Unity and Magic Leap libraries\n\n\n## Assets in project we did not create\n\n\nPhoto marker: Flickr [U.S. Geological Survey\n\"View on the Blue River near Mount Powell, looking up. Summit County, Colorado. 1874.\"](https://www.flickr.com/photos/usgeologicalsurvey/16927843961/)  \n\nFor The Bristlecone Snag by a computer (AI poetry bot by Zack Scholl) p. 32 [The Archive Fall 2011](https://issuu.com/dukeupb/docs/thearchive_fall2011)  \n\n[Red Button](https://www.turbosquid.com/3d-models/3d-button-emergency-1326689)  \n\n[Office Ambience](https://freesound.org/people/Nightwatcher98/sounds/407292/)\n\n\n## Inspiration\n\n\nInspired by the video games Fa\u00e7ade, Keep Talking and Nobody Explodes, Virtual Virtual Reality and immersive experiences like Draw Me Close which blend real actors within virtual experiences. \n\n\n## What it does\n\n\nAn AR (Magic Leap) storytelling experience with an actor. You are in a boring office environment, you put the headset on and a mysterious narrator helps you to escape. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/xrology",
            "title": "ARology",
            "blurb": "We are providing a new experience for Clinicians and Radiologists to interact with 3D Medical data",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312316073?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "in2utes",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46495186?height=180&v=4&width=180"
                },
                {
                    "name": "Derrick Wilson-Duncan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/514/038/datas/profile.jpg"
                }
            ],
            "built_with": [
                "datavisualization",
                "javascript",
                "webxr"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>ER Doctors, Trauma Surgeons, and Radiologic technologists spend a lot of time collaborating with one another to evaluate Medical images of their patients. Scenarios where these clinicians are not in the same City, State, Country, or even the same Time Zone are very common. </p>\n<h2>What it does</h2>\n<p>Allows Radiologic technologists and Health professionals to easily collaborate with one another to provide quick and efficient evaluations of MRI, CT, and Ultrasound Imagery.</p>\n<h2>How I built it</h2>\n<p>This experience was built with a combination of </p>\n<h2>Challenges I ran into</h2>\n<p>Choosing an efficient way </p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We made an awesome prototype for this hackathon and meet a lot of cool people along the way.</p>\n<h2>What I learned</h2>\n<p>We learned a lot about the emerging VR and AR tools and the evolving ecosystem these technologies and their use cases.</p>\n<h2>What's next for ARology</h2>\n<p>evaluate the progress and define use case for further development</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nER Doctors, Trauma Surgeons, and Radiologic technologists spend a lot of time collaborating with one another to evaluate Medical images of their patients. Scenarios where these clinicians are not in the same City, State, Country, or even the same Time Zone are very common. \n\n\n## What it does\n\n\nAllows Radiologic technologists and Health professionals to easily collaborate with one another to provide quick and efficient evaluations of MRI, CT, and Ultrasound Imagery.\n\n\n## How I built it\n\n\nThis experience was built with a combination of \n\n\n## Challenges I ran into\n\n\nChoosing an efficient way \n\n\n## Accomplishments that I'm proud of\n\n\nWe made an awesome prototype for this hackathon and meet a lot of cool people along the way.\n\n\n## What I learned\n\n\nWe learned a lot about the emerging VR and AR tools and the evolving ecosystem these technologies and their use cases.\n\n\n## What's next for ARology\n\n\nevaluate the progress and define use case for further development\n\n\n"
        },
        {
            "source": "https://devpost.com/software/file-destruction-vr",
            "title": "File Destruction VR ",
            "blurb": "Delete buttons are for losers. Torch, smash, explode and ultimately destroy your digital life with File DestructionVR",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "mhrahmani",
                    "about": "I participated in game design and narrative, plus some concept arts and sketches. I also did all the sound design in Audition. ",
                    "photo": "https://graph.facebook.com/10212805238942578/picture?height=180&width=180"
                },
                {
                    "name": "Michael Shumikhin",
                    "about": "File to game object conversion and windows api interaction",
                    "photo": "https://www.gravatar.com/avatar/a2cc7eac78703930e5dbca39f77299cc?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Thomas Watson",
                    "about": "I created some simple 3d assets in Blender, then put those and others into our Unity project. A lot of my time was spent tweaking things to better fit the feel we were after. I made springy buttons that are really fun to push.",
                    "photo": "https://avatars3.githubusercontent.com/u/35503719?height=180&v=4&width=180"
                },
                {
                    "name": "Lisa Lokshina",
                    "about": "I partook in the experience design of the project, creating the wireframes and making sure we address all interaction points in the experience. ",
                    "photo": "https://www.gravatar.com/avatar/44a1235cd3cc664f7adf9684fc8356b7?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "audtion",
                "blender",
                "csharp",
                "gimp",
                "maya",
                "unity"
            ],
            "content_html": "<div>\n<h2>What it does</h2>\n<p>File Destruction VR is a virtual reality experience that allows you to destroy your digital life and watch it die forever. </p>\n<p>Users enter a virtual room where they see files from their desktop on a virtual monitor. With a simple press of a button, those files materialize into the virtual room. Users are then provided with a variety of exquisite destruction tools to utterly obliterate the files into oblivion. </p>\n<p>Got a picture of your ex? Shred it to tiny pieces. Term paper got you all stressed out? Try blowtorching it to hell. </p>\n<p>Have you ever wondered - if you die in VR, do you die in real life? Well, in File Destruction VR, destroying files in VR will destroy the files for real, and forever, so you never ever see them again. </p>\n<h2>How we built it</h2>\n<p>The whole experience is built in Unity. We created some of our models in Blender, while others were free downloads from Turbosquid and Google Poly.</p>\n<h2>Inspiration</h2>\n<p>It\u2019s like Job Simulator or a Rage Room with real-world consequences, where the content is your own stuff and the goal is to destroy it. </p>\n<h2>Challenges we ran into</h2>\n<p>Unity, version control, scripting simultaneously for a single project is always a challenge. </p>\n<h2>What's next for File Destruction VR</h2>\n<p>Our vision for the next iteration is to provide users with more fun and engaging tools to destroy their files with and relieve some of their pent up anger. Think files being flushed down the toiler, teleports that can be activated so you can send your files into other universes, fire dragons, toilet flushes, and the likes. </p>\n</div>",
            "content_md": "\n## What it does\n\n\nFile Destruction VR is a virtual reality experience that allows you to destroy your digital life and watch it die forever. \n\n\nUsers enter a virtual room where they see files from their desktop on a virtual monitor. With a simple press of a button, those files materialize into the virtual room. Users are then provided with a variety of exquisite destruction tools to utterly obliterate the files into oblivion. \n\n\nGot a picture of your ex? Shred it to tiny pieces. Term paper got you all stressed out? Try blowtorching it to hell. \n\n\nHave you ever wondered - if you die in VR, do you die in real life? Well, in File Destruction VR, destroying files in VR will destroy the files for real, and forever, so you never ever see them again. \n\n\n## How we built it\n\n\nThe whole experience is built in Unity. We created some of our models in Blender, while others were free downloads from Turbosquid and Google Poly.\n\n\n## Inspiration\n\n\nIt\u2019s like Job Simulator or a Rage Room with real-world consequences, where the content is your own stuff and the goal is to destroy it. \n\n\n## Challenges we ran into\n\n\nUnity, version control, scripting simultaneously for a single project is always a challenge. \n\n\n## What's next for File Destruction VR\n\n\nOur vision for the next iteration is to provide users with more fun and engaging tools to destroy their files with and relieve some of their pent up anger. Think files being flushed down the toiler, teleports that can be activated so you can send your files into other universes, fire dragons, toilet flushes, and the likes. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/clue-7xr9ts",
            "title": "IN.CLASS",
            "blurb": "*IN.CLASS* is an immersive in-classroom tool for students and teachers that helps with focus and learning.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312386857?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "IN.CLASS",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/743/408/datas/original.jpg"
                },
                {
                    "title": "Concept Art: Student watching teacher",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/499/datas/original.jpg"
                },
                {
                    "title": "Desktop during math class",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/536/datas/original.jpg"
                },
                {
                    "title": "Desktop during science",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/537/datas/original.jpg"
                },
                {
                    "title": "Looking up and seeing live closed-captioning",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/539/datas/original.jpg"
                },
                {
                    "title": "Sequence 1 (Closed Caption UI)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/584/datas/original.png"
                },
                {
                    "title": "Sequence 2 (CC + Desk UI))",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/585/datas/original.png"
                },
                {
                    "title": "Sequence 3 (Desk UI)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/586/datas/original.png"
                },
                {
                    "title": "Sequence 4 (Desk UI)",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/587/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Tuba Ozkan",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/21373984?height=180&v=4&width=180"
                },
                {
                    "name": "JJ Rohrer",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/372/datas/profile.jpg"
                },
                {
                    "name": "Taeyeon Kim",
                    "about": "",
                    "photo": "https://graph.facebook.com/1924651354269918/picture?height=180&width=180"
                },
                {
                    "name": "Nouf A.",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/22034616?height=180&v=4&width=180"
                },
                {
                    "name": "Edward Lok",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c36628556d3c1dca3bc4d404adef027f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "ibm-watson",
                "magic-leap",
                "poly-toolkit",
                "unity"
            ],
            "content_html": "<div>\n<h2>Tools/SDKs/APIs</h2>\n<p>Magic Leap, Unity, IBM Watson Unity Plugin (Speech-to-Text), Free models from Unity Asset Store.</p>\n<h2>Inspiration</h2>\n<p>Modern schools are still based upon an old factory paradigm that struggles to meet the individualized needs that each student deserves. Schools won't go away anytime soon. Student variation will never go away. With <strong>In. Class,</strong> it is the boring, uniform classroom that goes away.</p>\n<p>We started with a <strong>North Star</strong> of helping english learners. Research shows the students learn a language best when shown closed-captions below a video.  We wanted to add this to real-life transcription to classrooms.  Once we brought that technology to the student, it opened a rich world classroom possibilities.  <strong>IN.CLASS</strong> is <strong>accessible</strong> by providing <em>closed captioning</em>, by tracking <em>focus</em>, and by facilitating truly <em>differentiated learning</em>.</p>\n<p><strong>IN.CLASS</strong> is an immersive in-classroom tool for students and teachers that helps with focus and learning.\nBy providing unique tools, such as closed captions and 3D visual aids, it increases students comprehension and eases the teaching burden. While teachers can review the engagement level of the students, real-time transcriptions of the course support english learners. 3D tools let teachers push 3d objects to helps everyone better learn the material.  Differentiation lets a teacher push, say, a <em>3D heart</em> to each student's desk, but each students may receive different models of varying complexity, depending upon their own learning plan.</p>\n<h2>What it does</h2>\n<p><strong>IN.CLASS</strong> is a platform that helps both the teacher, and the student.</p>\n<p><strong>IN.CLASS</strong> brings augmented reality to the classroom, and gives teachers a control panel to monitor student progress, follow lesson plans, and bring Mixed-Reality-aware tools to their fingertips. </p>\n<p>Students see their desks transformed from a boring slab, into a dynamic workbench. The student can receive pushed learning-objects from the teacher.  Items can range from simple quiz questions, to personalized learning aids, such as 3D visual aids and experiential simulations. </p>\n<p>Feedback tools, such as automatic attentiveness tracking, give teachers ways to more easily tailor the the class in real-time.</p>\n<h2>Technology Differentiation</h2>\n<ul>\n<li>Real-Time closed-captioning of the teacher</li>\n<li>Teacher initiated on-desk quizzes</li>\n<li>Teacher initiated 3D objects to the desktop</li>\n<li>Attention tracking of students, relaying to teacher</li>\n</ul>\n<h2>Value Differentiation</h2>\n<ul>\n<li>Built-In Differentiated Experiences (The teacher could push a 3D <em>heart</em> to the class, and AP students would receive a more detailed model, but standard student could receive a less details model appropriate to their own learning plan).</li>\n<li>Each student has a different set of needs, and can receive a unique experience, depending upon their situation.</li>\n<li>English Learners can receive real-time transcription</li>\n<li><p>Deaf students could read transcription, or potentially pipe-in real-time signing services</p></li>\n<li><p>A large class demands much from a teacher, our <strong>Teaching Control Panel</strong> helps</p></li>\n<li><p>Instantly track attention and/or confusion (via eye tracking) and tailor the lesson, accordingly.</p></li>\n<li><p>Quickly and dynamically push 3D visual aids to students</p></li>\n</ul>\n<h2>How we built it</h2>\n<p>We're using the Magic Leap to create a mixed-reality environment for the student.  Unity is our engine, and IBM Watson provides our real-time transcription services.</p>\n<h2>Challenges we ran into</h2>\n<p>None of us had worked with the Magic Leap before, so a lot of time was with system basics. We explored the capabilities of the system and spent way to long getting the dev tools installed.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>The transcription concept works surprisingly well.</p>\n<h2>What we learned</h2>\n<p>Brainstorming with a diverse group can lead to great new ideas.</p>\n<h2>What's next for <strong>In. Class</strong></h2>\n<p><strong>IN.CLASS</strong> is a fun proof of concept that shows a possible future for the classroom, but the underlying idea of the  teacher-student connected platform is a powerful concept that could implemented with existing technologies and budgets.  Our team hopes to incubate the concept into a practical product.</p>\n<p>What would a one-year product look like? Imagine starting with Magic Leap classroom as the vision, but targeting classes with Smart Boards and iPads, which are in place today. A teacher you could still pushing quizzes, questions, and 3d objects to each iPad. Objects and questions could still be differentiated based upon the student. Teachers could project a room-sized object and students would lift their iPad to see an augmented view of the room, seeing that room-sized object.  </p>\n<p>Unrelated to learning objects, realtime transcripts, pushed to an mobile phone or to simpler over-the-eye displays (like Google Glass, or Epson Smart Glasses), could be implemented and tested in-class within the year.</p>\n</div>",
            "content_md": "\n## Tools/SDKs/APIs\n\n\nMagic Leap, Unity, IBM Watson Unity Plugin (Speech-to-Text), Free models from Unity Asset Store.\n\n\n## Inspiration\n\n\nModern schools are still based upon an old factory paradigm that struggles to meet the individualized needs that each student deserves. Schools won't go away anytime soon. Student variation will never go away. With **In. Class,** it is the boring, uniform classroom that goes away.\n\n\nWe started with a **North Star** of helping english learners. Research shows the students learn a language best when shown closed-captions below a video. We wanted to add this to real-life transcription to classrooms. Once we brought that technology to the student, it opened a rich world classroom possibilities. **IN.CLASS** is **accessible** by providing *closed captioning*, by tracking *focus*, and by facilitating truly *differentiated learning*.\n\n\n**IN.CLASS** is an immersive in-classroom tool for students and teachers that helps with focus and learning.\nBy providing unique tools, such as closed captions and 3D visual aids, it increases students comprehension and eases the teaching burden. While teachers can review the engagement level of the students, real-time transcriptions of the course support english learners. 3D tools let teachers push 3d objects to helps everyone better learn the material. Differentiation lets a teacher push, say, a *3D heart* to each student's desk, but each students may receive different models of varying complexity, depending upon their own learning plan.\n\n\n## What it does\n\n\n**IN.CLASS** is a platform that helps both the teacher, and the student.\n\n\n**IN.CLASS** brings augmented reality to the classroom, and gives teachers a control panel to monitor student progress, follow lesson plans, and bring Mixed-Reality-aware tools to their fingertips. \n\n\nStudents see their desks transformed from a boring slab, into a dynamic workbench. The student can receive pushed learning-objects from the teacher. Items can range from simple quiz questions, to personalized learning aids, such as 3D visual aids and experiential simulations. \n\n\nFeedback tools, such as automatic attentiveness tracking, give teachers ways to more easily tailor the the class in real-time.\n\n\n## Technology Differentiation\n\n\n* Real-Time closed-captioning of the teacher\n* Teacher initiated on-desk quizzes\n* Teacher initiated 3D objects to the desktop\n* Attention tracking of students, relaying to teacher\n\n\n## Value Differentiation\n\n\n* Built-In Differentiated Experiences (The teacher could push a 3D *heart* to the class, and AP students would receive a more detailed model, but standard student could receive a less details model appropriate to their own learning plan).\n* Each student has a different set of needs, and can receive a unique experience, depending upon their situation.\n* English Learners can receive real-time transcription\n* Deaf students could read transcription, or potentially pipe-in real-time signing services\n* A large class demands much from a teacher, our **Teaching Control Panel** helps\n* Instantly track attention and/or confusion (via eye tracking) and tailor the lesson, accordingly.\n* Quickly and dynamically push 3D visual aids to students\n\n\n## How we built it\n\n\nWe're using the Magic Leap to create a mixed-reality environment for the student. Unity is our engine, and IBM Watson provides our real-time transcription services.\n\n\n## Challenges we ran into\n\n\nNone of us had worked with the Magic Leap before, so a lot of time was with system basics. We explored the capabilities of the system and spent way to long getting the dev tools installed.\n\n\n## Accomplishments that we're proud of\n\n\nThe transcription concept works surprisingly well.\n\n\n## What we learned\n\n\nBrainstorming with a diverse group can lead to great new ideas.\n\n\n## What's next for **In. Class**\n\n\n**IN.CLASS** is a fun proof of concept that shows a possible future for the classroom, but the underlying idea of the teacher-student connected platform is a powerful concept that could implemented with existing technologies and budgets. Our team hopes to incubate the concept into a practical product.\n\n\nWhat would a one-year product look like? Imagine starting with Magic Leap classroom as the vision, but targeting classes with Smart Boards and iPads, which are in place today. A teacher you could still pushing quizzes, questions, and 3d objects to each iPad. Objects and questions could still be differentiated based upon the student. Teachers could project a room-sized object and students would lift their iPad to see an augmented view of the room, seeing that room-sized object. \n\n\nUnrelated to learning objects, realtime transcripts, pushed to an mobile phone or to simpler over-the-eye displays (like Google Glass, or Epson Smart Glasses), could be implemented and tested in-class within the year.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/form-1mhrpg",
            "title": "FORM. for Magic Leap",
            "blurb": "touch, feel, + play with emotions in magic leap using real time brainwave data from muse",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Daniel Sabio",
                    "about": "Sound Design, Audio Implementation, Technical Art",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/199/datas/profile.jpg"
                },
                {
                    "name": "Aidan Wolf",
                    "about": "AR Unity Development, Technical Art, Ideation",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/262/870/datas/profile.jpg"
                },
                {
                    "name": "winter willoughby",
                    "about": "Design, Technical Art, Ideation, UX, Project Management",
                    "photo": "https://www.gravatar.com/avatar/ad1b807ca62eb0e9951fc6bd65296f3a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Matthew Hoe",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/16183329?height=180&v=4&width=180"
                },
                {
                    "name": "Devon Kennedy",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/738/786/datas/profile.jpg"
                }
            ],
            "built_with": [
                "audio",
                "augmented-reality",
                "c#",
                "eeg",
                "magic-leap",
                "muse",
                "osc",
                "unity"
            ],
            "content_html": "<div>\n<p>MEET YOUR WORST FEARS, REVISIT YOUR BEST MEMORIES, EXPERIENCE ALL THE FEELS. your brainwave frequencies resonate with all sorts of mind states. the emotions you consciously + subconsciously experience will take form as particles with materials, sounds, and personalities to touch, throw, stretch, and pop. this is our ultimate mental health + wellness playground. the positive, the negative. love it. play with it. crush it. throw it across the room.</p>\n<h2>Inspiration</h2>\n<p>We are inspired by the power of emotions, the things that make us humans. We want to create a collective environment to see, touch, share, and ultimately actualize and form our emotions from realtime brainwave frequencies that translate into unique colors, materials, and particles before our very eyes.</p>\n<p>There is a dire need to revolutionize how we view mental health today. We wanted to de-stigmatize mental health by showing the world at the Hackathon that our emotions which naturally radiate from each of us are beautiful. Uniquely, we looked to create an experience that\u2019s impactful and meaningful by witnessing our own emotions emit before us from our very brain\u2014 and we are beyond ecstatic to announce that we have made FORM., the first mental health and wellness playground.</p>\n<h2>What it does</h2>\n<p>Using Magic Leap and the Muse brain sensing headband, FORM turns your emotions into sentient particle creatures, which you can then play with or let soak in like paint into the surfaces around you. At the end of the experience, everyone's emotions will be saved along with the room, to be shared as one incredible collaborative spatial art piece.</p>\n<h2>Built with..</h2>\n<h2>Engine: Unity <a href=\"https://unity3d.com/\" rel=\"nofollow\">https://unity3d.com/</a></h2>\n<h2>Platform: Magic Leap <a href=\"https://www.magicleap.com/\" rel=\"nofollow\">https://www.magicleap.com/</a></h2>\n<h2>Modeling &amp; Animation: <a href=\"https://www.blender.org/\" rel=\"nofollow\">https://www.blender.org/</a></h2>\n<h2>Music &amp; Audio: <a href=\"https://www.ableton.com/\" rel=\"nofollow\">https://www.ableton.com/</a></h2>\n<h2>Libraries: <a href=\"https://github.com/heaversm/unity-osc-receiver\" rel=\"nofollow\">https://github.com/heaversm/unity-osc-receiver</a></h2>\n<h2>Particle Effects: <a href=\"https://assetstore.unity.com/packages/tools/particles-effects/real-time-procedural-cable-simple-102196\" rel=\"nofollow\">https://assetstore.unity.com/packages/tools/particles-effects/real-time-procedural-cable-simple-102196</a></h2>\n<h2>ML: <a href=\"https://github.com/gjrgj/mltools\" rel=\"nofollow\">https://github.com/gjrgj/mltools</a></h2>\n<p>-</p>\n<h2>Challenges we ran into</h2>\n<p>Getting data from the Muse headband to the Magic Leap was a tricky process since it hasn't been done before. Luckily our audio mastermind Daniel worked with the Muse previoWe learned so much about the physical body, brainwaves, and how they interface with the human experience. It was amazing to learn about different brain states and their roles in our everyday lives, and how they serve as a metadata layer of our physical subconscious. In addition, we shared insightful conversations about vulnerability and openness in regards to emotions, and environments where we've felt encouraged to share, or inhibited to do so.</p>\n<p>On the technical side, we learned a lot about working with Magic Leap in Unity (as it was many of our first experiences with the device), and about using OSC (open sound control) as a data source for real-time bio-metric control wireless-ly between devices. There were also many small victories and \"wow\" moments we shared with each other through our diverse skill sets during the process.usly and came up with a great solution using OSC.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We feel proud to use the Muse headband and Magic Leap AR headset to help users empathize with themselves and others by creating art together with their EEG signals.</p>\n<h2>What we learned</h2>\n<p>We learned so much about the physical body, brainwaves, and how they interface with the human experience. It was amazing to learn about different brain states and their roles in our everyday lives, and how they serve as a metadata layer of our physical subconscious. In addition, we shared insightful conversations about vulnerability and openness in regards to emotions, and environments where we've felt encouraged to share, or inhibited to do so.</p>\n<p>On the technical side, we learned a lot about working with Magic Leap in Unity (as it was many of our first experiences with the device), and about using OSC (open sound control) as a data source for real-time bio-metric control wireless-ly between devices. There were also many small victories and \"wow\" moments we shared with each other through our diverse skill sets during the process.</p>\n<h2>What's next for FORM. for Magic Leap</h2>\n<p>There will be a mass, consumer-friendly iteration of Magic Leap within our lifetime. No longer bound to just a room-scale experience, this future device will be enabled by the AR cloud to create meshes of the physical world. With geo-localization of spatial information, FORM. allows its users to express their emotional states in the real world on an individual level -as well as in collaboration with others. The experience of FORM. has dual intentions: therapeutic effect &amp; creative outlet. There is a therapeutic effect to expressing your emotions which alleviates mental health strain, and painting is often used as a form of self-expression. As a result, a user would be able to bring their spatial computer with them to lunch, walk around in a local park, and paint their emotions -whether they are venting or reflecting.</p>\n</div>",
            "content_md": "\nMEET YOUR WORST FEARS, REVISIT YOUR BEST MEMORIES, EXPERIENCE ALL THE FEELS. your brainwave frequencies resonate with all sorts of mind states. the emotions you consciously + subconsciously experience will take form as particles with materials, sounds, and personalities to touch, throw, stretch, and pop. this is our ultimate mental health + wellness playground. the positive, the negative. love it. play with it. crush it. throw it across the room.\n\n\n## Inspiration\n\n\nWe are inspired by the power of emotions, the things that make us humans. We want to create a collective environment to see, touch, share, and ultimately actualize and form our emotions from realtime brainwave frequencies that translate into unique colors, materials, and particles before our very eyes.\n\n\nThere is a dire need to revolutionize how we view mental health today. We wanted to de-stigmatize mental health by showing the world at the Hackathon that our emotions which naturally radiate from each of us are beautiful. Uniquely, we looked to create an experience that\u2019s impactful and meaningful by witnessing our own emotions emit before us from our very brain\u2014 and we are beyond ecstatic to announce that we have made FORM., the first mental health and wellness playground.\n\n\n## What it does\n\n\nUsing Magic Leap and the Muse brain sensing headband, FORM turns your emotions into sentient particle creatures, which you can then play with or let soak in like paint into the surfaces around you. At the end of the experience, everyone's emotions will be saved along with the room, to be shared as one incredible collaborative spatial art piece.\n\n\n## Built with..\n\n\n## Engine: Unity <https://unity3d.com/>\n\n\n## Platform: Magic Leap <https://www.magicleap.com/>\n\n\n## Modeling & Animation: <https://www.blender.org/>\n\n\n## Music & Audio: <https://www.ableton.com/>\n\n\n## Libraries: <https://github.com/heaversm/unity-osc-receiver>\n\n\n## Particle Effects: <https://assetstore.unity.com/packages/tools/particles-effects/real-time-procedural-cable-simple-102196>\n\n\n## ML: <https://github.com/gjrgj/mltools>\n\n\n-\n\n\n## Challenges we ran into\n\n\nGetting data from the Muse headband to the Magic Leap was a tricky process since it hasn't been done before. Luckily our audio mastermind Daniel worked with the Muse previoWe learned so much about the physical body, brainwaves, and how they interface with the human experience. It was amazing to learn about different brain states and their roles in our everyday lives, and how they serve as a metadata layer of our physical subconscious. In addition, we shared insightful conversations about vulnerability and openness in regards to emotions, and environments where we've felt encouraged to share, or inhibited to do so.\n\n\nOn the technical side, we learned a lot about working with Magic Leap in Unity (as it was many of our first experiences with the device), and about using OSC (open sound control) as a data source for real-time bio-metric control wireless-ly between devices. There were also many small victories and \"wow\" moments we shared with each other through our diverse skill sets during the process.usly and came up with a great solution using OSC.\n\n\n## Accomplishments that we're proud of\n\n\nWe feel proud to use the Muse headband and Magic Leap AR headset to help users empathize with themselves and others by creating art together with their EEG signals.\n\n\n## What we learned\n\n\nWe learned so much about the physical body, brainwaves, and how they interface with the human experience. It was amazing to learn about different brain states and their roles in our everyday lives, and how they serve as a metadata layer of our physical subconscious. In addition, we shared insightful conversations about vulnerability and openness in regards to emotions, and environments where we've felt encouraged to share, or inhibited to do so.\n\n\nOn the technical side, we learned a lot about working with Magic Leap in Unity (as it was many of our first experiences with the device), and about using OSC (open sound control) as a data source for real-time bio-metric control wireless-ly between devices. There were also many small victories and \"wow\" moments we shared with each other through our diverse skill sets during the process.\n\n\n## What's next for FORM. for Magic Leap\n\n\nThere will be a mass, consumer-friendly iteration of Magic Leap within our lifetime. No longer bound to just a room-scale experience, this future device will be enabled by the AR cloud to create meshes of the physical world. With geo-localization of spatial information, FORM. allows its users to express their emotional states in the real world on an individual level -as well as in collaboration with others. The experience of FORM. has dual intentions: therapeutic effect & creative outlet. There is a therapeutic effect to expressing your emotions which alleviates mental health strain, and painting is often used as a form of self-expression. As a result, a user would be able to bring their spatial computer with them to lunch, walk around in a local park, and paint their emotions -whether they are venting or reflecting.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ablation-cascade-vr",
            "title": "Ablation Cascade VR",
            "blurb": "Space debris is a growing and terrifying problem. A problem you will solve with whatever tools you can find. In space",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/f_IUKyhEy9w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/067/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Costas Frost",
                    "about": "I made Girraffics",
                    "photo": "https://avatars2.githubusercontent.com/u/7755346?height=180&v=3&width=180"
                },
                {
                    "name": "Trevor Morrisey",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/095e515072e90eb6f0f0a6f460e334f4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "casparmkerr",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/28448519?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "photoshop",
                "steamvr",
                "substance-designer",
                "substance-painter",
                "unity",
                "zbrush"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Space debris is a huge threat to the future of space exploration but has been allowed to steadily build up for decades. It's far away and small which makes it easy to ignore. For now...</p>\n<h2>What it does</h2>\n<p>Ablation Cascade VR brings space debris to the spotlight. The user takes on the role of an astronaut who has been stranded in orbit after his shuttle was struck by debris. As more debris sails at the player, they'll need to destroy or dodge it. Although any debris they dodge will cycle around towards them again. Ultimately, with never-ending waves of increasing intensity, the player will inevitably lose eventually. The increasing intensity of the waves of debris represent the actual build up of space junk in Earth's atmosphere. Ablation Cascade VR is an arcade style game inspired by the classic Asteroid and is meant to be light-hearted and fun but it also provides an engaging way to draw attention to and quantify a growing problem.</p>\n<h2>How I built it</h2>\n<p>Ablation Cascade VR was built with Unity. We used SteamVR to manage input so it could be played with any major headset.</p>\n<h2>Challenges I ran into</h2>\n<p>Getting collisions to work correctly, lack of time</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>We made a VR game in a weekend. That's pretty cool</p>\n<h2>What I learned</h2>\n<p>Developing games is both fun and hard.</p>\n<h2>What's next for Ablation Cascade VR</h2>\n<p>We want to polish it, improve mechanics and perhaps publish it somewhere.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSpace debris is a huge threat to the future of space exploration but has been allowed to steadily build up for decades. It's far away and small which makes it easy to ignore. For now...\n\n\n## What it does\n\n\nAblation Cascade VR brings space debris to the spotlight. The user takes on the role of an astronaut who has been stranded in orbit after his shuttle was struck by debris. As more debris sails at the player, they'll need to destroy or dodge it. Although any debris they dodge will cycle around towards them again. Ultimately, with never-ending waves of increasing intensity, the player will inevitably lose eventually. The increasing intensity of the waves of debris represent the actual build up of space junk in Earth's atmosphere. Ablation Cascade VR is an arcade style game inspired by the classic Asteroid and is meant to be light-hearted and fun but it also provides an engaging way to draw attention to and quantify a growing problem.\n\n\n## How I built it\n\n\nAblation Cascade VR was built with Unity. We used SteamVR to manage input so it could be played with any major headset.\n\n\n## Challenges I ran into\n\n\nGetting collisions to work correctly, lack of time\n\n\n## Accomplishments that I'm proud of\n\n\nWe made a VR game in a weekend. That's pretty cool\n\n\n## What I learned\n\n\nDeveloping games is both fun and hard.\n\n\n## What's next for Ablation Cascade VR\n\n\nWe want to polish it, improve mechanics and perhaps publish it somewhere.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/driva",
            "title": "Driva",
            "blurb": "@MIT Media Lab 6th floor | An AR application for driver to be away from tired driving (for a long time)",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/1cFCw4NA5_w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jing Liu",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQEsf53-IZIJZA/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=bOwi1WHzbqA6IKJpvMD70Uw8rp7hhpT1IlPbrC6QZvI&v=beta&width=180"
                },
                {
                    "name": "\u9038\u8c6a \u9b4f",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5103AQFczj5wa1wv_A/profile-displayphoto-shrink_100_100/0?e=1553731200&height=180&t=TlrvwJwHGdIFKfuvbEF9JFCDR_V79-8ukvuyFwptH7c&v=beta&width=180"
                },
                {
                    "name": "Danny Lee (Ting-Hsun Lee)",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQEH_38AlMD9OQ/profile-displayphoto-shrink_100_100/0?e=1553731200&height=180&t=Ke80w1XVFoc8C9MpTNiudHzehAMtG-BQOwPOZVR59B0&v=beta&width=180"
                },
                {
                    "name": "Shaoying Tan",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/22299143?height=180&v=4&width=180"
                },
                {
                    "name": "Michael Peguero",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/421/007/datas/profile.PNG"
                }
            ],
            "built_with": [
                "android-studio",
                "blender",
                "carla",
                "maya",
                "true-ar-sdk",
                "unreal-engine"
            ],
            "content_html": "<div>\n<h1>Say goodbye to drowsy driving!</h1>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-87d2f94d00cfa634.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--N-43_lTb--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-87d2f94d00cfa634.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p>Hi, Driva! This is a wonderful AR application for drivers, which can help to ease drowsy feeling while Driving.</p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-8e1b2cf10b289d1c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--SaAfaPOw--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-8e1b2cf10b289d1c.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p>The inspiration comes from our daily life - almost everyone has a family member who drives a lot. Long Time Driving is one of the biggest factors of car accidents.</p>\n<p>And it is also because of the place, the moment, the feeling we get together and we want to value these days.</p>\n<p>Great thanks for our sponsor - Wayray Team, they help us on so many details and even improve their True AR SDK in the process. </p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-796188570e54c32f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--PGh4uPTM--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-796188570e54c32f.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<h2>What it does</h2>\n<p>Driva plays an important part in the AR application,Her mood drops as your drowse level rises while driving! Keep her happy and she keeps you safe :) Driva wants to meet you in the AR world protect you from this dangerous driving behavior.</p>\n<p>How to ease drowsy feeling while Driving? We have the three tips foir you:</p>\n<ul>\n<li>Move and Stretch</li>\n<li>Get Caffeine</li>\n<li>Have fun (Enjoy gallery)</li>\n</ul>\n<p>In fact, We are thinking about many parts of the future application. For example, because we can only run the app on simulator, It's cool to customize map as we wish or based on real geodata, so that we can add more POIs (like a virtual yelp), which also open a door towards potential business collboration.</p>\n<p>We hope it becomes a new lifestyle for drivers to enjoy a unique feeling of self-driving in your travelling. </p>\n<h2>How we built it</h2>\n<p>We build it with True AR SDK on Android Studio. With the help of Unreal Engine and CARLA, we build the custom map.</p>\n<p>Designing is the second important task for us. It is pixel style and driva has many statues which makes it a cool girl and can impress others the first time they see her.</p>\n<h2>Challenges we ran into</h2>\n<p>There are so many challenges when we develop with True SDK, for example:</p>\n<ul>\n<li>Can't customize the 2D UI because there is a lack of 2D UI API.</li>\n<li>It doesn't support sound effect.</li>\n<li>We can't change the opacity of the view.</li>\n<li>Hard to custom map.</li>\n<li>...</li>\n</ul>\n<p>However the challenges has been solved with the help from Wayray team(really appraciate!). </p>\n<p>The top challenge for us is a new team(built 3 days ago) and totally new SDK, UE, 3D Max... We have to learn fast and find the most efficient way to solve problems. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>During this period, we not only finish the project(maybe beta), but also collaborate with Wayray team to improve True AR SDK to make it more friendly, for example:</p>\n<ul>\n<li><p>Define your own map in simulator\n<img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-f98d25306726a189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--TMTuUDKW--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-f98d25306726a189.png%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p></li>\n<li><p>Realize 2D UI on Screen without existing API</p></li>\n</ul>\n<p>We hope to have the chance to finish it better.</p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-41736be08e807a2f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--VcyYVgSc--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-41736be08e807a2f.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<h2>What is the future of Driva</h2>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-da2195d591134d24.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--2VHsVRqx--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-da2195d591134d24.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<h2>Screenshoot</h2>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-de8b210ffd2c193e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--FHgGNngq--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-de8b210ffd2c193e.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-6753e072b7444ebf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--5zbhiqmD--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-6753e072b7444ebf.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-acd7917490e37657.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--OHhJzw3o--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-acd7917490e37657.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-463e88631e742ece.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--_oyYMceZ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-463e88631e742ece.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-78fff529e234a247.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--8xJywSkd--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-78fff529e234a247.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<h2>Video</h2>\n<p>Demo video: <a href=\"https://youtu.be/1cFCw4NA5_w\" rel=\"nofollow\">https://youtu.be/1cFCw4NA5_w</a>\nTeam Video: <a href=\"https://www.youtube.com/watch?v=hQCOEsuXxDo&amp;feature=youtu.be\" rel=\"nofollow\">https://www.youtube.com/watch?v=hQCOEsuXxDo&amp;feature=youtu.be</a></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-3987ed9710fceb05.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--x5c6FWy9--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-3987ed9710fceb05.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-dc52929e9f40558a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--tSHtNDSy--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-dc52929e9f40558a.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n<p><img alt=\"\" data-canonical-url=\"https://upload-images.jianshu.io/upload_images/1551023-662f7abc004f4a5d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" src=\"https://res.cloudinary.com/devpost/image/fetch/s--246mPQuz--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-662f7abc004f4a5d.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240\"/></p>\n</div>",
            "content_md": "\n# Say goodbye to drowsy driving!\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--N-43_lTb--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-87d2f94d00cfa634.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\nHi, Driva! This is a wonderful AR application for drivers, which can help to ease drowsy feeling while Driving.\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--SaAfaPOw--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-8e1b2cf10b289d1c.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\nThe inspiration comes from our daily life - almost everyone has a family member who drives a lot. Long Time Driving is one of the biggest factors of car accidents.\n\n\nAnd it is also because of the place, the moment, the feeling we get together and we want to value these days.\n\n\nGreat thanks for our sponsor - Wayray Team, they help us on so many details and even improve their True AR SDK in the process. \n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--PGh4uPTM--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-796188570e54c32f.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n## What it does\n\n\nDriva plays an important part in the AR application,Her mood drops as your drowse level rises while driving! Keep her happy and she keeps you safe :) Driva wants to meet you in the AR world protect you from this dangerous driving behavior.\n\n\nHow to ease drowsy feeling while Driving? We have the three tips foir you:\n\n\n* Move and Stretch\n* Get Caffeine\n* Have fun (Enjoy gallery)\n\n\nIn fact, We are thinking about many parts of the future application. For example, because we can only run the app on simulator, It's cool to customize map as we wish or based on real geodata, so that we can add more POIs (like a virtual yelp), which also open a door towards potential business collboration.\n\n\nWe hope it becomes a new lifestyle for drivers to enjoy a unique feeling of self-driving in your travelling. \n\n\n## How we built it\n\n\nWe build it with True AR SDK on Android Studio. With the help of Unreal Engine and CARLA, we build the custom map.\n\n\nDesigning is the second important task for us. It is pixel style and driva has many statues which makes it a cool girl and can impress others the first time they see her.\n\n\n## Challenges we ran into\n\n\nThere are so many challenges when we develop with True SDK, for example:\n\n\n* Can't customize the 2D UI because there is a lack of 2D UI API.\n* It doesn't support sound effect.\n* We can't change the opacity of the view.\n* Hard to custom map.\n* ...\n\n\nHowever the challenges has been solved with the help from Wayray team(really appraciate!). \n\n\nThe top challenge for us is a new team(built 3 days ago) and totally new SDK, UE, 3D Max... We have to learn fast and find the most efficient way to solve problems. \n\n\n## Accomplishments that we're proud of\n\n\nDuring this period, we not only finish the project(maybe beta), but also collaborate with Wayray team to improve True AR SDK to make it more friendly, for example:\n\n\n* Define your own map in simulator\n![](https://res.cloudinary.com/devpost/image/fetch/s--TMTuUDKW--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-f98d25306726a189.png%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n* Realize 2D UI on Screen without existing API\n\n\nWe hope to have the chance to finish it better.\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--VcyYVgSc--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-41736be08e807a2f.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n## What is the future of Driva\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--2VHsVRqx--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-da2195d591134d24.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n## Screenshoot\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--FHgGNngq--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-de8b210ffd2c193e.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--5zbhiqmD--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-6753e072b7444ebf.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--OHhJzw3o--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-acd7917490e37657.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--_oyYMceZ--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-463e88631e742ece.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--8xJywSkd--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-78fff529e234a247.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n## Video\n\n\nDemo video: <https://youtu.be/1cFCw4NA5_w>\nTeam Video: <https://www.youtube.com/watch?v=hQCOEsuXxDo&feature=youtu.be>\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--x5c6FWy9--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-3987ed9710fceb05.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--tSHtNDSy--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-dc52929e9f40558a.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n![](https://res.cloudinary.com/devpost/image/fetch/s--246mPQuz--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://upload-images.jianshu.io/upload_images/1551023-662f7abc004f4a5d.jpg%3FimageMogr2/auto-orient/strip%257CimageView2/2/w/1240)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/gutenberg-ar",
            "title": "Gutenberg AR",
            "blurb": "The entire Gutenberg Project (50k public domain books) optimized for AR HoloLens",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/BqSj87ICHEo?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jitinder Thakur",
                    "about": "Worked with unity and hololense at kit \nReinstated old code",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/729/711/datas/profile.jpg"
                },
                {
                    "name": "Alan Foster",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQH4xievny0uLQ/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=14iRe8enpptHfjEDFbCnh6OR-49J8EXBB9WR-UXjAhU&v=beta&width=180"
                },
                {
                    "name": "Akram Zeyada",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/662/274/datas/profile.jpg"
                },
                {
                    "name": "Trang Le",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0384e3523445e4454dba9e5022c8bf25?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "JIN Thakur",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/383/411/datas/profile.jpg"
                }
            ],
            "built_with": [
                "ar",
                "c#",
                "love",
                "microsoft-hololens",
                "mr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Gutenberg AR is designed to reformat and optimize ebooks and other 2D text documents to be searched and experienced using immersive computing and smartglasses.  The project was inspired by LoreBooks, which converted ebooks into 3D holograms, but couldn't be used to navigate different libraries and collections.</p>\n<h2>What it does</h2>\n<p>-Gutenberg AR allows users to surround themselves with a collection of random books pulled from the Gutenberg Project.  This experience is designed to optimize exploration and discovery of new content within the 50,000 public domain collection.</p>\n<h2>How we built it</h2>\n<p>-This project was constructed in Unity on the Microsoft HoloLens with assets from the LoreBooks project.</p>\n<h2>Challenges we ran into</h2>\n<p>-Because the LoreBooks project was created in 2016, it uses older versions of Unity (Unity 5.5.2) and Visual Studio 2015.  A majority of the code had become corrupted or was no longer functioning, which significantly reduced our ability to build on top of the project directly.</p>\n<p>-As a result, our exploration app was created separately until LoreBooks could be integrated with the newest versions of Unity.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>-Safety for a lot of E-books .\n-Easy to find your way to your E-books .\n-Help a lot of people save time and efforts .\n-Make reading session more comfortable and pace of mind.</p>\n<h2>What we learned</h2>\n<p>-Visual studio coding -  Unity - Vuforia - Web VR</p>\n<h2>What's next for Gutenberg AR</h2>\n<p>-The future goals for Gutenberg AR is to fully integrate the entire 50,000 books to function on AR, so that readers with special needs may use mixed reality headsets and smartglasses to more easily experience literature.  As headsets become cheaper and lighter, the quality of the experience will increase and allow for longer and more comfortable reading sessions.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nGutenberg AR is designed to reformat and optimize ebooks and other 2D text documents to be searched and experienced using immersive computing and smartglasses. The project was inspired by LoreBooks, which converted ebooks into 3D holograms, but couldn't be used to navigate different libraries and collections.\n\n\n## What it does\n\n\n-Gutenberg AR allows users to surround themselves with a collection of random books pulled from the Gutenberg Project. This experience is designed to optimize exploration and discovery of new content within the 50,000 public domain collection.\n\n\n## How we built it\n\n\n-This project was constructed in Unity on the Microsoft HoloLens with assets from the LoreBooks project.\n\n\n## Challenges we ran into\n\n\n-Because the LoreBooks project was created in 2016, it uses older versions of Unity (Unity 5.5.2) and Visual Studio 2015. A majority of the code had become corrupted or was no longer functioning, which significantly reduced our ability to build on top of the project directly.\n\n\n-As a result, our exploration app was created separately until LoreBooks could be integrated with the newest versions of Unity.\n\n\n## Accomplishments that we're proud of\n\n\n-Safety for a lot of E-books .\n-Easy to find your way to your E-books .\n-Help a lot of people save time and efforts .\n-Make reading session more comfortable and pace of mind.\n\n\n## What we learned\n\n\n-Visual studio coding - Unity - Vuforia - Web VR\n\n\n## What's next for Gutenberg AR\n\n\n-The future goals for Gutenberg AR is to fully integrate the entire 50,000 books to function on AR, so that readers with special needs may use mixed reality headsets and smartglasses to more easily experience literature. As headsets become cheaper and lighter, the quality of the experience will increase and allow for longer and more comfortable reading sessions.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/plantmail",
            "title": "PlantMail",
            "blurb": "An organic plant-based MR mail client",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Jack Wesson",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/104/datas/profile.jpg"
                },
                {
                    "name": "ChelleySherman",
                    "about": "",
                    "photo": "https://graph.facebook.com/10155841749466782/picture?height=180&width=180"
                },
                {
                    "name": "Vladimir Storm",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/480/datas/profile.png"
                },
                {
                    "name": "Lucas Rizzotto",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/457/250/datas/profile.jpg"
                }
            ],
            "built_with": [
                "ar",
                "data-visualization",
                "internet",
                "magic-leap",
                "plants",
                "unity"
            ],
            "content_html": "<div>\n<p>PlantMail is a Mixed Reality plant... and nature-based e-mail client!</p>\n<p>Here individual e-mails are displayed as white berries which you can pluck off the plant - the more e-mails you have in your inbox, the larger the plant becomes. Flowers represent messages from people you love. Berries change in size depending on the length of the e-mail. Plants that are not catered to age and die if you ignore them.</p>\n<p>Mixed Reality is an exciting new medium, but for the last few years most applications have been built following 2D graphical user interfaces. We asked ourselves, are paper-based metaphors truly the best way to communicate ideas in this new space? What if we let go of GUIs and tried to develop a new way to think about information representation that's truly inherent to 3D and this new way of understanding reality.</p>\n<p>So we went to nature for inspiration. Literally.</p>\n<p>By creating PlantMail, we designed the first Organic User Interface, making full use of nature-based metaphors to communicate information in software in an entirely new way that's intuitive, charming and healthy.</p>\n<p>This taught us a tremendous amount about generating content for people, and learning intuitive design and interaction through experience. PlantMail wants to become even more powerful, add voice interpretation for email composition, and even bring in some outside social components to see or interact with the plants for or with your shared community. </p>\n<p>What we used:</p>\n<ul>\n<li>More Effective Coroutines: <a href=\"https://assetstore.unity.com/packages/tools/animation/more-effective-coroutines-free-54975\" rel=\"nofollow\">https://assetstore.unity.com/packages/tools/animation/more-effective-coroutines-free-54975</a></li>\n<li>Low Poly Water GPU: <a href=\"https://github.com/danielzeller/Lowpoly-Water-Unity\" rel=\"nofollow\">https://github.com/danielzeller/Lowpoly-Water-Unity</a></li>\n<li>Dust Particles: <a href=\"https://forum.unity.com/threads/new-simple-dust-particle-system.341921/\" rel=\"nofollow\">https://forum.unity.com/threads/new-simple-dust-particle-system.341921/</a></li>\n<li>Procedural Tree Builder: <a href=\"https://github.com/mattatz/unity-procedural-tree\" rel=\"nofollow\">https://github.com/mattatz/unity-procedural-tree</a></li>\n</ul>\n</div>",
            "content_md": "\nPlantMail is a Mixed Reality plant... and nature-based e-mail client!\n\n\nHere individual e-mails are displayed as white berries which you can pluck off the plant - the more e-mails you have in your inbox, the larger the plant becomes. Flowers represent messages from people you love. Berries change in size depending on the length of the e-mail. Plants that are not catered to age and die if you ignore them.\n\n\nMixed Reality is an exciting new medium, but for the last few years most applications have been built following 2D graphical user interfaces. We asked ourselves, are paper-based metaphors truly the best way to communicate ideas in this new space? What if we let go of GUIs and tried to develop a new way to think about information representation that's truly inherent to 3D and this new way of understanding reality.\n\n\nSo we went to nature for inspiration. Literally.\n\n\nBy creating PlantMail, we designed the first Organic User Interface, making full use of nature-based metaphors to communicate information in software in an entirely new way that's intuitive, charming and healthy.\n\n\nThis taught us a tremendous amount about generating content for people, and learning intuitive design and interaction through experience. PlantMail wants to become even more powerful, add voice interpretation for email composition, and even bring in some outside social components to see or interact with the plants for or with your shared community. \n\n\nWhat we used:\n\n\n* More Effective Coroutines: <https://assetstore.unity.com/packages/tools/animation/more-effective-coroutines-free-54975>\n* Low Poly Water GPU: <https://github.com/danielzeller/Lowpoly-Water-Unity>\n* Dust Particles: <https://forum.unity.com/threads/new-simple-dust-particle-system.341921/>\n* Procedural Tree Builder: <https://github.com/mattatz/unity-procedural-tree>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/magic-mover-bl3e7p",
            "title": "Magic Mover",
            "blurb": "An AR game that works on motor, balance and cognitive rehabilitation either in the hospital or at home.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/gFib3k2hHVY?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Taha Vasowalla",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/561/020/datas/profile.jpg"
                },
                {
                    "name": "Bom Lee",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/41710237?height=180&v=4&width=180"
                },
                {
                    "name": "Paolo Meriggi",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/14932390?height=180&v=4&width=180"
                },
                {
                    "name": "ErinMitt",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/33756931?height=180&v=4&width=180"
                },
                {
                    "name": "Peter Abbondanzo",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/10366495?height=180&v=3&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "after-effects",
                "augmented-reality",
                "c#",
                "microsoft-hololens",
                "premiere",
                "unity",
                "visual-studio",
                "xd"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>AR technologies have been rapidly evolving only recently, and therefore have not been applied yet to the medical fields as much as VR has. However, AR seems to be promising in particular for home-based rehab because, unlike VR, AR overlaps virtual images onto the real world environments during simulation, thus possibly limiting the discomfort in patients. Recent evolutions of AR, like Hololens and Magic Leap One, by tracking the environmental structure where they are in, allow to literally place virtual objects in specific 3D positions, thus opening interesting perspectives for novel iteractive forms of rehabilitation.  </p>\n<p>We looked for fields where AR based rehab could contribute most, and found that post-stroke treatments could clearly benefit from the engaging interactive technology, which could help motivate and empower the patients to fully recover. In particular, this become important in the later stages of the rehab process or when patients no longer need hospital treatments, since they may want to work on their skills at home to get back to normal mobility and function. </p>\n<p>With Magic Mover, we could help by giving them the resources and motivation.</p>\n<h2>What it does</h2>\n<p>We decided to build an AR game involving interacting with (i.e. moving and stacking) virtual objects. The user would move/stack them by looking at the object and placing it on a virtual mat (both of them anchored in the real space). Over time, the levels will get harder, requiring the user to move around the real space with increasing the complexity and magnitude of movements.</p>\n<p>The fun looking objects and new technology would better motivate the patient to practice at home, and the app would get the patient up and moving without straining them to move objects that may result too heavy. Each game level would have new objects, allowing the patient to have a diverse experience tailored to their current abilities. They can increase the game difficulty when they feel that they have mastered the level, or repeat the level with the objects in slightly different spots to maintain interest.</p>\n<h2>How we built it</h2>\n<p>We spent a large portion of time designing the different components of the product in a way that would have enough versatility to improve motor, balance, and cognitive skills, but also have a unifying theme throughout. We also made sure that there was possibility for expansion into other fields through our project. Finally, we aimed to make it as fun as possible while working on these skills to keep patients interested and engaged in order to ensure they would actually use at home.</p>\n<p>The project has been developed with Unity 3D and targeted to Microsoft Hololens. We chose appealing assets and programmed them to glow when the user is looking at them, and show a timer. When this timer is full, the virual object is picked up and it is displayed in front of the user. Then the user can move it onto the mat, which would also bring up a timer and at the end of the timer, drop the object where you put it (the timer would allow positioning). We programmed each of these actions in Unity, and used the Unity Spacial Mapping Toolkit to randomly generate objects in the desired section of the room.</p>\n<p>We designed our interface in Adobe Illustrator and XD. Our interface is made with large clickable buttons to avoid too fine of movement for stroke patients. We also gave our game a catchy name and animated our title to communicate the movement and the new technology the patients would get to use. Our UI/UX will make the product attractive for the patients.</p>\n<p>SDKs: Poly Tools V1.1.2, Mixed Reality ToolKit</p>\n<p>Assets: AJ1_Dimensions_Apassara_J_dd_IMO_ExC_ , Bamboo_Fence_Panel_Jarlan_Perez_dVwByrifEmj , Crate_Robin_Brook_fKox3UXY7sU , \nEmpty_Chest_Daniel_Timko_3I2OiZ92Nz7 , Fish_Tank_<em>Chris_Ross_2Rb9zxgkDEM , Zub_Sphere_TheWave_MusicForTheM1nd_AKA_Zyro1331_0mlS0GO454d , Zub</em>Spheres_Multi_TheWave_MusicForTheM1nd_AKA_Zyro1331_1PtcTZc49nz</p>\n<p><strong>Audio Assets</strong>: \nMusic: </p>\n<p>Inspired.mp3  ( <a href=\"https://incompetech.com/music/royalty-free/index.html?isrc=usuan1600022\" rel=\"nofollow\">https://incompetech.com/music/royalty-free/index.html?isrc=usuan1600022</a>)\nSummer  (<a href=\"https://www.bensound.com/royalty-free-music/track/summer\" rel=\"nofollow\">https://www.bensound.com/royalty-free-music/track/summer</a>)</p>\n<p><em>Sounds</em>: </p>\n<p>Start Beep.wav ( ???? )\nzapsplat_bell_service_desk_press_single_18040.mp3 (from: Zapsplat.com)\nzapsplat_fantasy_magic_twinkle_burst_001_25773.mp3 (from: Zapsplat.com)\nDrop Noise.wav ( ???? ) \nzapsplat_fantasy_magic_twinkle_burst_001_25773 (from: Zapsplat.com)\nBell Ring.wav ( ???? )\nzapsplat_fantasy_magic_wand_ping_retro_style_001_24799.mp3 (from: Zapsplat.com)</p>\n<h2>Challenges we ran into</h2>\n<p>Working with the Hololens was a challenge because in the present version it is not possible to integrate or track hand movement in a way that wouldn't be too difficult for the patient (some of us even had trouble doing the pinching motion required for the Hololens). We adapted our project to moving things visually, which instead requires our patients to walk around more and get used to their houses again.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of our development with the Microsoft Hololens, because of the many potential activities that can be done and monitored with our program, and, most of all, because we turned a potential issue (not tracking the hands) into an opportunity, by allowing the user to go through all options without having to use upper limbs, which may be too hard for some of our patients.</p>\n<p>We also developed an easily navigable, understandable and aesthetic interface with sound effects that allows the game to look fun and sophisticated.</p>\n<h2>What we learned</h2>\n<p>We learned about poly assets, which allowed us to get much prettier assets for free, improving our project.</p>\n<p>Our group came in with many different skill sets and learned quite a bit about others skills and how to implement and work with these skills, allowing a stronger project than any of us individually.</p>\n<p>We learned how to work with the Microsoft Hololens in AR, and expanded our knowledge of Unity, including object manipulation and eyesight interaction, and how to randomly place virtual objects onto a real space.</p>\n<p>We spoke to experts in the field, Rafael Grossman and Richard Isaacs. Rafael suggested we make sure to cover skills they are trying to work on as well as possible, and also that our product would work for some patients recovering from concussions (Traumatic Brain Injury) Richard Isaacs helped us make sure we had entertaining assets, suggesting ways to use color and balancing and other cool features for maximal engagement. We applied their ideas to our project, and put ideas we didn\u2019t get to implement into our future plans.</p>\n<h2>What's next for Magic Mover</h2>\n<p>From a purely technical point of view, we hope to integrate the Leap Motion  controller or the Magic Leap to include hand tracking among the several forms of possible input. This will allow a more natural interaction with virtual objects, easing the picking up virtual objects in the earlier levels, and better grasping and moving them in advanced ones.</p>\n<p>We also hope to develop more levels to require more movement around the room, more challenging stacking patterns, and even bimanual activities, pronosupination and fine movement.</p>\n<p>Other possible developments would be developing exercises more focused on the cognitive side, i.e. by asking patients to put in the proper sequence virtual objects which include simple actions related to getting dressed or washing teeth or any other daily activities to increasingly challenge the subjects.</p>\n<p>We will add more menu options to allow hospitals to create accounts for each patient and provide trackable data on improvement.</p>\n<p>One important development would be the transfer of data into a cloud based repository, allowing remote monitoring and continuous supervision of the patients, as well as paving the way to new advanced processing (i.e. big data)</p>\n<p>Location: 3rd Floor, E15, outside room 322</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nAR technologies have been rapidly evolving only recently, and therefore have not been applied yet to the medical fields as much as VR has. However, AR seems to be promising in particular for home-based rehab because, unlike VR, AR overlaps virtual images onto the real world environments during simulation, thus possibly limiting the discomfort in patients. Recent evolutions of AR, like Hololens and Magic Leap One, by tracking the environmental structure where they are in, allow to literally place virtual objects in specific 3D positions, thus opening interesting perspectives for novel iteractive forms of rehabilitation. \n\n\nWe looked for fields where AR based rehab could contribute most, and found that post-stroke treatments could clearly benefit from the engaging interactive technology, which could help motivate and empower the patients to fully recover. In particular, this become important in the later stages of the rehab process or when patients no longer need hospital treatments, since they may want to work on their skills at home to get back to normal mobility and function. \n\n\nWith Magic Mover, we could help by giving them the resources and motivation.\n\n\n## What it does\n\n\nWe decided to build an AR game involving interacting with (i.e. moving and stacking) virtual objects. The user would move/stack them by looking at the object and placing it on a virtual mat (both of them anchored in the real space). Over time, the levels will get harder, requiring the user to move around the real space with increasing the complexity and magnitude of movements.\n\n\nThe fun looking objects and new technology would better motivate the patient to practice at home, and the app would get the patient up and moving without straining them to move objects that may result too heavy. Each game level would have new objects, allowing the patient to have a diverse experience tailored to their current abilities. They can increase the game difficulty when they feel that they have mastered the level, or repeat the level with the objects in slightly different spots to maintain interest.\n\n\n## How we built it\n\n\nWe spent a large portion of time designing the different components of the product in a way that would have enough versatility to improve motor, balance, and cognitive skills, but also have a unifying theme throughout. We also made sure that there was possibility for expansion into other fields through our project. Finally, we aimed to make it as fun as possible while working on these skills to keep patients interested and engaged in order to ensure they would actually use at home.\n\n\nThe project has been developed with Unity 3D and targeted to Microsoft Hololens. We chose appealing assets and programmed them to glow when the user is looking at them, and show a timer. When this timer is full, the virual object is picked up and it is displayed in front of the user. Then the user can move it onto the mat, which would also bring up a timer and at the end of the timer, drop the object where you put it (the timer would allow positioning). We programmed each of these actions in Unity, and used the Unity Spacial Mapping Toolkit to randomly generate objects in the desired section of the room.\n\n\nWe designed our interface in Adobe Illustrator and XD. Our interface is made with large clickable buttons to avoid too fine of movement for stroke patients. We also gave our game a catchy name and animated our title to communicate the movement and the new technology the patients would get to use. Our UI/UX will make the product attractive for the patients.\n\n\nSDKs: Poly Tools V1.1.2, Mixed Reality ToolKit\n\n\nAssets: AJ1\\_Dimensions\\_Apassara\\_J\\_dd\\_IMO\\_ExC\\_ , Bamboo\\_Fence\\_Panel\\_Jarlan\\_Perez\\_dVwByrifEmj , Crate\\_Robin\\_Brook\\_fKox3UXY7sU , \nEmpty\\_Chest\\_Daniel\\_Timko\\_3I2OiZ92Nz7 , Fish\\_Tank\\_*Chris\\_Ross\\_2Rb9zxgkDEM , Zub\\_Sphere\\_TheWave\\_MusicForTheM1nd\\_AKA\\_Zyro1331\\_0mlS0GO454d , Zub*Spheres\\_Multi\\_TheWave\\_MusicForTheM1nd\\_AKA\\_Zyro1331\\_1PtcTZc49nz\n\n\n**Audio Assets**: \nMusic: \n\n\nInspired.mp3 ( <https://incompetech.com/music/royalty-free/index.html?isrc=usuan1600022>)\nSummer (<https://www.bensound.com/royalty-free-music/track/summer>)\n\n\n*Sounds*: \n\n\nStart Beep.wav ( ???? )\nzapsplat\\_bell\\_service\\_desk\\_press\\_single\\_18040.mp3 (from: Zapsplat.com)\nzapsplat\\_fantasy\\_magic\\_twinkle\\_burst\\_001\\_25773.mp3 (from: Zapsplat.com)\nDrop Noise.wav ( ???? ) \nzapsplat\\_fantasy\\_magic\\_twinkle\\_burst\\_001\\_25773 (from: Zapsplat.com)\nBell Ring.wav ( ???? )\nzapsplat\\_fantasy\\_magic\\_wand\\_ping\\_retro\\_style\\_001\\_24799.mp3 (from: Zapsplat.com)\n\n\n## Challenges we ran into\n\n\nWorking with the Hololens was a challenge because in the present version it is not possible to integrate or track hand movement in a way that wouldn't be too difficult for the patient (some of us even had trouble doing the pinching motion required for the Hololens). We adapted our project to moving things visually, which instead requires our patients to walk around more and get used to their houses again.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of our development with the Microsoft Hololens, because of the many potential activities that can be done and monitored with our program, and, most of all, because we turned a potential issue (not tracking the hands) into an opportunity, by allowing the user to go through all options without having to use upper limbs, which may be too hard for some of our patients.\n\n\nWe also developed an easily navigable, understandable and aesthetic interface with sound effects that allows the game to look fun and sophisticated.\n\n\n## What we learned\n\n\nWe learned about poly assets, which allowed us to get much prettier assets for free, improving our project.\n\n\nOur group came in with many different skill sets and learned quite a bit about others skills and how to implement and work with these skills, allowing a stronger project than any of us individually.\n\n\nWe learned how to work with the Microsoft Hololens in AR, and expanded our knowledge of Unity, including object manipulation and eyesight interaction, and how to randomly place virtual objects onto a real space.\n\n\nWe spoke to experts in the field, Rafael Grossman and Richard Isaacs. Rafael suggested we make sure to cover skills they are trying to work on as well as possible, and also that our product would work for some patients recovering from concussions (Traumatic Brain Injury) Richard Isaacs helped us make sure we had entertaining assets, suggesting ways to use color and balancing and other cool features for maximal engagement. We applied their ideas to our project, and put ideas we didn\u2019t get to implement into our future plans.\n\n\n## What's next for Magic Mover\n\n\nFrom a purely technical point of view, we hope to integrate the Leap Motion controller or the Magic Leap to include hand tracking among the several forms of possible input. This will allow a more natural interaction with virtual objects, easing the picking up virtual objects in the earlier levels, and better grasping and moving them in advanced ones.\n\n\nWe also hope to develop more levels to require more movement around the room, more challenging stacking patterns, and even bimanual activities, pronosupination and fine movement.\n\n\nOther possible developments would be developing exercises more focused on the cognitive side, i.e. by asking patients to put in the proper sequence virtual objects which include simple actions related to getting dressed or washing teeth or any other daily activities to increasingly challenge the subjects.\n\n\nWe will add more menu options to allow hospitals to create accounts for each patient and provide trackable data on improvement.\n\n\nOne important development would be the transfer of data into a cloud based repository, allowing remote monitoring and continuous supervision of the patients, as well as paving the way to new advanced processing (i.e. big data)\n\n\nLocation: 3rd Floor, E15, outside room 322\n\n\n"
        },
        {
            "source": "https://devpost.com/software/xrt",
            "title": "XRT",
            "blurb": "From classical paintings to the latest viral image on Tumblr - the art world spills out into the room you're in.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/X2PjGBLUnOs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Nick Savarese",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/743/211/datas/profile.jpg"
                },
                {
                    "name": "Mike Yun",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/743/823/datas/profile.jpg"
                },
                {
                    "name": "Haoran Chang",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/ec0a15d2589116a16b0da4ed88550893?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lexi Townsend",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/30307583?height=180&v=4&width=180"
                },
                {
                    "name": "Travis Ho",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/432/636/datas/profile.PNG"
                }
            ],
            "built_with": [
                "arkit",
                "ios",
                "magic-leap",
                "unity",
                "unreal-engine",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>How to use new technologies to revitalize gallery art works? \nWe artwork come to life through Augmented Reality</p>\n<h2>What it does</h2>\n<p>XRT recognizes static images both in real life and on screens to augment and bring artwork to life. 2D animations, 3D animations, and sound will be blended into the static image to create the appearance of movement and dimension to flat images, with intractability.</p>\n<h2>How we built it</h2>\n<p>XRT uses the Magic Leap and Unreal to recognise the images and blend 2D and 3D models, assets and animations into the pre-existing artwork. It also provides the platform in order to view these images in an optimal format.</p>\n<h2>Challenges we ran into</h2>\n<p>Working with a new hardware like the Magic Leap did create challenges based on a lack of experience of the headset in general. Tracking issues in terms of perfection and blending shapes also proved difficult but adaptations were made to fully utilize its current capabilities.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Working with a completely new headset as well as developing for 2 different engines and devices is something the team is very proud of. We tried to increase the scope of different applications as well as development opportunities.</p>\n<h2>What we learned</h2>\n<p>Working with the Magic Leap, a headset most of our team hasn't worked with before, provided a great learning opportunity. We also learnt a lot regarding exporting assets from multiple softwares. Scoping is also an important element of hackathons that out team is now very aware of.</p>\n<h2>What's next for XRT</h2>\n<p>XRT aims to utilize the technology to create a completely new digital medium. Instead of moving images (Gifs) when browsing, images will be translated into a 3D and interactive space.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nHow to use new technologies to revitalize gallery art works? \nWe artwork come to life through Augmented Reality\n\n\n## What it does\n\n\nXRT recognizes static images both in real life and on screens to augment and bring artwork to life. 2D animations, 3D animations, and sound will be blended into the static image to create the appearance of movement and dimension to flat images, with intractability.\n\n\n## How we built it\n\n\nXRT uses the Magic Leap and Unreal to recognise the images and blend 2D and 3D models, assets and animations into the pre-existing artwork. It also provides the platform in order to view these images in an optimal format.\n\n\n## Challenges we ran into\n\n\nWorking with a new hardware like the Magic Leap did create challenges based on a lack of experience of the headset in general. Tracking issues in terms of perfection and blending shapes also proved difficult but adaptations were made to fully utilize its current capabilities.\n\n\n## Accomplishments that we're proud of\n\n\nWorking with a completely new headset as well as developing for 2 different engines and devices is something the team is very proud of. We tried to increase the scope of different applications as well as development opportunities.\n\n\n## What we learned\n\n\nWorking with the Magic Leap, a headset most of our team hasn't worked with before, provided a great learning opportunity. We also learnt a lot regarding exporting assets from multiple softwares. Scoping is also an important element of hackathons that out team is now very aware of.\n\n\n## What's next for XRT\n\n\nXRT aims to utilize the technology to create a completely new digital medium. Instead of moving images (Gifs) when browsing, images will be translated into a 3D and interactive space.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/drugmatic",
            "title": "Drugmatic",
            "blurb": "Drugmatic is an educational VR immersive experience to help teens fully understand the effects of drugs.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/5-fb9Ds3mkM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Wentao Pu",
                    "about": "Worked on integrating digital content into Unreal Engine and UE4 procedural content generation.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/748/337/datas/profile.jpg"
                },
                {
                    "name": "Shuting Jiang",
                    "about": "Worked on the environment building and UI/UX designing for this project.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/966/datas/profile.jpg"
                },
                {
                    "name": "Cindy Yunzhu Li",
                    "about": "I worked on the interaction design and 3D modeling for the project. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/513/datas/profile.jpg"
                },
                {
                    "name": "Tianyue Wu",
                    "about": "Worked as a 3D artist for the project.",
                    "photo": "https://avatars3.githubusercontent.com/u/31944616?height=180&v=4&width=180"
                },
                {
                    "name": "SimiGu",
                    "about": "Worked on UE4 content developing.",
                    "photo": "https://avatars3.githubusercontent.com/u/31868735?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "autodesk",
                "leap-motion",
                "oculus",
                "quill",
                "tilt-brush",
                "unreal-engine"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Drugs have always been a serious problem especially for the younger generation. Unfortunately, most of teenagers started their journey with drugs because of curiosity. So how might we help them get away from drugs with respect to their curiosity and imagination? </p>\n<h2>What It Does</h2>\n<p>\"Drugmatic\" is a virtual reality drug experience focusing on teenagers. Based on research of true feelings after taking drugs, teenagers will have a taste of the illusion. During this experience, they will also have chances to make a change, and to stop this dramatic wired and fake world. </p>\n<p>Once a decision is made, there is no going back. Fantastic or horrifying, they have to go through the entire illusion by themselves. By the end of this experience, they will realize the unpredictable result of drugs.</p>\n<h2>How We Built It</h2>\n<p>We started this project by exploring the VR drawing tools Tilt Brush and Quill VR in order to draw an illusions of drugs. \nWith this VR experience, we tried to rebuilt the scene where this event happened. Player could make their decision by themselves using leap motion.</p>\n<h4>Brainstorming and Problem definition</h4>\n<p>In order to decide the concept, we did a brain storming session. At first we thought about several ideas but in the end we made our mind to dive into a drug experience.</p>\n<h4>Research</h4>\n<p>For us, it's very difficult to recreate an experience without a real reference. Thus we started our project by doing some research about what illusion would be like after taking drugs. We looked at some articles, videos and movies, decided to create the entire illusion world based on the research.</p>\n<h4>Storyboard</h4>\n<p>It took us a lot of time on the storyboard. We hoped our player could enjoy the fancy part of the illusion but also the scary side. It's very important to let our players know that the illusion of drugs can lead you to a horrible world with the most scary result you may never have a chance to regret.</p>\n<h4>Style and Design</h4>\n<p>During our research, we found out that there are a lot of common things of drug illusion such as higher color contrast, fluid and strange world, strange physical contact...These changes reminds us of hand-drawing. We decided to use Tilt Brush and Quill to build the world and animations. We also wanted to have some interaction for our players in the illusion world.</p>\n<h4>Programming and Exporting Assets</h4>\n<p>This is the time when everything started to become messy. We watched a lot of tutorials, tried a million times how to export our drawings into Unreal Engine, learn how to build animations...The process is bitter but it's also a fun journey. </p>\n<h2>Challenges We Ran Into</h2>\n<ul>\n<li>Develop a solution to explore the world of drug effects based on research.</li>\n<li>Integrate Tilt Brush assets into different graphics engines</li>\n<li>Limit time and resources to create an educational experience which depends on high quality art pieces.</li>\n</ul>\n<h2>Accomplishments that We are Proud of</h2>\n<ul>\n<li>Successfully bring Tilt Brush VR painter into the whole creation work flow.</li>\n<li>Build an incredible illusion world.</li>\n</ul>\n<h2>Interaction</h2>\n<ul>\n<li>Natural hand movement instead of traditional controllers</li>\n<li>Create your own illusion by hand drawing</li>\n<li>Choose your own adventure</li>\n<li>Incorporate taste and touch into own VR experience</li>\n</ul>\n<h2>What We Learned</h2>\n<ul>\n<li>Drugs may make you high for a moment, but the cost of the illusion will be extremely <strong>high</strong>, too.</li>\n<li>New creation tools</li>\n<li>Teamwork</li>\n<li>Time Management is important </li>\n<li>Some toolkits are not compatible!! </li>\n</ul>\n<h2>What's Next for Drugmatic?</h2>\n<ul>\n<li>Refine project by adding interactions and visual effects</li>\n<li>Research more on drug effects</li>\n<li>Play test among teenagers</li>\n<li>Reach out schools, provide this new way for advisors and teachers.</li>\n<li>Reach out researchers and medical center, provide our project as a assistant to help people get rid of drugs.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nDrugs have always been a serious problem especially for the younger generation. Unfortunately, most of teenagers started their journey with drugs because of curiosity. So how might we help them get away from drugs with respect to their curiosity and imagination? \n\n\n## What It Does\n\n\n\"Drugmatic\" is a virtual reality drug experience focusing on teenagers. Based on research of true feelings after taking drugs, teenagers will have a taste of the illusion. During this experience, they will also have chances to make a change, and to stop this dramatic wired and fake world. \n\n\nOnce a decision is made, there is no going back. Fantastic or horrifying, they have to go through the entire illusion by themselves. By the end of this experience, they will realize the unpredictable result of drugs.\n\n\n## How We Built It\n\n\nWe started this project by exploring the VR drawing tools Tilt Brush and Quill VR in order to draw an illusions of drugs. \nWith this VR experience, we tried to rebuilt the scene where this event happened. Player could make their decision by themselves using leap motion.\n\n\n#### Brainstorming and Problem definition\n\n\nIn order to decide the concept, we did a brain storming session. At first we thought about several ideas but in the end we made our mind to dive into a drug experience.\n\n\n#### Research\n\n\nFor us, it's very difficult to recreate an experience without a real reference. Thus we started our project by doing some research about what illusion would be like after taking drugs. We looked at some articles, videos and movies, decided to create the entire illusion world based on the research.\n\n\n#### Storyboard\n\n\nIt took us a lot of time on the storyboard. We hoped our player could enjoy the fancy part of the illusion but also the scary side. It's very important to let our players know that the illusion of drugs can lead you to a horrible world with the most scary result you may never have a chance to regret.\n\n\n#### Style and Design\n\n\nDuring our research, we found out that there are a lot of common things of drug illusion such as higher color contrast, fluid and strange world, strange physical contact...These changes reminds us of hand-drawing. We decided to use Tilt Brush and Quill to build the world and animations. We also wanted to have some interaction for our players in the illusion world.\n\n\n#### Programming and Exporting Assets\n\n\nThis is the time when everything started to become messy. We watched a lot of tutorials, tried a million times how to export our drawings into Unreal Engine, learn how to build animations...The process is bitter but it's also a fun journey. \n\n\n## Challenges We Ran Into\n\n\n* Develop a solution to explore the world of drug effects based on research.\n* Integrate Tilt Brush assets into different graphics engines\n* Limit time and resources to create an educational experience which depends on high quality art pieces.\n\n\n## Accomplishments that We are Proud of\n\n\n* Successfully bring Tilt Brush VR painter into the whole creation work flow.\n* Build an incredible illusion world.\n\n\n## Interaction\n\n\n* Natural hand movement instead of traditional controllers\n* Create your own illusion by hand drawing\n* Choose your own adventure\n* Incorporate taste and touch into own VR experience\n\n\n## What We Learned\n\n\n* Drugs may make you high for a moment, but the cost of the illusion will be extremely **high**, too.\n* New creation tools\n* Teamwork\n* Time Management is important\n* Some toolkits are not compatible!!\n\n\n## What's Next for Drugmatic?\n\n\n* Refine project by adding interactions and visual effects\n* Research more on drug effects\n* Play test among teenagers\n* Reach out schools, provide this new way for advisors and teachers.\n* Reach out researchers and medical center, provide our project as a assistant to help people get rid of drugs.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-sound-garden",
            "title": "Garden of Sounds",
            "blurb": "A VR experience that promotes healthy gardens while providing an auditory and visual experience.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/FIGEGNidrVk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Team 83_ Garden of Sounds_VR Project",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/542/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Brian Hui",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/22160758?height=180&v=4&width=180"
                },
                {
                    "name": "Alejandro Gonzalez",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/46790277?height=180&v=4&width=180"
                },
                {
                    "name": "Kaclark468",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/46722305?height=180&v=4&width=180"
                },
                {
                    "name": "Imen Maaroufi",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_-m7HL3c6fJRjeSoBj0JHwXgFERETmuNB40RQ_NR6wj6hmpqw-EB6Sz9FHYvha0MFY0BWenvbiY68DHvcOjEqDzNwQY63DHKIOjEI6qTQSx1SFOwBtup5Q87B6IpAkHFVBYaXTQyfcV-?height=180&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "armswinger-vr-locomotion-system",
                "mixed-reality-toolkit",
                "rhino",
                "unity"
            ],
            "content_html": "<div>\n<h2>What it does</h2>\n<p>An immersive VR experience that takes the user through a unique gardening experience where every plant plays its own instrument taking part in a soundscape of \u201cplant-created\u201d music around the user. As the user waters the plants, music grows out of their garden. By watering each plant, the user can fill the \u201cwater status indicator\u201d for each plant, allowing for a beautiful outcome in the end; the imagery and sound of their efforts realized, and presented as a \u201cfruit\u201d of that effort.</p>\n<h2>How we built it</h2>\n<p>With a dedicated team that gathered different talent, Unity developer, CAD designer, UI/UX specialists, and a sound designer, we combined our efforts to be able to use Unity, Rhino, Adobe Illustrator, Microsoft's Mixed Reality Toolkit and ArmSwinger VR Locomotion System. We created a Circular Garden, taking into consideration the transition of sounds and the volume of the closest plants while establishing a grid illustration that allows usability, natural moves, and harmony of the sounds.</p>\n<p>We worked on creating original musical phrases of increasing intensity to be attributed to the plant assets. Each of the 9 different plants had 3 different musical phrases which are representations of the 3 potential plant states (Wilted, needs water, and health) The sound layers interchange as we get closer to other plants making 729 different possibilities of musical combinations possible as the user chooses which plants to water. </p>\n<h2>Challenges we ran into</h2>\n<p>We believe we grew through overcoming challenges as, the iterative processes within the team, being able to prioritize features for building an effective demo with the given timeframe. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are very proud from the beginning of the process: creating and brainstorming for an idea that combined two ideas coming from two groups of people came up with and decided to compromise in order to give birth to this new idea. Isn\u2019t that how great ideas are born?\nWe were also all eager to learn and ask for help. The team has great energy and is very complimentary. </p>\n<h2>What we learned</h2>\n<p>As a team, we learned that VR helped us bridge our Imagination from what we see in Reality.</p>\n<h2>What's next for Garden of Sounds</h2>\n<p>In development: At the end of our demo, the user is presented with an apple from which they can discover seeds for growing their next garden. Users will be able to grow their own garden of sounds.\nUse cases: This could be an open experience through the BSO connecting people to music the world around them in a way that they can only experience that in VR. Also, some museums would be interested in providing that experience, or even some education centers where we are involving technology in the curriculum to teach children about nature and Fauna and Flora.</p>\n<h2>Location, floor, room</h2>\n<p>Lower Level Atrium (E15), Table 108 (Table 75)</p>\n<h2>The development tools used to build the project</h2>\n<p>We used Unity, Rhino, and Adobe Illustrator.</p>\n<h2>SDKs used in the project</h2>\n<p>ArmSwinger VR Locomotion System, Windows Mixed Reality Toolkit</p>\n<h2>Any assets used in the project that you did not create</h2>\n<p>Colorful Brick Wall Texture: <a href=\"https://www.freepik.com/free-vector/colorful-brick-wall-texture_957410.htm\" rel=\"nofollow\">https://www.freepik.com/free-vector/colorful-brick-wall-texture_957410.htm</a>\nEarth Texture: <a href=\"https://www.freepik.com/free-vector/earth-texture_997013.htm\" rel=\"nofollow\">https://www.freepik.com/free-vector/earth-texture_997013.htm</a>\nCartoon Stone Texture: <a href=\"https://www.freepik.com/free-vector/cartoon-stone-texture_976364.htm\" rel=\"nofollow\">https://www.freepik.com/free-vector/cartoon-stone-texture_976364.htm</a>\nFir: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/764025\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/764025</a>\nOak: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/834759\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/834759</a>\nBush1: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/1251544\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/1251544</a>\nTrees4Bushe2: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/506851\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/506851</a>\nSplitTree4Bush3: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/880453\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/880453</a>\nPrimrose: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/516226\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/516226</a>\nTulips: <a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/1270600\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/1270600</a>\nSunflower: <a href=\"https://free3d.com/3d-model/-sunflower-v1--572329.html\" rel=\"nofollow\">https://free3d.com/3d-model/-sunflower-v1--572329.html</a></p>\n</div>",
            "content_md": "\n## What it does\n\n\nAn immersive VR experience that takes the user through a unique gardening experience where every plant plays its own instrument taking part in a soundscape of \u201cplant-created\u201d music around the user. As the user waters the plants, music grows out of their garden. By watering each plant, the user can fill the \u201cwater status indicator\u201d for each plant, allowing for a beautiful outcome in the end; the imagery and sound of their efforts realized, and presented as a \u201cfruit\u201d of that effort.\n\n\n## How we built it\n\n\nWith a dedicated team that gathered different talent, Unity developer, CAD designer, UI/UX specialists, and a sound designer, we combined our efforts to be able to use Unity, Rhino, Adobe Illustrator, Microsoft's Mixed Reality Toolkit and ArmSwinger VR Locomotion System. We created a Circular Garden, taking into consideration the transition of sounds and the volume of the closest plants while establishing a grid illustration that allows usability, natural moves, and harmony of the sounds.\n\n\nWe worked on creating original musical phrases of increasing intensity to be attributed to the plant assets. Each of the 9 different plants had 3 different musical phrases which are representations of the 3 potential plant states (Wilted, needs water, and health) The sound layers interchange as we get closer to other plants making 729 different possibilities of musical combinations possible as the user chooses which plants to water. \n\n\n## Challenges we ran into\n\n\nWe believe we grew through overcoming challenges as, the iterative processes within the team, being able to prioritize features for building an effective demo with the given timeframe. \n\n\n## Accomplishments that we're proud of\n\n\nWe are very proud from the beginning of the process: creating and brainstorming for an idea that combined two ideas coming from two groups of people came up with and decided to compromise in order to give birth to this new idea. Isn\u2019t that how great ideas are born?\nWe were also all eager to learn and ask for help. The team has great energy and is very complimentary. \n\n\n## What we learned\n\n\nAs a team, we learned that VR helped us bridge our Imagination from what we see in Reality.\n\n\n## What's next for Garden of Sounds\n\n\nIn development: At the end of our demo, the user is presented with an apple from which they can discover seeds for growing their next garden. Users will be able to grow their own garden of sounds.\nUse cases: This could be an open experience through the BSO connecting people to music the world around them in a way that they can only experience that in VR. Also, some museums would be interested in providing that experience, or even some education centers where we are involving technology in the curriculum to teach children about nature and Fauna and Flora.\n\n\n## Location, floor, room\n\n\nLower Level Atrium (E15), Table 108 (Table 75)\n\n\n## The development tools used to build the project\n\n\nWe used Unity, Rhino, and Adobe Illustrator.\n\n\n## SDKs used in the project\n\n\nArmSwinger VR Locomotion System, Windows Mixed Reality Toolkit\n\n\n## Any assets used in the project that you did not create\n\n\nColorful Brick Wall Texture: <https://www.freepik.com/free-vector/colorful-brick-wall-texture_957410.htm>\nEarth Texture: <https://www.freepik.com/free-vector/earth-texture_997013.htm>\nCartoon Stone Texture: <https://www.freepik.com/free-vector/cartoon-stone-texture_976364.htm>\nFir: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/764025>\nOak: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/834759>\nBush1: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/1251544>\nTrees4Bushe2: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/506851>\nSplitTree4Bush3: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/880453>\nPrimrose: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/516226>\nTulips: <https://www.turbosquid.com/FullPreview/Index.cfm/ID/1270600>\nSunflower: <https://free3d.com/3d-model/-sunflower-v1--572329.html>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/weflow",
            "title": "WeFlow",
            "blurb": "WeFlow is an AR artistic multiplayer game that offers a fun experience for eldery people to gain a healthier body.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/yYakrqEWZAU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Grace Oh",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/2134ecd7d3e484fb6108ccdc1dc12e85?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Karen El Asmar",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b1fe616ef5dd57874ba403936c61ab5b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "mariabelenponcio",
                    "about": "",
                    "photo": "https://graph.facebook.com/10156066005958589/picture?height=180&width=180"
                },
                {
                    "name": "Alexander Gao",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/44277980?height=180&v=4&width=180"
                },
                {
                    "name": "Jomar G",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/601/750/datas/profile.jpg"
                }
            ],
            "built_with": [
                "adobe-audition",
                "adobe-xd",
                "magic-leap",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>8.5 percent of people worldwide and 47.8 million of US citizens are 65 years old or older. This population usually suffers back pains and arthrosis and lose flexibility, balance and strength. Over 50% of residents of nurseing homes experience depression and isolation during their stay. We want to offer a new fun experience for the elderlies to have a good time a become healthier. </p>\n<h2>What it does</h2>\n<p>WeFlow is an AR artistic multiplayer game that offers a fun experience for elderly people (65-year-old and up) to gain a healthier body. The users make exercises based in yoga and mindfulness following music and creating with their movements a huge collective virtual art work. Some exercises are individual and some of them are collective. </p>\n<h2>How we built it</h2>\n<p>We developed this project for magic leap and worked with hand tracking. The animations and coding were designed in unity using the Magic Leap SDK. </p>\n<h2>Challenges we ran into</h2>\n<p>Hand tracking, position of interaction, creating a pathway for making the app collaborative on multiple Magic Leap devices.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Coding the hand tracking and interactions. As well as working with hardware and software unfamiliar to us. </p>\n<h2>What we learned</h2>\n<p>Greater understanding of Magic Leap and its SDK's in addition to working in Unity.</p>\n<h2>What's next for WeFlow</h2>\n<p>Making the app collaborative and adding more movements as well as environments. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\n8.5 percent of people worldwide and 47.8 million of US citizens are 65 years old or older. This population usually suffers back pains and arthrosis and lose flexibility, balance and strength. Over 50% of residents of nurseing homes experience depression and isolation during their stay. We want to offer a new fun experience for the elderlies to have a good time a become healthier. \n\n\n## What it does\n\n\nWeFlow is an AR artistic multiplayer game that offers a fun experience for elderly people (65-year-old and up) to gain a healthier body. The users make exercises based in yoga and mindfulness following music and creating with their movements a huge collective virtual art work. Some exercises are individual and some of them are collective. \n\n\n## How we built it\n\n\nWe developed this project for magic leap and worked with hand tracking. The animations and coding were designed in unity using the Magic Leap SDK. \n\n\n## Challenges we ran into\n\n\nHand tracking, position of interaction, creating a pathway for making the app collaborative on multiple Magic Leap devices.\n\n\n## Accomplishments that we're proud of\n\n\nCoding the hand tracking and interactions. As well as working with hardware and software unfamiliar to us. \n\n\n## What we learned\n\n\nGreater understanding of Magic Leap and its SDK's in addition to working in Unity.\n\n\n## What's next for WeFlow\n\n\nMaking the app collaborative and adding more movements as well as environments. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/syncup-l14ip2",
            "title": "SyncUp",
            "blurb": "\"Human Tetris\" -- a collaborative VR game",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/xcC0SOUf51o?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Game",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/487/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Marcella Prieto",
                    "about": "I contributed embodiment, empathy, process design, concept design, game design, narrative, and sound capture. ",
                    "photo": "https://avatars0.githubusercontent.com/u/46794378?height=180&v=4&width=180"
                },
                {
                    "name": "Kaochoy Danny Saetern",
                    "about": "I worked on game mechanics and ux implementations. It was my first time working with networks and photon. We got the network room and coop experience but the coop game mechanic is almost there. ",
                    "photo": "https://media.licdn.com/mpr/mprx/0_xgY8aXcdJUlTfNUvxMH8_XgH0UTtuXwvP26ael1dj4T1dNoLMMH86XAHRp-1dLZZVMQ3i9Oelw-PE8fkraUy6r0kqw--E8mvPaU_DAoW0EFY8nvd0xDDS5XZNOHGi8JkVS0uE_g2Fuy?height=180&width=180"
                },
                {
                    "name": "Krishnan Unnikrishnan",
                    "about": "As a designer, I had a vision to create a real-time collaborative experience in VR that required an embodied interaction.  With these guiding principles, Marcella and I worked together to brainstorm, design, and create a playful experience that captures the joyful childhood experience of Tetris. ",
                    "photo": "https://avatars2.githubusercontent.com/u/41970674?height=180&v=4&width=180"
                },
                {
                    "name": "Colin Keenan",
                    "about": "I composed the score and other audio assets, animated the island where the play happens, and made the score icons.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/458/datas/profile.jpg"
                },
                {
                    "name": "Wendy Wei",
                    "about": "I implemented game mechanics and UX. We designed a way to project the user\u2019s physical position and controller positions onto the game board, as a mirror of the user\u2019s actions.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/467/datas/profile.jpg"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>Our process begins with feeling the joy of childhood play. </p>\n<p>From there, we created an optimistic hypothesis for the future by applying (1) real-time collaborative play and (2) whole-body interaction to a game with universal appeal and few barriers to entry. </p>\n<p>Enter Tetris.</p>\n<p>By borrowing from the syntax and grammar of Tetris, we were able to quickly develop a hypothesis for two or more players in multiple locations to assemble puzzle pieces together and enter the combined piece as a single solution. We prioritized fundamentally social interaction within the context of informed advocacy.</p>\n<p>We built the hack on Unity for Oculus Rift using Photon Universal Network. We built our visual assets in Tilt Brush to manipulate the Google Poly library at human scale. We composed audio tracks using MixCraft with a MIDI input controller to build a digitally composed original score. We recorded a library of unique sounds from detritus that was caught in bushes located within 10' of the Media Lab perimeter. We created a physical art installation to present our hack by littering our workspace with an ocean of Crystal Geyser plastic bottles rescued from Trash-destined bins on the 6th floor of the Media Lab. We believe in the power of advocacy through gaming and hope that a player's time spent using SyncUp will change their relationship with plastic bottles, birds, and the ocean.</p>\n<p>Our primary obstacle was the implicit bias of the event's design.</p>\n<p>Our hypothesis of designing collaborative play intersected with the priorities of the Hackathon's supply design. Teams were allowed to borrow only one Rift for the duration of the event and every available unit was loaned in this manner, leaving 0 inventory to rent for testing. So our hypothesis of collaborative play was not imminently testable from the outset. Nevertheless, we persisted. </p>\n<p>We are particularly proud of three things: networked real-time play, team diversity, and design process.</p>\n<p>1) We successfully networked two players to interact with each other in real-time towards a common goal.</p>\n<p>2) Our team was diverse in experience. Our diversity of experience informed each other's creative process, casting emerging perspectives and new discoveries upon existing design practices. </p>\n<p>3) We refined an iterative, values-based design process that can be applied to other familiar games which, by extension, creates a rubric for us to continue to coalesce around values of collaboration and embodiment within the context of informed advocacy.</p>\n<p>What's next for SyncUp is to apply the iterative, values-based design process to other familiar games and make them fundamentally social through the values of collaborative play, embodied interaction, and informed advocacy.</p>\n</div>",
            "content_md": "\nOur process begins with feeling the joy of childhood play. \n\n\nFrom there, we created an optimistic hypothesis for the future by applying (1) real-time collaborative play and (2) whole-body interaction to a game with universal appeal and few barriers to entry. \n\n\nEnter Tetris.\n\n\nBy borrowing from the syntax and grammar of Tetris, we were able to quickly develop a hypothesis for two or more players in multiple locations to assemble puzzle pieces together and enter the combined piece as a single solution. We prioritized fundamentally social interaction within the context of informed advocacy.\n\n\nWe built the hack on Unity for Oculus Rift using Photon Universal Network. We built our visual assets in Tilt Brush to manipulate the Google Poly library at human scale. We composed audio tracks using MixCraft with a MIDI input controller to build a digitally composed original score. We recorded a library of unique sounds from detritus that was caught in bushes located within 10' of the Media Lab perimeter. We created a physical art installation to present our hack by littering our workspace with an ocean of Crystal Geyser plastic bottles rescued from Trash-destined bins on the 6th floor of the Media Lab. We believe in the power of advocacy through gaming and hope that a player's time spent using SyncUp will change their relationship with plastic bottles, birds, and the ocean.\n\n\nOur primary obstacle was the implicit bias of the event's design.\n\n\nOur hypothesis of designing collaborative play intersected with the priorities of the Hackathon's supply design. Teams were allowed to borrow only one Rift for the duration of the event and every available unit was loaned in this manner, leaving 0 inventory to rent for testing. So our hypothesis of collaborative play was not imminently testable from the outset. Nevertheless, we persisted. \n\n\nWe are particularly proud of three things: networked real-time play, team diversity, and design process.\n\n\n1) We successfully networked two players to interact with each other in real-time towards a common goal.\n\n\n2) Our team was diverse in experience. Our diversity of experience informed each other's creative process, casting emerging perspectives and new discoveries upon existing design practices. \n\n\n3) We refined an iterative, values-based design process that can be applied to other familiar games which, by extension, creates a rubric for us to continue to coalesce around values of collaboration and embodiment within the context of informed advocacy.\n\n\nWhat's next for SyncUp is to apply the iterative, values-based design process to other familiar games and make them fundamentally social through the values of collaborative play, embodied interaction, and informed advocacy.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/heart-304i5h",
            "title": "heARt",
            "blurb": "HeARt Health AR improves clinical data visualization & communication of the medical staff for efficient patient care.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/4noifsxC6yA?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Hannes Bend",
                    "about": "Proposed idea. Formed and managed team. Initiated the idea to integrate webcam of Magic Leap for e.g. detection of vital sign detection (heart rate), extended to identification via face recognition (both via machine learning). Created video, devpost and facilitated communication between team, mentors and organizers. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/463/085/datas/profile.jpeg"
                },
                {
                    "name": "Niraioza Oza",
                    "about": "I helped my team to brainstorm for the critical question of how might we improve patient care utilizing AR technology. As physician I was able to articulate the unmet need of doctors and frustrations that they share during the course of healthcare delivery. I created icons and designs for the project. I was able to create prototype of UI using sketch and inVision. ",
                    "photo": "https://avatars3.githubusercontent.com/u/46817218?height=180&v=4&width=180"
                },
                {
                    "name": "Matthew Fisher",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/197/946/datas/profile.jpeg"
                },
                {
                    "name": "asalgargoosh",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/46693250?height=180&v=4&width=180"
                },
                {
                    "name": "Tushar Purang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/917/768/datas/profile.png"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<p>HeARt Health AR improves clinical data visualization &amp; communication of the medical staff for efficient patient care.</p>\n<p>According to a 2018 poll by Stanford Medicine, most doctors agree that EHR/EMR (electronic health/medical records), greatly contributes to physician burnout and needs a complete overhaul. \nMost problems in healthcare derive from poor communication and connectivity. \nThe smart use of technology can help.\nHeARt is an AR application to improve clinical data visualization and communication, for a safer and more efficient patient care.</p>\n<p>HeARth combines\n\u2013\u00a0Face recognition and patient identification\n\u2013\u00a0Access of EHR/EMR (electronic medical records) \n\u2013\u00a0Vital sign detection (heart rate via camera and machine learning)\n\u2013\u00a0Accessing data with hand gestures\n\u2013\u00a0Listening to the patient summary with audio\n\u2013\u00a0Recording of notes via voice and microphone</p>\n<h2>What it does</h2>\n<p>The heARt application offers face recognition with the built-in camera of the Magic Leap to identify the patient and access the clinical data. \nIt also offers detection of vital signs such as heart rate via built-in camera and machine learning right at the patient.\nThe doctor is then able to pull up medical records and listen to the patient summary with integrated audio files while with the patient. </p>\n<p>Currently without heARt, over 60% devoted to each patient are being spent by doctors separately with the medical records, and an additional about 20% of time talking to the medical staff. The heARt application allows the doctor to access the medical records and recording of the medical staff while with the patient and to spend over 80% of the time with the patient. </p>\n<p>The use of heARt lessens the total hours of work for doctors, adds valuable time at the patient for care, strengthens the doctor-patient relationships and effectively improves inpatient care. </p>\n<p>The AR application can also connect with other sensors via Bluetooth to display vital signs and clinical to the doctor. </p>\n<p>In addition to the easier work flow, it also offers a more hygienic solution by using hand gestures without touch of any screens of documents, as well as audio to listen to medical staff feedback and voice to record the diagnosis of the doctor. </p>\n<h2>How we built it</h2>\n<p>Languages: C#, Python\nApis: Magic Leap API\nHardware: Magic Leap, Windows laptop</p>\n<h2>Challenges we ran into</h2>\n<p>Networking the devices...</p>\n<h2>What's next for heARt</h2>\n<p>First steps made to use Magic Leap camera with machine learning to detect heart rate and other vital signs, and for face recognition to identify patients. Further establishing this.\nAlso to add other sensors to integrate multiple vital signs and patient data into the AR application. \nImprove networking between computers and Magic Leap. \nRefine UI and UX. \nAlready great feedback by mentors (accomplished surgeon and physician).</p>\n<h2>Our team</h2>\n<p>Hannes Bend: Founder of breathing.ai to personalize screens with biometrics and machine learning, multiple award-winning VR/AR projects with US-research universities and US-county on health, science, art and mindfulness. </p>\n<p>Alaa Algargoosh: Ph.D. Candidate in Architecture | Building Technology &amp; Architectural Acoustics at The University of Michigan | Master of Science in Architecture, Sustainable Architecture at University of Dammam, Saudi Arabia.</p>\n<p>Nirali Oza:\nAs a tech-savvy medical graduate and an altruistic designer, I am passionate about utilizing data, technology, and design to tackle the complex challenges facing today\u2019s world. Bachelor of Medicine and Bachelor of Surgery, MBBS from the Government Medical College Bhavnagar, Gujarat, India </p>\n<p>Matthew Allen Fisher\nWebXR Developer, 3D Modeling Enthusiast, Machine Learning Student, and CEO/Co-founder of Component Entity.</p>\n<p>Tushar Purang\nDelhi-based engineer, AR/VR Developer, hackathon enthusiast with a passion for designing and building experimental user interfaces.</p>\n</div>",
            "content_md": "\nHeARt Health AR improves clinical data visualization & communication of the medical staff for efficient patient care.\n\n\nAccording to a 2018 poll by Stanford Medicine, most doctors agree that EHR/EMR (electronic health/medical records), greatly contributes to physician burnout and needs a complete overhaul. \nMost problems in healthcare derive from poor communication and connectivity. \nThe smart use of technology can help.\nHeARt is an AR application to improve clinical data visualization and communication, for a safer and more efficient patient care.\n\n\nHeARth combines\n\u2013\u00a0Face recognition and patient identification\n\u2013\u00a0Access of EHR/EMR (electronic medical records) \n\u2013\u00a0Vital sign detection (heart rate via camera and machine learning)\n\u2013\u00a0Accessing data with hand gestures\n\u2013\u00a0Listening to the patient summary with audio\n\u2013\u00a0Recording of notes via voice and microphone\n\n\n## What it does\n\n\nThe heARt application offers face recognition with the built-in camera of the Magic Leap to identify the patient and access the clinical data. \nIt also offers detection of vital signs such as heart rate via built-in camera and machine learning right at the patient.\nThe doctor is then able to pull up medical records and listen to the patient summary with integrated audio files while with the patient. \n\n\nCurrently without heARt, over 60% devoted to each patient are being spent by doctors separately with the medical records, and an additional about 20% of time talking to the medical staff. The heARt application allows the doctor to access the medical records and recording of the medical staff while with the patient and to spend over 80% of the time with the patient. \n\n\nThe use of heARt lessens the total hours of work for doctors, adds valuable time at the patient for care, strengthens the doctor-patient relationships and effectively improves inpatient care. \n\n\nThe AR application can also connect with other sensors via Bluetooth to display vital signs and clinical to the doctor. \n\n\nIn addition to the easier work flow, it also offers a more hygienic solution by using hand gestures without touch of any screens of documents, as well as audio to listen to medical staff feedback and voice to record the diagnosis of the doctor. \n\n\n## How we built it\n\n\nLanguages: C#, Python\nApis: Magic Leap API\nHardware: Magic Leap, Windows laptop\n\n\n## Challenges we ran into\n\n\nNetworking the devices...\n\n\n## What's next for heARt\n\n\nFirst steps made to use Magic Leap camera with machine learning to detect heart rate and other vital signs, and for face recognition to identify patients. Further establishing this.\nAlso to add other sensors to integrate multiple vital signs and patient data into the AR application. \nImprove networking between computers and Magic Leap. \nRefine UI and UX. \nAlready great feedback by mentors (accomplished surgeon and physician).\n\n\n## Our team\n\n\nHannes Bend: Founder of breathing.ai to personalize screens with biometrics and machine learning, multiple award-winning VR/AR projects with US-research universities and US-county on health, science, art and mindfulness. \n\n\nAlaa Algargoosh: Ph.D. Candidate in Architecture | Building Technology & Architectural Acoustics at The University of Michigan | Master of Science in Architecture, Sustainable Architecture at University of Dammam, Saudi Arabia.\n\n\nNirali Oza:\nAs a tech-savvy medical graduate and an altruistic designer, I am passionate about utilizing data, technology, and design to tackle the complex challenges facing today\u2019s world. Bachelor of Medicine and Bachelor of Surgery, MBBS from the Government Medical College Bhavnagar, Gujarat, India \n\n\nMatthew Allen Fisher\nWebXR Developer, 3D Modeling Enthusiast, Machine Learning Student, and CEO/Co-founder of Component Entity.\n\n\nTushar Purang\nDelhi-based engineer, AR/VR Developer, hackathon enthusiast with a passion for designing and building experimental user interfaces.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/xr44",
            "title": "XR44 ",
            "blurb": "In an emergency room, trauma surgeons/Emergency Doc\u2019s are under pressure to evaluate patient as quickly as possible. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/wHEmKkNEC3c?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "in2utes",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46495186?height=180&v=4&width=180"
                },
                {
                    "name": "Bruce Hecht",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/583/897/datas/profile.png"
                },
                {
                    "name": "DewdLabs Wilson-Duncan",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/63424456?height=180&v=4&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>In an emergency room, trauma surgeons/Emergency Doc\u2019s are under pressure to evaluate patient as quickly as possible. In cases where more detail review is needed they order a CT scan which is acquired in a different room. Often to review the images they leave the patient to go to the imaging room, this is time consuming and potentially dangerous as they are away from.</h2>\n<h2>The system allows clinicians to review 3D data and collaborate</h2>\n<h2>How we built it: used DICOM images and imported into Vuforia, and exported to the Hololens</h2>\n<h2>Challenges we ran into: data conversion from DICOM to a readable version by Unity/Vuforia</h2>\n<h2>Accomplishments that we're proud of: narrowing the problem and obtaining good sample data to import into the XR environment</h2>\n<h2>What we learned: many are trying to do this, but it is a difficult problem</h2>\n<h2>What's next for XR44 : streamline the workflow to get a medical image into an AR environment.</h2>\n</div>",
            "content_md": "\n## In an emergency room, trauma surgeons/Emergency Doc\u2019s are under pressure to evaluate patient as quickly as possible. In cases where more detail review is needed they order a CT scan which is acquired in a different room. Often to review the images they leave the patient to go to the imaging room, this is time consuming and potentially dangerous as they are away from.\n\n\n## The system allows clinicians to review 3D data and collaborate\n\n\n## How we built it: used DICOM images and imported into Vuforia, and exported to the Hololens\n\n\n## Challenges we ran into: data conversion from DICOM to a readable version by Unity/Vuforia\n\n\n## Accomplishments that we're proud of: narrowing the problem and obtaining good sample data to import into the XR environment\n\n\n## What we learned: many are trying to do this, but it is a difficult problem\n\n\n## What's next for XR44 : streamline the workflow to get a medical image into an AR environment.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/i-know-you",
            "title": "I Know You",
            "blurb": "A HoloLens app to help users remember the names of people s/he meets",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312312941?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Mario Bourgoin",
                    "about": "I worked on the framework of the app, including voice triggers, picture taking, face recognition, and labeling.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/750/datas/profile.jpg"
                },
                {
                    "name": "Mohamed Raouf Seyam",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/0a48accafec2f0b012912e5cbdca1292?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Kamo4",
                    "about": "",
                    "photo": "https://graph.facebook.com/10218399945612257/picture?height=180&width=180"
                },
                {
                    "name": "Robert Alexander",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/d92e219717337552f724635e1e782615?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "azure",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration: At some point, we all ran into the awkward situation when you see someone that remembers your name while you do not remember their's</h2>\n<h2>What it does: I Know You (our app's name) stores the names of people you meet, and associates the name to their face. When you meet the same person again, the app recognizes her/his face, then displays the associated name next to the person. That way, the app reminds you who the person you're talking to is and saves you from potential embarrassment. To save a face and a name, the user simply has to have the target acquaintance in the app's field of view, then triggers the name save by saying phrases such as \"Nice to meet you, \". The app will then store the name with that person's face, and will pull the name whenever the same person is in the FoV. The app responds only to the user, so it avoids interference from similar conversations ocuring in the background.</h2>\n<h2>How I built it: Our team combined elements of face recognition, image editing, machine learning, and voice recognition</h2>\n<h2>Challenges I ran into: We struggled with having the app focus only on 1 face when there are more than 2 people in the FoV</h2>\n<h2>Accomplishments that I'm proud of: My team and I learned to collaborate and expanded our coding skills. I am proud to work with such amazing people!</h2>\n<h2>What I learned: Problem solving, troubleshooting, self-confidence to say the least</h2>\n<h2>What's next for I Know You: We hope to improve on this app so that image recall and analysis is faster and seamless to the user so that it integrates smoothly within that natural flow of a typical 1st conversation in the professional world. We also hope to integrate more than just a person's name - following a similar approach, we could get the app to capture the person's alma mater, company, birthday, key interests, and other common aspects</h2>\n</div>",
            "content_md": "\n## Inspiration: At some point, we all ran into the awkward situation when you see someone that remembers your name while you do not remember their's\n\n\n## What it does: I Know You (our app's name) stores the names of people you meet, and associates the name to their face. When you meet the same person again, the app recognizes her/his face, then displays the associated name next to the person. That way, the app reminds you who the person you're talking to is and saves you from potential embarrassment. To save a face and a name, the user simply has to have the target acquaintance in the app's field of view, then triggers the name save by saying phrases such as \"Nice to meet you, \". The app will then store the name with that person's face, and will pull the name whenever the same person is in the FoV. The app responds only to the user, so it avoids interference from similar conversations ocuring in the background.\n\n\n## How I built it: Our team combined elements of face recognition, image editing, machine learning, and voice recognition\n\n\n## Challenges I ran into: We struggled with having the app focus only on 1 face when there are more than 2 people in the FoV\n\n\n## Accomplishments that I'm proud of: My team and I learned to collaborate and expanded our coding skills. I am proud to work with such amazing people!\n\n\n## What I learned: Problem solving, troubleshooting, self-confidence to say the least\n\n\n## What's next for I Know You: We hope to improve on this app so that image recall and analysis is faster and seamless to the user so that it integrates smoothly within that natural flow of a typical 1st conversation in the professional world. We also hope to integrate more than just a person's name - following a similar approach, we could get the app to capture the person's alma mater, company, birthday, key interests, and other common aspects\n\n\n"
        },
        {
            "source": "https://devpost.com/software/community-engine",
            "title": "Community Engine",
            "blurb": "A XR citizen participation engine to help communities better visualize change (in large infrastructure projects)",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/0eEPh1iDO_U?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jordan Pelovitz",
                    "about": "I worked on art assets, interface, and concept.",
                    "photo": "https://www.gravatar.com/avatar/da69e64971e4e4c7c9c2b9b040bf4305?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Marlon Romero",
                    "about": "I work in 3D assets.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/505/255/datas/profile.jpg"
                },
                {
                    "name": "Ezequiellenard",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/32619536?height=180&v=4&width=180"
                },
                {
                    "name": "Siddhesh Gupte",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5103AQEeKu0pGwy-qA/profile-displayphoto-shrink_100_100/0?e=1547078400&height=180&t=LCIshJjvKO4C2GXoJ6DYt8L024rUtvfJX_iD9hyYy5w&v=beta&width=180"
                },
                {
                    "name": "Yosun Chang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/406/935/datas/profile.jpg"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Many community members struggle to convey the impact of long term projects to their people. Statistics, words and even pictures are often insufficient to communicate the necessity of starting (or stopping) an investment. What\u2019s more, people often absorb this information in a vacuum, unaware of their larger community\u2019s thoughts on the matter.</p>\n<p>Community Engine is a Collaborative XR (CEXR) app that gives any individual the ability to share their vison for infrastructure developments that impact their lives. It also lets them build a case for their idea by collecting feedback from the community based on both time and space.</p>\n<h2>Innovation</h2>\n<ul>\n<li>Each survey response is a position + time coordinate, and can easily be shared using a XRurl.xyz </li>\n<li>Combines AR and VR in a single view for data visualizations in large scale survey results.</li>\n<li>The analogy of VR as the empathy machine carries when we switch to seeing the individual's literal perspective in the aggregate data.<br/></li>\n<li>\n</li></ul>\n<h2>What it does</h2>\n<ul>\n<li><p>Help communities understand complex data and long term investment plans.</p></li>\n<li><p>Give individuals an opportunity to give feedback on proposed developments and learn how others in their community feel.</p></li>\n<li><p>Empower even individuals to easily get their idea in front of their peers in an engaging way and also generate highly granular research on their opinions about a proposal.</p></li>\n<li><p>An AR marker PostCard-based activism campaign</p></li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMany community members struggle to convey the impact of long term projects to their people. Statistics, words and even pictures are often insufficient to communicate the necessity of starting (or stopping) an investment. What\u2019s more, people often absorb this information in a vacuum, unaware of their larger community\u2019s thoughts on the matter.\n\n\nCommunity Engine is a Collaborative XR (CEXR) app that gives any individual the ability to share their vison for infrastructure developments that impact their lives. It also lets them build a case for their idea by collecting feedback from the community based on both time and space.\n\n\n## Innovation\n\n\n* Each survey response is a position + time coordinate, and can easily be shared using a XRurl.xyz\n* Combines AR and VR in a single view for data visualizations in large scale survey results.\n* The analogy of VR as the empathy machine carries when we switch to seeing the individual's literal perspective in the aggregate data.\n* \n\n\n## What it does\n\n\n* Help communities understand complex data and long term investment plans.\n* Give individuals an opportunity to give feedback on proposed developments and learn how others in their community feel.\n* Empower even individuals to easily get their idea in front of their peers in an engaging way and also generate highly granular research on their opinions about a proposal.\n* An AR marker PostCard-based activism campaign\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dnar",
            "title": "DNAr",
            "blurb": "An app that helps teachers explain difficult subjects, like DNA, through the use of AR synced with voice recognition.",
            "awards": [],
            "videos": [],
            "images": [
                {
                    "title": "To begin, click start! ",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/727/datas/original.jpg"
                },
                {
                    "title": "After you learn about how DNA is made, you know that we share 16% with bananas. Say Bananas!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/728/datas/original.jpg"
                },
                {
                    "title": "Watch as the DNA forms a banana.",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/729/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Emily Steele",
                    "about": "I was an an animator and designer!",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQGfkQZyAeu1kg/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=5EFDv1m3hCLsJHKfseEY2JV6zD0TvGcEoSGcXZT86HA&v=beta&width=180"
                },
                {
                    "name": "Juan Pablo",
                    "about": "I worked as a programmer. With the help of the other developers we decided to do voice recognition to trigger augmented reality",
                    "photo": "https://avatars2.githubusercontent.com/u/29048064?height=180&v=4&width=180"
                },
                {
                    "name": "Pascal Phoa",
                    "about": "I was a Unity developer with two others and mainly helped integrated the animation from Maya into Unity, and compiled the sound effects.",
                    "photo": "https://www.gravatar.com/avatar/b8b41772396087c685a741feb699176d?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Jingorho",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/11937176?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "github",
                "google",
                "hlsl",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>What it does</h2>\n<p>It transforms the teaching of difficult subjects (like DNA and microbiology) into a fun, interactive, group building experience. Through the use of an AR narrative trigged by the kids\u2019 own verbal responses to questions asked by the teacher, we promote engagement and reward good reading without taking away from the teacher\u2019s control and ability to run the class. So everybody learns together at the same pace.</p>\n<h2>How we built it</h2>\n<p>We used UNITY, VUFORIA and GOOGLE\u2019s VOICE API, powered by voice activation.</p>\n<h2>Challenges we ran into</h2>\n<p>There weren\u2019t any prior examples of educational experiences that combined the use of AR and Voice Recognition to control the narrative that we could find. That led us to have to find a way to combine VUFORIA\u2019s engine with Google\u2019s voice API and use the result to drive the narrative. The mentors us helped a lot in that.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>To combine the two technologies into one single experience that can run according to a previous class scripts of Questions and Answers.</p>\n<h2>What we learned</h2>\n<p>We learned that not only is necessary to find a technological way of solving a problem, but also it is necessary to check whether that educational approach is sound and exciting. Our first script was much more \u201cpresentational\u201d whereas the second one used more of the Q&amp;A format that stimulates the community of students to find the solutions together organically. In this approach, the teacher becomes both the facilitator and the one who controls the experience, even though it is an AR experience. We think that the best ones are in the crossover between real and virtual.</p>\n<h2>What's next for DNAr</h2>\n<p>What DNAr really can be in the future in a platform for the teaching of any difficult subjects, not only microbiology. Also we wanted to incorporate into the App a Quiz at the end, so the students could immediately tested on their ability to grasp what\u2019s being taught.</p>\n</div>",
            "content_md": "\n## What it does\n\n\nIt transforms the teaching of difficult subjects (like DNA and microbiology) into a fun, interactive, group building experience. Through the use of an AR narrative trigged by the kids\u2019 own verbal responses to questions asked by the teacher, we promote engagement and reward good reading without taking away from the teacher\u2019s control and ability to run the class. So everybody learns together at the same pace.\n\n\n## How we built it\n\n\nWe used UNITY, VUFORIA and GOOGLE\u2019s VOICE API, powered by voice activation.\n\n\n## Challenges we ran into\n\n\nThere weren\u2019t any prior examples of educational experiences that combined the use of AR and Voice Recognition to control the narrative that we could find. That led us to have to find a way to combine VUFORIA\u2019s engine with Google\u2019s voice API and use the result to drive the narrative. The mentors us helped a lot in that.\n\n\n## Accomplishments that we're proud of\n\n\nTo combine the two technologies into one single experience that can run according to a previous class scripts of Questions and Answers.\n\n\n## What we learned\n\n\nWe learned that not only is necessary to find a technological way of solving a problem, but also it is necessary to check whether that educational approach is sound and exciting. Our first script was much more \u201cpresentational\u201d whereas the second one used more of the Q&A format that stimulates the community of students to find the solutions together organically. In this approach, the teacher becomes both the facilitator and the one who controls the experience, even though it is an AR experience. We think that the best ones are in the crossover between real and virtual.\n\n\n## What's next for DNAr\n\n\nWhat DNAr really can be in the future in a platform for the teaching of any difficult subjects, not only microbiology. Also we wanted to incorporate into the App a Quiz at the end, so the students could immediately tested on their ability to grasp what\u2019s being taught.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/keep-searching-and-nobody-explodes",
            "title": "Keep Searching and Nobody Explodes",
            "blurb": "Turn-based bomb defusion game for Magic Leap.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/OL22EorEbVk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Andres Ornelas",
                    "about": "I worked on the game flow and overall mechanics, threading together the work of the team. I also contributed with 3D assets made with Spatiate.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/688/660/datas/profile.jpg"
                },
                {
                    "name": "Dylan Dawkins",
                    "about": "I worked on programming the object handling mechanics as well as the explosions in the scene. ",
                    "photo": "https://avatars1.githubusercontent.com/u/43074920?height=180&v=4&width=180"
                },
                {
                    "name": "Janet Hwang",
                    "about": "",
                    "photo": "https://graph.facebook.com/2156828784342172/picture?height=180&width=180"
                },
                {
                    "name": "Allison Ing",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/3e9f282242498d0d3d5cbb949026f009?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Coby Palivathukal",
                    "about": "",
                    "photo": "https://graph.facebook.com/10218792081939973/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "magic-leap",
                "maya",
                "photoshop",
                "rider",
                "spatiate",
                "unity",
                "unitygltf",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We were inspired by our memories of childhood games like scavenger hunts, hide and seek, and 'I spy'. These games were simple but had the ability to elevate the magic of play in everyday environments.</p>\n<p>Our team started with the idea of 'Lava Leap', an AR, Magic Leap rendition of the analog game, 'The Floor is Lava!'. Further ideation on multi-player mechanics in spatial design led us to rethink how we could best leverage the real/virtual properties of an XR game. We realized the best way to leverage Magic Leap's spatial mapping, object occlusion, and controller interactivity was to pivot towards a game involving hiding and searching for a 3D object. The bomb aspect added some stakes to the game's entertaining narrative.</p>\n<h2>What it does</h2>\n<p>The game is a two player, turn-based game that uses a single Magic Leap One (ML1) headset. Set in a light-hearted and playful context, Player 1 sets a bomb and hides the tool needed to defuse it, while player 2 must then find the tool and use it to defuse the bomb before it explodes. Both players experience unexpected and fun surprises throughout the game that leads to playful and surreal interactions with 3D assets viewed using ML1.</p>\n<h2>How we built it</h2>\n<p>Unity Code, Spatiate, MLSDK<br/>\nSweat and tears.</p>\n<h2>Challenges we ran into</h2>\n<p>Some challenges we faced were implementing the relatively novel game mechanics and formulating a narrative to compliment our premise.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Implementing effective spatial design and intuitive user interactions in AR; creating unique assets using Spatiate; testing the game using the Magic Leap Simulator.</p>\n<h2>What we learned</h2>\n<p>How to create a compelling AR user experience with spatial design; how to effectively collaborate on a Magic Leap project using Unity and Github.</p>\n<h2>What's next for Keep Searching and Nobody Explodes</h2>\n<p>Adding additional levels to the game that pose additional challenges to the user; creating new and amusing ways for the user to use hand motions to interact with 3D assets in AR. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe were inspired by our memories of childhood games like scavenger hunts, hide and seek, and 'I spy'. These games were simple but had the ability to elevate the magic of play in everyday environments.\n\n\nOur team started with the idea of 'Lava Leap', an AR, Magic Leap rendition of the analog game, 'The Floor is Lava!'. Further ideation on multi-player mechanics in spatial design led us to rethink how we could best leverage the real/virtual properties of an XR game. We realized the best way to leverage Magic Leap's spatial mapping, object occlusion, and controller interactivity was to pivot towards a game involving hiding and searching for a 3D object. The bomb aspect added some stakes to the game's entertaining narrative.\n\n\n## What it does\n\n\nThe game is a two player, turn-based game that uses a single Magic Leap One (ML1) headset. Set in a light-hearted and playful context, Player 1 sets a bomb and hides the tool needed to defuse it, while player 2 must then find the tool and use it to defuse the bomb before it explodes. Both players experience unexpected and fun surprises throughout the game that leads to playful and surreal interactions with 3D assets viewed using ML1.\n\n\n## How we built it\n\n\nUnity Code, Spatiate, MLSDK  \n\nSweat and tears.\n\n\n## Challenges we ran into\n\n\nSome challenges we faced were implementing the relatively novel game mechanics and formulating a narrative to compliment our premise.\n\n\n## Accomplishments that we're proud of\n\n\nImplementing effective spatial design and intuitive user interactions in AR; creating unique assets using Spatiate; testing the game using the Magic Leap Simulator.\n\n\n## What we learned\n\n\nHow to create a compelling AR user experience with spatial design; how to effectively collaborate on a Magic Leap project using Unity and Github.\n\n\n## What's next for Keep Searching and Nobody Explodes\n\n\nAdding additional levels to the game that pose additional challenges to the user; creating new and amusing ways for the user to use hand motions to interact with 3D assets in AR. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/weather-window",
            "title": "Weather Window",
            "blurb": "Immersive AR weather app that visualizes forecast of world weather through a magic window.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/oOuufk62-YM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ji Young Chun",
                    "about": "Ideation, art direction, and Unity development (API request / data mapping and visualization) ",
                    "photo": "https://avatars2.githubusercontent.com/u/22206775?height=180&v=4&width=180"
                },
                {
                    "name": "Joohyun Park",
                    "about": "Developing particle system and physics in Unity scene.",
                    "photo": "https://avatars2.githubusercontent.com/u/31669188?height=180&v=4&width=180"
                },
                {
                    "name": "Itay Niv",
                    "about": "Unity development, scripting, design and modeling",
                    "photo": "https://avatars1.githubusercontent.com/u/5209486?height=180&v=4&width=180"
                },
                {
                    "name": "Dav",
                    "about": "I worked on shaders, and the UI. We all also contributed to the entire design thinking process.",
                    "photo": "https://avatars2.githubusercontent.com/u/34929787?height=180&v=4&width=180"
                },
                {
                    "name": "Xuhan Yang",
                    "about": "I worked as the designer on the team. My main tasks are to design, prepare and organize the 2D graphics and 3D assets needed for Weather Window.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/749/datas/profile.JPG"
                }
            ],
            "built_with": [
                "photoshop",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Most of the time, weather is either mundane or disappointing. We open our weather apps and hope for a miracle, either a warm day in January, or a cool wind in august. But days like that are few and far between. We wanted to add a little magic to the boring monotony of checking the weather, and make checking the weather a little more enjoyable.</p>\n<p>We also drew inspiration from 2 primary places, people who live in dense urban cities, and snow snow globes. Many people in dense cities don\u2019t have windows, or views beyond a brick wall next door. It hard to realize how much a bright window with dynamic weather can help someone's mood while living in a situation like this. Snowglobes can offer a fun perspective of familier city. </p>\n<p>When the Weather Channel went viral for posting a video where a flood surrounding the reporter using augmented reality in order to give the viewer an idea of the size and depth of the flood, the world realized how powerful a tool AR could be while helping people imagine the weather.</p>\n<p>As a side benefit, we built a way to transport the user away from inclimate weather, to see and experience better days in other parts of the world. On days that are cold and miserable in New York, transport yourself in augmented reality to sunny San Diego. </p>\n<h2>What it does</h2>\n<p>Our Weather Window enables users to have an immersive experience of the weather of any cities at any time. Users can get the information about the weather both visually through the AR built window view, and grasp the accurate data shown along the side. Besides, the slide bar underneath provide the smooth experience of exploring weather changes in a 24 hr range.</p>\n<h2>How we built it</h2>\n<p>It was mainly built with Unity 3D. We created a three layered scene, with photoshopped views of the city, an obj model of the empire state building, and a skybox. We added a particle system to simulate snow and rain. And we then placed all of these components into a portal, so that the scene was only visible while looking through a window. Finally, we made an API call from the \"Open Weather Map\" and connected to the components to visualize the real data over time in three different cities. </p>\n<p>In ARkit, we also used the plane detection function to make sure that the window was always at eye level.</p>\n<h2>Challenges we ran into</h2>\n<p>Partly displaying scene throughout the frame was the big challenge. We approached making a transparent box to cover five sides and show the front. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Success in building multi-layer AR frame that shows elements within elements as well as applying physics between particles and elements in AR scene.</p>\n<h2>What we learned</h2>\n<p>How to share projects with Unity Colab and Github. How to write shaders.</p>\n<h2>What's next for Weather Window</h2>\n<p>The app built during the Hackathon is a prototype. We will definitely include more options for the cities. We would like to add a search function for selecting cities and adding the cities into a customized index. We will also make the scenes inside the window more dynamic and interesting. We will map the background skybox and 3d models of sun and moon with the real timeline to make the scenes look more accurate. We will also put some birds flying around for \"sunny\" and trees blown away for \"windy\". </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMost of the time, weather is either mundane or disappointing. We open our weather apps and hope for a miracle, either a warm day in January, or a cool wind in august. But days like that are few and far between. We wanted to add a little magic to the boring monotony of checking the weather, and make checking the weather a little more enjoyable.\n\n\nWe also drew inspiration from 2 primary places, people who live in dense urban cities, and snow snow globes. Many people in dense cities don\u2019t have windows, or views beyond a brick wall next door. It hard to realize how much a bright window with dynamic weather can help someone's mood while living in a situation like this. Snowglobes can offer a fun perspective of familier city. \n\n\nWhen the Weather Channel went viral for posting a video where a flood surrounding the reporter using augmented reality in order to give the viewer an idea of the size and depth of the flood, the world realized how powerful a tool AR could be while helping people imagine the weather.\n\n\nAs a side benefit, we built a way to transport the user away from inclimate weather, to see and experience better days in other parts of the world. On days that are cold and miserable in New York, transport yourself in augmented reality to sunny San Diego. \n\n\n## What it does\n\n\nOur Weather Window enables users to have an immersive experience of the weather of any cities at any time. Users can get the information about the weather both visually through the AR built window view, and grasp the accurate data shown along the side. Besides, the slide bar underneath provide the smooth experience of exploring weather changes in a 24 hr range.\n\n\n## How we built it\n\n\nIt was mainly built with Unity 3D. We created a three layered scene, with photoshopped views of the city, an obj model of the empire state building, and a skybox. We added a particle system to simulate snow and rain. And we then placed all of these components into a portal, so that the scene was only visible while looking through a window. Finally, we made an API call from the \"Open Weather Map\" and connected to the components to visualize the real data over time in three different cities. \n\n\nIn ARkit, we also used the plane detection function to make sure that the window was always at eye level.\n\n\n## Challenges we ran into\n\n\nPartly displaying scene throughout the frame was the big challenge. We approached making a transparent box to cover five sides and show the front. \n\n\n## Accomplishments that we're proud of\n\n\nSuccess in building multi-layer AR frame that shows elements within elements as well as applying physics between particles and elements in AR scene.\n\n\n## What we learned\n\n\nHow to share projects with Unity Colab and Github. How to write shaders.\n\n\n## What's next for Weather Window\n\n\nThe app built during the Hackathon is a prototype. We will definitely include more options for the cities. We would like to add a search function for selecting cities and adding the cities into a customized index. We will also make the scenes inside the window more dynamic and interesting. We will map the background skybox and 3d models of sun and moon with the real timeline to make the scenes look more accurate. We will also put some birds flying around for \"sunny\" and trees blown away for \"windy\". \n\n\n"
        },
        {
            "source": "https://devpost.com/software/xable",
            "title": "XAble",
            "blurb": "Exploring ways to close the growing accessibility divide within XR, Both experimental and ADA ",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312313908?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "F O",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/980/054/datas/profile.jpg"
                },
                {
                    "name": "Scott Niejadlik",
                    "about": "",
                    "photo": "https://lh3.googleusercontent.com/a-/AOh14GiY3WX8vYskRERZ8mqDEx8TqszGipo1hZzuyH3R4A?height=180&width=180"
                },
                {
                    "name": "Jordan Higgins",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/163/117/datas/profile.png"
                },
                {
                    "name": "Mike Festa",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/394/443/datas/profile.jpg"
                },
                {
                    "name": "Susan Johnson McCabe",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/93a93d4ac29734031ecbceefaf4e9e20?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "google-daydream",
                "holo-toolkit",
                "microsoft-hololens",
                "unity",
                "xbox-controllers"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>XAble is exploring ways to close the accessibility divide within XR by visualizing an open-source toolkit for developers that helps them integrate support with cross-platform compliance to support users with disabilities</p>\n<h2>What it does</h2>\n<p>Our team created 5 demos to illustrate variety of ways to make XR accessible:</p>\n<p>DEMO A - Low Vision Accessibility: Created a demo scene with 3 objects that can be better understood by a person with low vision. One object is active at a time and has a red outline to make it easier to see against the background. The active object can be brought closer to the user by holding the top button down on the Daydream controller. If they want to see the object from a different angle or the other side, they can simply turn their head before clicking the enlarge button and it will be rotated in front of them. These objects also give the developer the option of including alternate text and/or audio. When the user holds the button on the right side of the controller, the text is presented in large print one word at a time using a speed reading technique called Spritz. Each word has one letter in red font that is always aligned in the same location, the rest of the word is white font on a black background. The speed of the playback can be adjusted by the user in their accessibility settings. This technique allows a lot of text to be read quickly by both individuals with low vision as well as those with normal vision. If the object has an audio file, pressing the left button on the side of Daydream controller plays that audio. A future goal would be to use a text to speech service if no audio was specified. We would also expand the feature to use the camera to identify text in the real world and present it enlarged or as audio to the user in mixed reality.</p>\n<p>DEMO B - Visual Color Accessibility: We explored and built illustrative proof of concept of visual color accessibility challenges, specifically protanopia one of the most common conditions of color blindness. We looked at this experience as a way at both a way for people to experience what it is like to be color blind, as well as a look beyond the current interface accessibility options and think of how the emergence of computer vision, ai, and sensors can start to open up the world in ways that were not possible.  Namely being able to interpret greens and blues and overlay them with unique patterns that could potentially help someone start to perceive a difference while only still having sensitivity to two wave lengths.</p>\n<p>DEMO C - We explored some accessible approaches for people with cognitive or memory issues, we looked at a solution involving computer vision and speech to text in order to help locate lost items or identify something that an elderly user would need but was unable to remember or incapable of identifying where it was.</p>\n<p>DEMO D - Icons for Accessibility: We created an initial draft of an icon library that can be used cross platform to make accessibility features easier to include in an user interface, and more recognizable across multiple devices. These icons will be available after the hackathon, and hopefully will grow as we expand </p>\n<p>DEMO E - HoloLens Adaptive Controller: The \u201cnatural UI\u201d of the HoloLens is very engaging, but what happens when air-tap and tap &amp; hold aren\u2019t natural, or even possible? We wanted to see if an adaptive controller for the Xbox could be set-up to make experiences on the HoloLens more accessible. Our concept of a new UI paradigm to make HoloLens and MixedReality experiences accessible opens up new types of experiences.</p>\n<p>We drafted a README file  with Hackathon project overview and  starting point for inviting developers into collaboration on accessibility for XR</p>\n<h2>How we built it</h2>\n<p>DEMO A - Low Vision Accessibility: This demo was built in Unity and prototyped on the Oculus Rift as well as the Google Pixel using Daydream. It started with a 360 image inside an aquarium that had a lot of small elements which may be hard to see for someone with reduced eyesight. My son, who is legally blind due to his Albinism, was able to help me understand what his acquity was like by telling me if he could see the difference between different versions of that environment. I used Photoshop to to create several versions with different levels of blur (Filter &gt; Gaussian Blur). The different levels were 5px, 5px applied twice, 5px applied three times, 10px, 10px applied twice, 10px applied three times. For my son, he could not tell the difference between the high res image and the 5px blur. Knowing this information help us design aids to enhance the small content in the scene. I placed a 3D scan of a shark positioned to match other sharks in the image. I then wrote enhancements to enlarge the shark in place. This was good, by my son preferred to have it move up close so that he could observe all of the details. The next iteration was to move the Shark up close while holding down a button. As a nice side effect, we discovered that by turning his head, he could see the other side of the object when enlarging it. After getting one object working, I added two others - a small falcon and a handicapped parking sign. I added a method for selecting one object at a time and bundled all of the code into a unitypackage that could be imported easily into any unity project. Any object in the scene with the XableObject script on it would be selectable and accessible. Only the active object would be enlarged at a time. The thinking was to emulate the tab focus feature found in HTML for web pages. Next was adding an outline filter to highlight and indicate what object was active. Next, I added the speed reading feature to show the alt text in large print in front of the user. We referenced the speed reading app called Spritz and used the Unity Text Mesh Pro object on a white background. Spritz shows one word at a time with a single letter in red and the rest in white. Our brains are able to read much faster with this presentation and it enables a long message to be shown quickly to the user. I added that functionality to a new button press and the user could then select to enlarge the object by holding one button or speed read the text by holding another. Finally, I added a third feature to play an audio clip for the object. The next feature would have been to use a text-to-speech service if the audio clip was not specified, but the time limit was reached and that will have to be added as the continuation of this project.</p>\n<p>DEMO B- Color blind augmented study and accessibility simulator: This demo was an attempt to help emulate the visual difficulties color blind individuals have in seeing their everyday world.  This simple idea replicates some previous experiences around accessibility but brings it into a 3d photo scanned room.  A GPU shader was written that is capable of adjusting the surface colors of the scanned meshes and mutes the red and green color channels to match the effect of a having protanopia.  We further explored this concept and worked the shader to splice in two divergent patterns for both the red and green color spectrum and attempt to allow a color blind person to differentiate between the two with blended patterns. It will be interesting to build this out and see if it is actually effective for someone how experiences this disorder.</p>\n<p>DEMO C - For this demo we used a combination of an object detection algorithm and a speech to text one. For the former we implemented YOLO against a camera feed successfully and accurately detecting a great number of objects. For the latter, we implemented calls to Google API in order to translate the user speech to text. Using that text we were able to connect both applications and isolate the objects that we were interested in localize.</p>\n<p>DEMO D- Icons were created in Adobe XD and exported as PNG files for developers across demos to use. Future plans include making available in PNG and SVG as well as fully transparent versions.</p>\n<h2>Challenges we ran into</h2>\n<p>DEMO A- Getting the enlarged object to be placed in front of where the user was looking. Using two colors on a text-mesh-pro object. Rapidly prototyping on Daydream (finding the instant preview solved this).</p>\n<p>DEMO B- Working the shader code to correctly simulate the visual effect of color blindness was tricky, but the real issue arose when the attempt to augment the green and red color spectrum with patterns. The effect works for now but It needs refinement and testing to figure out what patterns work best and what range of colors are useful for edge detection.</p>\n<p>DEMO C- The api calls </p>\n<p>DEMO D- How can we expand familiar icons to address specific accessibility features in XR? Coming up with symbols that represent concepts like color blindness, low vision, and manipulating a hologram (or other form of digital object) are an opportunity to build a new design language for accessible design.</p>\n<p>DEMO E- While the Xbox controller is technically supported by the HoloToolkit, we encountered great difficulty triggering specific actions from the controllers. After consulting with Microsoft we believe it\u2019s a temporary limitation due to the transition to a newer and better supported Mixed Reality Toolkit, and we look forward to exploring this after the Hackathon.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We were able to use come together as a team quickly using Design Thinking as a shared language/methodology  as well as draw from diverse range of expertise on team to imagine future possibilities. </p>\n<p>We created an initial draft of an icon library that can be used cross platform.</p>\n<p>We drafted a README file  with Hackathon project overview and  starting point for inviting developers into collaboration on accessibility for XR</p>\n<p>Coming up with a new UX/UI paradigm for experiencing spatial computing and immersive VR using adaptive technology.</p>\n<p>Implementing the speed reading in VR works really well and would be great for other use cases, such as reading news, books, websites, etc. This is great for full sighted individuals as well.</p>\n<h2>What we learned</h2>\n<p>XR is still a new medium and there are still a lot of accessibility gaps that still need to be addressed. We are proud of the ideas and demos we came up with in a weekend, but are more excited to continue building this out to address additional disabilities. XR technology has the ability to go beyond just making content accessible to everyone, it can enhance the abilities of users to better understand and interact in the real world as well.</p>\n<h2>What's next for XAble</h2>\n<p>Publishing on GitHub and sparking a broader conversation in the XR and accessibility communities to contribute and establish tools and practices. Additional documentation will explore how XR can viewed through different lenses of accessibility using the WAI standards as a starting point. We bought the domain name xable.org to promote our continued efforts.\nWe will continue to evolve the README file as starting point for inviting developers into collaboration on accessibility for XR.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nXAble is exploring ways to close the accessibility divide within XR by visualizing an open-source toolkit for developers that helps them integrate support with cross-platform compliance to support users with disabilities\n\n\n## What it does\n\n\nOur team created 5 demos to illustrate variety of ways to make XR accessible:\n\n\nDEMO A - Low Vision Accessibility: Created a demo scene with 3 objects that can be better understood by a person with low vision. One object is active at a time and has a red outline to make it easier to see against the background. The active object can be brought closer to the user by holding the top button down on the Daydream controller. If they want to see the object from a different angle or the other side, they can simply turn their head before clicking the enlarge button and it will be rotated in front of them. These objects also give the developer the option of including alternate text and/or audio. When the user holds the button on the right side of the controller, the text is presented in large print one word at a time using a speed reading technique called Spritz. Each word has one letter in red font that is always aligned in the same location, the rest of the word is white font on a black background. The speed of the playback can be adjusted by the user in their accessibility settings. This technique allows a lot of text to be read quickly by both individuals with low vision as well as those with normal vision. If the object has an audio file, pressing the left button on the side of Daydream controller plays that audio. A future goal would be to use a text to speech service if no audio was specified. We would also expand the feature to use the camera to identify text in the real world and present it enlarged or as audio to the user in mixed reality.\n\n\nDEMO B - Visual Color Accessibility: We explored and built illustrative proof of concept of visual color accessibility challenges, specifically protanopia one of the most common conditions of color blindness. We looked at this experience as a way at both a way for people to experience what it is like to be color blind, as well as a look beyond the current interface accessibility options and think of how the emergence of computer vision, ai, and sensors can start to open up the world in ways that were not possible. Namely being able to interpret greens and blues and overlay them with unique patterns that could potentially help someone start to perceive a difference while only still having sensitivity to two wave lengths.\n\n\nDEMO C - We explored some accessible approaches for people with cognitive or memory issues, we looked at a solution involving computer vision and speech to text in order to help locate lost items or identify something that an elderly user would need but was unable to remember or incapable of identifying where it was.\n\n\nDEMO D - Icons for Accessibility: We created an initial draft of an icon library that can be used cross platform to make accessibility features easier to include in an user interface, and more recognizable across multiple devices. These icons will be available after the hackathon, and hopefully will grow as we expand \n\n\nDEMO E - HoloLens Adaptive Controller: The \u201cnatural UI\u201d of the HoloLens is very engaging, but what happens when air-tap and tap & hold aren\u2019t natural, or even possible? We wanted to see if an adaptive controller for the Xbox could be set-up to make experiences on the HoloLens more accessible. Our concept of a new UI paradigm to make HoloLens and MixedReality experiences accessible opens up new types of experiences.\n\n\nWe drafted a README file with Hackathon project overview and starting point for inviting developers into collaboration on accessibility for XR\n\n\n## How we built it\n\n\nDEMO A - Low Vision Accessibility: This demo was built in Unity and prototyped on the Oculus Rift as well as the Google Pixel using Daydream. It started with a 360 image inside an aquarium that had a lot of small elements which may be hard to see for someone with reduced eyesight. My son, who is legally blind due to his Albinism, was able to help me understand what his acquity was like by telling me if he could see the difference between different versions of that environment. I used Photoshop to to create several versions with different levels of blur (Filter > Gaussian Blur). The different levels were 5px, 5px applied twice, 5px applied three times, 10px, 10px applied twice, 10px applied three times. For my son, he could not tell the difference between the high res image and the 5px blur. Knowing this information help us design aids to enhance the small content in the scene. I placed a 3D scan of a shark positioned to match other sharks in the image. I then wrote enhancements to enlarge the shark in place. This was good, by my son preferred to have it move up close so that he could observe all of the details. The next iteration was to move the Shark up close while holding down a button. As a nice side effect, we discovered that by turning his head, he could see the other side of the object when enlarging it. After getting one object working, I added two others - a small falcon and a handicapped parking sign. I added a method for selecting one object at a time and bundled all of the code into a unitypackage that could be imported easily into any unity project. Any object in the scene with the XableObject script on it would be selectable and accessible. Only the active object would be enlarged at a time. The thinking was to emulate the tab focus feature found in HTML for web pages. Next was adding an outline filter to highlight and indicate what object was active. Next, I added the speed reading feature to show the alt text in large print in front of the user. We referenced the speed reading app called Spritz and used the Unity Text Mesh Pro object on a white background. Spritz shows one word at a time with a single letter in red and the rest in white. Our brains are able to read much faster with this presentation and it enables a long message to be shown quickly to the user. I added that functionality to a new button press and the user could then select to enlarge the object by holding one button or speed read the text by holding another. Finally, I added a third feature to play an audio clip for the object. The next feature would have been to use a text-to-speech service if the audio clip was not specified, but the time limit was reached and that will have to be added as the continuation of this project.\n\n\nDEMO B- Color blind augmented study and accessibility simulator: This demo was an attempt to help emulate the visual difficulties color blind individuals have in seeing their everyday world. This simple idea replicates some previous experiences around accessibility but brings it into a 3d photo scanned room. A GPU shader was written that is capable of adjusting the surface colors of the scanned meshes and mutes the red and green color channels to match the effect of a having protanopia. We further explored this concept and worked the shader to splice in two divergent patterns for both the red and green color spectrum and attempt to allow a color blind person to differentiate between the two with blended patterns. It will be interesting to build this out and see if it is actually effective for someone how experiences this disorder.\n\n\nDEMO C - For this demo we used a combination of an object detection algorithm and a speech to text one. For the former we implemented YOLO against a camera feed successfully and accurately detecting a great number of objects. For the latter, we implemented calls to Google API in order to translate the user speech to text. Using that text we were able to connect both applications and isolate the objects that we were interested in localize.\n\n\nDEMO D- Icons were created in Adobe XD and exported as PNG files for developers across demos to use. Future plans include making available in PNG and SVG as well as fully transparent versions.\n\n\n## Challenges we ran into\n\n\nDEMO A- Getting the enlarged object to be placed in front of where the user was looking. Using two colors on a text-mesh-pro object. Rapidly prototyping on Daydream (finding the instant preview solved this).\n\n\nDEMO B- Working the shader code to correctly simulate the visual effect of color blindness was tricky, but the real issue arose when the attempt to augment the green and red color spectrum with patterns. The effect works for now but It needs refinement and testing to figure out what patterns work best and what range of colors are useful for edge detection.\n\n\nDEMO C- The api calls \n\n\nDEMO D- How can we expand familiar icons to address specific accessibility features in XR? Coming up with symbols that represent concepts like color blindness, low vision, and manipulating a hologram (or other form of digital object) are an opportunity to build a new design language for accessible design.\n\n\nDEMO E- While the Xbox controller is technically supported by the HoloToolkit, we encountered great difficulty triggering specific actions from the controllers. After consulting with Microsoft we believe it\u2019s a temporary limitation due to the transition to a newer and better supported Mixed Reality Toolkit, and we look forward to exploring this after the Hackathon.\n\n\n## Accomplishments that we're proud of\n\n\nWe were able to use come together as a team quickly using Design Thinking as a shared language/methodology as well as draw from diverse range of expertise on team to imagine future possibilities. \n\n\nWe created an initial draft of an icon library that can be used cross platform.\n\n\nWe drafted a README file with Hackathon project overview and starting point for inviting developers into collaboration on accessibility for XR\n\n\nComing up with a new UX/UI paradigm for experiencing spatial computing and immersive VR using adaptive technology.\n\n\nImplementing the speed reading in VR works really well and would be great for other use cases, such as reading news, books, websites, etc. This is great for full sighted individuals as well.\n\n\n## What we learned\n\n\nXR is still a new medium and there are still a lot of accessibility gaps that still need to be addressed. We are proud of the ideas and demos we came up with in a weekend, but are more excited to continue building this out to address additional disabilities. XR technology has the ability to go beyond just making content accessible to everyone, it can enhance the abilities of users to better understand and interact in the real world as well.\n\n\n## What's next for XAble\n\n\nPublishing on GitHub and sparking a broader conversation in the XR and accessibility communities to contribute and establish tools and practices. Additional documentation will explore how XR can viewed through different lenses of accessibility using the WAI standards as a starting point. We bought the domain name xable.org to promote our continued efforts.\nWe will continue to evolve the README file as starting point for inviting developers into collaboration on accessibility for XR.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/caution-no-bear-in-area",
            "title": "Caution: No Bear in Area",
            "blurb": "Can we help people anywhere in the world to help slow global warming at home?",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/pZgkOl4fOEU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "LongzeXia Xia",
                    "about": "Design & 3D Modelling",
                    "photo": "https://avatars1.githubusercontent.com/u/16907034?height=180&v=4&width=180"
                },
                {
                    "name": "Matt Kaminski",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/01d2f4695b5c4b0e8d3c7f937c315d3e?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sandeep Bangalore Venkatesh",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/131ddfe0d8f6457f0639379e031b7baf?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Axel S. Toro Vega",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/250/285/datas/profile.JPG"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "blender",
                "cinema-4d",
                "google-cloud-text-to-speech",
                "photoshop",
                "unity",
                "visual-studio"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>How can individuals at home impact a global problem?  Can we teach people anywhere in the world to take action at home that will help slow global warming?</p>\n<h2>What it does</h2>\n<p>The experience charges people with the task of learning activities that can be done in their home that could help slow global warming.  Users are encouraged to communicate the tasks with others.</p>\n<h2>How we built it</h2>\n<p>We started with the desire to educate people about global warming.  We researched on the web to educate ourselves and help define the problem.  We went through ideation, design, and implementation, then iteration.  In each phase we used different materials and opportunities.  We used interviews with adults to understand their knowledge.  We decided to target young adults and adults.</p>\n<p>Global warming is a macro problem.  We decided to make a connection between young adults and global warming problem.  We choose simple educational language.  To help focus attention, we chose a specific species which is impacted - Polar bears.  A sign sometimes has the caution text \"Bear in Area\"; we think we need to caution people \"No Bear in Area\".  Oculus with Unity was chosen to create an immersive environment.   We iterated and refined the learning experiences, visuals, language and interactions.</p>\n<h2>Challenges we ran into</h2>\n<p>The design challenge for us was how can we translate this complex issue into language that anyone can understand.  How can we make an experience that is both attractive and compelling, and give the users a sense of meaning and urgency to perform the tasks.</p>\n<p>We needed to integrate the grabbing gesture and teleportation in Oculus. </p>\n<p>Wanted to create elements dynamically to show the dynamically loaded text strings.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We know we needed to balance between our expectations and ability given the short timeframe.  We were able to meet and make adjustments, and design decisions, as needed.  Long video sample (<a href=\"https://youtu.be/aX28r7El-o4\" rel=\"nofollow\">https://youtu.be/aX28r7El-o4</a>).</p>\n<h2>What we learned</h2>\n<p>We learned some startling facts about global warming.  We were exposed to cross discipline tools and concepts\nAs a team we had broad experience and were able to learn from one another conceptually and technically.</p>\n<h2>What's next for Caution: No Bear in Area</h2>\n<p>Please contact us with questions or ideas.  Our project would benefit from widespread user testing to refine interactions.  We would like to create other meaningful elements in the experience which users can discover.  This project can be easily localized - we used external string files - so that it can be experienced throughout the globe</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nHow can individuals at home impact a global problem? Can we teach people anywhere in the world to take action at home that will help slow global warming?\n\n\n## What it does\n\n\nThe experience charges people with the task of learning activities that can be done in their home that could help slow global warming. Users are encouraged to communicate the tasks with others.\n\n\n## How we built it\n\n\nWe started with the desire to educate people about global warming. We researched on the web to educate ourselves and help define the problem. We went through ideation, design, and implementation, then iteration. In each phase we used different materials and opportunities. We used interviews with adults to understand their knowledge. We decided to target young adults and adults.\n\n\nGlobal warming is a macro problem. We decided to make a connection between young adults and global warming problem. We choose simple educational language. To help focus attention, we chose a specific species which is impacted - Polar bears. A sign sometimes has the caution text \"Bear in Area\"; we think we need to caution people \"No Bear in Area\". Oculus with Unity was chosen to create an immersive environment. We iterated and refined the learning experiences, visuals, language and interactions.\n\n\n## Challenges we ran into\n\n\nThe design challenge for us was how can we translate this complex issue into language that anyone can understand. How can we make an experience that is both attractive and compelling, and give the users a sense of meaning and urgency to perform the tasks.\n\n\nWe needed to integrate the grabbing gesture and teleportation in Oculus. \n\n\nWanted to create elements dynamically to show the dynamically loaded text strings.\n\n\n## Accomplishments that we're proud of\n\n\nWe know we needed to balance between our expectations and ability given the short timeframe. We were able to meet and make adjustments, and design decisions, as needed. Long video sample (<https://youtu.be/aX28r7El-o4>).\n\n\n## What we learned\n\n\nWe learned some startling facts about global warming. We were exposed to cross discipline tools and concepts\nAs a team we had broad experience and were able to learn from one another conceptually and technically.\n\n\n## What's next for Caution: No Bear in Area\n\n\nPlease contact us with questions or ideas. Our project would benefit from widespread user testing to refine interactions. We would like to create other meaningful elements in the experience which users can discover. This project can be easily localized - we used external string files - so that it can be experienced throughout the globe\n\n\n"
        },
        {
            "source": "https://devpost.com/software/fit-nl0cr5",
            "title": "FIT",
            "blurb": "When clothes shopping online or in a catalog, our app shows how clothing would look on your body type in 3D space",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/ccUGsMpzjTk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "John Knox",
                    "about": "I managed the team and workflow. I developed all the Unity and HoloLens scripts.  Worked with the team to port the code to iOS for demo on smartphone.",
                    "photo": "https://avatars3.githubusercontent.com/u/38356430?height=180&v=4&width=180"
                },
                {
                    "name": "steveice",
                    "about": "",
                    "photo": "https://graph.facebook.com/2331137156898466/picture?height=180&width=180"
                },
                {
                    "name": "mariaspark",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46804238?height=180&v=4&width=180"
                },
                {
                    "name": "Minerstudio",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/45796624?height=180&v=4&width=180"
                },
                {
                    "name": "Namsoo Vincent Kim",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQExqEppLA3a5w/profile-displayphoto-shrink_100_100/0?e=1553731200&height=180&t=Y9gL6-mnCzrDXWJP-kiD4YKYZ-PpkEkxULHvL_B2QWs&v=beta&width=180"
                }
            ],
            "built_with": [
                "c#",
                "invision",
                "unity",
                "vuforia"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>The Original Idea for FIT, was to be more of an augmented reality style app, but after receiving feedback from mentors and peers, our team agreed that focusing more on the fit of the clothes rather than the style for online shoppers was a bigger issue, and one not thoroughly explored in the fashion market.</p>\n<h2>What it does</h2>\n<p>It allows the user to set up a profile where they input their Body Type parameters for their 3D model (avatar). Then the user can view a virtual representation of their body type in 3D space using their smartphone or tablet. The customer will be able to see how the clothes found during their online shopping experience will look on their body type, as well as where the tight and loose places are, from a large database of clothes (For the demo, it is just shirts).</p>\n<h2>How we built it</h2>\n<p>Our team used Unity and Vuforia for the 3D demonstration and Invision for the profile creator. </p>\n<h2>Challenges we ran into</h2>\n<p>Learning how to work with IOS Devices. The product was originally going to be on a HoloLens since the team had more experience with this device. However, we realized it would be tedious to enter the customer profile using the limited gesture control on the HoloLens. The team decided that the app would be better suited for smartphones and Tablets.  The team needed to quickly learn the iPhone and tablet touch screen functionality and create an operational GUI interface.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Having not worked with IOS devices before, our team was proud of the fact we were able to get apps deployed on our phones. Also the designers on our team took on some programming challenges with Vuforia and succeeded in getting the scripts to work. </p>\n<h2>What we learned</h2>\n<p>Working with Unity and iOS. Using ground plane stage in Vuforia.     Understanding how to pitch an idea and form a team. How to effectively work with a team of people that just met.</p>\n<h2>What's next for FIT</h2>\n<p>Share data between smartphone and HoloLens</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThe Original Idea for FIT, was to be more of an augmented reality style app, but after receiving feedback from mentors and peers, our team agreed that focusing more on the fit of the clothes rather than the style for online shoppers was a bigger issue, and one not thoroughly explored in the fashion market.\n\n\n## What it does\n\n\nIt allows the user to set up a profile where they input their Body Type parameters for their 3D model (avatar). Then the user can view a virtual representation of their body type in 3D space using their smartphone or tablet. The customer will be able to see how the clothes found during their online shopping experience will look on their body type, as well as where the tight and loose places are, from a large database of clothes (For the demo, it is just shirts).\n\n\n## How we built it\n\n\nOur team used Unity and Vuforia for the 3D demonstration and Invision for the profile creator. \n\n\n## Challenges we ran into\n\n\nLearning how to work with IOS Devices. The product was originally going to be on a HoloLens since the team had more experience with this device. However, we realized it would be tedious to enter the customer profile using the limited gesture control on the HoloLens. The team decided that the app would be better suited for smartphones and Tablets. The team needed to quickly learn the iPhone and tablet touch screen functionality and create an operational GUI interface.\n\n\n## Accomplishments that we're proud of\n\n\nHaving not worked with IOS devices before, our team was proud of the fact we were able to get apps deployed on our phones. Also the designers on our team took on some programming challenges with Vuforia and succeeded in getting the scripts to work. \n\n\n## What we learned\n\n\nWorking with Unity and iOS. Using ground plane stage in Vuforia. Understanding how to pitch an idea and form a team. How to effectively work with a team of people that just met.\n\n\n## What's next for FIT\n\n\nShare data between smartphone and HoloLens\n\n\n"
        },
        {
            "source": "https://devpost.com/software/dreams-of-cajal",
            "title": "Dreams of Cajal",
            "blurb": "Inside the mind of a discoverer of the mind.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/SFIuPrvuyP8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Jeffery DelViscio",
                    "about": "I conceived the idea of \"Dreams of Cajal.\" I also contributed as a 3D artist, concept designer, sound recordist and editor. It's amazing to me that a group of strangers could come together so fast and so harmoniously to build such an ambitious piece of immersive science storytelling.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/753/datas/profile.jpg"
                },
                {
                    "name": "Pakinam Amer",
                    "about": "I contributed as researcher, script writer, story designer and asset curator. I had tremendous fun visualizing narratives in Tiltbrush for our project, and on the sidelines, learning how to create 3D models in Blender for the very first time. I may be biased but I believe we have the most kickass team in this hackathon! ",
                    "photo": "https://avatars1.githubusercontent.com/u/46689846?height=180&v=4&width=180"
                },
                {
                    "name": "reinosomartin",
                    "about": "I am the voice of Cajal on this project, thanks to a Spanish accent that my teammates describe as intimate and charming. I developed the main lab scene, basic interactions, transitions and animations. I also set up the programming environment and kept the repository and code updated. The bulk of my work was on Unity, connecting 3D neural models with the evolving scenes and core VR interactions. ",
                    "photo": "https://graph.facebook.com/10210462753643575/picture?height=180&width=180"
                },
                {
                    "name": "Sean Manton",
                    "about": "I contributed as a developer, interaction designer, and musician. On the dev side, I worked on the main tracing and trigger interactions within the neuron scenes, as well as various other little bits.",
                    "photo": "https://avatars2.githubusercontent.com/u/4335536?height=180&v=4&width=180"
                },
                {
                    "name": "Arwa Alsaati",
                    "about": "I contributed to concept and story design, built 3D models of human neurons in Maya, in addition to creating art in Tiltbrush and acquiring sound effects.",
                    "photo": "https://avatars2.githubusercontent.com/u/36837528?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "maya",
                "tiltbrush",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>\u201cLike the entomologist in search of colorful butterflies, my attention has chased in the gardens of the grey matter cells with delicate and elegant shapes, the mysterious butterflies of the soul, whose beating of wings may one day reveal to us the secrets of the mind.\u201d -- Santiago Ramon Y Cajal</p>\n<p>There\u2019s neuroscience before Santiago Ramon Y Cajal, and neuroscience after, and they\u2019re not the same. Long before MRIs, Cajal was interrogating consciousness, dream states and the inner universe of the brain through the lens of his microscope. He was co-awarded the Nobel prize in 1906 \"in recognition of [his] work on the structure of the nervous system.\"</p>\n<h2>What it does</h2>\n<p>\"Dreams of Cajal\" is a virtual dive into mind of the forefather of neuroscience. Viewers are transported through his lab onto the microscope where he discovered, in vibrant detail, the structures that power the human brain. </p>\n<h2>How we built it</h2>\n<p>\"Dreams of Cajal\" was built in Maya, Tiltbrush, Unity, and Audition. </p>\n<h2>Challenges we ran into</h2>\n<p>You try making accurate representations of human neurons into playable game objects some time.</p>\n<p>The tracing mechanic is so intuitive in concept, but it turned out to require quite a few hacks in order to make it work.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Making accurate representations of human neurons into playable game objects. </p>\n<p>Crafting a satisfying feeling tracing mechanic.</p>\n<p>Budgeting our time and adapting our original concept repeatedly in order to get not only a working prototype of the core interactions, but successfully putting everything together into a narrative experience.</p>\n<h2>What we learned</h2>\n<p>We learned that there's a rich vein of opportunity for immersive storytelling at the intersection between science and art. Also, it takes many talents + a heaping pile of collaboration to pull something like this off in 2.5 days. </p>\n<h2>What's next for \"Dreams of Cajal\"</h2>\n<p>We would love to go back and really iron out fine grained user interaction, make all custom objects and refine the environments, and we would love to just generally build more richness into the experience. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\n\u201cLike the entomologist in search of colorful butterflies, my attention has chased in the gardens of the grey matter cells with delicate and elegant shapes, the mysterious butterflies of the soul, whose beating of wings may one day reveal to us the secrets of the mind.\u201d -- Santiago Ramon Y Cajal\n\n\nThere\u2019s neuroscience before Santiago Ramon Y Cajal, and neuroscience after, and they\u2019re not the same. Long before MRIs, Cajal was interrogating consciousness, dream states and the inner universe of the brain through the lens of his microscope. He was co-awarded the Nobel prize in 1906 \"in recognition of [his] work on the structure of the nervous system.\"\n\n\n## What it does\n\n\n\"Dreams of Cajal\" is a virtual dive into mind of the forefather of neuroscience. Viewers are transported through his lab onto the microscope where he discovered, in vibrant detail, the structures that power the human brain. \n\n\n## How we built it\n\n\n\"Dreams of Cajal\" was built in Maya, Tiltbrush, Unity, and Audition. \n\n\n## Challenges we ran into\n\n\nYou try making accurate representations of human neurons into playable game objects some time.\n\n\nThe tracing mechanic is so intuitive in concept, but it turned out to require quite a few hacks in order to make it work.\n\n\n## Accomplishments that we're proud of\n\n\nMaking accurate representations of human neurons into playable game objects. \n\n\nCrafting a satisfying feeling tracing mechanic.\n\n\nBudgeting our time and adapting our original concept repeatedly in order to get not only a working prototype of the core interactions, but successfully putting everything together into a narrative experience.\n\n\n## What we learned\n\n\nWe learned that there's a rich vein of opportunity for immersive storytelling at the intersection between science and art. Also, it takes many talents + a heaping pile of collaboration to pull something like this off in 2.5 days. \n\n\n## What's next for \"Dreams of Cajal\"\n\n\nWe would love to go back and really iron out fine grained user interaction, make all custom objects and refine the environments, and we would love to just generally build more richness into the experience. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/track-builder",
            "title": "Track Builder",
            "blurb": "Get your ball to the end zone by building a tubular track that avoids dangerous obstacles and scores points.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/3b6LYW6hun8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "ElliottBaygan",
                    "about": "I worked on the procedural generation of the drones and walls in the play space. I also positioned and rotated the play space boundary to encapsulate the drones and keep them in the play space. Lastly I created the \"win script\" and helped out with other developer work.",
                    "photo": "https://graph.facebook.com/10157099792528921/picture?height=180&width=180"
                },
                {
                    "name": "Ralston Louie",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/50455d6065b1388e3fed4d2fe4929aa3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Robert Silverberg",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/bf019f9a1ce7e457a640220f52d16f23?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "magic-leap",
                "maya",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We were inspired by 2D games, such as <strong>Line Rider</strong> and <strong>Track Mania</strong>, envisioning what these games might look like in mixed reality. We were also inspired by Rube Goldberg machines and how to translate them into mixed reality.</p>\n<h2>What it does</h2>\n<p>In order to advance their ball to the end zone, players use the Magic Leap to build and visualize their custom track paths in mixed reality while avoiding dangerous obstacles and scoring points along the way.</p>\n<h2>How we built it</h2>\n<p>We built our app in Unity, created the 3D art in Maya, and deployed to the Magic Leap.</p>\n<h2>Challenges we ran into</h2>\n<p>We ran into the challenge of building and designing for the Magic Leap headset, which has no established design precedents, so we felt like true mixed reality pioneers in creating this game. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're proud of building a fun, rewarding game that takes full advantage of the affordances of the Magic Leap.</p>\n<h2>What we learned</h2>\n<p>We learned how to build a game that utilizes the unique affordances of the Magic Leap headset, including spatial mapping and spatial audio.</p>\n<h2>What's next for Track Builder</h2>\n<p>We plan to continue developing and polishing the game and ultimately launch the game on the Magic Leap store. We want to add new levels, create new obstacles and power ups, and integrate new sound effects.</p>\n<h2>Location, floor, and room</h2>\n<p>MIT Media Lab, Floor 3, Room E-15</p>\n<h2>The development tools used to build the project</h2>\n<p>Unity, Maya, and Magic Leap</p>\n<h2>SDKs used in the project</h2>\n<p>Magic Leap/Lumin SDK</p>\n<h2>Any assets used in the project that you did not create</h2>\n<p>Drone model courtesy of Dawn Chan</p>\n<h2>A link to a video of a screen capture of the application on Youtube</h2>\n<p><a href=\"https://youtu.be/hDJ5qlSSTs4\" rel=\"nofollow\">https://youtu.be/hDJ5qlSSTs4</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe were inspired by 2D games, such as **Line Rider** and **Track Mania**, envisioning what these games might look like in mixed reality. We were also inspired by Rube Goldberg machines and how to translate them into mixed reality.\n\n\n## What it does\n\n\nIn order to advance their ball to the end zone, players use the Magic Leap to build and visualize their custom track paths in mixed reality while avoiding dangerous obstacles and scoring points along the way.\n\n\n## How we built it\n\n\nWe built our app in Unity, created the 3D art in Maya, and deployed to the Magic Leap.\n\n\n## Challenges we ran into\n\n\nWe ran into the challenge of building and designing for the Magic Leap headset, which has no established design precedents, so we felt like true mixed reality pioneers in creating this game. \n\n\n## Accomplishments that we're proud of\n\n\nWe're proud of building a fun, rewarding game that takes full advantage of the affordances of the Magic Leap.\n\n\n## What we learned\n\n\nWe learned how to build a game that utilizes the unique affordances of the Magic Leap headset, including spatial mapping and spatial audio.\n\n\n## What's next for Track Builder\n\n\nWe plan to continue developing and polishing the game and ultimately launch the game on the Magic Leap store. We want to add new levels, create new obstacles and power ups, and integrate new sound effects.\n\n\n## Location, floor, and room\n\n\nMIT Media Lab, Floor 3, Room E-15\n\n\n## The development tools used to build the project\n\n\nUnity, Maya, and Magic Leap\n\n\n## SDKs used in the project\n\n\nMagic Leap/Lumin SDK\n\n\n## Any assets used in the project that you did not create\n\n\nDrone model courtesy of Dawn Chan\n\n\n## A link to a video of a screen capture of the application on Youtube\n\n\n<https://youtu.be/hDJ5qlSSTs4>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/tribal-knowledge",
            "title": "Tribal Knowledge",
            "blurb": "A way to capture memories from the world, revisit them in VR, and share them in AR",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/jgJlZ3cskc8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Problem Space",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/255/datas/original.jpeg"
                },
                {
                    "title": "Evolution of comms. But capture?",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/254/datas/original.jpeg"
                },
                {
                    "title": "Value Prop",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/257/datas/original.jpeg"
                },
                {
                    "title": "System Map",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/248/datas/original.jpeg"
                },
                {
                    "title": "User Flow",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/258/datas/original.jpeg"
                },
                {
                    "title": "Proof of Use Case",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/249/datas/original.jpeg"
                },
                {
                    "title": "Capture",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/259/datas/original.jpeg"
                },
                {
                    "title": "Load Memory in VR - Production Version",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/260/datas/original.jpeg"
                },
                {
                    "title": "Load Memory in VR -Play Testing",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/261/datas/original.jpeg"
                },
                {
                    "title": "Load Memory in VR -Initial Prototype",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/622/datas/original.jpeg"
                },
                {
                    "title": "Share in AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/251/datas/original.jpeg"
                },
                {
                    "title": "Share in AR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/252/datas/original.jpeg"
                },
                {
                    "title": "Innovation Journey",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/275/datas/original.jpeg"
                },
                {
                    "title": "Tribal Knowledge - A Tribe - A Team",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/278/datas/original.jpeg"
                }
            ],
            "team": [
                {
                    "name": "Ali Jaafar",
                    "about": "Software Developer. Volumetric Photo Capturer",
                    "photo": "https://media.licdn.com/dms/image/C4E03AQFJvi23NPh-9w/profile-displayphoto-shrink_100_100/0?e=1550102400&height=180&t=yKvYu-o25ZE1LYrGRn5TCI_02P8rcWTwTkhOnspyeBU&v=beta&width=180"
                },
                {
                    "name": "Anne-Elise Chung",
                    "about": "Unity, design, develop, generalist",
                    "photo": "https://avatars0.githubusercontent.com/u/30807117?height=180&v=4&width=180"
                },
                {
                    "name": "Samuel Brewton",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/564/datas/profile.png"
                },
                {
                    "name": "James Dunn",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQEbK6YfZkfaBg/profile-displayphoto-shrink_100_100/0?e=1547683200&height=180&t=X7XmwImTcP_1XyCIthxnWDicb-TuQtFleEZ-qjW-8q8&v=beta&width=180"
                },
                {
                    "name": "Zsa Wen",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/737/839/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "c#"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Memories degrade with age. Knowledge is lost with time if not documented.</p>\n<h2>What it does</h2>\n<p>Tribal Knowledge allows you to capture, remember, and relive experiences that are important to you.</p>\n<h2>How we built it</h2>\n<p>Unity, Android Studio, xCode for prototyping and viewing the 3D assets in AR, GO and Postman to utilize the AutoForge API, Torch for quick AR prototyping.</p>\n<h2>Innovation Journey</h2>\n<p>Starting with our concept for capturing in AR and localizing 3D content to the real world to preserve our memories. We explored the Esri SDK along with Augmented Images with ARCore to pin memories to the real world and we realized VR would be a better medium for first access these experiences virtually. </p>\n<p>While using the AutoDesk Forge API, ARKit and the Structure sensor. We were able to create complex 3D assets effortlessly via Mobile Devices.</p>\n<p>Once our memories where digitally preserved. We began creating a virtual space to display these memories and interact with other users using Unity, HTC Vive headset and HP MR headset .</p>\n<h2>Challenges we ran into</h2>\n<p>Creating 3D captures from user input proved difficult. We found the most success using the Structure sensor, mixed results with the 20 image limit on Autodesk Forge ReCap, and fun but untextured captures via the Hololens.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Capturing volumetric memories and revisiting them in VR, and sharing simultaneously in AR.</p>\n<h2>What we learned</h2>\n<p>The toolkit for creating VR and AR experience has evolved over the past past few years, but it has light years of ground to cover before we can experience and create localized applications that are contextual to the space we are in the needs of unique users.</p>\n<h2>What's next for Tribal Knowledge</h2>\n<p>Optimizing the usage of AutoDesk Forge API.</p>\n<ul>\n<li>Further our capture technology with object detection to separate the object of interest from the scene to quicken the upload speed and forge better models with less noise from the surrounding scene.</li>\n</ul>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMemories degrade with age. Knowledge is lost with time if not documented.\n\n\n## What it does\n\n\nTribal Knowledge allows you to capture, remember, and relive experiences that are important to you.\n\n\n## How we built it\n\n\nUnity, Android Studio, xCode for prototyping and viewing the 3D assets in AR, GO and Postman to utilize the AutoForge API, Torch for quick AR prototyping.\n\n\n## Innovation Journey\n\n\nStarting with our concept for capturing in AR and localizing 3D content to the real world to preserve our memories. We explored the Esri SDK along with Augmented Images with ARCore to pin memories to the real world and we realized VR would be a better medium for first access these experiences virtually. \n\n\nWhile using the AutoDesk Forge API, ARKit and the Structure sensor. We were able to create complex 3D assets effortlessly via Mobile Devices.\n\n\nOnce our memories where digitally preserved. We began creating a virtual space to display these memories and interact with other users using Unity, HTC Vive headset and HP MR headset .\n\n\n## Challenges we ran into\n\n\nCreating 3D captures from user input proved difficult. We found the most success using the Structure sensor, mixed results with the 20 image limit on Autodesk Forge ReCap, and fun but untextured captures via the Hololens.\n\n\n## Accomplishments that we're proud of\n\n\nCapturing volumetric memories and revisiting them in VR, and sharing simultaneously in AR.\n\n\n## What we learned\n\n\nThe toolkit for creating VR and AR experience has evolved over the past past few years, but it has light years of ground to cover before we can experience and create localized applications that are contextual to the space we are in the needs of unique users.\n\n\n## What's next for Tribal Knowledge\n\n\nOptimizing the usage of AutoDesk Forge API.\n\n\n* Further our capture technology with object detection to separate the object of interest from the scene to quicken the upload speed and forge better models with less noise from the surrounding scene.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bodies-in-motion",
            "title": "Bodies In Motion",
            "blurb": "Computer Vision in VR to improve Physical Therapy and Exercise Form",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/GNX8vEDfg2A?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Tania De Gasperis",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQEq6Sr53qpT9Q/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=KJIBj9YU7wXswxnDr6v2FAzawlJQo4WtT1uk69SI5c4&v=beta&width=180"
                },
                {
                    "name": "Brianne Baker",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/45960041?height=180&v=4&width=180"
                },
                {
                    "name": "Daniel Bryand",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/c854e9372564492cef7fd0b692e9bf4c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Max Orozco",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/13499370?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "google-poly",
                "mocapclub.com",
                "node.js",
                "opencv",
                "poly-toolkit",
                "probuilder",
                "simplejson",
                "socket.io",
                "steamvr",
                "tensorflow.js",
                "unity",
                "unityassetstore"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Physical Therapy can be a daunting and lengthy process. For outpatients, compliance to the proper form for physical therapy can dramatically improve outcomes. However, once outside the doctor's office it is difficult to remember the intricacies of the movements. </p>\n<h2>What it does</h2>\n<p>Android device uses Computer Vision to collect user's movement. These motions are then mirrored inside the HMD providing the user with a real time feed of personal body movement. An animated character provides an example of proper movements, and the user can improve form by self correcting posture vs example. </p>\n<h2>How I built it</h2>\n<p>Android OpenCV captures user movement. Web Sockets send data to HMD. Animations are using Motion Capture data. Environment is created using Unity. </p>\n<h2>Challenges I ran into</h2>\n<p>Establishing Websockets. Animation</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Sending OpenCV to a HMD. Application towards rehabilitation. </p>\n<h2>What I learned</h2>\n<p>Node.JS, Socket.IO, learned websockets. Some members first time using Unity. </p>\n<h2>What's next for Bodies In Motion</h2>\n<p>Visual and audio cues to user when deviations from correct form are observed. Incorporation of various Physical Therapy exercises. Incorporation of fitness programs, providing feedback on sports performance. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nPhysical Therapy can be a daunting and lengthy process. For outpatients, compliance to the proper form for physical therapy can dramatically improve outcomes. However, once outside the doctor's office it is difficult to remember the intricacies of the movements. \n\n\n## What it does\n\n\nAndroid device uses Computer Vision to collect user's movement. These motions are then mirrored inside the HMD providing the user with a real time feed of personal body movement. An animated character provides an example of proper movements, and the user can improve form by self correcting posture vs example. \n\n\n## How I built it\n\n\nAndroid OpenCV captures user movement. Web Sockets send data to HMD. Animations are using Motion Capture data. Environment is created using Unity. \n\n\n## Challenges I ran into\n\n\nEstablishing Websockets. Animation\n\n\n## Accomplishments that I'm proud of\n\n\nSending OpenCV to a HMD. Application towards rehabilitation. \n\n\n## What I learned\n\n\nNode.JS, Socket.IO, learned websockets. Some members first time using Unity. \n\n\n## What's next for Bodies In Motion\n\n\nVisual and audio cues to user when deviations from correct form are observed. Incorporation of various Physical Therapy exercises. Incorporation of fitness programs, providing feedback on sports performance. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/vr-fitness",
            "title": "VR Fitness",
            "blurb": "A VR game that motivates the player to move their body and exercise",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312416755?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Brian McDevitt",
                    "about": "I helped with the developing in Unity. I created the functionality of the hit objects in the application and built the menu that guided the user to different exercises.",
                    "photo": "https://avatars1.githubusercontent.com/u/8967395?height=180&v=3&width=180"
                },
                {
                    "name": "Lucid Dreams VR",
                    "about": "I proposed the idea and recruited and ran the team. I kept us organized using Trello, wrote several of the C# Unity scripts, helped teach other members, and designed and built the exercise routines and prefabs in the Unity editor.",
                    "photo": "https://avatars3.githubusercontent.com/u/28881505?height=180&v=4&width=180"
                },
                {
                    "name": "Kristen Palmer",
                    "about": "I worked on the shaders and created some of the paths for the objects to follow. I also helped build some of the exercise routines.",
                    "photo": "https://avatars3.githubusercontent.com/u/18505940?height=180&v=4&width=180"
                },
                {
                    "name": "Private user",
                    "about": "I worked on music composition and sound  design. ",
                    "photo": "https://devpost-challengepost.netdna-ssl.com/assets/defaults/no-avatar-180-caa7628ae0aae09831858639d32ace2a.png"
                },
                {
                    "name": "Melissa Chiu",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/6010a90961eb7de2b25bc4dbf9c52e6b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "htc-vive",
                "unity"
            ],
            "content_html": "<div>\n<p>Inspired by 80s jazzercise videos, VR Fitness aims to motivate players to move their bodies and exercise. And it looks like an 80s screensaver! Also includes a seated mode for those who cannot exercise standing up. Obesity is a major challenge and making exercise fun and accessible is a good start. </p>\n<p>Theme: Health and Wellness</p>\n<p>Team 18</p>\n<p>MIT Media Lab, 6th floor, E14-674, Table 4</p>\n<p>Assets Used:</p>\n<ul>\n<li>VIU</li>\n<li>UCLA Wireframe Shader</li>\n<li>UETools</li>\n<li>VR Capture</li>\n<li>freesound.org</li>\n<li>Google fonts</li>\n</ul>\n</div>",
            "content_md": "\nInspired by 80s jazzercise videos, VR Fitness aims to motivate players to move their bodies and exercise. And it looks like an 80s screensaver! Also includes a seated mode for those who cannot exercise standing up. Obesity is a major challenge and making exercise fun and accessible is a good start. \n\n\nTheme: Health and Wellness\n\n\nTeam 18\n\n\nMIT Media Lab, 6th floor, E14-674, Table 4\n\n\nAssets Used:\n\n\n* VIU\n* UCLA Wireframe Shader\n* UETools\n* VR Capture\n* freesound.org\n* Google fonts\n\n\n"
        },
        {
            "source": "https://devpost.com/software/crane",
            "title": "Crane",
            "blurb": "Crane is a design language used for 6DoF XR interactions, inspired by traditional origami style",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312383180?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "Music player",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/612/datas/original.jpg"
                },
                {
                    "title": "Weather + music player widgets",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/883/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Yu Jen Chen",
                    "about": "Interaction demo animation",
                    "photo": "https://avatars0.githubusercontent.com/u/30551434?height=180&v=4&width=180"
                },
                {
                    "name": "Sara Birchard",
                    "about": "UX/UI Designer",
                    "photo": "https://avatars2.githubusercontent.com/u/6678010?height=180&v=4&width=180"
                },
                {
                    "name": "Earl NIE",
                    "about": "UX Designer and creative technologist, created and built interactive motions in Unity.\n",
                    "photo": "https://avatars3.githubusercontent.com/u/30550746?height=180&v=4&width=180"
                },
                {
                    "name": "Ke Ding",
                    "about": "Design Lead of Project: Crane. XR Designer.",
                    "photo": "https://avatars2.githubusercontent.com/u/11837655?height=180&v=4&width=180"
                },
                {
                    "name": "Wei Dai",
                    "about": "UI/UX Designer and developer",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/747/064/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "c#",
                "cinema4d",
                "oculus",
                "sketch",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>6 DoF controllers &amp; freedom to use hands in XR</p>\n<h2>What it is</h2>\n<p>Crane is a design language used for 6DoF XR interactions, inspired by traditional origami style.</p>\n<p>Our XR interface utilizes the space surrounding the user.  It\u2019s not necessary to have a lot of space to navigate through XR, especially when we are using our hands as controllers and cannot reach too far away.</p>\n<p>The visual style and the interaction feedback in today\u2019s XR sometimes feel inconsistent. With a systematic design language, we can unify the experience and make it even more <strong>intuitive</strong>.</p>\n<p>Crane uses a set of interfaces and interactions that inherit &amp; simulate origami.</p>\n<p>Our goal is to make the experience in XR robust, tangible &amp; intuitive.</p>\n<h2>How we built it</h2>\n<p>Unity, Leap Motion, Oculus Rift, Cinema4D, After Effects, Sketch</p>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for Crane</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n6 DoF controllers & freedom to use hands in XR\n\n\n## What it is\n\n\nCrane is a design language used for 6DoF XR interactions, inspired by traditional origami style.\n\n\nOur XR interface utilizes the space surrounding the user. It\u2019s not necessary to have a lot of space to navigate through XR, especially when we are using our hands as controllers and cannot reach too far away.\n\n\nThe visual style and the interaction feedback in today\u2019s XR sometimes feel inconsistent. With a systematic design language, we can unify the experience and make it even more **intuitive**.\n\n\nCrane uses a set of interfaces and interactions that inherit & simulate origami.\n\n\nOur goal is to make the experience in XR robust, tangible & intuitive.\n\n\n## How we built it\n\n\nUnity, Leap Motion, Oculus Rift, Cinema4D, After Effects, Sketch\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for Crane\n\n\n"
        },
        {
            "source": "https://devpost.com/software/spatial-programming-sk1ry8",
            "title": "Spatial Programming",
            "blurb": "MIT Hackathon 2019. Learn programming intuitively and spatially in mixed reality",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312312003?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Suzan Oslin",
                    "about": "Interface design and user experience.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/632/963/datas/profile.jpg"
                },
                {
                    "name": "et3rnald",
                    "about": "I focused on 3D modeling and animation, contributed to group discussion as we homed in on our project idea and its game mechanics and narrative, and worked on sound design as well.",
                    "photo": "https://graph.facebook.com/10155917091572805/picture?height=180&width=180"
                },
                {
                    "name": "Aakash Shah",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQHylR1wa2OfNg/profile-displayphoto-shrink_100_100/0?e=1526187600&height=180&t=y9sGUoWtTR_M7byBxpUAU7jlhYBFL42BQgtPRroRT4s&v=alpha&width=180"
                },
                {
                    "name": "Leonard Wedderburn",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/424/958/datas/profile.jpg"
                },
                {
                    "name": "Hat Nguyen",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/e8cc9122dd96e41edfdf29fa1789149b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "unity-mixedrealitytoolkit"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Spatial programming is the beginnings of a node based programming interface, like <a href=\"https://scratch.mit.edu/\" rel=\"nofollow\">Scratch</a>, but in mixed reality. The idea stemmed from a desire to learn to program while rapidly prototyping for mixed reality. The productivity potentials for a system like caught our imagination as much as the educational potentials.</p>\n<h2>What it does</h2>\n<p>Control a robot by organizing a sequence of node-based commands in MR</p>\n<h2>How we built it</h2>\n<p>Built in unity with the mixed reality toolkit</p>\n<h2>Challenges we ran into</h2>\n<p>Figuring out what was possible to achieve in the limited time-frame</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're proud of our use of natural interfaces, took advantage of the affordances of the WMR interface.</p>\n<h2>What we learned</h2>\n<p>We learned what it takes to create a pseudo-programming language. We learned programming languages are very complex and to break it down spatial. It involves more than just throwing objects together.</p>\n<h2>What's next for Spatial Programming</h2>\n<p>Phase One Features: finish fleshing out the library of nodes to get it closer to a full programming language.\nPhase Two: add features such as:</p>\n<ul>\n<li>Multi-user cross-platform AR/VR collaboration</li>\n<li>Connect IOT devices to literally program your environment</li>\n<li>Import &amp; parse scripts to visualize more complex program flows</li>\n</ul>\n<p>Generally pushing towards being an open source productivity tool.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSpatial programming is the beginnings of a node based programming interface, like [Scratch](https://scratch.mit.edu/), but in mixed reality. The idea stemmed from a desire to learn to program while rapidly prototyping for mixed reality. The productivity potentials for a system like caught our imagination as much as the educational potentials.\n\n\n## What it does\n\n\nControl a robot by organizing a sequence of node-based commands in MR\n\n\n## How we built it\n\n\nBuilt in unity with the mixed reality toolkit\n\n\n## Challenges we ran into\n\n\nFiguring out what was possible to achieve in the limited time-frame\n\n\n## Accomplishments that we're proud of\n\n\nWe're proud of our use of natural interfaces, took advantage of the affordances of the WMR interface.\n\n\n## What we learned\n\n\nWe learned what it takes to create a pseudo-programming language. We learned programming languages are very complex and to break it down spatial. It involves more than just throwing objects together.\n\n\n## What's next for Spatial Programming\n\n\nPhase One Features: finish fleshing out the library of nodes to get it closer to a full programming language.\nPhase Two: add features such as:\n\n\n* Multi-user cross-platform AR/VR collaboration\n* Connect IOT devices to literally program your environment\n* Import & parse scripts to visualize more complex program flows\n\n\nGenerally pushing towards being an open source productivity tool.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/goldfish-0oh51j",
            "title": "Goldfish",
            "blurb": "A joyful interaction between VR and the reality through the perspective of a Goldfish.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312404537?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Kimberly Lin",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/31666429?height=180&v=4&width=180"
                },
                {
                    "name": "Maximegau",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/26672876?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "apis",
                "hardware",
                "hosts",
                "languages",
                "leap-motion",
                "libraries",
                "ui-kits",
                "unity",
                "vive",
                "wwise"
            ],
            "content_html": "<div>\n<h2>Abstract</h2>\n<p>Exploring the dynamic between an immersed human and observers, Goldfish is a joyful interaction between VR and the reality through the perspective of a Goldfish. </p>\n<h2>Inspiration</h2>\n<p>In most VR games, we are sitting in a small space alone looking into a boundless world. What if we revert the dynamic, experience VR in a swimming pool, free from the constraint of gravity while we interact with the environment above us? How does it feel like being a small fish looking at the world outside of the fish bowl? With an unconventional approach to VR, we want create a performative VR sculpture that resembles life through the lens of a goldfish.</p>\n<h2>What it does</h2>\n<p>Two players are involved. Player 1 is put into the perspective of a goldfish via a VR headset and a Leap motion that turns Player 1\u2019s hands into fins. He/she explores the world he/she lives in contained by a fish tank. Player 2 acts as a cat holding a cat paw with tracker attached. He/she uses his/her paw to attack Player 1 where his/her claw will appear in Player 1\u2019s point of view. </p>\n<h2>How we built it</h2>\n<p>Besides Unity, Leap Motion, HTC Vive, we built a physical cat paw out of recycling materials we found at MIT\u2019s junkyard. </p>\n<h2>Challenges</h2>\n<p>Working with a relatively big team on a same project is rather challenging. We had a lot of conflicts and merging issues with GitHub. We also weren\u2019t able to scale the environment correctly in time due to limited VR headsets we have. Initially, we wanted to also add another interaction of an additional player fishing the goldfish with a marshmallow but we ran out of time. </p>\n<h2>Accomplishments</h2>\n<p>We have successfully completed our cat interaction with the goldfish as a proof of concept but more importantly, we are really happy that we found a team of talents who are passionate about the same topic. </p>\n<h2>What we learned</h2>\n<p>Interdisciplinary group work is very rewarding on a personal level but as well on a product level. </p>\n<h2>What\u2019s next</h2>\n<p>Moving forward, we would like to experiment how the sensations of our project can be enhanced by submerging in a swimming pool. We are in conversation with a fellow hacker we met during the Hackathon, Pierre Friquet, who have created an underwater VR headset. We also envision Goldfish to be an interactive installation at an aquarium or a science museum hence we plan to reach out to venues to show our piece. We are also interested in how haptics can improve motor functions and hand accuracy of stroke and parkinson's patients and would like to work with institutions to extend our research. </p>\n</div>",
            "content_md": "\n## Abstract\n\n\nExploring the dynamic between an immersed human and observers, Goldfish is a joyful interaction between VR and the reality through the perspective of a Goldfish. \n\n\n## Inspiration\n\n\nIn most VR games, we are sitting in a small space alone looking into a boundless world. What if we revert the dynamic, experience VR in a swimming pool, free from the constraint of gravity while we interact with the environment above us? How does it feel like being a small fish looking at the world outside of the fish bowl? With an unconventional approach to VR, we want create a performative VR sculpture that resembles life through the lens of a goldfish.\n\n\n## What it does\n\n\nTwo players are involved. Player 1 is put into the perspective of a goldfish via a VR headset and a Leap motion that turns Player 1\u2019s hands into fins. He/she explores the world he/she lives in contained by a fish tank. Player 2 acts as a cat holding a cat paw with tracker attached. He/she uses his/her paw to attack Player 1 where his/her claw will appear in Player 1\u2019s point of view. \n\n\n## How we built it\n\n\nBesides Unity, Leap Motion, HTC Vive, we built a physical cat paw out of recycling materials we found at MIT\u2019s junkyard. \n\n\n## Challenges\n\n\nWorking with a relatively big team on a same project is rather challenging. We had a lot of conflicts and merging issues with GitHub. We also weren\u2019t able to scale the environment correctly in time due to limited VR headsets we have. Initially, we wanted to also add another interaction of an additional player fishing the goldfish with a marshmallow but we ran out of time. \n\n\n## Accomplishments\n\n\nWe have successfully completed our cat interaction with the goldfish as a proof of concept but more importantly, we are really happy that we found a team of talents who are passionate about the same topic. \n\n\n## What we learned\n\n\nInterdisciplinary group work is very rewarding on a personal level but as well on a product level. \n\n\n## What\u2019s next\n\n\nMoving forward, we would like to experiment how the sensations of our project can be enhanced by submerging in a swimming pool. We are in conversation with a fellow hacker we met during the Hackathon, Pierre Friquet, who have created an underwater VR headset. We also envision Goldfish to be an interactive installation at an aquarium or a science museum hence we plan to reach out to venues to show our piece. We are also interested in how haptics can improve motor functions and hand accuracy of stroke and parkinson's patients and would like to work with institutions to extend our research. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/ar-itus",
            "title": "AR-itus",
            "blurb": "Rehabilitating individuals who are suffering with hand-eye coordination problems.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/tKCXJWlN_xw?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Zhehai (Mark) Zhang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/224/540/datas/profile.jpg"
                },
                {
                    "name": "Anish Aggarwal",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/30203792?height=180&v=4&width=180"
                },
                {
                    "name": "Noor Nasri",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/37914931?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "ar",
                "c#",
                "leap-motion",
                "unity",
                "vive-pro"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>One of our team members has an uncle who has Arthritis. It's very difficult for him to perform daily tasks that require hands and arms. This is a problem not just affecting people suffering Arthritis, but is also present in other diseases such as Parkinson's, Huntington's. Elderly also suffer the same problem. In order to help these individuals to regain their basic motor skills, our team decided to solve this problem. </p>\n<h2>What it does</h2>\n<p>AR-itus has 2 main features: Start and Goals. With three exercises - Gestures, dropping, and balance, these exercises evaluate the individual's time, accuracy, and shakiness. By analyzing these three factors, the user is able to practice daily and eventually see themselves progress and improve. With this technology, the user is not only able to train his/her hand eye coordination, but is also able to set SMART goals and view their progress along their journey to a healthier lifestyle.</p>\n<h2>How we built it</h2>\n<p>We used Unity, Leap Motion (motion sensor) and the Vive Pro Headset with AR. Our models were made with Cinema 4D and Photoshop.</p>\n<h2>Challenges we ran into</h2>\n<p>We had a lot of trouble setting up the hardware for Leap Motion and Vive Pro since SteamVR and the software were not compatible, so we had to edit the settings inside SteamVR to re enable the camera on the Vive Pro. Then, we had an issue of merging two projects since they were on different versions of Unity, and additionally, verifying both SDK's work on the same version of Unity. Additionally, we had other problems with hardware such as missing specific types of cables, and initially not having the Leap Motion until the second day. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We're proud of being able to implement AR and Leap Motion in this process. Additionally, being able to get the tech to work after spending so much time is a big accomplishment we're proud of.</p>\n<h2>What we learned</h2>\n<p>We learnt how to use joints on the rig to determine specific hand signals based on the Vector positions, and how AR setup is different compared to VR setup on Unity. We also learnt how to make a flowing UI for better user interaction.</p>\n<h2>What's next for AR-itus</h2>\n<p>In order to make AR-itus more fun and interactive, implementing a virtual store where users can buy customization skins from earning credits by completing exercises would greatly improve reception of this application.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOne of our team members has an uncle who has Arthritis. It's very difficult for him to perform daily tasks that require hands and arms. This is a problem not just affecting people suffering Arthritis, but is also present in other diseases such as Parkinson's, Huntington's. Elderly also suffer the same problem. In order to help these individuals to regain their basic motor skills, our team decided to solve this problem. \n\n\n## What it does\n\n\nAR-itus has 2 main features: Start and Goals. With three exercises - Gestures, dropping, and balance, these exercises evaluate the individual's time, accuracy, and shakiness. By analyzing these three factors, the user is able to practice daily and eventually see themselves progress and improve. With this technology, the user is not only able to train his/her hand eye coordination, but is also able to set SMART goals and view their progress along their journey to a healthier lifestyle.\n\n\n## How we built it\n\n\nWe used Unity, Leap Motion (motion sensor) and the Vive Pro Headset with AR. Our models were made with Cinema 4D and Photoshop.\n\n\n## Challenges we ran into\n\n\nWe had a lot of trouble setting up the hardware for Leap Motion and Vive Pro since SteamVR and the software were not compatible, so we had to edit the settings inside SteamVR to re enable the camera on the Vive Pro. Then, we had an issue of merging two projects since they were on different versions of Unity, and additionally, verifying both SDK's work on the same version of Unity. Additionally, we had other problems with hardware such as missing specific types of cables, and initially not having the Leap Motion until the second day. \n\n\n## Accomplishments that we're proud of\n\n\nWe're proud of being able to implement AR and Leap Motion in this process. Additionally, being able to get the tech to work after spending so much time is a big accomplishment we're proud of.\n\n\n## What we learned\n\n\nWe learnt how to use joints on the rig to determine specific hand signals based on the Vector positions, and how AR setup is different compared to VR setup on Unity. We also learnt how to make a flowing UI for better user interaction.\n\n\n## What's next for AR-itus\n\n\nIn order to make AR-itus more fun and interactive, implementing a virtual store where users can buy customization skins from earning credits by completing exercises would greatly improve reception of this application.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/the-big-picture-e1mf7h",
            "title": "The Big Picture",
            "blurb": "A platform for artists to scale location based installations, from small concepts to big ideas.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/8_2OvBazPho?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Albert Schweitzer",
                    "about": "AWS web-based APIs, Raspberry Pi and Photon integrations. General timekeeping and tidyness. ",
                    "photo": "https://www.gravatar.com/avatar/c5d805c5862bac69139d705e75f789b3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Sean Continuum",
                    "about": "Virtual Reality Artist / Animator",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/743/993/datas/profile.jpg"
                },
                {
                    "name": "Ian Sterling",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0__bBK7OLYNySePLgn5liKCR2YKd2e9z0Uh_iKdodxlyAd9L2nL_7Kaabx1e7UvLOc8_iAX7e04IaWrAiQ7qtuboL1gIaHrAeVWqtPuIrOnoRkVFPQL5Xj2xOfjYq4MA0IhB9yIzKdDrD?height=180&width=180"
                },
                {
                    "name": "Swaroop Pal",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQFVPdeacBYpAw/profile-displayphoto-shrink_100_100/0?e=1545264000&height=180&t=HyfG0Mr4quzjuUV2RTiKTX-NmjJtjHUosseMZ86GIL8&v=beta&width=180"
                }
            ],
            "built_with": [
                "amazon-web-services",
                "c#",
                "gravity-sketch",
                "javascript",
                "particle-photon",
                "pir-sensor",
                "raspberry-pi",
                "sense-hat",
                "tvori",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Mixed reality brings the ability to tell stories across a scale that was previously available only in our imaginations. However, storytellers eager to embrace workflows and mediums of the future face a number of hurdles in their journey from idea to reality.</p>\n<p>We hacked together a platform and workflow for artists, event producers and anyone interested in using XR to create new and memorable experiences that help users translate small lessons into ideas that can change the world.</p>\n<h2>What it does</h2>\n<p>The project highlights some tools and workflows that can be combined to build an interactive MR experience. We built a demo on top of the platform that illustrates our relationship to energy use and waste in a way that highlights the scale of the problem and our role in tackling it.</p>\n<h2>How we built it</h2>\n<p>After ideation, VR artist Sean Rodrigo used Tvori to create scenes and characters that are incorporated into a Mixed Reality world using a Magic Leap headset. The headset interfaces with real world objects in the exhibition via a serverless API built with AWS Lambda and API Gateway. The real world can interface with the application via sensors connected to a Particle Photon or Rasbperry Pi. </p>\n<h2>Challenges we ran into</h2>\n<p>Getting the artist workflow into Unity gave us some difficulties because the animations were not exporting correctly, but we were able to debug this by breaking down the export process and trying different tools along the way. It was also difficult to build both the platform and the demo at the same time! </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Location-based mixed-reality! \nReplicable and scalable!\nOpen source for the people!\nEmpowering artists and creatives!</p>\n<h2>What we learned</h2>\n<h2>What's next for The Big Picture</h2>\n<p>The Big Picture is addressing the trend of temporary/pop up installations as a new form of creative expression for brands and artists. Current pop-up installation experiences such as Meow Wolf, The Museum of Ice Cream, and Happy Place show that there is public support for this form of art.</p>\n<p>As a platform, Big Picture offers the opportunity to create a blended physical and digital experience by merging analog interactions, smart sensors, and Magic Leap technology. As a pop-up location-based experience, Big Picture could be implemented essentially anywhere, and could collaborate with artists, brands, or institutions to offer novel experiences. Because of this, Big Picture is a scalable mixed reality experience not dependent on users owning a device themselves, but only on them visiting a location and purchasing a ticket. We would push for a presence on social media, thus helping create an interest in the consumer market. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nMixed reality brings the ability to tell stories across a scale that was previously available only in our imaginations. However, storytellers eager to embrace workflows and mediums of the future face a number of hurdles in their journey from idea to reality.\n\n\nWe hacked together a platform and workflow for artists, event producers and anyone interested in using XR to create new and memorable experiences that help users translate small lessons into ideas that can change the world.\n\n\n## What it does\n\n\nThe project highlights some tools and workflows that can be combined to build an interactive MR experience. We built a demo on top of the platform that illustrates our relationship to energy use and waste in a way that highlights the scale of the problem and our role in tackling it.\n\n\n## How we built it\n\n\nAfter ideation, VR artist Sean Rodrigo used Tvori to create scenes and characters that are incorporated into a Mixed Reality world using a Magic Leap headset. The headset interfaces with real world objects in the exhibition via a serverless API built with AWS Lambda and API Gateway. The real world can interface with the application via sensors connected to a Particle Photon or Rasbperry Pi. \n\n\n## Challenges we ran into\n\n\nGetting the artist workflow into Unity gave us some difficulties because the animations were not exporting correctly, but we were able to debug this by breaking down the export process and trying different tools along the way. It was also difficult to build both the platform and the demo at the same time! \n\n\n## Accomplishments that we're proud of\n\n\nLocation-based mixed-reality! \nReplicable and scalable!\nOpen source for the people!\nEmpowering artists and creatives!\n\n\n## What we learned\n\n\n## What's next for The Big Picture\n\n\nThe Big Picture is addressing the trend of temporary/pop up installations as a new form of creative expression for brands and artists. Current pop-up installation experiences such as Meow Wolf, The Museum of Ice Cream, and Happy Place show that there is public support for this form of art.\n\n\nAs a platform, Big Picture offers the opportunity to create a blended physical and digital experience by merging analog interactions, smart sensors, and Magic Leap technology. As a pop-up location-based experience, Big Picture could be implemented essentially anywhere, and could collaborate with artists, brands, or institutions to offer novel experiences. Because of this, Big Picture is a scalable mixed reality experience not dependent on users owning a device themselves, but only on them visiting a location and purchasing a ticket. We would push for a presence on social media, thus helping create an interest in the consumer market. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/boop-xaikn6",
            "title": "Boop",
            "blurb": "Conceal your emotions while your opponent does questionable things to your face. Played on Magic Leap One / iPhone X.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/4y-N3qcJg48?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Brenda Chen",
                    "about": "I collaborated to design the game, build the IPhone AR app, create the art assets, and animate. ",
                    "photo": "https://avatars3.githubusercontent.com/u/25263586?height=180&v=4&width=180"
                },
                {
                    "name": "Sagar Ramesh",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/16869869?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Social multiplayer games IRL. </p>\n<h2>What it does</h2>\n<p>iPhone player conceals his / her emotions while the Magic Leap player throws and drops objects (eggs, steaks, flip flops, etc.) on them to make them laugh. The Magic Leap player can also change what the iPhone player looks like, all in an attempt to make the iPhone player break his / her straight face.</p>\n<h2>How we built it</h2>\n<p>Unity, C#, ARKit, Magic Leap SDK</p>\n<h2>Challenges we ran into</h2>\n<p>Ensuring that the Magic Leap One, iPhone XR, and computer(s) running the game are able to talk to each other seamlessly. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our silly visual style and interactions. </p>\n<h2>What we learned</h2>\n<p>Scoping down and how to work with new hardware (Magic Leap).</p>\n<h2>What's next for Boop</h2>\n<p>More polish!</p>\n<h2>RV Requirements</h2>\n<p>Main work area, Sixth Floor.\nMade with Unity, C#, ARKit, Magic Leap SDK, LeanTween.\nAll other assets in project are original.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSocial multiplayer games IRL. \n\n\n## What it does\n\n\niPhone player conceals his / her emotions while the Magic Leap player throws and drops objects (eggs, steaks, flip flops, etc.) on them to make them laugh. The Magic Leap player can also change what the iPhone player looks like, all in an attempt to make the iPhone player break his / her straight face.\n\n\n## How we built it\n\n\nUnity, C#, ARKit, Magic Leap SDK\n\n\n## Challenges we ran into\n\n\nEnsuring that the Magic Leap One, iPhone XR, and computer(s) running the game are able to talk to each other seamlessly. \n\n\n## Accomplishments that we're proud of\n\n\nOur silly visual style and interactions. \n\n\n## What we learned\n\n\nScoping down and how to work with new hardware (Magic Leap).\n\n\n## What's next for Boop\n\n\nMore polish!\n\n\n## RV Requirements\n\n\nMain work area, Sixth Floor.\nMade with Unity, C#, ARKit, Magic Leap SDK, LeanTween.\nAll other assets in project are original.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/animaid",
            "title": "AnimAid",
            "blurb": "AI/AR tool to allow anyone w/ an idea and a modern smartphone to prototype visual story concepts",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/2-PuKfeCO_s?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "made with AnimAid",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/699/datas/original.png"
                },
                {
                    "title": "made with AnimAid",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/665/datas/original.png"
                },
                {
                    "title": "Deep learning network architecture",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/700/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Ryan Reede",
                    "about": "I built the menu systems, did a lot of modeling in Blender and helped define the product direction",
                    "photo": "https://avatars0.githubusercontent.com/u/12736540?height=180&v=4&width=180"
                },
                {
                    "name": "Nicholas Grana",
                    "about": "AR UI/UX Engineer. Worked on 3D environment design, user controls, and animations. Used Unity to apply scale, position, rotation, live camera view, selection, and duplication actions to easily and quickly manipulate scenes.",
                    "photo": "https://avatars1.githubusercontent.com/u/16421258?height=180&v=4&width=180"
                },
                {
                    "name": "Matt Kelsey",
                    "about": "Machine learning developer. Implemented a convolutional neural net to convert 2D sketches to 3D models. Wrote Python server to interface with this neural net. Consumed copious amounts of coffee.",
                    "photo": "https://avatars2.githubusercontent.com/u/6994202?height=180&v=4&width=180"
                },
                {
                    "name": "Thomas Suarez",
                    "about": "AR Integration / Logic / Networking",
                    "photo": "https://avatars3.githubusercontent.com/u/5103968?height=180&v=4&width=180"
                },
                {
                    "name": "Sam Roquitte",
                    "about": "Built multipurpose scene to draw 2D images to be processed into 3D via the neural net.",
                    "photo": "https://avatars3.githubusercontent.com/u/3688992?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "autodesk",
                "blender",
                "forge",
                "python",
                "teleportal",
                "tensorflow",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Speed up and democratize the ability to produce convincing storyboards and layouts for visual storytellers.</p>\n<h2>What it does</h2>\n<p>AnimAid is the ultimate tool for rapid prototyping of story concepts. Accessible to millions (available on all ARKit and ARCore devices), AnimAid seeks to democratize the skill and tools used for creating concept art and storyboards. There are 3 main components to the system.</p>\n<ol>\n<li>Drawing mode to feed two 2D viewports of a hypothetical 3D model. Uses deep learning to synthesize a basic 3d model from the two viewpoints.</li>\n<li>Laying out a scene's geometry in AR and moving cameras to get the perfect shot. Ability to add custom geometry formed by the deep learning system in (1).</li>\n<li>Capture a nice angle into the scene in AR, then drawing story content on top (chars, stage directions, etc..)</li>\n</ol>\n<h2>Check out our Slideshow for engineering details, challenges faced, and what is next for AnimAid:</h2>\n<p><a href=\"https://docs.google.com/presentation/d/e/2PACX-1vR53MlL9SgA1m3QzYEOIrYrteRKwDKQDDFpYX5wUN-NQRadJaaDn_6i2N99v56fwzBkCpnMV8jFo7y4/pub?start=true&amp;loop=false&amp;delayms=3000\" rel=\"nofollow\">https://docs.google.com/presentation/d/e/2PACX-1vR53MlL9SgA1m3QzYEOIrYrteRKwDKQDDFpYX5wUN-NQRadJaaDn_6i2N99v56fwzBkCpnMV8jFo7y4/pub?start=true&amp;loop=false&amp;delayms=3000</a></p>\n<h2>Also on Github here:</h2>\n<p><a href=\"https://github.com/RealityVirtually2019/ComposAR\" rel=\"nofollow\">https://github.com/RealityVirtually2019/ComposAR</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nSpeed up and democratize the ability to produce convincing storyboards and layouts for visual storytellers.\n\n\n## What it does\n\n\nAnimAid is the ultimate tool for rapid prototyping of story concepts. Accessible to millions (available on all ARKit and ARCore devices), AnimAid seeks to democratize the skill and tools used for creating concept art and storyboards. There are 3 main components to the system.\n\n\n1. Drawing mode to feed two 2D viewports of a hypothetical 3D model. Uses deep learning to synthesize a basic 3d model from the two viewpoints.\n2. Laying out a scene's geometry in AR and moving cameras to get the perfect shot. Ability to add custom geometry formed by the deep learning system in (1).\n3. Capture a nice angle into the scene in AR, then drawing story content on top (chars, stage directions, etc..)\n\n\n## Check out our Slideshow for engineering details, challenges faced, and what is next for AnimAid:\n\n\n<https://docs.google.com/presentation/d/e/2PACX-1vR53MlL9SgA1m3QzYEOIrYrteRKwDKQDDFpYX5wUN-NQRadJaaDn_6i2N99v56fwzBkCpnMV8jFo7y4/pub?start=true&loop=false&delayms=3000>\n\n\n## Also on Github here:\n\n\n<https://github.com/RealityVirtually2019/ComposAR>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/chronicle",
            "title": "Chronicle",
            "blurb": "Swipe from your phone to Magic Leap, curate memories in AR",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/316869925?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "iOS",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/979/datas/original.PNG"
                },
                {
                    "title": "iOS",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/980/datas/original.PNG"
                },
                {
                    "title": "iOS app",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/981/datas/original.PNG"
                },
                {
                    "title": "iOS",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/745/982/datas/original.PNG"
                }
            ],
            "team": [
                {
                    "name": "Gaston Montemayor",
                    "about": "",
                    "photo": "https://avatars.githubusercontent.com/u/6088934?height=180&v=3&width=180"
                },
                {
                    "name": "Josh",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/9309884?height=180&v=4&width=180"
                },
                {
                    "name": "scherrer-joshua",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/25680609?height=180&v=4&width=180"
                },
                {
                    "name": "Leopold Mebazaa",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/667e0b7597485c303f5bc386f0380cad?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "nnewberg",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/7318768?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "blender",
                "c#",
                "heroku",
                "magic-leap",
                "node.js",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>We wanted to play with the connection between phones and Magic Leap. We also wanted a way to make AR more social, personal, and emotional.</p>\n<p>Some sources of inspiration: scrapbooks, hyperlinks + HTML, Myspace, memory palaces, Benedict Cumberbatch.</p>\n<h2>What it does</h2>\n<ol>\n<li>Swipe images, video, songs, and 3D objects from your phone to AR</li>\n<li>Arrange and curate collections of objects into memories</li>\n<li>Save memories to \"stickers\" in the real world</li>\n<li>Share them with others</li>\n</ol>\n<h2>How we built it</h2>\n<p>Unity, C#, REST api, Magic Leap, React Native iOS app, Google Poly, Node.js backend server, Heroku server</p>\n<h2>Challenges we ran into</h2>\n<p>Web socket hell</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>OMG it works</p>\n<h2>What we learned</h2>\n<p>TBD</p>\n<h2>What's next for Chronicle</h2>\n<ol>\n<li>Dual-wield your phone in AR, i.e. more phone interactions</li>\n<li>Bluetooth connection from phone to Magic Leap (depends on Magic Leap API)</li>\n<li>Raycast snapping</li>\n<li>Eye gaze actions</li>\n</ol>\n</div>",
            "content_md": "\n## Inspiration\n\n\nWe wanted to play with the connection between phones and Magic Leap. We also wanted a way to make AR more social, personal, and emotional.\n\n\nSome sources of inspiration: scrapbooks, hyperlinks + HTML, Myspace, memory palaces, Benedict Cumberbatch.\n\n\n## What it does\n\n\n1. Swipe images, video, songs, and 3D objects from your phone to AR\n2. Arrange and curate collections of objects into memories\n3. Save memories to \"stickers\" in the real world\n4. Share them with others\n\n\n## How we built it\n\n\nUnity, C#, REST api, Magic Leap, React Native iOS app, Google Poly, Node.js backend server, Heroku server\n\n\n## Challenges we ran into\n\n\nWeb socket hell\n\n\n## Accomplishments that we're proud of\n\n\nOMG it works\n\n\n## What we learned\n\n\nTBD\n\n\n## What's next for Chronicle\n\n\n1. Dual-wield your phone in AR, i.e. more phone interactions\n2. Bluetooth connection from phone to Magic Leap (depends on Magic Leap API)\n3. Raycast snapping\n4. Eye gaze actions\n\n\n"
        },
        {
            "source": "https://devpost.com/software/adelante-offerings-of-rememberance",
            "title": "Adelante: Offerings of Remembrance",
            "blurb": "A VR app for people dealing with the loss of a loved one with interactive elements to honor the loved one & the loss.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/aIUKCawTvRg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ryan Sullivan",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/13654313?height=180&v=4&width=180"
                },
                {
                    "name": "achoko Allela",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46793932?height=180&v=4&width=180"
                },
                {
                    "name": "Susan Kapuscinski Gaylord",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b375c340f71225a4ed39c957d76a4ddb?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "steamvr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Location, floor, and room</h2>\n<p>Vive area, 6th floor Media Lab</p>\n<h2>The development tools used to build the project</h2>\n<p>Unity, PolyToolkit</p>\n<h2>SDKs used in the project</h2>\n<p>SteamVR</p>\n<h2>APIs used in the project</h2>\n<h2>Any assets used in the project that you did not create</h2>\n<p>Unity Standard Assets</p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/environments/landscapes/free-rocks-19288\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/environments/landscapes/free-rocks-19288</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/vfx/shaders/water-effect-fits-for-lowpoly-style-87810\" rel=\"nofollow\">https://assetstore.unity.com/packages/vfx/shaders/water-effect-fits-for-lowpoly-style-87810</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/tools/terrain/microsplat-96478\" rel=\"nofollow\">https://assetstore.unity.com/packages/tools/terrain/microsplat-96478</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/environments/landscapes/realistic-terrain-collection-lite-47726\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/environments/landscapes/realistic-terrain-collection-lite-47726</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/environments/landscapes/winter-zone-mini-107583\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/environments/landscapes/winter-zone-mini-107583</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/fallen-tree-barrier-free-49089\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/fallen-tree-barrier-free-49089</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/vfx/shaders/mk-glass-free-100712\" rel=\"nofollow\">https://assetstore.unity.com/packages/vfx/shaders/mk-glass-free-100712</a></p>\n<p><a href=\"https://free3d.com/3d-model/tree-house-69127.html\" rel=\"nofollow\">https://free3d.com/3d-model/tree-house-69127.html</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/props/food/bretwalda-halloween-74177\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/props/food/bretwalda-halloween-74177</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/environments/landscapes/photoscanned-moutainsrocks-pbr-130876\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/environments/landscapes/photoscanned-moutainsrocks-pbr-130876</a></p>\n<p><a href=\"https://assetstore.unity.com/packages/3d/characters/animals/living-birds-15649\" rel=\"nofollow\">https://assetstore.unity.com/packages/3d/characters/animals/living-birds-15649</a></p>\n<h2>Any libraries used in the project</h2>\n<h2>Any components not created at the hackathon</h2>\n<h2>A link to a video of a screen capture of the application on Youtube</h2>\n</div>",
            "content_md": "\n## Location, floor, and room\n\n\nVive area, 6th floor Media Lab\n\n\n## The development tools used to build the project\n\n\nUnity, PolyToolkit\n\n\n## SDKs used in the project\n\n\nSteamVR\n\n\n## APIs used in the project\n\n\n## Any assets used in the project that you did not create\n\n\nUnity Standard Assets\n\n\n<https://assetstore.unity.com/packages/3d/environments/landscapes/free-rocks-19288>\n\n\n<https://assetstore.unity.com/packages/vfx/shaders/water-effect-fits-for-lowpoly-style-87810>\n\n\n<https://assetstore.unity.com/packages/tools/terrain/microsplat-96478>\n\n\n<https://assetstore.unity.com/packages/3d/environments/landscapes/realistic-terrain-collection-lite-47726>\n\n\n<https://assetstore.unity.com/packages/3d/environments/landscapes/winter-zone-mini-107583>\n\n\n<https://assetstore.unity.com/packages/3d/fallen-tree-barrier-free-49089>\n\n\n<https://assetstore.unity.com/packages/vfx/shaders/mk-glass-free-100712>\n\n\n<https://free3d.com/3d-model/tree-house-69127.html>\n\n\n<https://assetstore.unity.com/packages/3d/props/food/bretwalda-halloween-74177>\n\n\n<https://assetstore.unity.com/packages/3d/environments/landscapes/photoscanned-moutainsrocks-pbr-130876>\n\n\n<https://assetstore.unity.com/packages/3d/characters/animals/living-birds-15649>\n\n\n## Any libraries used in the project\n\n\n## Any components not created at the hackathon\n\n\n## A link to a video of a screen capture of the application on Youtube\n\n\n"
        },
        {
            "source": "https://devpost.com/software/ice-breakers-wa829m",
            "title": "Ice Breakers",
            "blurb": "A multiplayer AR game that breaks down communication barriers and encourages positive social interactions. ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/OpTZdWx0stM?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/491/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "krispatl Pilcher",
                    "about": "I'm the main developer!\n",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/050/046/datas/profile.png"
                },
                {
                    "name": "Siciliana Trevino",
                    "about": "I was inspired to create Ice Breakers standing in line at the Hackathon. I designed and created the game identity and assets.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/486/datas/profile.jpg"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "arcore",
                "medium",
                "photoshop",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our team was inspired by the potential of shared digital experiences and their power to bring people together. We think breaking down communication barriers between people is one of our most pressing challenges in society today.</p>\n<h2>What it does</h2>\n<p>Two players enter a playroom where they are separated by a wall of ice cubes. Their objective is to break the ice cubes with a charmed projectile. When the players breakdown the wall everyone wins, incentivizing players to dismantle the divide between isolation and shared moments of delight. </p>\n<h2>How we built it</h2>\n<p>Developer Kris Pilcher built the game using Google's cloud integration in Unity. Team Lead and designer, Siciliana Trevino made game assets in Medium, photoshop and Illustrator. We also used Blender. </p>\n<h2>Challenges we ran into</h2>\n<p>In addition to only being a two-person team, we're inexperienced in networking and have intermediate programming skills that while challenging, presented a unique opportunity to learn about multi-player cloud AR. We didn't reach our stretch goal of having the ice cubes randomly release charms or a prompt to act out a short ice breaker activity like the hokey pokey or sharing with your partner a positive affirmation. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Being on a team, making important decisions quickly, networking the devices, getting our concept to work. Finishing!</p>\n<h2>What we learned</h2>\n<p>How to network devices using Google Cloud and Unity, how to build a mobile AR multiplayer XR experience.</p>\n<h2>What's next for Ice Breakers: Bringing the world closer together by taking down one wall at a time.</h2>\n<p>==================</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur team was inspired by the potential of shared digital experiences and their power to bring people together. We think breaking down communication barriers between people is one of our most pressing challenges in society today.\n\n\n## What it does\n\n\nTwo players enter a playroom where they are separated by a wall of ice cubes. Their objective is to break the ice cubes with a charmed projectile. When the players breakdown the wall everyone wins, incentivizing players to dismantle the divide between isolation and shared moments of delight. \n\n\n## How we built it\n\n\nDeveloper Kris Pilcher built the game using Google's cloud integration in Unity. Team Lead and designer, Siciliana Trevino made game assets in Medium, photoshop and Illustrator. We also used Blender. \n\n\n## Challenges we ran into\n\n\nIn addition to only being a two-person team, we're inexperienced in networking and have intermediate programming skills that while challenging, presented a unique opportunity to learn about multi-player cloud AR. We didn't reach our stretch goal of having the ice cubes randomly release charms or a prompt to act out a short ice breaker activity like the hokey pokey or sharing with your partner a positive affirmation. \n\n\n## Accomplishments that we're proud of\n\n\nBeing on a team, making important decisions quickly, networking the devices, getting our concept to work. Finishing!\n\n\n## What we learned\n\n\nHow to network devices using Google Cloud and Unity, how to build a mobile AR multiplayer XR experience.\n\n\n## What's next for Ice Breakers: Bringing the world closer together by taking down one wall at a time.\n\n\n==================\n\n\n"
        },
        {
            "source": "https://devpost.com/software/scalenaut",
            "title": "Scalenaut",
            "blurb": "A VR game to explore infinite spaces, from macro to micro",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/o8mKZhgvfrQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Louis Charron",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/32399381?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "grasshopper",
                "rhinoceros",
                "unity"
            ],
            "content_html": "<div>\n<h2>Information</h2>\n<p>Location: Room E15 054\nTools: Unity, Grasshopper, Rhinoceros, Mixamo\nSDK: none\nAPI: none</p>\n</div>",
            "content_md": "\n## Information\n\n\nLocation: Room E15 054\nTools: Unity, Grasshopper, Rhinoceros, Mixamo\nSDK: none\nAPI: none\n\n\n"
        },
        {
            "source": "https://devpost.com/software/project-awkward",
            "title": "Project Awkward",
            "blurb": "A simulation of awkward situations to help people handle those situations",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/x62-X9dzHe4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Private user",
                    "about": "I worked for the Unity3D programming part. I learned how to integrate NLP in VR games.",
                    "photo": "https://devpost-challengepost.netdna-ssl.com/assets/defaults/no-avatar-180-caa7628ae0aae09831858639d32ace2a.png"
                },
                {
                    "name": "nampham148",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/20184749?height=180&v=4&width=180"
                },
                {
                    "name": "Yana Pyryalina",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/74115aa3035372aa185fb96af9565815?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Ari Weinkle",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a5271cd8e8a96829d79d50cecdc84406?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Location for Judging:</h2>\n<p>E14 building, 6th floor, Purple room, far back left corner next to windows</p>\n<h2>Inspiration</h2>\n<p>You forget someone's name. You don't know anyone at the party. You're not sure who's paying for the meal. Your ex is at the bar. Our inspiration comes from these every-day awkward situations, from all of those times when you say \"Uh, I could've handled this so much better\". \nOur goal is to give people practice and suggestions on how to handle these awkward situations, so that they can confidently say \"I can handle this, I\u2019ve done this a thousand times by now\". </p>\n<h2>What it does</h2>\n<p>Our VR app simulates situations of different awkwardness levels. As users look around, we show them hints to get out of the situation, giving feedback, explanations, as well as things to stay to make the situation less awkward.</p>\n<h2>How we built it</h2>\n<p>We used Unity to develop on Oculus Rift. To understand human voice inputs, we used Azure Language Processing. To design and animate the characters, we used Blender and Cinema 4D. We also made real recordings to create the characters' dialogues. </p>\n<h2>Challenges we ran into</h2>\n<p>We had a hard time capturing Oculus Rift input, because we didn't use the Oculus Integration for Unity and we couldn't find good tutorials dealing with inputs. We also spent a lot of time discussing scenarios and deciding how we should design them to be both gamified and educational. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our application has some really cool scenarios that we're excited to show people! Also, our app can understand human voice input, catch where the users are looking, and uses real conversation records, creating a very natural interaction with users. We want to simulate realistic situations after all!</p>\n<h2>What we learned</h2>\n<ul>\n<li>Creating realistic awkward scenarios in VR </li>\n<li>Uniting various story-telling, educational, and game design aspects in one project</li>\n<li>Creating a VR project from the ground up just in 3 days</li>\n<li>Writing and recording realistic conversations with real people's voices</li>\n<li>Capturing users' speech in VR using NLP</li>\n<li>Using gaze tracking to create more immersive, realistic, and responsive VR environments</li>\n<li>Balancing reality and gamified experiences when designing the visuals of environments and non-player characters</li>\n<li>Getting Oculus Rift inputs</li>\n<li>Working as a true team</li>\n</ul>\n<h2>What's next for Project Awkward</h2>\n<p>We see this project going a long way. We would like to create more scenarios to cover a wider range of situations, as well as use machine learning to produce natural continuous conversations between users and neural networks. It is also a possibility that the app will host multiple users in one scenario, possibly creating awkward situations among real users. Also, we would like to do more research to refine the interaction between users and non-player characters, so that the experience is as realistic as possible. The more realistic, the more effective!</p>\n</div>",
            "content_md": "\n## Location for Judging:\n\n\nE14 building, 6th floor, Purple room, far back left corner next to windows\n\n\n## Inspiration\n\n\nYou forget someone's name. You don't know anyone at the party. You're not sure who's paying for the meal. Your ex is at the bar. Our inspiration comes from these every-day awkward situations, from all of those times when you say \"Uh, I could've handled this so much better\". \nOur goal is to give people practice and suggestions on how to handle these awkward situations, so that they can confidently say \"I can handle this, I\u2019ve done this a thousand times by now\". \n\n\n## What it does\n\n\nOur VR app simulates situations of different awkwardness levels. As users look around, we show them hints to get out of the situation, giving feedback, explanations, as well as things to stay to make the situation less awkward.\n\n\n## How we built it\n\n\nWe used Unity to develop on Oculus Rift. To understand human voice inputs, we used Azure Language Processing. To design and animate the characters, we used Blender and Cinema 4D. We also made real recordings to create the characters' dialogues. \n\n\n## Challenges we ran into\n\n\nWe had a hard time capturing Oculus Rift input, because we didn't use the Oculus Integration for Unity and we couldn't find good tutorials dealing with inputs. We also spent a lot of time discussing scenarios and deciding how we should design them to be both gamified and educational. \n\n\n## Accomplishments that we're proud of\n\n\nOur application has some really cool scenarios that we're excited to show people! Also, our app can understand human voice input, catch where the users are looking, and uses real conversation records, creating a very natural interaction with users. We want to simulate realistic situations after all!\n\n\n## What we learned\n\n\n* Creating realistic awkward scenarios in VR\n* Uniting various story-telling, educational, and game design aspects in one project\n* Creating a VR project from the ground up just in 3 days\n* Writing and recording realistic conversations with real people's voices\n* Capturing users' speech in VR using NLP\n* Using gaze tracking to create more immersive, realistic, and responsive VR environments\n* Balancing reality and gamified experiences when designing the visuals of environments and non-player characters\n* Getting Oculus Rift inputs\n* Working as a true team\n\n\n## What's next for Project Awkward\n\n\nWe see this project going a long way. We would like to create more scenarios to cover a wider range of situations, as well as use machine learning to produce natural continuous conversations between users and neural networks. It is also a possibility that the app will host multiple users in one scenario, possibly creating awkward situations among real users. Also, we would like to do more research to refine the interaction between users and non-player characters, so that the experience is as realistic as possible. The more realistic, the more effective!\n\n\n"
        },
        {
            "source": "https://devpost.com/software/flockar",
            "title": "FlockAR",
            "blurb": "AR art in nature experience to bring awareness to the habitat loss of the near threatened chimney swift.",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312311129?byline=0&portrait=0&title=0#t="
            ],
            "images": [
                {
                    "title": "FlockAR",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/742/543/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Margarita Benitez",
                    "about": "We learned tons about Augmented Reality (A-Frame, three.js, and AR.js). We love chimney swifts and hope you will, too!",
                    "photo": "https://graph.facebook.com/v3.3/10161181034405401/picture?height=180&width=180"
                },
                {
                    "name": "Markus Vogl",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/535/datas/profile.jpg"
                }
            ],
            "built_with": [
                "a-frame",
                "ar.js",
                "gblock.js",
                "grasshopper",
                "poly",
                "rhino",
                "three.js"
            ],
            "content_html": "<div>\n<h2>FlockAR</h2>\n<p>Margarita Benitez (Modeler + Designer)</p>\n<p>Markus Vogl (\"Developer\" + Designer)</p>\n<p>Location: E15 - Lower Level Atrium - Table 100</p>\n<h2>BUILT WITH:</h2>\n<p>3D models: Rhino, Grasshopper, Poly</p>\n<p>AR dev framework: A-Frame</p>\n<p>Libraries: Three.js, AR.js, gblock.js</p>\n<p>Custom AR Marker created <a href=\"https://jeromeetienne.github.io/AR.js/three.js/examples/marker-training/examples/generator.html\" rel=\"nofollow\">AR.js Marker Training</a>.</p>\n<p>Grasshopper definition is based on a tutorial file that was originally tested on 12/27/2018.</p>\n<h2>Inspiration</h2>\n<p>AR art in nature experience to bring awareness to the habitat loss of the near threatened chimney swift.\nThe Chimney Swift is a very fascinating bird, as they cannot perch, but only hang on to walls. They are found in certain places in North America (including Ohio) and in some instances their migration can take them down to parts in South America. </p>\n<p><a href=\"https://commons.wikimedia.org/wiki/File:Chimney_Swift_range.png\" rel=\"nofollow\">Chimney Swift Migration Range</a></p>\n<p>They spend all their time in the air except when roosting or nesting. They make their nests in Chimneys, but as industrialization moves away from American mainland and homeowners lock them out from their chimneys by capping them, they have lost over 70% of their habitat since the 1960\u2019s and are now being considered a near-threatened bird species in their US habitat. One tower will hold a large number of migrating chimney swifts but only one breeding pair. The Cornell Lab of Ornithology states \u201cduring migration, as many as 10,000 swifts may circle in a tornado-like flock at dusk and funnel into a roosting chimney to spend the night. The lives of these widespread urban birds are surprisingly unstudied, because of their inaccessible nesting and roosting sites and their aerial lifestyle.\u201d There has been efforts of bird lovers to mitigate this crisis, by building artificial chimneys out of cinderblocks, wood or metal sheeting.</p>\n<p>When we first learned about the chimney swift, as designers we felt we could push the aesthetic design of these artificial chimneys through the use of digital fabrication and manufacturing. We have been working with Dr. Lara Roketenetz from the University of Akron Bath Nature Preserve Field Station, Dr. Judy Semroc from the Cleveland Museum of Natural History Museum and using the book \u201cChimney Swift Towers: New Habitat for America\u2019s Mysterious Birds, a Construction Guide\u201d by Paul and Georgean Kyle to develop the designs for our digitally designed and manufactured tower.</p>\n<p>With this AR App, we wanted to share a project close to our hearts and bring to light the plight of the chimney swift. Our AR App uses a marker to place a chimney tower on site. We hope FlockAR to be used by field stations, parks, and schools to bring awareness about the chimney swift population decline and help the preservation of habitat for these awesome birds. It may even serve as a way to motivate the community and individuals to create their own chimney swift towers to help raise the population numbers of these adorable birds.</p>\n<h2>What it does</h2>\n<p>Flock AR is a WebAR App utilizing a custom chimney swift marker to place a virtual 3D chimney tower on site \n(ultimately to be built in nature).</p>\n<h2>How we built it</h2>\n<p>We used A-Frame with Jerome Etienne's A-Frame AR (thank you, Jerome!). Our Chimney tower models were created in Rhino using Grasshopper (parametric modeling) which were then loaded via poly. Custom .patt marker was created via Jerome's <a href=\"https://jeromeetienne.github.io/AR.js/three.js/examples/marker-training/examples/generator.html\" rel=\"nofollow\">AR.js Marker Training</a>.</p>\n<h2>Challenges we ran into</h2>\n<p>Ideation and team building were the first challenges we faced. We decided on our idea mid-Friday and we didn't feel it was possible to find a developer at that time to work with use so we focused on making this a learning experience. We ran into issues importing the model - some would import in Poly, others would not - turns out you have to turn on the allow for remix setting for the model to be able to be called into the app. We originally had an ASUS Zen Phone and a Google Daydream and it became instantly obvious that the Daydream is not an AR capable headset (no hole for the camera). We tested a VIVE Focus (as well as the Microsoft headset) but it felt like overkill as we wanted to keep the application as a web based, mobile AR experience. We had some issues with the marker triggering but the model not hiding once the marker was off camera. Mentor Or Fleisher was incredibly helpful in demystifying how models load and parts of three.js. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of having learned about how to approach AR for mobile devices during the hackathon and that even though our project is technically \"simple\", we made something that is close to our heart in topic - the conservation of chimney swifts.</p>\n<h2>What we learned</h2>\n<p>Our takeaway from the keynote was to learn and have fun. We learned so much about the current hardware and software. The workshops and community talks were interesting and full of insights that we will take with us moving forward with our explorations in XR. Or helped us realize three.js is approachable.</p>\n<h2>What's next for FlockAR</h2>\n<p>The tower model will be changed to reflect actual tower design that will be physically constructed. We will be releasing the webAR at Bath Nature Preserve in Ohio so that the visitors there can visualize the chimney swift tower we will be installing there later this year. We plan on switching to a GPS markerless AR experience and plan to add a flock of chimney swifts at the top of the tower. We hope to add more content about the chimney swift (facts, photos, conservation info). </p>\n</div>",
            "content_md": "\n## FlockAR\n\n\nMargarita Benitez (Modeler + Designer)\n\n\nMarkus Vogl (\"Developer\" + Designer)\n\n\nLocation: E15 - Lower Level Atrium - Table 100\n\n\n## BUILT WITH:\n\n\n3D models: Rhino, Grasshopper, Poly\n\n\nAR dev framework: A-Frame\n\n\nLibraries: Three.js, AR.js, gblock.js\n\n\nCustom AR Marker created [AR.js Marker Training](https://jeromeetienne.github.io/AR.js/three.js/examples/marker-training/examples/generator.html).\n\n\nGrasshopper definition is based on a tutorial file that was originally tested on 12/27/2018.\n\n\n## Inspiration\n\n\nAR art in nature experience to bring awareness to the habitat loss of the near threatened chimney swift.\nThe Chimney Swift is a very fascinating bird, as they cannot perch, but only hang on to walls. They are found in certain places in North America (including Ohio) and in some instances their migration can take them down to parts in South America. \n\n\n[Chimney Swift Migration Range](https://commons.wikimedia.org/wiki/File:Chimney_Swift_range.png)\n\n\nThey spend all their time in the air except when roosting or nesting. They make their nests in Chimneys, but as industrialization moves away from American mainland and homeowners lock them out from their chimneys by capping them, they have lost over 70% of their habitat since the 1960\u2019s and are now being considered a near-threatened bird species in their US habitat. One tower will hold a large number of migrating chimney swifts but only one breeding pair. The Cornell Lab of Ornithology states \u201cduring migration, as many as 10,000 swifts may circle in a tornado-like flock at dusk and funnel into a roosting chimney to spend the night. The lives of these widespread urban birds are surprisingly unstudied, because of their inaccessible nesting and roosting sites and their aerial lifestyle.\u201d There has been efforts of bird lovers to mitigate this crisis, by building artificial chimneys out of cinderblocks, wood or metal sheeting.\n\n\nWhen we first learned about the chimney swift, as designers we felt we could push the aesthetic design of these artificial chimneys through the use of digital fabrication and manufacturing. We have been working with Dr. Lara Roketenetz from the University of Akron Bath Nature Preserve Field Station, Dr. Judy Semroc from the Cleveland Museum of Natural History Museum and using the book \u201cChimney Swift Towers: New Habitat for America\u2019s Mysterious Birds, a Construction Guide\u201d by Paul and Georgean Kyle to develop the designs for our digitally designed and manufactured tower.\n\n\nWith this AR App, we wanted to share a project close to our hearts and bring to light the plight of the chimney swift. Our AR App uses a marker to place a chimney tower on site. We hope FlockAR to be used by field stations, parks, and schools to bring awareness about the chimney swift population decline and help the preservation of habitat for these awesome birds. It may even serve as a way to motivate the community and individuals to create their own chimney swift towers to help raise the population numbers of these adorable birds.\n\n\n## What it does\n\n\nFlock AR is a WebAR App utilizing a custom chimney swift marker to place a virtual 3D chimney tower on site \n(ultimately to be built in nature).\n\n\n## How we built it\n\n\nWe used A-Frame with Jerome Etienne's A-Frame AR (thank you, Jerome!). Our Chimney tower models were created in Rhino using Grasshopper (parametric modeling) which were then loaded via poly. Custom .patt marker was created via Jerome's [AR.js Marker Training](https://jeromeetienne.github.io/AR.js/three.js/examples/marker-training/examples/generator.html).\n\n\n## Challenges we ran into\n\n\nIdeation and team building were the first challenges we faced. We decided on our idea mid-Friday and we didn't feel it was possible to find a developer at that time to work with use so we focused on making this a learning experience. We ran into issues importing the model - some would import in Poly, others would not - turns out you have to turn on the allow for remix setting for the model to be able to be called into the app. We originally had an ASUS Zen Phone and a Google Daydream and it became instantly obvious that the Daydream is not an AR capable headset (no hole for the camera). We tested a VIVE Focus (as well as the Microsoft headset) but it felt like overkill as we wanted to keep the application as a web based, mobile AR experience. We had some issues with the marker triggering but the model not hiding once the marker was off camera. Mentor Or Fleisher was incredibly helpful in demystifying how models load and parts of three.js. \n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of having learned about how to approach AR for mobile devices during the hackathon and that even though our project is technically \"simple\", we made something that is close to our heart in topic - the conservation of chimney swifts.\n\n\n## What we learned\n\n\nOur takeaway from the keynote was to learn and have fun. We learned so much about the current hardware and software. The workshops and community talks were interesting and full of insights that we will take with us moving forward with our explorations in XR. Or helped us realize three.js is approachable.\n\n\n## What's next for FlockAR\n\n\nThe tower model will be changed to reflect actual tower design that will be physically constructed. We will be releasing the webAR at Bath Nature Preserve in Ohio so that the visitors there can visualize the chimney swift tower we will be installing there later this year. We plan on switching to a GPS markerless AR experience and plan to add a flock of chimney swifts at the top of the tower. We hope to add more content about the chimney swift (facts, photos, conservation info). \n\n\n"
        },
        {
            "source": "https://devpost.com/software/buddha-box",
            "title": "CLOUD BREATHER",
            "blurb": "Let your negative thoughts pass by like clouds in the sky. A breath-responsive underwater VR experience.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/IVZHirfcqPk?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "nado",
                    "about": "Worked on sound analysis",
                    "photo": "https://graph.facebook.com/2200484376681741/picture?height=180&width=180"
                },
                {
                    "name": "Cyrus Vachha",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/b33e2737b71d9aea4593743327e8792c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "monsieurpyare",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/46818484?height=180&v=4&width=180"
                },
                {
                    "name": "zibas",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/3812875?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "audacity",
                "c#",
                "poly",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>\"Between stimulus and response there is a space, in that space lies our power to choose our response, in our response lies our growth and our freedom.\" Viktor Frankel</p>\n<p>Mindfulness-based stress reduction is a technique developed by John Kabat-Zinn at University Mass. Medical Center in 1970s, which focus on attention feelings, thoughts and sensations that flow through individuals. We have focused on breathing technique and use an underwater VR headset for wellness purpose. We explore the use of meditation as a VR experience to help to improve everybody to sharpen their attention.</p>\n<h2>What it does</h2>\n<p>Be weightlessness in water, while being \"somewhere else\" is unique. CLOUD BREATHER is a mind-body experience visualising biofeedback. It can be used for specific therapeutic reasons like teenagers suffering from attention disorder and elderly having anxiety. We are trying to make people detach, objective and non-judgemental to themselves. Then they would be less inclined to fixate, react with negativity or label. </p>\n<h2>How we built it</h2>\n<p>We used Unity.</p>\n<h2>Challenges we ran into is to find the patterns of calmness, through the real-time recordings of the breath.</h2>\n<h2>Accomplishments that we're proud of finding a simple and yet exciting concept to create a fully fledged meditative experience.</h2>\n<h2>What we learned is a VR headset used underwater generates new possibilities of the immersive medium. It pushes the limit and definition of VR. It's the ultimate poetic machine.</h2>\n<h2>What's next for CLOUD BREATHER is to start a collaboration with Mindfulness-based stress reduction's clinics. We want them to implement our underwater VR experience.</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\n\"Between stimulus and response there is a space, in that space lies our power to choose our response, in our response lies our growth and our freedom.\" Viktor Frankel\n\n\nMindfulness-based stress reduction is a technique developed by John Kabat-Zinn at University Mass. Medical Center in 1970s, which focus on attention feelings, thoughts and sensations that flow through individuals. We have focused on breathing technique and use an underwater VR headset for wellness purpose. We explore the use of meditation as a VR experience to help to improve everybody to sharpen their attention.\n\n\n## What it does\n\n\nBe weightlessness in water, while being \"somewhere else\" is unique. CLOUD BREATHER is a mind-body experience visualising biofeedback. It can be used for specific therapeutic reasons like teenagers suffering from attention disorder and elderly having anxiety. We are trying to make people detach, objective and non-judgemental to themselves. Then they would be less inclined to fixate, react with negativity or label. \n\n\n## How we built it\n\n\nWe used Unity.\n\n\n## Challenges we ran into is to find the patterns of calmness, through the real-time recordings of the breath.\n\n\n## Accomplishments that we're proud of finding a simple and yet exciting concept to create a fully fledged meditative experience.\n\n\n## What we learned is a VR headset used underwater generates new possibilities of the immersive medium. It pushes the limit and definition of VR. It's the ultimate poetic machine.\n\n\n## What's next for CLOUD BREATHER is to start a collaboration with Mindfulness-based stress reduction's clinics. We want them to implement our underwater VR experience.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/soundbath",
            "title": "SoundBath",
            "blurb": "Creating tones with your voice, you create immersive sound objects which bathe you in a tonal hum ",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Vladimir Storm",
                    "about": "VFX",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/753/480/datas/profile.png"
                },
                {
                    "name": "ChelleySherman",
                    "about": "",
                    "photo": "https://graph.facebook.com/10155841749466782/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "c#"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>something</p>\n<h2>What it does</h2>\n<h2>How we built it</h2>\n<p>sdfsdfsdf</p>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for SoundBath</h2>\n<p>sdf</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nsomething\n\n\n## What it does\n\n\n## How we built it\n\n\nsdfsdfsdf\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for SoundBath\n\n\nsdf\n\n\n"
        },
        {
            "source": "https://devpost.com/software/urblox",
            "title": "Urblox",
            "blurb": "An urban planning tool that you can step into. Plan a location from above and then get in! ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/Y6U7UCtfgw8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Urblox Logo",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/237/datas/original.png"
                }
            ],
            "team": [
                {
                    "name": "Tay",
                    "about": "Primarily made the 3d assets for the projects, with the exception of the house models, and also 2d graphics- Logo. ",
                    "photo": "https://avatars2.githubusercontent.com/u/41489853?height=180&v=4&width=180"
                },
                {
                    "name": "Christopher Trinh",
                    "about": "I developed most of the VR interactions and mechanics, along with the main feature programming. It wouldn't have been successful at all without Khalea and Taylor though!",
                    "photo": "https://avatars0.githubusercontent.com/u/29661369?height=180&v=4&width=180"
                },
                {
                    "name": "Khalea Berry",
                    "about": "I came up with the idea of building an urban design application. I worked mostly on physics (collisions, interactions, etc) and level design! Also, lots of debugging & testing!",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/097/191/datas/profile.jpg"
                }
            ],
            "built_with": [
                "htc-vive",
                "steamvr-2.0",
                "unity"
            ],
            "content_html": "<div>\n<h2>Urblox Concept</h2>\n<p>Urblox an urban planning tool that you can step into! It's aimed at helping architects, urban designers, students, and hobbyists see how their ideas play out in an immersive environment.</p>\n<p>1) Import 3D assets into the \"studio\".\n2) Build an environment from scratch, or use a pre-existing environment to modify &amp; add on to.\n3) Teleport to street level and interact with the environment.</p>\n<h2>Basic Mechanics</h2>\n<p>The \"Design Studio\" is composed of the <em>Asset Display Table</em> and the <em>Sandbox Table</em> (which houses the environment you modify). </p>\n<p>The Asset table allows you to pull a copy of a 3D asset and move it into the Sandbox Table as you wish.</p>\n<h2>Future Prospects &amp; Features</h2>\n<p>Khalea (City Data &amp; Models): I make architectural models in my free time and use a product called Lumion 9 to render them. Lumion lets you import city data and will place grey blocks where buildings are, thus simulating a city. I wanted to have a similar feature in Urblox but we didn't have enough time to acquire that knowledge... but now we do :)</p>\n<h2>Accomplishments</h2>\n<p>We are a small but mighty team of 3 who <em>all</em> came in with little or no Unity scripting experience. We are proud that we pushed out a solid experience given the amount of time we had and learning that we had to catch up on.</p>\n<h2>Made With...</h2>\n<p>Unity, Vive Pro, Rhino (3D Modelling), SteamVR 2.0 (SDK)</p>\n<h2>Asset Attribution</h2>\n<ul>\n<li>SteamVR 2.0 default assets</li>\n<li>Ground Texture - <a href=\"https://freestocktextures.com/texture/soil-dirt-ground,268.html\" rel=\"nofollow\">https://freestocktextures.com/texture/soil-dirt-ground,268.html</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/building-apartment-3d-model-1315556\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/building-apartment-3d-model-1315556</a> </li>\n<li><a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/690329\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/690329</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/3d-medium-model/813122\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/3d-medium-model/813122</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/3ds-max-medium-cn-tower/813127\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/3ds-max-medium-cn-tower/813127</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/building-3d-model-1362131\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/building-3d-model-1362131</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/building-01-3d-model-1362124\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/building-01-3d-model-1362124</a></li>\n<li><a href=\"https://www.turbosquid.com/FullPreview/Index.cfm/ID/1326529\" rel=\"nofollow\">https://www.turbosquid.com/FullPreview/Index.cfm/ID/1326529</a></li>\n<li><a href=\"https://www.turbosquid.com/3d-models/lwo-football-stadium/930309\" rel=\"nofollow\">https://www.turbosquid.com/3d-models/lwo-football-stadium/930309</a></li>\n<li><a href=\"https://free3d.com/3d-model/red-brick-building-35698.html\" rel=\"nofollow\">https://free3d.com/3d-model/red-brick-building-35698.html</a></li>\n<li><a href=\"https://www.cgtrader.com/free-3d-models/exterior/house/apartment-ext\" rel=\"nofollow\">https://www.cgtrader.com/free-3d-models/exterior/house/apartment-ext</a></li>\n<li><a href=\"http://texturelib.com/texture/?path=/Textures/brick/pavement/brick_pavement_0098\" rel=\"nofollow\">http://texturelib.com/texture/?path=/Textures/brick/pavement/brick_pavement_0098</a></li>\n</ul>\n</div>",
            "content_md": "\n## Urblox Concept\n\n\nUrblox an urban planning tool that you can step into! It's aimed at helping architects, urban designers, students, and hobbyists see how their ideas play out in an immersive environment.\n\n\n1) Import 3D assets into the \"studio\".\n2) Build an environment from scratch, or use a pre-existing environment to modify & add on to.\n3) Teleport to street level and interact with the environment.\n\n\n## Basic Mechanics\n\n\nThe \"Design Studio\" is composed of the *Asset Display Table* and the *Sandbox Table* (which houses the environment you modify). \n\n\nThe Asset table allows you to pull a copy of a 3D asset and move it into the Sandbox Table as you wish.\n\n\n## Future Prospects & Features\n\n\nKhalea (City Data & Models): I make architectural models in my free time and use a product called Lumion 9 to render them. Lumion lets you import city data and will place grey blocks where buildings are, thus simulating a city. I wanted to have a similar feature in Urblox but we didn't have enough time to acquire that knowledge... but now we do :)\n\n\n## Accomplishments\n\n\nWe are a small but mighty team of 3 who *all* came in with little or no Unity scripting experience. We are proud that we pushed out a solid experience given the amount of time we had and learning that we had to catch up on.\n\n\n## Made With...\n\n\nUnity, Vive Pro, Rhino (3D Modelling), SteamVR 2.0 (SDK)\n\n\n## Asset Attribution\n\n\n* SteamVR 2.0 default assets\n* Ground Texture - <https://freestocktextures.com/texture/soil-dirt-ground,268.html>\n* <https://www.turbosquid.com/3d-models/building-apartment-3d-model-1315556>\n* <https://www.turbosquid.com/FullPreview/Index.cfm/ID/690329>\n* <https://www.turbosquid.com/3d-models/3d-medium-model/813122>\n* <https://www.turbosquid.com/3d-models/3ds-max-medium-cn-tower/813127>\n* <https://www.turbosquid.com/3d-models/building-3d-model-1362131>\n* <https://www.turbosquid.com/3d-models/building-01-3d-model-1362124>\n* <https://www.turbosquid.com/FullPreview/Index.cfm/ID/1326529>\n* <https://www.turbosquid.com/3d-models/lwo-football-stadium/930309>\n* <https://free3d.com/3d-model/red-brick-building-35698.html>\n* <https://www.cgtrader.com/free-3d-models/exterior/house/apartment-ext>\n* <http://texturelib.com/texture/?path=/Textures/brick/pavement/brick_pavement_0098>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/virtual-escape-room-bj9lwv",
            "title": "Escape the Witch's Grotto",
            "blurb": "Use magic, potions, and your wits to escape to freedom!",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/j4SkaYllHz4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Michael Zhang",
                    "about": "",
                    "photo": "https://graph.facebook.com/2233076753626265/picture?height=180&width=180"
                },
                {
                    "name": "Alina Christenbury",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/8976597?height=180&v=4&width=180"
                },
                {
                    "name": "Mark",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/29577479?height=180&v=4&width=180"
                },
                {
                    "name": "Chase Laux",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/204/datas/profile.jpg"
                },
                {
                    "name": "Dan Donato",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/10944648?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "htc-vive",
                "poly",
                "unity"
            ],
            "content_html": "<div>\n<h2>Abstract</h2>\n<p>After walking home from a regular-old hackathon, you found yourself suddenly kidnapped and locked in a dark, mystical room! With only 10 minutes to spare, you'll need to use magic, potions, and your wits to explore and navigate through this intricate trap. Will you escape the witch's grotto?</p>\n<h2>What it Does</h2>\n<p>The chest contains a final key that allows the player to escape and win the game. In order to open the chest, the player must create three potions. A magma, ice, and acid potion must all be poured onto the chest to unlock it.</p>\n<p>The escape room puzzle system takes advantage of a modular brewing system in which you can take any object, put it into the cauldron, and then pull a potion out of the cauldron based on the contents you put into it. Each asset has a value for three different attributes, and when mixed together, correspond to different potions. </p>\n<p>With these combinations, a player is able to make a potion from a variety of combinations. For example, a player can make a fire potion by throwing a torch into the cauldron because the torch has the hot, rough, and red characteristics. Alternatively, the player could also make a fire potion using a red mushroom which is smooth and red, combined with a candle which is hot.</p>\n<p>If the player makes the correct potion, the player will be able to pick up a potion that has either red, blue, or green liquid. If the player makes the incorrect potion, the player will receive a potion with black liquid inside. The green button next to the cauldron will allow the player to empty the cauldron and start over.</p>\n<h2>Our Team</h2>\n<ul>\n<li><p>Alina Christenbury is a computer science student at the University of Deleware. With over 2 years of experience with Unity, she leads an undergrad research project for VR/AR technology. </p></li>\n<li><p>Dan Donato <a href=\"https://dvdonato.com/\" rel=\"nofollow\">https://dvdonato.com/</a> is a computer science student with over 6 years of Unity experience. He's previously worked in quality assurance in the video game industry, and worked with multiple programming languages.</p></li>\n<li><p>Michael Zhang is a business student at the University of Michigan. He leads a VR/AR student organization called ARI and also helps produce a weekly VR talkshow called TheHiveVR.</p></li>\n<li><p>Mark Ma is finishing his masters in Design and Development of Digital Games at Columbia University. He's previously worked with Unity, JavaScript, and Java.</p></li>\n<li><p>Chase Laux recently graduated from California State University Fullerton, and specializes in effects and modeling. He also leads the Orange County Houdini User Group. </p></li>\n</ul>\n<h2>How We Built It</h2>\n<p><strong>Location:</strong> Vive Lounge, 3rd Floor, E15-335 \n<strong>Theme:</strong> Games and Learning\n<strong>Category:</strong> Videogame</p>\n<p>We built our HTC Vive experience using the Unity Game Engine and C#. We built the witch's house in Google Poly, and imported several Creative Commons License assets, sounds, and visual effects. These came from the Unity Store, Google Poly, Free Sound, and Sketchfab. The list of assets used are below:</p>\n<p><strong>Assets:</strong></p>\n<ul>\n<li>Stool Poly by Google: <a href=\"https://poly.google.com/view/cLydFlVg-wI\" rel=\"nofollow\">https://poly.google.com/view/cLydFlVg-wI</a> </li>\n<li>Wizard Hat Poly: <a href=\"https://poly.google.com/view/7VVumyY7L_u\" rel=\"nofollow\">https://poly.google.com/view/7VVumyY7L_u</a> </li>\n<li>Small stack of papers: <a href=\"https://poly.google.com/view/aiBozYlPe--\" rel=\"nofollow\">https://poly.google.com/view/aiBozYlPe--</a> </li>\n<li>Pumpkin Flash Sprinkles: <a href=\"https://poly.google.com/view/20c9qxsjcdQ\" rel=\"nofollow\">https://poly.google.com/view/20c9qxsjcdQ</a> </li>\n</ul>\n<p>Table 1</p>\n<ul>\n<li>Skull: <a href=\"https://sketchfab.com/models/bc302c319cff4dd5a67e2a7d92a4fb66\" rel=\"nofollow\">https://sketchfab.com/models/bc302c319cff4dd5a67e2a7d92a4fb66</a></li>\n<li>Candle: <a href=\"https://poly.google.com/view/8apROCXjeN4\" rel=\"nofollow\">https://poly.google.com/view/8apROCXjeN4</a><br/></li>\n<li>Full Rejuv Potion: <a href=\"https://sketchfab.com/models/c29531095aa24d7399fcc777e821e90b\" rel=\"nofollow\">https://sketchfab.com/models/c29531095aa24d7399fcc777e821e90b</a></li>\n<li>Alembic: <a href=\"https://sketchfab.com/models/811618ea79ba4aac95d7f7312a2128ec\" rel=\"nofollow\">https://sketchfab.com/models/811618ea79ba4aac95d7f7312a2128ec</a> </li>\n<li>Low Poly Book: <a href=\"https://poly.google.com/view/0-_UGETLPsX\" rel=\"nofollow\">https://poly.google.com/view/0-_UGETLPsX</a> </li>\n<li>Atom Crystal: <a href=\"https://sketchfab.com/models/81193f25ee064a899727581f34793486\" rel=\"nofollow\">https://sketchfab.com/models/81193f25ee064a899727581f34793486</a> </li>\n<li>New Table (1): <a href=\"https://sketchfab.com/models/ce6977c9de1d43048838e4802e21f7d4\" rel=\"nofollow\">https://sketchfab.com/models/ce6977c9de1d43048838e4802e21f7d4</a> </li>\n<li>Lab Equipment: <a href=\"https://poly.google.com/view/fSWYfMERNak\" rel=\"nofollow\">https://poly.google.com/view/fSWYfMERNak</a> </li>\n</ul>\n<p>Crystal Ball Clock</p>\n<ul>\n<li>Crystal Ball (1): <a href=\"https://poly.google.com/view/6X_VejNZ7Ok\" rel=\"nofollow\">https://poly.google.com/view/6X_VejNZ7Ok</a> </li>\n<li>Stem_Base_VR: <a href=\"https://poly.google.com/view/5cpYDJ4TZAP\" rel=\"nofollow\">https://poly.google.com/view/5cpYDJ4TZAP</a></li>\n</ul>\n<p>Table 2</p>\n<ul>\n<li>frog: <a href=\"https://sketchfab.com/models/6e12855ea9d84c5a9087dc02807352f0#download\" rel=\"nofollow\">https://sketchfab.com/models/6e12855ea9d84c5a9087dc02807352f0#download</a> </li>\n<li>healer: <a href=\"https://sketchfab.com/models/63b311b87a06425aa0b9a790e828e55c\" rel=\"nofollow\">https://sketchfab.com/models/63b311b87a06425aa0b9a790e828e55c</a> </li>\n<li>Cube crystal: <a href=\"https://sketchfab.com/models/20addc5a445840e3a6dc79c2d929785c\" rel=\"nofollow\">https://sketchfab.com/models/20addc5a445840e3a6dc79c2d929785c</a> </li>\n<li>Simple wooden tankard: <a href=\"https://poly.google.com/view/b3qGuE7F56q\" rel=\"nofollow\">https://poly.google.com/view/b3qGuE7F56q</a> </li>\n<li>Knife Poly: <a href=\"https://poly.google.com/view/0X5xcxjszwI\" rel=\"nofollow\">https://poly.google.com/view/0X5xcxjszwI</a> </li>\n</ul>\n<p>Bookshelf</p>\n<ul>\n<li><p>Staff <a href=\"https://sketchfab.com/models/db1b4a5fe6854f05818259465c9d1b0f#download\" rel=\"nofollow\">https://sketchfab.com/models/db1b4a5fe6854f05818259465c9d1b0f#download</a> </p></li>\n<li><p>Bookcase_Nikki_mor: <a href=\"https://poly.google.com/view/6p0f-u26DDh\" rel=\"nofollow\">https://poly.google.com/view/6p0f-u26DDh</a> </p></li>\n<li><p>Trapjaw: <a href=\"https://poly.google.com/view/fen5tCyl8UK\" rel=\"nofollow\">https://poly.google.com/view/fen5tCyl8UK</a> </p></li>\n<li><p>Spoon poly: <a href=\"https://poly.google.com/view/9_F2bQrTAmM\" rel=\"nofollow\">https://poly.google.com/view/9_F2bQrTAmM</a> </p></li>\n<li><p>Plant poly: <a href=\"https://poly.google.com/view/8KMKYzAqWjp\" rel=\"nofollow\">https://poly.google.com/view/8KMKYzAqWjp</a> </p></li>\n<li><p>Peach poly: <a href=\"https://poly.google.com/view/bvUNGzHch5q\" rel=\"nofollow\">https://poly.google.com/view/bvUNGzHch5q</a> </p></li>\n<li><p>Mushroom Poly: <a href=\"https://poly.google.com/view/2DAaKHD48ZP\" rel=\"nofollow\">https://poly.google.com/view/2DAaKHD48ZP</a> </p></li>\n<li><p>Mortar and pestle: <a href=\"https://poly.google.com/view/4oAnkhb0XdE\" rel=\"nofollow\">https://poly.google.com/view/4oAnkhb0XdE</a> </p></li>\n<li><p>Monster Cactus: <a href=\"https://poly.google.com/view/6Xd-zf5zCGu\" rel=\"nofollow\">https://poly.google.com/view/6Xd-zf5zCGu</a> </p></li>\n<li><p>Key Poly: <a href=\"https://poly.google.com/view/9_pU3cY465w\" rel=\"nofollow\">https://poly.google.com/view/9_pU3cY465w</a> </p></li>\n<li><p>Ginger Root: <a href=\"https://poly.google.com/view/7BRpTI0RVnW\" rel=\"nofollow\">https://poly.google.com/view/7BRpTI0RVnW</a> </p></li>\n<li><p>Coffee Plant: <a href=\"https://poly.google.com/view/dam5c-OPC8w\" rel=\"nofollow\">https://poly.google.com/view/dam5c-OPC8w</a> </p></li>\n<li><p>Chicken Foot: <a href=\"https://poly.google.com/view/3HIcQ6E-yGo\" rel=\"nofollow\">https://poly.google.com/view/3HIcQ6E-yGo</a> </p></li>\n<li><p>Broom: <a href=\"https://poly.google.com/view/bzCi_ZLQPN4\" rel=\"nofollow\">https://poly.google.com/view/bzCi_ZLQPN4</a> </p></li>\n<li><p>Cabbage Poly: <a href=\"https://poly.google.com/view/dNUS4xC0D6C\" rel=\"nofollow\">https://poly.google.com/view/dNUS4xC0D6C</a> </p></li>\n<li><p>Bucket: <a href=\"https://sketchfab.com/models/ce504120be3a4d958b637e82c7bb7562\" rel=\"nofollow\">https://sketchfab.com/models/ce504120be3a4d958b637e82c7bb7562</a></p></li>\n<li><p>Witch Cauldron: <a href=\"https://poly.google.com/view/4bb6qREpt-k\" rel=\"nofollow\">https://poly.google.com/view/4bb6qREpt-k</a> </p></li>\n<li><p>Potions: <a href=\"https://poly.google.com/view/3FqCCzpqpds\" rel=\"nofollow\">https://poly.google.com/view/3FqCCzpqpds</a> </p></li>\n<li><p>Chest: <a href=\"https://sketchfab.com/models/6dc66aebca894dd0ab958d9528b69d91\" rel=\"nofollow\">https://sketchfab.com/models/6dc66aebca894dd0ab958d9528b69d91</a></p></li>\n<li><p>Model Cage: <a href=\"https://poly.google.com/view/0d4ExMmqXUV\" rel=\"nofollow\">https://poly.google.com/view/0d4ExMmqXUV</a> </p></li>\n<li><p>Bird: <a href=\"https://sketchfab.com/models/99ed138bb3d64d67bc9ddc1400b211a9\" rel=\"nofollow\">https://sketchfab.com/models/99ed138bb3d64d67bc9ddc1400b211a9</a></p></li>\n<li><p>Wood: <a href=\"https://poly.google.com/view/dkRLlPSdgdR\" rel=\"nofollow\">https://poly.google.com/view/dkRLlPSdgdR</a> </p></li>\n<li><p>Torch: <a href=\"https://poly.google.com/view/duXLJbjjdtZ\" rel=\"nofollow\">https://poly.google.com/view/duXLJbjjdtZ</a> </p></li>\n<li><p>Rug: <a href=\"https://poly.google.com/u/1/view/8-vPv5_yqKb\" rel=\"nofollow\">https://poly.google.com/u/1/view/8-vPv5_yqKb</a> </p></li>\n<li><p>Fire <a href=\"https://poly.google.com/view/1QpMTUO7P-G\" rel=\"nofollow\">https://poly.google.com/view/1QpMTUO7P-G</a> </p></li>\n<li><p>Ice <a href=\"https://poly.google.com/view/9U2KjmvOJvY\" rel=\"nofollow\">https://poly.google.com/view/9U2KjmvOJvY</a></p></li>\n<li><p>Star <a href=\"https://poly.google.com/view/0ddNZ3EsIhw\" rel=\"nofollow\">https://poly.google.com/view/0ddNZ3EsIhw</a></p></li>\n<li><p>Cloud <a href=\"https://poly.google.com/view/eaQNOl_5iYQ\" rel=\"nofollow\">https://poly.google.com/view/eaQNOl_5iYQ</a> </p></li>\n</ul>\n<p><strong>Visual Effects</strong></p>\n<ul>\n<li>Potion Effects: <a href=\"https://assetstore.unity.com/packages/vfx/particles/fire-explosions/tinyfire-vfx-1-0-97898\" rel=\"nofollow\">https://assetstore.unity.com/packages/vfx/particles/fire-explosions/tinyfire-vfx-1-0-97898</a> </li>\n</ul>\n<p><strong>Sound Effects</strong></p>\n<ul>\n<li>Ambience/Cauldron: <a href=\"https://freesound.org/people/martats/sounds/138018/\" rel=\"nofollow\">https://freesound.org/people/martats/sounds/138018/</a> </li>\n</ul>\n<h2>Challenges We Ran Into</h2>\n<p>Due to issues with connecting to Github, our team was forced to create a new repository which also prevented several of our members from accessing the project. We pivoted to using Unity Collaborate for the majority of the hackathon as it served as a better platform for us to manage multiple iterations of our product. Our Github site shows a screen capture of the multiple commits we made using Unity Collaborate.  </p>\n<h2>What's Next</h2>\n<p>We accomplished our original goal of creating a prototype of a witch-themed VR escape room. There were also several other puzzles that we wanted to implement but were not able to complete in the scope of the hackathon. We originally wanted to add missing potion recipes that needed to be found by using potions created by the player. For example, we planned to add a shrinking and growth potion that would allow the player to walk into a mousehole to find a missing page. In addition, we planned to make a crow in a birdcage prevent the player from reaching a page unless a sleep potion was used. These are all several additions that would add puzzles to the game.</p>\n<p>Our prototype could be expanded into a escape room level-editor platform. Rather than users creating assets from scratch, adding colliders, and developing game mechanics, we can create these components so anyone can more intuitvely build a VR escape room in VR, and test it using their own VR headset.</p>\n</div>",
            "content_md": "\n## Abstract\n\n\nAfter walking home from a regular-old hackathon, you found yourself suddenly kidnapped and locked in a dark, mystical room! With only 10 minutes to spare, you'll need to use magic, potions, and your wits to explore and navigate through this intricate trap. Will you escape the witch's grotto?\n\n\n## What it Does\n\n\nThe chest contains a final key that allows the player to escape and win the game. In order to open the chest, the player must create three potions. A magma, ice, and acid potion must all be poured onto the chest to unlock it.\n\n\nThe escape room puzzle system takes advantage of a modular brewing system in which you can take any object, put it into the cauldron, and then pull a potion out of the cauldron based on the contents you put into it. Each asset has a value for three different attributes, and when mixed together, correspond to different potions. \n\n\nWith these combinations, a player is able to make a potion from a variety of combinations. For example, a player can make a fire potion by throwing a torch into the cauldron because the torch has the hot, rough, and red characteristics. Alternatively, the player could also make a fire potion using a red mushroom which is smooth and red, combined with a candle which is hot.\n\n\nIf the player makes the correct potion, the player will be able to pick up a potion that has either red, blue, or green liquid. If the player makes the incorrect potion, the player will receive a potion with black liquid inside. The green button next to the cauldron will allow the player to empty the cauldron and start over.\n\n\n## Our Team\n\n\n* Alina Christenbury is a computer science student at the University of Deleware. With over 2 years of experience with Unity, she leads an undergrad research project for VR/AR technology.\n* Dan Donato <https://dvdonato.com/> is a computer science student with over 6 years of Unity experience. He's previously worked in quality assurance in the video game industry, and worked with multiple programming languages.\n* Michael Zhang is a business student at the University of Michigan. He leads a VR/AR student organization called ARI and also helps produce a weekly VR talkshow called TheHiveVR.\n* Mark Ma is finishing his masters in Design and Development of Digital Games at Columbia University. He's previously worked with Unity, JavaScript, and Java.\n* Chase Laux recently graduated from California State University Fullerton, and specializes in effects and modeling. He also leads the Orange County Houdini User Group.\n\n\n## How We Built It\n\n\n**Location:** Vive Lounge, 3rd Floor, E15-335 \n**Theme:** Games and Learning\n**Category:** Videogame\n\n\nWe built our HTC Vive experience using the Unity Game Engine and C#. We built the witch's house in Google Poly, and imported several Creative Commons License assets, sounds, and visual effects. These came from the Unity Store, Google Poly, Free Sound, and Sketchfab. The list of assets used are below:\n\n\n**Assets:**\n\n\n* Stool Poly by Google: <https://poly.google.com/view/cLydFlVg-wI>\n* Wizard Hat Poly: <https://poly.google.com/view/7VVumyY7L_u>\n* Small stack of papers: <https://poly.google.com/view/aiBozYlPe-->\n* Pumpkin Flash Sprinkles: <https://poly.google.com/view/20c9qxsjcdQ>\n\n\nTable 1\n\n\n* Skull: <https://sketchfab.com/models/bc302c319cff4dd5a67e2a7d92a4fb66>\n* Candle: <https://poly.google.com/view/8apROCXjeN4>\n* Full Rejuv Potion: <https://sketchfab.com/models/c29531095aa24d7399fcc777e821e90b>\n* Alembic: <https://sketchfab.com/models/811618ea79ba4aac95d7f7312a2128ec>\n* Low Poly Book: <https://poly.google.com/view/0-_UGETLPsX>\n* Atom Crystal: <https://sketchfab.com/models/81193f25ee064a899727581f34793486>\n* New Table (1): <https://sketchfab.com/models/ce6977c9de1d43048838e4802e21f7d4>\n* Lab Equipment: <https://poly.google.com/view/fSWYfMERNak>\n\n\nCrystal Ball Clock\n\n\n* Crystal Ball (1): <https://poly.google.com/view/6X_VejNZ7Ok>\n* Stem\\_Base\\_VR: <https://poly.google.com/view/5cpYDJ4TZAP>\n\n\nTable 2\n\n\n* frog: <https://sketchfab.com/models/6e12855ea9d84c5a9087dc02807352f0#download>\n* healer: <https://sketchfab.com/models/63b311b87a06425aa0b9a790e828e55c>\n* Cube crystal: <https://sketchfab.com/models/20addc5a445840e3a6dc79c2d929785c>\n* Simple wooden tankard: <https://poly.google.com/view/b3qGuE7F56q>\n* Knife Poly: <https://poly.google.com/view/0X5xcxjszwI>\n\n\nBookshelf\n\n\n* Staff <https://sketchfab.com/models/db1b4a5fe6854f05818259465c9d1b0f#download>\n* Bookcase\\_Nikki\\_mor: <https://poly.google.com/view/6p0f-u26DDh>\n* Trapjaw: <https://poly.google.com/view/fen5tCyl8UK>\n* Spoon poly: <https://poly.google.com/view/9_F2bQrTAmM>\n* Plant poly: <https://poly.google.com/view/8KMKYzAqWjp>\n* Peach poly: <https://poly.google.com/view/bvUNGzHch5q>\n* Mushroom Poly: <https://poly.google.com/view/2DAaKHD48ZP>\n* Mortar and pestle: <https://poly.google.com/view/4oAnkhb0XdE>\n* Monster Cactus: <https://poly.google.com/view/6Xd-zf5zCGu>\n* Key Poly: <https://poly.google.com/view/9_pU3cY465w>\n* Ginger Root: <https://poly.google.com/view/7BRpTI0RVnW>\n* Coffee Plant: <https://poly.google.com/view/dam5c-OPC8w>\n* Chicken Foot: <https://poly.google.com/view/3HIcQ6E-yGo>\n* Broom: <https://poly.google.com/view/bzCi_ZLQPN4>\n* Cabbage Poly: <https://poly.google.com/view/dNUS4xC0D6C>\n* Bucket: <https://sketchfab.com/models/ce504120be3a4d958b637e82c7bb7562>\n* Witch Cauldron: <https://poly.google.com/view/4bb6qREpt-k>\n* Potions: <https://poly.google.com/view/3FqCCzpqpds>\n* Chest: <https://sketchfab.com/models/6dc66aebca894dd0ab958d9528b69d91>\n* Model Cage: <https://poly.google.com/view/0d4ExMmqXUV>\n* Bird: <https://sketchfab.com/models/99ed138bb3d64d67bc9ddc1400b211a9>\n* Wood: <https://poly.google.com/view/dkRLlPSdgdR>\n* Torch: <https://poly.google.com/view/duXLJbjjdtZ>\n* Rug: <https://poly.google.com/u/1/view/8-vPv5_yqKb>\n* Fire <https://poly.google.com/view/1QpMTUO7P-G>\n* Ice <https://poly.google.com/view/9U2KjmvOJvY>\n* Star <https://poly.google.com/view/0ddNZ3EsIhw>\n* Cloud <https://poly.google.com/view/eaQNOl_5iYQ>\n\n\n**Visual Effects**\n\n\n* Potion Effects: <https://assetstore.unity.com/packages/vfx/particles/fire-explosions/tinyfire-vfx-1-0-97898>\n\n\n**Sound Effects**\n\n\n* Ambience/Cauldron: <https://freesound.org/people/martats/sounds/138018/>\n\n\n## Challenges We Ran Into\n\n\nDue to issues with connecting to Github, our team was forced to create a new repository which also prevented several of our members from accessing the project. We pivoted to using Unity Collaborate for the majority of the hackathon as it served as a better platform for us to manage multiple iterations of our product. Our Github site shows a screen capture of the multiple commits we made using Unity Collaborate. \n\n\n## What's Next\n\n\nWe accomplished our original goal of creating a prototype of a witch-themed VR escape room. There were also several other puzzles that we wanted to implement but were not able to complete in the scope of the hackathon. We originally wanted to add missing potion recipes that needed to be found by using potions created by the player. For example, we planned to add a shrinking and growth potion that would allow the player to walk into a mousehole to find a missing page. In addition, we planned to make a crow in a birdcage prevent the player from reaching a page unless a sleep potion was used. These are all several additions that would add puzzles to the game.\n\n\nOur prototype could be expanded into a escape room level-editor platform. Rather than users creating assets from scratch, adding colliders, and developing game mechanics, we can create these components so anyone can more intuitvely build a VR escape room in VR, and test it using their own VR headset.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/playground",
            "title": "playground",
            "blurb": "A space to try new games, learn new skills, and connect with others",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/1iUn02kz0vg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Ata Dogan",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/831/347/datas/profile.jpg"
                },
                {
                    "name": "Samip Jain",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/cb3af4a0f249781e425be5428ed1f1d4?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Savanna",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/17018224?height=180&v=4&width=180"
                },
                {
                    "name": "shrma21294",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/26113003?height=180&v=4&width=180"
                },
                {
                    "name": "Yolanda Lam",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/a3afa0935d6486278239bbfbc5fe395a?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                }
            ],
            "built_with": [
                "mixed-reality",
                "steamvr",
                "unity"
            ],
            "content_html": "<div>\n<h2>Playground</h2>\n<p>Playground is a space to try new skills and hobbies, and test out their interests in the real world while having fun. In the environment, users can also try out activities that they may not be able to on a regular basis. Playground's goal is to connect users to those in the space who have the same interest and to the places in the real world that can help them learn these fun skills. It's easy to use and a great intro to VR for anyone new!</p>\n<h2>Our mission</h2>\n<p>We are passionate about building environments that can bridge the virtual world with the physical real world by encouraging positive social interactions. Our mission is to help young adults build healthier behaviors, learning habits, and have fun.</p>\n<h2>Inspiration</h2>\n<p>With every emerging technology, we continue to adapt ourselves and our relationships to the digital age. So, how do we use existing technologies to promote a healthier lifestyle and encourage more of that human element that we all need?</p>\n<p>From our research, we found that 18% of the American population in that age range are dealing with depression and many are consulting medical professionals for medication. However, studies have shown that behavioral and activity-based changes can significantly improve levels of dopamine in the brain and are more sustainable methods of improving not only depression but also overall mental wellness.</p>\n<h2>Who we are designing for</h2>\n<p>Our target audience includes young adults, specifically from the ages of 18-25 years old, who experience many new lifestyle changes (e.g. graduating from a safe school community, living on their own, getting married etc.). These new adjustments may take a toll on many of their needs, including their social needs.</p>\n<p>Read Jordan\u2019s story with Playground \u2014\nJordan is 21 years old and a recent graduate. She moved to Boston for her first job and is living on her own for the first time. It\u2019s busy season in the accounting firm she works for and recently, she\u2019s feeling emotional and physically exhausted, losing motivation, and finding it difficult to keep her negative thoughts away. She may not know it yet, but Jordan is displaying common symptoms of mild depression. In the mood for a short and low-commitment game, Jordan opens Playground and starts playing around with axe throwing. She's never tried it and was surprised by how much she enjoyed it. Through Playground, Jordan selects the closest axe throwing bar to her and sends herself an email to get started. Now, Jordan is an axe-throwing enthusiast.</p>\n<h2>How we built it &amp; the technical things</h2>\n<p>Github - <a href=\"https://github.com/RealityVirtually2019/Playground\" rel=\"nofollow\">https://github.com/RealityVirtually2019/Playground</a>\nSDKs - OpenVR\nAPIs - SteamVR\nLibrary - TBD\nList of sourced assets used in our project can be found here - <a href=\"https://goo.gl/U34sUo\" rel=\"nofollow\">https://goo.gl/U34sUo</a></p>\n</div>",
            "content_md": "\n## Playground\n\n\nPlayground is a space to try new skills and hobbies, and test out their interests in the real world while having fun. In the environment, users can also try out activities that they may not be able to on a regular basis. Playground's goal is to connect users to those in the space who have the same interest and to the places in the real world that can help them learn these fun skills. It's easy to use and a great intro to VR for anyone new!\n\n\n## Our mission\n\n\nWe are passionate about building environments that can bridge the virtual world with the physical real world by encouraging positive social interactions. Our mission is to help young adults build healthier behaviors, learning habits, and have fun.\n\n\n## Inspiration\n\n\nWith every emerging technology, we continue to adapt ourselves and our relationships to the digital age. So, how do we use existing technologies to promote a healthier lifestyle and encourage more of that human element that we all need?\n\n\nFrom our research, we found that 18% of the American population in that age range are dealing with depression and many are consulting medical professionals for medication. However, studies have shown that behavioral and activity-based changes can significantly improve levels of dopamine in the brain and are more sustainable methods of improving not only depression but also overall mental wellness.\n\n\n## Who we are designing for\n\n\nOur target audience includes young adults, specifically from the ages of 18-25 years old, who experience many new lifestyle changes (e.g. graduating from a safe school community, living on their own, getting married etc.). These new adjustments may take a toll on many of their needs, including their social needs.\n\n\nRead Jordan\u2019s story with Playground \u2014\nJordan is 21 years old and a recent graduate. She moved to Boston for her first job and is living on her own for the first time. It\u2019s busy season in the accounting firm she works for and recently, she\u2019s feeling emotional and physically exhausted, losing motivation, and finding it difficult to keep her negative thoughts away. She may not know it yet, but Jordan is displaying common symptoms of mild depression. In the mood for a short and low-commitment game, Jordan opens Playground and starts playing around with axe throwing. She's never tried it and was surprised by how much she enjoyed it. Through Playground, Jordan selects the closest axe throwing bar to her and sends herself an email to get started. Now, Jordan is an axe-throwing enthusiast.\n\n\n## How we built it & the technical things\n\n\nGithub - <https://github.com/RealityVirtually2019/Playground>\nSDKs - OpenVR\nAPIs - SteamVR\nLibrary - TBD\nList of sourced assets used in our project can be found here - <https://goo.gl/U34sUo>\n\n\n"
        },
        {
            "source": "https://devpost.com/software/holo-learn",
            "title": "HoloLeARn - IVO",
            "blurb": "Interactive, self-learning Hololens app. Teach an avatar your customized content to increase retention & have fun",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/2QUVgjR78A4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [
                {
                    "title": "Hololearn - IVO",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/646/datas/original.jpg"
                },
                {
                    "title": "Hololearn on Hololens",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/649/datas/original.jpg"
                },
                {
                    "title": "Welcome screen",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/648/datas/original.jpg"
                },
                {
                    "title": "Flash cards",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/647/datas/original.jpg"
                },
                {
                    "title": "Teach Ivo!",
                    "src": "https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/746/645/datas/original.jpg"
                }
            ],
            "team": [
                {
                    "name": "Brodey Lajoie",
                    "about": "The lead developer on this project, I built HoloLeARn in Unity with Joe. I handled the systems and mechanics of the application, the avatar-user interaction, and the Hololens capability integration into the application.",
                    "photo": "https://avatars2.githubusercontent.com/u/32546217?height=180&v=4&width=180"
                },
                {
                    "name": "Eileen Hoff",
                    "about": "Worked with the team on user journey and story boards.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/964/datas/profile.jpg"
                },
                {
                    "name": "Joe Chung",
                    "about": "I worked on the character  animation controller, scripts for audio and animations. I also helped with developing pedagogical features and logo + graphic design.",
                    "photo": "https://graph.facebook.com/912294205218/picture?height=180&width=180"
                },
                {
                    "name": "Madeleine M",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQHZ4rAvZ4XDcA/profile-displayphoto-shrink_100_100/0?e=1553126400&height=180&t=qC8W6z-iczqL6XmCUA8FWelhZbrUig6s8P564qIPbY0&v=beta&width=180"
                },
                {
                    "name": "Masaya Sasaki",
                    "about": "",
                    "photo": "https://graph.facebook.com/1328302520597572/picture?height=180&width=180"
                }
            ],
            "built_with": [],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Education has enormous transformative power, and the potential to change the world. However, the potential is unrealized and we strongly believe this can be changed with AR.</p>\n<p>Studying alone is boring, especially when you only read or simply listen. It's also ineffective, as content is only retained at a 10% rate with this method. This leads to frustrated students and teachers. What a waste of time! </p>\n<p>Studying is fun and engaging when you can explain the material to someone else. It's also the most effective way of retaining information, at a rate of 90%. Win-Win! </p>\n<p>IVO - The name of the avatar \u201cIVO\u201d comes from the Japanese word with the same pronunciation \u201cAibou\u201d which means pal, partner, or buddy.</p>\n<h2>What it does</h2>\n<p>Customized Content: </p>\n<p>Users insert customized content of what they are learning at school, such as; definitions, concepts, and key terms. The content is from the users traditional educational materials, such as; textbooks, study guides, or flashcards. The content is inputted through speech recognition or via the web. </p>\n<p>Avatar:</p>\n<p>The user is greeted by an avatar upon setting up their Hololens AR headset. The avatar has a positive, asset-based minded personality, who encourages the growth mindset and lifelong learning of the user. The avatar is always curious about the topic that the user is learning, and really wants to learn more. </p>\n<p>Stage 1 Learn:: </p>\n<p>In stage 1, the user has a time period to review flashcards of the content. The time period is dependent on the amount of content that has been inputted. Next to the flashcards, there are retrieval image cues that are related to keywords within the content. </p>\n<p>Stage 2 Explain:</p>\n<p>In stage 2, the avatar is ready and excited to learn from you.The avatar asks you to explain the topic of each flashcard, with various conversational prompts. When the avatar prompts the user, the retrieval image cue is shown.  The conversation is structured within a time period based on the amount of content. When the user is answering the prompt, there is a visual in the form of a brain, that is progressively colored in while the user is answering the prompt correctly. </p>\n<p>The avatar reacts with body gestures to the user responses based on whether the response is correct or incorrect. </p>\n<p>Once all of the prompts have been answered, the avatar either congratulates the user or directs the user to review the flashcards again in stage 1. </p>\n<h2>How we built it</h2>\n<p>This application was built in Unity using the UWP platform. We used Microsoft's Mixed Reality Toolkit in order to build the groundwork for HoloLeARn.</p>\n<p>The animation was built using a character and animations from mixamo. The voice of the avatar was recorded by one of the team members. </p>\n<h2>Challenges we ran into</h2>\n<p>We started the project with Magic Leap, and ran into issues with the speech recognition capabilities. After switching platforms to the Hololens, we had to tackle the issue of keyword and phrase recognition. We also ran into issues with the Unity build process and application lag.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We are proud of identifying a huge gap in the education space and leveraging AR to develop to open retrieval practice, the most effective method of enhancing memory, to everyone. The avatar that we have built, embodies characteristics that encourage positive-minded and lifelong learners, through the connection built during the game. </p>\n<p>Creating a virtual avatar that will interact and react with the user in Mixed Reality is a huge accomplishment as well. Getting the avatar to respond and actually listen to and recognize our voices was a big step for us.</p>\n<h2>What we learned</h2>\n<p>Majoring learning came from starting with the Magic Leap. We learned what the platform is capable of and isn't capable of. We also learned more about the Hololens platform and how spatial perception comes into play. </p>\n<h2>What's next for HoloLeARn - IVO</h2>\n<p>Adding additional avatars to have a wide variety of representation for users to identify with. </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nEducation has enormous transformative power, and the potential to change the world. However, the potential is unrealized and we strongly believe this can be changed with AR.\n\n\nStudying alone is boring, especially when you only read or simply listen. It's also ineffective, as content is only retained at a 10% rate with this method. This leads to frustrated students and teachers. What a waste of time! \n\n\nStudying is fun and engaging when you can explain the material to someone else. It's also the most effective way of retaining information, at a rate of 90%. Win-Win! \n\n\nIVO - The name of the avatar \u201cIVO\u201d comes from the Japanese word with the same pronunciation \u201cAibou\u201d which means pal, partner, or buddy.\n\n\n## What it does\n\n\nCustomized Content: \n\n\nUsers insert customized content of what they are learning at school, such as; definitions, concepts, and key terms. The content is from the users traditional educational materials, such as; textbooks, study guides, or flashcards. The content is inputted through speech recognition or via the web. \n\n\nAvatar:\n\n\nThe user is greeted by an avatar upon setting up their Hololens AR headset. The avatar has a positive, asset-based minded personality, who encourages the growth mindset and lifelong learning of the user. The avatar is always curious about the topic that the user is learning, and really wants to learn more. \n\n\nStage 1 Learn:: \n\n\nIn stage 1, the user has a time period to review flashcards of the content. The time period is dependent on the amount of content that has been inputted. Next to the flashcards, there are retrieval image cues that are related to keywords within the content. \n\n\nStage 2 Explain:\n\n\nIn stage 2, the avatar is ready and excited to learn from you.The avatar asks you to explain the topic of each flashcard, with various conversational prompts. When the avatar prompts the user, the retrieval image cue is shown. The conversation is structured within a time period based on the amount of content. When the user is answering the prompt, there is a visual in the form of a brain, that is progressively colored in while the user is answering the prompt correctly. \n\n\nThe avatar reacts with body gestures to the user responses based on whether the response is correct or incorrect. \n\n\nOnce all of the prompts have been answered, the avatar either congratulates the user or directs the user to review the flashcards again in stage 1. \n\n\n## How we built it\n\n\nThis application was built in Unity using the UWP platform. We used Microsoft's Mixed Reality Toolkit in order to build the groundwork for HoloLeARn.\n\n\nThe animation was built using a character and animations from mixamo. The voice of the avatar was recorded by one of the team members. \n\n\n## Challenges we ran into\n\n\nWe started the project with Magic Leap, and ran into issues with the speech recognition capabilities. After switching platforms to the Hololens, we had to tackle the issue of keyword and phrase recognition. We also ran into issues with the Unity build process and application lag.\n\n\n## Accomplishments that we're proud of\n\n\nWe are proud of identifying a huge gap in the education space and leveraging AR to develop to open retrieval practice, the most effective method of enhancing memory, to everyone. The avatar that we have built, embodies characteristics that encourage positive-minded and lifelong learners, through the connection built during the game. \n\n\nCreating a virtual avatar that will interact and react with the user in Mixed Reality is a huge accomplishment as well. Getting the avatar to respond and actually listen to and recognize our voices was a big step for us.\n\n\n## What we learned\n\n\nMajoring learning came from starting with the Magic Leap. We learned what the platform is capable of and isn't capable of. We also learned more about the Hololens platform and how spatial perception comes into play. \n\n\n## What's next for HoloLeARn - IVO\n\n\nAdding additional avatars to have a wide variety of representation for users to identify with. \n\n\n"
        },
        {
            "source": "https://devpost.com/software/smilehelper",
            "title": "Smilehelper",
            "blurb": "Creates enhanced social experiences for Autistic people by translating facial emotions into animated imagery & sounds",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/ZvKdD6O2Qw8?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "itspavana",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/46673408?height=180&v=4&width=180"
                },
                {
                    "name": "jsmentch",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/20958589?height=180&v=4&width=180"
                },
                {
                    "name": "Erin Woo",
                    "about": "",
                    "photo": "https://graph.facebook.com/1415752685144105/picture?height=180&width=180"
                },
                {
                    "name": "rightfromleft",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/10914061?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "adobe-illustrator",
                "azure",
                "azure-face-api",
                "fl-studio",
                "maya",
                "microsoft-hololens",
                "mr-emulator",
                "unity"
            ],
            "content_html": "<div>\n<p>Location: MIT Media Lab, 6th Floor, Atrium, Table right across Wayfair booth</p>\n<h2>Inspiration</h2>\n<p>It is challenging for those with autism to connect with those around them and understand societal emotions and expressions. Hence they do not react accordingly. This results in diminished social experience and sometimes social alienation. Our app solves this problem and increases sense of belonging for those with autism. We are mainly targeting Austistic children. </p>\n<h2>What it does</h2>\n<p>Smilehelper creates enhanced social experiences for those with autism by translating the facial emotions into optimal sensory stimuli. We use familiar imagery and familiar sounds that are associated with the corresponding emotion. The app user can grasp, feel and react to the emotions faster and better </p>\n<h2>How I built it</h2>\n<p>We have a diverse team of developers, 3D modeler, Austism researcher and product manager.\nWe have a diverse team of developers, 3D modeler, Austism researcher and product manager. We used Unity, Azure Face API, hololens and Maya to build the application</p>\n<h1>Challenges I ran into</h1>\n<p>Initial set up of unity with hololens was extremely time consuming</p>\n<h2>Accomplishments that I'm proud of</h2>\n<p>Getting Face API to work, 3D modeling of the squirrel</p>\n<h2>What I learned</h2>\n<p>We learned how to use github and unity. We learnt Azure Face API and hololens set up and operation. We are all first timers for hackathon. We learned how to work together with complete strangers under pressure.</p>\n<h2>What's next for Smilehelper</h2>\n<p>We are aiming for customized translations thru machine learning for amplified emotion translations and imagery representing child's familiar and favorite objects (Toys, legos). Our app can also be used for enhancing collaboration with Autistic people</p>\n</div>",
            "content_md": "\nLocation: MIT Media Lab, 6th Floor, Atrium, Table right across Wayfair booth\n\n\n## Inspiration\n\n\nIt is challenging for those with autism to connect with those around them and understand societal emotions and expressions. Hence they do not react accordingly. This results in diminished social experience and sometimes social alienation. Our app solves this problem and increases sense of belonging for those with autism. We are mainly targeting Austistic children. \n\n\n## What it does\n\n\nSmilehelper creates enhanced social experiences for those with autism by translating the facial emotions into optimal sensory stimuli. We use familiar imagery and familiar sounds that are associated with the corresponding emotion. The app user can grasp, feel and react to the emotions faster and better \n\n\n## How I built it\n\n\nWe have a diverse team of developers, 3D modeler, Austism researcher and product manager.\nWe have a diverse team of developers, 3D modeler, Austism researcher and product manager. We used Unity, Azure Face API, hololens and Maya to build the application\n\n\n# Challenges I ran into\n\n\nInitial set up of unity with hololens was extremely time consuming\n\n\n## Accomplishments that I'm proud of\n\n\nGetting Face API to work, 3D modeling of the squirrel\n\n\n## What I learned\n\n\nWe learned how to use github and unity. We learnt Azure Face API and hololens set up and operation. We are all first timers for hackathon. We learned how to work together with complete strangers under pressure.\n\n\n## What's next for Smilehelper\n\n\nWe are aiming for customized translations thru machine learning for amplified emotion translations and imagery representing child's familiar and favorite objects (Toys, legos). Our app can also be used for enhancing collaboration with Autistic people\n\n\n"
        },
        {
            "source": "https://devpost.com/software/veggieslam",
            "title": "VeggieSlam",
            "blurb": "The kitchen is your arena. It's time to prove your mastery over vegetables.",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Samantha Garcia",
                    "about": "Worked on Ball Physics, Implementing Multiplayer using Photon's PUN Unity Asset, Integrated VR for Daydream and Oculus Go",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/537/870/datas/profile.jpg"
                },
                {
                    "name": "Abel Paguio",
                    "about": "",
                    "photo": "https://graph.facebook.com/1683672631651822/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "android",
                "c#",
                "oculus",
                "oculus-go",
                "photon",
                "poly",
                "unity"
            ],
            "content_html": "<div>\n<h2>What it does</h2>\n</div>",
            "content_md": "\n## What it does\n\n\n"
        },
        {
            "source": "https://devpost.com/software/rocket-powar",
            "title": "Rocket PowAR",
            "blurb": "Terraforming Mars using a Brain Compter Interaface and Magic physiotherapist",
            "awards": [],
            "videos": [
                "https://player.vimeo.com/video/312439324?byline=0&portrait=0&title=0#t="
            ],
            "images": [],
            "team": [
                {
                    "name": "Brian Jordan",
                    "about": "Coming from a Unity & web development background, I partnered up with Sophia on the Node.js/Express OpenBCI/Web <> Unity connector and with Rogue on the development of a Unity-based Magic Leap app.",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/746/613/datas/profile.jpeg"
                },
                {
                    "name": "Rogue Fong",
                    "about": "Narrative, UX, Sound Design/SFX, Unity Programming/Game Logic",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/455/740/datas/profile.JPG"
                },
                {
                    "name": "Brandon Powers",
                    "about": "I created the narrative design and focused on user experience to users would want to come back and continue their therapy. ",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/696/951/datas/profile.JPG"
                },
                {
                    "name": "Sophia Batchelor",
                    "about": "",
                    "photo": "https://graph.facebook.com/10212406625628338/picture?height=180&width=180"
                },
                {
                    "name": "Alexandria Heston",
                    "about": "",
                    "photo": "https://media.licdn.com/mpr/mprx/0_PotKeB6dBpRPEkPH9aXY85kHtgxmdq3H0UeKS_oHNgSfdkhZva6ruTFHKwxPEzlMMUeKLlkeUUp7fKN5xMBu8_5kOUpafKkNMMBPwCKWzsdCLQxHzH3jI9YZYa6OQK15rV1y2rx7ZGo?height=180&width=180"
                },
                {
                    "name": "snowtski",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/45925748?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "c#",
                "magic-leap",
                "node.js",
                "openbci",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Physical therapy is difficult, painful, and often boring in addition to the fact that it\u2019s hard to know if you're actually doing the exercises correctly.</p>\n<p><strong>Enter RocketPowAR:</strong> an AR app that picks up the signals from your muscles and uses their power to fly a rocket.</p>\n<p>Our interface guides you through where to place the sensors on your body in order to fly the rocket. The rocket's power comes from the using the electrical activity of your muscles.</p>\n<p>We were also inspired by brain computer interfaces and the potential they offer for bringing greater accessibility to the virtual world. </p>\n<h2>What it does</h2>\n<p>The BCI device picks up electrical activity from your muscles (EMG) and sends the information to Unity which uses it as a marker to launch rockets. </p>\n<p>The EMG markers are chosen based on physical therapy exercises to check if you are recruiting the correct muscle groups for the exercise (and to confirm that you are not using muscles you shouldn\u2019t be). </p>\n<h2>Use cases</h2>\n<p>The initial application for RocketPowAR centers around muscle rehabilitation and physical therapy.\nRecruiting and using the correct muscle group(s) during an exercise is a crucial part of physical therapy to strengthen weak areas and support injured structures. </p>\n<p>Current physical therapy devices such as TENS or e-Stim use electrical signals either to bypass a self directed recruitment (you choosing to turn a muscle \u201con\u201d) by sending an\u00a0electrical\u00a0current through electrodes and into the muscle to stimulate nerve pathways. This is incredibly unhelpful as users can\u2019t develop an understanding of how to recruit the muscle themselves, so the moment they stop using the TENS/e-Stim device (TENS/e-Stim sessions last 10-20 minutes maybe 1/weekly), they cannot do the rehab. </p>\n<p>These devices are cumbersome to set up, difficult to use, and the at-home kits do not support at-home users. We wanted to fill in the gaps to help. So many people live with pain that they brush off as \"a part of getting old\". It does not have to be that way.</p>\n<h2>How we built it</h2>\n<p>We used electrodes to get a muscle signal to the OpenBCI Ganglion board. This board acted as a signal amplifier and used a Simplee BLE Radio module. The signals were picked up over bluetooth by using a node.js library and creating a local HTP server which the Magic Leap One client receives.  We processed the appropriate live signal and chose a threshold for when a muscle would register as passively active, to being \u201crecruited\u201d for the exercise within the node scripts and any signals higher than threshold were used as a marker for \u201cgo\u201d by Unity to instantiate the rocket thrusters. Once received by Unity, the animation Magic happens and our AR rocket is launched.</p>\n<p><strong>To sum up:</strong> \nMuscle electrical activity is sent to the Ganglion which amplifies the signal, the signal is then sent and parsed, it\u2019s then sent again which extracts out if the muscle activation meets a set threshold, at that point it\u2019s then sent to Unity which accepts it as a \u201cgo\u201d signal to fire up the rocket\u2019s engines, which sends it to the Magic Leap and the rocket is poweARd. </p>\n<h2>Challenges we ran into</h2>\n<p><em>Oh the bluetooth issues you'll have</em> </p>\n<ol>\n<li><p>We needed to use 5 pieces of software to get from the chip to Unity (and then Magic Leap). Of those - on Windows bluetooth wouldn\u2019t connect, the OpenBCI software wouldn\u2019t open (and when we found a work around it wouldn\u2019t pick up the Ganglion). The signal processes software only runs on Windows. Running a virtual machine on the Mac broke the Mac (oops). We then tried using docker to run windows on Mac. Same issues as above. Then we installed Unbuntu onto a external disk\u2026but if the laptop was ever turned off or shutdown then we would lose everything. Eventually we thought to use web sockets except that the documentation on that was very very very misinformed. </p></li>\n<li><p>So now the Ganglion is connected. But what is the data coming in. We had \u201crandom\u201d (what we thought was voltage) coming in from channels that weren\u2019t being used in the form of a 4 size array. And that data was not related in any way to muscle activity. So after both hardware and software debugging, we found a working channel, we found the actual voltage information, and extracted the signal which was the muscle contraction. </p></li>\n<li><p>Did we mention that we had no programs to interpret or push the signal? Thank you mentors for a team crash (literally) course in terminal </p></li>\n<li><p>Sending data end to end was a massive massive challenge </p></li>\n<li><p>Bluetooth would dropped every 4 seconds </p></li>\n</ol>\n<p><strong>Quote from a mentor:</strong> \u201clearning how to use OpenBCI is it\u2019s own masters degree and you need a PhD just to understand the documentation\u201d (all of which was out of date). </p>\n<h2>Accomplishments</h2>\n<p>None of us had any experience with OpenBCI or EMG signals before, only one of us had ever worked with Magic Leap, and we had just enough to keep us going. Our \u201cwins\u201d were: </p>\n<p><em>Picking up heart rate signal (ECG) using Ganglion in ECG specific software built for OpenBCI</em></p>\n<p><em>\u201cBluetooth pairing successful\u201d</em></p>\n<p><em>Pick up a signal and have it display in terminal</em> </p>\n<p><em>Stream</em>\n<em>\u2014&gt; live</em></p>\n<p><em>Getting a single bit sent end to end (Ganglion to Magic Leap)</em></p>\n<p><em>Finding a a channel that sends signals</em> </p>\n<p><em>Parsing the signal into something that could be understood</em> </p>\n<h2>What we learned</h2>\n<p>How to use OpenBCI (and why correct documentation is important)\nHow to process and use biometric data \nHow to send biometric data using websockets \nDeveloping for the Magic Leap \nUsing terminal \nEverything related to bluetooth \nNetworking </p>\n<h2>What\u2019s next for RocketPowAR</h2>\n<p>Adding in additional physical therapy exercises for each injury (fly the rocket through the levels of the stratosphere, avoid meteors), and then adding in different \"missions\" based on different exercises (e.g. strengthening up VMO after a ACL or MCL tear).\nWe want to find a better way to work with the EMG signals, and can spend more time looking into signal processing software. We also want to spend more time on the send along of the data instead of having to use sockets.</p>\n<p>Discovered </p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nPhysical therapy is difficult, painful, and often boring in addition to the fact that it\u2019s hard to know if you're actually doing the exercises correctly.\n\n\n**Enter RocketPowAR:** an AR app that picks up the signals from your muscles and uses their power to fly a rocket.\n\n\nOur interface guides you through where to place the sensors on your body in order to fly the rocket. The rocket's power comes from the using the electrical activity of your muscles.\n\n\nWe were also inspired by brain computer interfaces and the potential they offer for bringing greater accessibility to the virtual world. \n\n\n## What it does\n\n\nThe BCI device picks up electrical activity from your muscles (EMG) and sends the information to Unity which uses it as a marker to launch rockets. \n\n\nThe EMG markers are chosen based on physical therapy exercises to check if you are recruiting the correct muscle groups for the exercise (and to confirm that you are not using muscles you shouldn\u2019t be). \n\n\n## Use cases\n\n\nThe initial application for RocketPowAR centers around muscle rehabilitation and physical therapy.\nRecruiting and using the correct muscle group(s) during an exercise is a crucial part of physical therapy to strengthen weak areas and support injured structures. \n\n\nCurrent physical therapy devices such as TENS or e-Stim use electrical signals either to bypass a self directed recruitment (you choosing to turn a muscle \u201con\u201d) by sending an\u00a0electrical\u00a0current through electrodes and into the muscle to stimulate nerve pathways. This is incredibly unhelpful as users can\u2019t develop an understanding of how to recruit the muscle themselves, so the moment they stop using the TENS/e-Stim device (TENS/e-Stim sessions last 10-20 minutes maybe 1/weekly), they cannot do the rehab. \n\n\nThese devices are cumbersome to set up, difficult to use, and the at-home kits do not support at-home users. We wanted to fill in the gaps to help. So many people live with pain that they brush off as \"a part of getting old\". It does not have to be that way.\n\n\n## How we built it\n\n\nWe used electrodes to get a muscle signal to the OpenBCI Ganglion board. This board acted as a signal amplifier and used a Simplee BLE Radio module. The signals were picked up over bluetooth by using a node.js library and creating a local HTP server which the Magic Leap One client receives. We processed the appropriate live signal and chose a threshold for when a muscle would register as passively active, to being \u201crecruited\u201d for the exercise within the node scripts and any signals higher than threshold were used as a marker for \u201cgo\u201d by Unity to instantiate the rocket thrusters. Once received by Unity, the animation Magic happens and our AR rocket is launched.\n\n\n**To sum up:** \nMuscle electrical activity is sent to the Ganglion which amplifies the signal, the signal is then sent and parsed, it\u2019s then sent again which extracts out if the muscle activation meets a set threshold, at that point it\u2019s then sent to Unity which accepts it as a \u201cgo\u201d signal to fire up the rocket\u2019s engines, which sends it to the Magic Leap and the rocket is poweARd. \n\n\n## Challenges we ran into\n\n\n*Oh the bluetooth issues you'll have* \n\n\n1. We needed to use 5 pieces of software to get from the chip to Unity (and then Magic Leap). Of those - on Windows bluetooth wouldn\u2019t connect, the OpenBCI software wouldn\u2019t open (and when we found a work around it wouldn\u2019t pick up the Ganglion). The signal processes software only runs on Windows. Running a virtual machine on the Mac broke the Mac (oops). We then tried using docker to run windows on Mac. Same issues as above. Then we installed Unbuntu onto a external disk\u2026but if the laptop was ever turned off or shutdown then we would lose everything. Eventually we thought to use web sockets except that the documentation on that was very very very misinformed.\n2. So now the Ganglion is connected. But what is the data coming in. We had \u201crandom\u201d (what we thought was voltage) coming in from channels that weren\u2019t being used in the form of a 4 size array. And that data was not related in any way to muscle activity. So after both hardware and software debugging, we found a working channel, we found the actual voltage information, and extracted the signal which was the muscle contraction.\n3. Did we mention that we had no programs to interpret or push the signal? Thank you mentors for a team crash (literally) course in terminal\n4. Sending data end to end was a massive massive challenge\n5. Bluetooth would dropped every 4 seconds\n\n\n**Quote from a mentor:** \u201clearning how to use OpenBCI is it\u2019s own masters degree and you need a PhD just to understand the documentation\u201d (all of which was out of date). \n\n\n## Accomplishments\n\n\nNone of us had any experience with OpenBCI or EMG signals before, only one of us had ever worked with Magic Leap, and we had just enough to keep us going. Our \u201cwins\u201d were: \n\n\n*Picking up heart rate signal (ECG) using Ganglion in ECG specific software built for OpenBCI*\n\n\n*\u201cBluetooth pairing successful\u201d*\n\n\n*Pick up a signal and have it display in terminal* \n\n\n*Stream*\n*\u2014> live*\n\n\n*Getting a single bit sent end to end (Ganglion to Magic Leap)*\n\n\n*Finding a a channel that sends signals* \n\n\n*Parsing the signal into something that could be understood* \n\n\n## What we learned\n\n\nHow to use OpenBCI (and why correct documentation is important)\nHow to process and use biometric data \nHow to send biometric data using websockets \nDeveloping for the Magic Leap \nUsing terminal \nEverything related to bluetooth \nNetworking \n\n\n## What\u2019s next for RocketPowAR\n\n\nAdding in additional physical therapy exercises for each injury (fly the rocket through the levels of the stratosphere, avoid meteors), and then adding in different \"missions\" based on different exercises (e.g. strengthening up VMO after a ACL or MCL tear).\nWe want to find a better way to work with the EMG signals, and can spend more time looking into signal processing software. We also want to spend more time on the send along of the data instead of having to use sockets.\n\n\nDiscovered \n\n\n"
        },
        {
            "source": "https://devpost.com/software/aurar",
            "title": "AurAR",
            "blurb": "AR experience that let you see your aura ",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/UCG_o-ZMD2d-C-REEx4xGR7w?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Mengzhen Xiao",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fe511dac228b2bebe81ea5db71d0673f?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Lin Zhang",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/205/datas/profile.jpg"
                }
            ],
            "built_with": [
                "aframe",
                "clmtrackr",
                "javascript",
                "posenet",
                "three.js"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our inspiration comes from the aura photo trend. We have seen a lot of people sharing their aura photo online, like this: <a href=\"https://www.instagram.com/p/BstxMarFCDt/?utm_source=ig_web_options_share_sheet\" rel=\"nofollow\">https://www.instagram.com/p/BstxMarFCDt/?utm_source=ig_web_options_share_sheet</a>. We like the idea and think we can bring this experience in a more interactive and dynamic setting in AR. </p>\n<h2>What it does</h2>\n<p>We plan to give as many people as possible to access this experience; therefore we are building for web AR. The user will open our website on a web browser. They are instructed to take a selfie and it will be analyzed and show their aura displayed. </p>\n<h2>How we built it</h2>\n<p>In this project, we are using aframe-ar, three.js, clmtrackr, and posenet.</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nOur inspiration comes from the aura photo trend. We have seen a lot of people sharing their aura photo online, like this: <https://www.instagram.com/p/BstxMarFCDt/?utm_source=ig_web_options_share_sheet>. We like the idea and think we can bring this experience in a more interactive and dynamic setting in AR. \n\n\n## What it does\n\n\nWe plan to give as many people as possible to access this experience; therefore we are building for web AR. The user will open our website on a web browser. They are instructed to take a selfie and it will be analyzed and show their aura displayed. \n\n\n## How we built it\n\n\nIn this project, we are using aframe-ar, three.js, clmtrackr, and posenet.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/crouton",
            "title": "Crouton",
            "blurb": "Every crumb of your persona as an AR avatar",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/JO6Y3X8_oRg?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "Catherine Qiu",
                    "about": "I worked on 3d assets for the project",
                    "photo": "https://www.gravatar.com/avatar/d76a512158cd52b4deb64cbeecf97436?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Marcus Wong",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/fa25bd8e41ef17865e52a5f03ff58c12?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Nigel Gillis",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/32252985?height=180&v=4&width=180"
                },
                {
                    "name": "Viktor Makarskyy",
                    "about": "",
                    "photo": "https://media.licdn.com/dms/image/C5603AQFbNQr9f4HUQA/profile-displayphoto-shrink_100_100/0?e=1526486400&height=180&t=oL0CHOhSqr_1NeL8oH1E8rTAWrhTd9sTcCKDY4StNS8&v=alpha&width=180"
                },
                {
                    "name": "Rizky Wellyanto",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/745/451/datas/profile.jpeg"
                }
            ],
            "built_with": [
                "microsoft-hololens",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Expressing yourself is a part of being human. We all seek to be able to show the world our stories and interests liberates our soul. Currently, our profile information is stuck in the 2D-world; a flat profile picture and a short bio is the most common layout in most social media! What happens to profile pictures when we move on from mobile phones to AR glasses in the future? Well, Crouton is how we imagined profile information will be shown when both the real world and our digital world combined. With Crouton, you can show every crumb of your persona as an AR avatar</p>\n<h2>What it does</h2>\n<p>Crouton lets you upload any 3D models: An expressive art piece you created in TiltBrush, a 3D model badge, a cute bunny, any special effect, and overlay them around your face. Crouton out-of-the-box comes with face tracking, face detection, and face alignment for an immersive Mixed Reality experience that maps perfectly to the faces around you. Anyone who has an AR headset will be able to see your Croutons, in real time, when your face is within their vision. </p>\n<h2>How we built it</h2>\n<ol>\n<li><p>We implemented real-time local face tracking on Hololens platform. We found it to be quite challenging because it's not easy to take picture, do neural net inference to localize a face, and overlay 3D mesh within the constraint of a head-mounted device. Luckily we found a research paper that's done a very good job exploring this topic [1]. So, we learned, used, and implemented their findings to solve our need</p></li>\n<li><p>We used Microsoft's Azure face recognition API to do 1-1 mapping between croutons and faces. Microsoft's API is very easy to use, we love it. However, integrating API-based individual face recognition to the real time face tracker is pretty challenging</p></li>\n<li><p>We spent a lot of time making personal 3D assets because we really care about expressing our personal beliefs and things that we care about, and we think everyone should do too</p></li>\n</ol>\n<h2>Challenges we ran into</h2>\n<ol>\n<li><p>We spent a little bit of time setting up developer environment for Hololens, as Unity + Visual Studio + Hololens Dev Kit + Universal Windows Platform takes surprisingly a bajillion bytes to download</p></li>\n<li><p>We also borrowed 2 PCs because 2/3 of our developers only brought a Mac</p></li>\n<li><p>Again, Implementing real-time local face tracking is very challenging because of the computational and power constraint of AR glasses. At the same time we need to identify a face, track it, and overlay meshes with &lt; 5 inches error from the center of the face achieve good UI/UX</p></li>\n<li><p>Combining API-based face detection and local face localization is also tricky</p></li>\n</ol>\n<h2>Accomplishments that we're proud of</h2>\n<ol>\n<li>Got local face tracking to work</li>\n<li>Got face recognition to work</li>\n<li>Got face rendering to work</li>\n<li>Got all this to work real-time (it's a pretty big engineering feat)</li>\n</ol>\n<h2>What we learned</h2>\n<ol>\n<li>We learned that Hololens Development is exclusively PC</li>\n<li>We learned that 3D spatial UI/UX is completely different than screen-based UI/UX</li>\n</ol>\n<h2>What's next for Crouton</h2>\n<p>[1] Kowalski et al. HoloFace: Augmenting Human-to-Human Interactions on HoloLens <a href=\"https://arxiv.org/abs/1802.00278\" rel=\"nofollow\">Paper</a></p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nExpressing yourself is a part of being human. We all seek to be able to show the world our stories and interests liberates our soul. Currently, our profile information is stuck in the 2D-world; a flat profile picture and a short bio is the most common layout in most social media! What happens to profile pictures when we move on from mobile phones to AR glasses in the future? Well, Crouton is how we imagined profile information will be shown when both the real world and our digital world combined. With Crouton, you can show every crumb of your persona as an AR avatar\n\n\n## What it does\n\n\nCrouton lets you upload any 3D models: An expressive art piece you created in TiltBrush, a 3D model badge, a cute bunny, any special effect, and overlay them around your face. Crouton out-of-the-box comes with face tracking, face detection, and face alignment for an immersive Mixed Reality experience that maps perfectly to the faces around you. Anyone who has an AR headset will be able to see your Croutons, in real time, when your face is within their vision. \n\n\n## How we built it\n\n\n1. We implemented real-time local face tracking on Hololens platform. We found it to be quite challenging because it's not easy to take picture, do neural net inference to localize a face, and overlay 3D mesh within the constraint of a head-mounted device. Luckily we found a research paper that's done a very good job exploring this topic [1]. So, we learned, used, and implemented their findings to solve our need\n2. We used Microsoft's Azure face recognition API to do 1-1 mapping between croutons and faces. Microsoft's API is very easy to use, we love it. However, integrating API-based individual face recognition to the real time face tracker is pretty challenging\n3. We spent a lot of time making personal 3D assets because we really care about expressing our personal beliefs and things that we care about, and we think everyone should do too\n\n\n## Challenges we ran into\n\n\n1. We spent a little bit of time setting up developer environment for Hololens, as Unity + Visual Studio + Hololens Dev Kit + Universal Windows Platform takes surprisingly a bajillion bytes to download\n2. We also borrowed 2 PCs because 2/3 of our developers only brought a Mac\n3. Again, Implementing real-time local face tracking is very challenging because of the computational and power constraint of AR glasses. At the same time we need to identify a face, track it, and overlay meshes with < 5 inches error from the center of the face achieve good UI/UX\n4. Combining API-based face detection and local face localization is also tricky\n\n\n## Accomplishments that we're proud of\n\n\n1. Got local face tracking to work\n2. Got face recognition to work\n3. Got face rendering to work\n4. Got all this to work real-time (it's a pretty big engineering feat)\n\n\n## What we learned\n\n\n1. We learned that Hololens Development is exclusively PC\n2. We learned that 3D spatial UI/UX is completely different than screen-based UI/UX\n\n\n## What's next for Crouton\n\n\n[1] Kowalski et al. HoloFace: Augmenting Human-to-Human Interactions on HoloLens [Paper](https://arxiv.org/abs/1802.00278)\n\n\n"
        },
        {
            "source": "https://devpost.com/software/bottled-up",
            "title": "Bottled Up",
            "blurb": "a virtual reality alchemist's lab",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/CGBW--ZoaEU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "ddgond",
                    "about": "",
                    "photo": "https://avatars1.githubusercontent.com/u/7691152?height=180&v=4&width=180"
                },
                {
                    "name": "Elissa He",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/82de3764aa9cb448c12b09ba1a66890c?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Danny Gelman",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/7d7b3bb228a489f7b162a8e8adfd51f2?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "Isabel Lee",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/18541240?height=180&v=4&width=180"
                }
            ],
            "built_with": [
                "unity"
            ],
            "content_html": "<div>\n<h2>Location</h2>\n<p>Media Lab, 3rd Floor, outside E15-313.</p>\n<h2>Inspiration</h2>\n<p>Where do the potions in RPGs come from? Who makes them? Humanity has spent centuries searching for an answer to this age-old question. But the search is at an end - now <em>you</em> can make the potions.</p>\n<h2>What it does</h2>\n<p>Bottled Up is a game that invites players to invent and explore. Players chop, smash and boil ingredients to make their own magical potions.</p>\n<h2>How we built it</h2>\n<p>We used Unity and SteamVR, as well as Blender and Maya for modeling.</p>\n<h2>3rd Party Assets and libraries</h2>\n<p><a href=\"https://sketchfab.com/models/dfc462fb314a4ca2985b6ed7090d1e90\" rel=\"nofollow\">https://sketchfab.com/models/dfc462fb314a4ca2985b6ed7090d1e90</a>\n<a href=\"https://sketchfab.com/models/47c554254fd24ac29966e71819c2a0be\" rel=\"nofollow\">https://sketchfab.com/models/47c554254fd24ac29966e71819c2a0be</a>\n<a href=\"https://sketchfab.com/models/1be00a3695c046c69eca1e0a0381e188\" rel=\"nofollow\">https://sketchfab.com/models/1be00a3695c046c69eca1e0a0381e188</a>\n<a href=\"https://sketchfab.com/models/b264c4fbd0534372a278a9def943f52a\" rel=\"nofollow\">https://sketchfab.com/models/b264c4fbd0534372a278a9def943f52a</a>\n<a href=\"https://sketchfab.com/models/747a541d6dca4c8c8df2f4ec0f37c00e\" rel=\"nofollow\">https://sketchfab.com/models/747a541d6dca4c8c8df2f4ec0f37c00e</a>\n<a href=\"https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647\" rel=\"nofollow\">https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647</a>\n<a href=\"https://sketchfab.com/models/ea227e0235a54af48ae11142402b4323\" rel=\"nofollow\">https://sketchfab.com/models/ea227e0235a54af48ae11142402b4323</a></p>\n<h2>Challenges we ran into</h2>\n<p>Couldn't get a Vive for testing the first two days.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We worked well together and made a lot of progress despite being unable to test in VR.</p>\n<h2>What we learned</h2>\n<p>Waking up early for equipment checkout is important.</p>\n<h2>What's next for Bottled Up</h2>\n<p>More crafting recipes and potions.</p>\n</div>",
            "content_md": "\n## Location\n\n\nMedia Lab, 3rd Floor, outside E15-313.\n\n\n## Inspiration\n\n\nWhere do the potions in RPGs come from? Who makes them? Humanity has spent centuries searching for an answer to this age-old question. But the search is at an end - now *you* can make the potions.\n\n\n## What it does\n\n\nBottled Up is a game that invites players to invent and explore. Players chop, smash and boil ingredients to make their own magical potions.\n\n\n## How we built it\n\n\nWe used Unity and SteamVR, as well as Blender and Maya for modeling.\n\n\n## 3rd Party Assets and libraries\n\n\n<https://sketchfab.com/models/dfc462fb314a4ca2985b6ed7090d1e90>\n<https://sketchfab.com/models/47c554254fd24ac29966e71819c2a0be>\n<https://sketchfab.com/models/1be00a3695c046c69eca1e0a0381e188>\n<https://sketchfab.com/models/b264c4fbd0534372a278a9def943f52a>\n<https://sketchfab.com/models/747a541d6dca4c8c8df2f4ec0f37c00e>\n<https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647>\n<https://sketchfab.com/models/ea227e0235a54af48ae11142402b4323>\n\n\n## Challenges we ran into\n\n\nCouldn't get a Vive for testing the first two days.\n\n\n## Accomplishments that we're proud of\n\n\nWe worked well together and made a lot of progress despite being unable to test in VR.\n\n\n## What we learned\n\n\nWaking up early for equipment checkout is important.\n\n\n## What's next for Bottled Up\n\n\nMore crafting recipes and potions.\n\n\n"
        },
        {
            "source": "https://devpost.com/software/herotherapy",
            "title": "HeroTherapy",
            "blurb": "A VR exp. for kids during chemo sessions. Chemo within VR makes them superheroes and gives powers to fight cancer.",
            "awards": [],
            "videos": [
                "https://www.youtube.com/embed/yz1sIWBnqk4?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
            ],
            "images": [],
            "team": [
                {
                    "name": "nikkieto",
                    "about": "",
                    "photo": "https://avatars2.githubusercontent.com/u/46796781?height=180&v=4&width=180"
                },
                {
                    "name": "Anish-Sinha",
                    "about": "",
                    "photo": "https://avatars0.githubusercontent.com/u/22457135?height=180&v=4&width=180"
                },
                {
                    "name": "Mr Chow",
                    "about": "",
                    "photo": "https://avatars3.githubusercontent.com/u/29829448?height=180&v=4&width=180"
                },
                {
                    "name": "Eileen Hing",
                    "about": "",
                    "photo": "https://www.gravatar.com/avatar/66a0fe260659b0639864543843335ab3?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
                },
                {
                    "name": "lojke",
                    "about": "",
                    "photo": "https://graph.facebook.com/10219428775492690/picture?height=180&width=180"
                }
            ],
            "built_with": [
                "oculus",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Personal experience. Feeling hopeless and alone during battle with cancer.</p>\n<h2>What it does</h2>\n<p>HeroTherapy is an application based on Unity that allows single or multiple users to have a VR experience for chemotherapy sessions for children of cancer clinics.  In VR - thank to chemo going through their veins - they are becoming superheroes(!) and with superpowers, destroying cancer cells. The children will feel calmer, stronger, and empowered during their VR chemotherapy session.</p>\n<h2>How we built it</h2>\n<p>Unity 3D</p>\n<h2>Challenges we ran into</h2>\n<p>We learned about Unity. We have learned about cooperation. Team leadership.\nEmpathy and power of visualisation in medical treatment.</p>\n<h2>Accomplishments that we're proud of</h2>\n<p>Our team learned everything that we've made this weekend from scratch.</p>\n<h2>What we learned</h2>\n<p>We learned about Unity. We have learned about cooperation. Team leadership.\nEmpathy and power of visualisation in medical treatment.</p>\n<h2>What's next for Herotherapy</h2>\n<p>Implementation of a game into global system of oncology clinics</p>\n<p>Development of social VR platform to connect children in their chemo session to interact ans support each other</p>\n<p>Cooperation with superhero producers / franchaise to join together in good cause and implement\ufffdMARVEL, Disney, Harry potter character </p>\n<p>Expeding Immersion level to actual usage of saying spells with Magic Wand gestures:\ufffdfor example \u201eWingardium Leviosa\u201d will make you fly</p>\n</div>",
            "content_md": "\n## Inspiration\n\n\nPersonal experience. Feeling hopeless and alone during battle with cancer.\n\n\n## What it does\n\n\nHeroTherapy is an application based on Unity that allows single or multiple users to have a VR experience for chemotherapy sessions for children of cancer clinics. In VR - thank to chemo going through their veins - they are becoming superheroes(!) and with superpowers, destroying cancer cells. The children will feel calmer, stronger, and empowered during their VR chemotherapy session.\n\n\n## How we built it\n\n\nUnity 3D\n\n\n## Challenges we ran into\n\n\nWe learned about Unity. We have learned about cooperation. Team leadership.\nEmpathy and power of visualisation in medical treatment.\n\n\n## Accomplishments that we're proud of\n\n\nOur team learned everything that we've made this weekend from scratch.\n\n\n## What we learned\n\n\nWe learned about Unity. We have learned about cooperation. Team leadership.\nEmpathy and power of visualisation in medical treatment.\n\n\n## What's next for Herotherapy\n\n\nImplementation of a game into global system of oncology clinics\n\n\nDevelopment of social VR platform to connect children in their chemo session to interact ans support each other\n\n\nCooperation with superhero producers / franchaise to join together in good cause and implement\ufffdMARVEL, Disney, Harry potter character \n\n\nExpeding Immersion level to actual usage of saying spells with Magic Wand gestures:\ufffdfor example \u201eWingardium Leviosa\u201d will make you fly\n\n\n"
        },
        {
            "source": "https://devpost.com/software/terra-form-ar",
            "title": "Terra Form AR",
            "blurb": "AR enabled point and \u2018shoot\u2019 aeriel map view that provides contextual data of nearby landmarks and points of interest",
            "awards": [],
            "videos": [],
            "images": [],
            "team": [
                {
                    "name": "Suzan Oslin",
                    "about": "",
                    "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/632/963/datas/profile.jpg"
                }
            ],
            "built_with": [
                "a-frame",
                "ar.js",
                "esri",
                "open-data",
                "three.js",
                "unity"
            ],
            "content_html": "<div>\n<h2>Inspiration</h2>\n<p>This app was inspired by the existing app, Horizontal Explorer AR, a potentially really useful app but with a not so great interface.  This version is device agnostic and built-in web XR and can be deployed to a phone, VR or AR headset.</p>\n<h2>What it does</h2>\n<p>Terra Forma AR gives form to user-defined geospatial data as an AR overlay.</p>\n<h2>How we built it</h2>\n<h2>Challenges we ran into</h2>\n<h2>Accomplishments that we're proud of</h2>\n<h2>What we learned</h2>\n<h2>What's next for Terra Form AR</h2>\n</div>",
            "content_md": "\n## Inspiration\n\n\nThis app was inspired by the existing app, Horizontal Explorer AR, a potentially really useful app but with a not so great interface. This version is device agnostic and built-in web XR and can be deployed to a phone, VR or AR headset.\n\n\n## What it does\n\n\nTerra Forma AR gives form to user-defined geospatial data as an AR overlay.\n\n\n## How we built it\n\n\n## Challenges we ran into\n\n\n## Accomplishments that we're proud of\n\n\n## What we learned\n\n\n## What's next for Terra Form AR\n\n\n"
        }
    ]
}