{
    "source": "https://devpost.com/software/project-x0bvzw8s654m",
    "title": "#\u00b3",
    "blurb": "#\u00b3 is a tool to create and curate spatial content. Easily leave notes and media in Augmented Reality using the phone\u2019s native keyboard without taking off your wearable. ",
    "awards": [
        "Best use of Nreal Light"
    ],
    "videos": [
        "https://www.youtube.com/embed/D8KYSyvnFCU?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
    ],
    "images": [],
    "team": [
        {
            "name": "William Liu",
            "about": "I worked on the Nreal Android client in Unity and developed the backend.",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/001/342/276/datas/profile.jpg"
        },
        {
            "name": "Colin Keenan",
            "about": "I built sounds, wrote story, tinkered with Unity, and went on a very trying trip to the Cambridge Dunkin Donuts.",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/458/datas/profile.jpg"
        },
        {
            "name": "Kathy Wang",
            "about": "I worked on UI design for the android app, 3D modeling and rendering, all the visual elements, hashtag contents, video editing and animations.",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/027/datas/profile.jpeg"
        },
        {
            "name": "Leon Wu",
            "about": "",
            "photo": "https://avatars.githubusercontent.com/u/13173037?height=180&v=3&width=180"
        },
        {
            "name": "JaredMonkey",
            "about": "",
            "photo": "https://avatars0.githubusercontent.com/u/356520?height=180&v=4&width=180"
        }
    ],
    "built_with": [
        "android",
        "azure",
        "c#",
        "figma",
        "mobile",
        "mongodb",
        "nreal",
        "python",
        "unity",
        "vectary"
    ],
    "content_html": "<div>\n<p>On Day 1 of Reality Hack, our team members found one another through our shared interest in how spatial realities relate to one another, how the display of location-persistent text and models should be prioritized relative to one another, and how to facilitate user-centered curation of the spatial media relevant to their needs.</p>\n<p>For Leon, coming to Cambridge for the hackathon brought him back to last summer and the many happy moments he shared with dancers on the MIT ballroom team. Yuting and Colin were lost between workshops sessions, wishing they could call up relevant wayfinding but anticipating that such an interface could be just as cluttered as the cork boards in the university hallways. Jared and William came to the project specifically to answer, \u201cWhat is a hashtag for spatial reality?\u201d</p>\n<p>Our team\u2019s design philosophy examines the precedent social media to clarify the utility of hashtags and user-assigned metadata channels within existing virtual spaces and forums. While user-submitted tags differ from platform to platform, they share a curation function- connecting users with channels of communication relevant to their wants and needs. From Internet Relay Chat to Twitter and beyond, hashtags have an established lineage as a stand-in for a user saying, \u201cI think this content is relevant to topic X.\u201d We immediately saw a need for equivalent functionality in virtual spaces and across digital realities. Once users are empowered to post content affixed to the underlying \u2018default reality,\u2019 the volume of content will quickly exceed the physical space\u2019s ability to comfortably accommodate this content. </p>\n<p>AR is still an emerging technology, and many of the challenges we faced came when we pushed the technology right up to its current limitations. Things we take for granted, even using a smartphone keyboard or posting a video, are radically different because we\u2019re working with tech that\u2019s still rough around the edges.</p>\n<p>We learned about how the world of atoms and the world of bits interact. You wouldn\u2019t think that a lightweight pair of glasses could be uncomfortable, but try wearing it as its embedded computer starts warming up your eyebrows. Or try wearing your regular pair of glasses behind the Nreal, pushing it closer to the tip of your nose.</p>\n<p>We learned that there\u2019s so much room to grow in this space, and we\u2019re unbelievably excited to see what the future holds!</p>\n<h2>What languages, APIs, hardware, hosts, libraries, UI Kits or frameworks are you using?</h2>\n<p>The client is built with Unity (in C#) for Nreal using the beta NRSDK. We used many of the unique functionalities of Nreal including accurate image tracking and using the phone as a controller. We also experimented with the offline mapping system currently in beta, but found that changing ambient conditions at our workspace significantly affected our anchoring system. As such, we currently base our anchoring on tracked images, but plan to switch to the SDK\u2019s offline mapping system once it becomes more robust. </p>\n<p>Nreal\u2019s choice of using an Android phone as the controller provided us with many unique opportunities to use the phone\u2019s services. As such, we were able to implement a user-friendly UI on the phone screen. In input fields, the builtin Android keyboard gives a much more fluid experience than typing experiences on other AR or VR devices, and responsive button presses on the phone screen also contribute to the experience. We also poll the phone\u2019s builtin location and compass data in order to tag objects with location data. Finally, the Android phone\u2019s ability to use mobile data fits perfectly with our vision of being able to see and place media outdoors where the user may not have wifi.</p>\n<p>The backend is written in Python (with pymongo) and is deployed on Azure as a Functions App. The backend functions as a bridge between our client and our Azure Cosmos database by providing handlers for various HTTP requests. In order to provide scalability, we created our web API following the REST guidelines. We wrote a custom RESTful client in Unity via the UnityWebRequest class to interact with the database from the client.</p>\n<p>In order to experiment with how our program could scale to VR devices, we used Aardvark, an open source framework to develop AR apps in VR. We used Figma, Rhino and Vectary to prototype and design UI elements and user experience. The video was created with Premiere Pro and After Effects.</p>\n<h2>What is your team number?</h2>\n<p>28</p>\n<h2>What is your table number?</h2>\n<p>6B-07</p>\n<h2>Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)</h2>\n<p>Best of AR</p>\n<h2>Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)</h2>\n<p>Best of VR</p>\n</div>",
    "content_md": "\nOn Day 1 of Reality Hack, our team members found one another through our shared interest in how spatial realities relate to one another, how the display of location-persistent text and models should be prioritized relative to one another, and how to facilitate user-centered curation of the spatial media relevant to their needs.\n\n\nFor Leon, coming to Cambridge for the hackathon brought him back to last summer and the many happy moments he shared with dancers on the MIT ballroom team. Yuting and Colin were lost between workshops sessions, wishing they could call up relevant wayfinding but anticipating that such an interface could be just as cluttered as the cork boards in the university hallways. Jared and William came to the project specifically to answer, \u201cWhat is a hashtag for spatial reality?\u201d\n\n\nOur team\u2019s design philosophy examines the precedent social media to clarify the utility of hashtags and user-assigned metadata channels within existing virtual spaces and forums. While user-submitted tags differ from platform to platform, they share a curation function- connecting users with channels of communication relevant to their wants and needs. From Internet Relay Chat to Twitter and beyond, hashtags have an established lineage as a stand-in for a user saying, \u201cI think this content is relevant to topic X.\u201d We immediately saw a need for equivalent functionality in virtual spaces and across digital realities. Once users are empowered to post content affixed to the underlying \u2018default reality,\u2019 the volume of content will quickly exceed the physical space\u2019s ability to comfortably accommodate this content. \n\n\nAR is still an emerging technology, and many of the challenges we faced came when we pushed the technology right up to its current limitations. Things we take for granted, even using a smartphone keyboard or posting a video, are radically different because we\u2019re working with tech that\u2019s still rough around the edges.\n\n\nWe learned about how the world of atoms and the world of bits interact. You wouldn\u2019t think that a lightweight pair of glasses could be uncomfortable, but try wearing it as its embedded computer starts warming up your eyebrows. Or try wearing your regular pair of glasses behind the Nreal, pushing it closer to the tip of your nose.\n\n\nWe learned that there\u2019s so much room to grow in this space, and we\u2019re unbelievably excited to see what the future holds!\n\n\n## What languages, APIs, hardware, hosts, libraries, UI Kits or frameworks are you using?\n\n\nThe client is built with Unity (in C#) for Nreal using the beta NRSDK. We used many of the unique functionalities of Nreal including accurate image tracking and using the phone as a controller. We also experimented with the offline mapping system currently in beta, but found that changing ambient conditions at our workspace significantly affected our anchoring system. As such, we currently base our anchoring on tracked images, but plan to switch to the SDK\u2019s offline mapping system once it becomes more robust. \n\n\nNreal\u2019s choice of using an Android phone as the controller provided us with many unique opportunities to use the phone\u2019s services. As such, we were able to implement a user-friendly UI on the phone screen. In input fields, the builtin Android keyboard gives a much more fluid experience than typing experiences on other AR or VR devices, and responsive button presses on the phone screen also contribute to the experience. We also poll the phone\u2019s builtin location and compass data in order to tag objects with location data. Finally, the Android phone\u2019s ability to use mobile data fits perfectly with our vision of being able to see and place media outdoors where the user may not have wifi.\n\n\nThe backend is written in Python (with pymongo) and is deployed on Azure as a Functions App. The backend functions as a bridge between our client and our Azure Cosmos database by providing handlers for various HTTP requests. In order to provide scalability, we created our web API following the REST guidelines. We wrote a custom RESTful client in Unity via the UnityWebRequest class to interact with the database from the client.\n\n\nIn order to experiment with how our program could scale to VR devices, we used Aardvark, an open source framework to develop AR apps in VR. We used Figma, Rhino and Vectary to prototype and design UI elements and user experience. The video was created with Premiere Pro and After Effects.\n\n\n## What is your team number?\n\n\n28\n\n\n## What is your table number?\n\n\n6B-07\n\n\n## Grand Challenge Choice #1 (For your project to be eligible for a prize, choose the MAIN category for which this project is submitted for judging:)\n\n\nBest of AR\n\n\n## Grand Challenge Choice #2 (optional) (If your project fits more than one category, what is the SECONDARY category for which this project is submitted for judging?)\n\n\nBest of VR\n\n\n"
}