{
    "source": "https://devpost.com/software/helpinghand-nwm0k6",
    "title": "HelpingHand",
    "blurb": "Portable therapeutic AR application for amputees for treatment of phantom limb pain",
    "awards": [],
    "videos": [
        "https://www.youtube.com/embed/Gf1Cn7w-qfQ?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
    ],
    "images": [],
    "team": [
        {
            "name": "Julien BOUVIER-VOLAILLE",
            "about": "I worked on java module to collecte iMotion data extracted from Shimmer devices, filter and generate events. I integrated those event in an Unity demo.",
            "photo": "https://www.gravatar.com/avatar/4d3a070fe16c728138c36ff037328e29?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
        },
        {
            "name": "Mehdi Lefouili",
            "about": "",
            "photo": "https://www.gravatar.com/avatar/7001f02a5195e4e9015b2eff6155df83?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
        },
        {
            "name": "Lindsay Lin",
            "about": "",
            "photo": "https://www.gravatar.com/avatar/776cf39df980711e80fc02317eb64649?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
        },
        {
            "name": "Jingru Guo",
            "about": "",
            "photo": "https://www.gravatar.com/avatar/8f94bd1b7b4449f359bfed93d62b53a1?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
        }
    ],
    "built_with": [
        "c#",
        "emg",
        "imotion",
        "java",
        "microsoft-hololens",
        "shimmer3",
        "unity"
    ],
    "content_html": "<div>\n<h2>Inspiration</h2>\n<p>There are nearly 2 million people living with limb loss in the United States. Approximately 60 to 80% of such amputees experience phantom sensations in their amputated limb, and the majority of the sensations are painful (this pain is called \"phantom limb pain\" (PLP)). Pharmacotherapy, surgery, and traditional adjuvant therapy (e.g. physiotherapy, massage, and ultrasound) are also not consistently effective. There has been clinical research showing that mirror therapy -- a way of positioning mirrors so that it visually convinces the amputee that he/she still has the missing limb -- is effective for some patients. We believe that an AR system that enables myoelectric control of a virtual limb and provides a visual simulation of the missing limb will be an effective and useful tool in the arsenal of treatments against PLP. There have been clinical studies that demonstrate high levels of effectiveness for AR in reducing PLP (See, e.g., <a href=\"http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full\" rel=\"nofollow\">http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19191061\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pubmed/19191061</a>) but there are currently no commercial, public  implementations out there. Moreover, we wanted to leverage the benefits of computational rehabilitation systems in being able to track patient progress, adjust task difficulty, and engage patients.</p>\n<p>In addition to visualization of the lost limb, we also want to enable the virtual arm to interact with the real world in useful applications. We believe this will be more convincing and effective for patients who are suffering PLP. Also, in the long run, perhaps the AR system will be able to transcend therapy and enhance day-to-day functionality of people who are missing limbs.</p>\n<h2>What it does</h2>\n<p>Our HoloLens application uses myoelectric input from the iMotions Shimmer3 ECG/EMG sensor to predict the amputee's intended muscle movements. We use this input to create an AR visualization of a hand that moves in interaction with the real world. The ultimate experience gives amputees the illusion that they have \"regained\" their lost limb. This responsive visual feedback helps patients slowly change their inner cortical map (an internal brain map that keeps track of limbs) and reduce PLP. </p>\n<p>We also enabled the virtual limb to be able to impact the real world. For example, we can launch music from the virtual limb directly on a computer in the real world.</p>\n<h2>How we built it</h2>\n<p>Our project is open source. We've created a backend for the iMotion platform to send data to the HoloLens. This backend can be used in conjunction with any sensor supported by the iMotion platform and is used to push events to the HoloLens. This technology can be applied ubiquitously by other developers to create  projects in the HoloLens featuring iMotion biometric sensors. We also created an algorithm to remove noise and detect changes in muscle movements for the iMotion Shimmer3 ECG/EMG. This algorithm can also be used by other developers to create an AR experience that is linked to myoelectric signals. </p>\n<p>We wanted the graphics to be immersive thus we used a realistic looking hand. Our Hololens application is built with Unity and is able to pull and push events from and to the real world. We have a complete chain of event management that relies on an local backend, a local server and external server.</p>\n<h2>Challenges we ran into</h2>\n<p>The iMotion Shimmer3 did not work at the beginning \u2014 it took a lot of tinkering to get it to start working. \nInherant complexity to connect all application modules: Shimmer, iMotion plateform, Hololens, backends and servers \nWe were not experts in Unity or the HoloLens so we had a steep learning curve. \nThe HoloLens SDK runs only on Windows and we only had one Windows computer. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We made the end to end connectivity from the iMotion sensor to the HoloLens!\nWe have a working demo!</p>\n<h2>What we learned</h2>\n<p>We started off knowing very little about Unity and the HoloLens. In the past couple of days, we learned a lot about Unity and the HoloLens! We also learned how to incorporate iMotion sensors with the HoloLens.</p>\n<h2>What's next for HelpingHand</h2>\n<p>We did not have time to build all the features that we wanted. We currently have simple actions and only one interaction between the virtual arm and a real object. We want to add: (1) a full range of motions and (2) more actions between virtual arm and real objects. Ultimately, it would be cool to have the virtual limb be able to interact with many real world smart objects.</p>\n<h2>Notes</h2>\n<p><strong>Team Lead:</strong> Julien Bouvier</p>\n<p><strong>Phone:</strong> 781-298-8165</p>\n<p><strong>Location:</strong> Room E14-674, Table 38</p>\n<p><strong>Vertical Category:</strong> Human Well-Being (Education/Health/Wellness/Activism)</p>\n<p><strong>GitHub:</strong> <a href=\"https://github.com/bouviervj/RealityVirtually_HelpingHand\" rel=\"nofollow\">https://github.com/bouviervj/RealityVirtually_HelpingHand</a></p>\n</div>",
    "content_md": "\n## Inspiration\n\n\nThere are nearly 2 million people living with limb loss in the United States. Approximately 60 to 80% of such amputees experience phantom sensations in their amputated limb, and the majority of the sensations are painful (this pain is called \"phantom limb pain\" (PLP)). Pharmacotherapy, surgery, and traditional adjuvant therapy (e.g. physiotherapy, massage, and ultrasound) are also not consistently effective. There has been clinical research showing that mirror therapy -- a way of positioning mirrors so that it visually convinces the amputee that he/she still has the missing limb -- is effective for some patients. We believe that an AR system that enables myoelectric control of a virtual limb and provides a visual simulation of the missing limb will be an effective and useful tool in the arsenal of treatments against PLP. There have been clinical studies that demonstrate high levels of effectiveness for AR in reducing PLP (See, e.g., <http://journal.frontiersin.org/article/10.3389/fnins.2014.00024/full>, <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3935120/>, <https://www.ncbi.nlm.nih.gov/pubmed/19191061>) but there are currently no commercial, public implementations out there. Moreover, we wanted to leverage the benefits of computational rehabilitation systems in being able to track patient progress, adjust task difficulty, and engage patients.\n\n\nIn addition to visualization of the lost limb, we also want to enable the virtual arm to interact with the real world in useful applications. We believe this will be more convincing and effective for patients who are suffering PLP. Also, in the long run, perhaps the AR system will be able to transcend therapy and enhance day-to-day functionality of people who are missing limbs.\n\n\n## What it does\n\n\nOur HoloLens application uses myoelectric input from the iMotions Shimmer3 ECG/EMG sensor to predict the amputee's intended muscle movements. We use this input to create an AR visualization of a hand that moves in interaction with the real world. The ultimate experience gives amputees the illusion that they have \"regained\" their lost limb. This responsive visual feedback helps patients slowly change their inner cortical map (an internal brain map that keeps track of limbs) and reduce PLP. \n\n\nWe also enabled the virtual limb to be able to impact the real world. For example, we can launch music from the virtual limb directly on a computer in the real world.\n\n\n## How we built it\n\n\nOur project is open source. We've created a backend for the iMotion platform to send data to the HoloLens. This backend can be used in conjunction with any sensor supported by the iMotion platform and is used to push events to the HoloLens. This technology can be applied ubiquitously by other developers to create projects in the HoloLens featuring iMotion biometric sensors. We also created an algorithm to remove noise and detect changes in muscle movements for the iMotion Shimmer3 ECG/EMG. This algorithm can also be used by other developers to create an AR experience that is linked to myoelectric signals. \n\n\nWe wanted the graphics to be immersive thus we used a realistic looking hand. Our Hololens application is built with Unity and is able to pull and push events from and to the real world. We have a complete chain of event management that relies on an local backend, a local server and external server.\n\n\n## Challenges we ran into\n\n\nThe iMotion Shimmer3 did not work at the beginning \u2014 it took a lot of tinkering to get it to start working. \nInherant complexity to connect all application modules: Shimmer, iMotion plateform, Hololens, backends and servers \nWe were not experts in Unity or the HoloLens so we had a steep learning curve. \nThe HoloLens SDK runs only on Windows and we only had one Windows computer. \n\n\n## Accomplishments that we're proud of\n\n\nWe made the end to end connectivity from the iMotion sensor to the HoloLens!\nWe have a working demo!\n\n\n## What we learned\n\n\nWe started off knowing very little about Unity and the HoloLens. In the past couple of days, we learned a lot about Unity and the HoloLens! We also learned how to incorporate iMotion sensors with the HoloLens.\n\n\n## What's next for HelpingHand\n\n\nWe did not have time to build all the features that we wanted. We currently have simple actions and only one interaction between the virtual arm and a real object. We want to add: (1) a full range of motions and (2) more actions between virtual arm and real objects. Ultimately, it would be cool to have the virtual limb be able to interact with many real world smart objects.\n\n\n## Notes\n\n\n**Team Lead:** Julien Bouvier\n\n\n**Phone:** 781-298-8165\n\n\n**Location:** Room E14-674, Table 38\n\n\n**Vertical Category:** Human Well-Being (Education/Health/Wellness/Activism)\n\n\n**GitHub:** <https://github.com/bouviervj/RealityVirtually_HelpingHand>\n\n\n"
}