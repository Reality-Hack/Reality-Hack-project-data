{
    "source": "https://devpost.com/software/accessiblelocomotionwebxr",
    "title": "AccessibleLocomotionWebXR",
    "blurb": "A-Frame component that enables quadriplegic users to navigate webvr spaces with binary input controls",
    "awards": [
        "Wayfair's Way-more",
        "Best application for accessibility"
    ],
    "videos": [
        "https://www.youtube.com/embed/9wxEeiWeGMs?enablejsapi=1&hl=en_US&rel=0&start=&version=3&wmode=transparent"
    ],
    "images": [],
    "team": [
        {
            "name": "Roland Dubois",
            "about": "Concept & Idea, team lead, A-Frame design, code, and implementation, UX and mechanics",
            "photo": "https://avatars.githubusercontent.com/u/347570?height=180&v=3&width=180"
        },
        {
            "name": "Selena de Leon",
            "about": "I worked on the background information, cross-sectional research, and storytelling/presentation of our project. I was unfamiliar with the ways assistive technology is used to operate software and hardware for people with different abilities. I also drafted the DevPost write up.",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/515/datas/profile.jpg"
        },
        {
            "name": "Jan Kalfus",
            "about": "I developed conceptional sketches, infographics and animated diagrams to illustrate the mechanics and user experience, created original 3D assets. I helped the team to gain a broader understanding on designing accessible spaces based on real life work experience in architecture.",
            "photo": "https://www.gravatar.com/avatar/866b03076c1b005493046b030a0e107b?d=https%3A%2F%2Fdevpost-challengepost.netdna-ssl.com%2Fassets%2Fdefaults%2Fno-avatar-180.png&s=180"
        },
        {
            "name": "Pilar Aranda",
            "about": "Designed the case study environment for this demo, while learning how to retrieve the Waifair model library and optimize the design and polycount for Web XR. During this hackathon I created a pipeline to quickly and efficiently deploy custom environments to gltf format.",
            "photo": "//challengepost-s3-challengepost.netdna-ssl.com/photos/production/user_photos/000/742/764/datas/profile.jpg"
        }
    ],
    "built_with": [
        "a-frame",
        "google-drive",
        "javascript"
    ],
    "content_html": "<div>\n<h2>Inspiration</h2>\n<p>Our project was inspired by the false assumption that there are immersive experiences available for all people, while they are actually only accessible to able-bodied people. The underlying assumption, within the world of immersive technology, is that everyone has a free range of motion. This leads to a lack of integration for assistive technology for individuals with different levels of mobility. </p>\n<p>Controllers demand the use of limbs, hands, and fingers, which is not available to people who are affected by quadriplegia. Our goal is to establish a new standard in the field, which used binary control so that people with different abilities have access to WebXR.</p>\n<h2>What it does</h2>\n<p>Our project uses a binary input function from the analysis of sip-and-puff mechanics -- technology created in the 1960s for individuals who are unable to use their limbs, and therefore, use their mouths to \u201csip\u201d or \u201cpuff\u201d air into a device (usually a straw or tube) that is linked to hardware or software that enables them to carry out day-to-day functions. <a href=\"https://www.youtube.com/watch?v=Bhj5vs9P5cw\" rel=\"nofollow\">https://www.youtube.com/watch?v=Bhj5vs9P5cw</a></p>\n<h2>How we built it</h2>\n<p>We used JavaScript &amp; A-frame to create the VR experience, as well as Maya for reducing poly-count and editing models, Rhino for building free-models to scale, and Google Drive for writing notes and a slide presentation. All models are integrated into A-Frame.</p>\n<h2>Challenges we ran into</h2>\n<p>We were being challenged to learn more efficient ways of executing our tasks and methods of input that consider breathing as the baseline to trigger interaction. We were also learning how to integrate assistive technology independent mechanics of binary input functions in VR. Converting files has been a common theme of challenges we have faced so far. </p>\n<h2>Accomplishments that we're proud of</h2>\n<p>We explored the possibilities of creating access to VR for people with different physical abilities and introducing a change within the field of VR for the greater good of humanity.\nWe built a functioning first version of an A-Frame component \"binary-controls\" that will be released to the A-Frame registry to coexist with other more common input controls like \"gamepad, keyboard or hand\" controllers\nWe defined three basic timed binary input sequences as sufficient mechanics to navigate and interact within VR: \nClick Event to dispatch (click): on - [0ms - 500ms] - off\nSkip/Tab Event to change focus (long-click): on - [500ms - 1000ms] - off\nToggle Event to change input mode interaction&lt;&gt;locomotion (click&amp;hold): on - [3000ms - 4000ms] - off\n<a href=\"https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing\" rel=\"nofollow\">https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing</a></p>\n<h2>What we learned</h2>\n<p>Navigating in the embodiment of a quadriplegic user gives you a new definition of the time and determination it takes to execute simple tasks that able-bodied users are likely to overlook. The demo makes you more patient and empathetic but also opens up opportunities for social inclusion for quadriplegic users in VR. </p>\n<h2>What's next for AccessibleLocomotionWebXR</h2>\n<p>After a few performance and animation updates, we will be publishing the component on the A-Frame registry. We will try the component with the quadriplegic users in collaboration with EqualEntry hosts of A11YNYC.\nA projection for Accessible Locomotion XR is the AR component\u2026 We are currently creating a model for VR experience. Eventually, we hope that the methods we introduce here are taken into a broad range of XR experiences and creates an inclusive environment for locomotion.</p>\n</div>",
    "content_md": "\n## Inspiration\n\n\nOur project was inspired by the false assumption that there are immersive experiences available for all people, while they are actually only accessible to able-bodied people. The underlying assumption, within the world of immersive technology, is that everyone has a free range of motion. This leads to a lack of integration for assistive technology for individuals with different levels of mobility. \n\n\nControllers demand the use of limbs, hands, and fingers, which is not available to people who are affected by quadriplegia. Our goal is to establish a new standard in the field, which used binary control so that people with different abilities have access to WebXR.\n\n\n## What it does\n\n\nOur project uses a binary input function from the analysis of sip-and-puff mechanics -- technology created in the 1960s for individuals who are unable to use their limbs, and therefore, use their mouths to \u201csip\u201d or \u201cpuff\u201d air into a device (usually a straw or tube) that is linked to hardware or software that enables them to carry out day-to-day functions. <https://www.youtube.com/watch?v=Bhj5vs9P5cw>\n\n\n## How we built it\n\n\nWe used JavaScript & A-frame to create the VR experience, as well as Maya for reducing poly-count and editing models, Rhino for building free-models to scale, and Google Drive for writing notes and a slide presentation. All models are integrated into A-Frame.\n\n\n## Challenges we ran into\n\n\nWe were being challenged to learn more efficient ways of executing our tasks and methods of input that consider breathing as the baseline to trigger interaction. We were also learning how to integrate assistive technology independent mechanics of binary input functions in VR. Converting files has been a common theme of challenges we have faced so far. \n\n\n## Accomplishments that we're proud of\n\n\nWe explored the possibilities of creating access to VR for people with different physical abilities and introducing a change within the field of VR for the greater good of humanity.\nWe built a functioning first version of an A-Frame component \"binary-controls\" that will be released to the A-Frame registry to coexist with other more common input controls like \"gamepad, keyboard or hand\" controllers\nWe defined three basic timed binary input sequences as sufficient mechanics to navigate and interact within VR: \nClick Event to dispatch (click): on - [0ms - 500ms] - off\nSkip/Tab Event to change focus (long-click): on - [500ms - 1000ms] - off\nToggle Event to change input mode interaction<>locomotion (click&hold): on - [3000ms - 4000ms] - off\n<https://docs.google.com/presentation/d/16rS1c6x0khgrrDCbxteArcZk8oBHtkEyH-wlqHWvQLA/edit?usp=sharing>\n\n\n## What we learned\n\n\nNavigating in the embodiment of a quadriplegic user gives you a new definition of the time and determination it takes to execute simple tasks that able-bodied users are likely to overlook. The demo makes you more patient and empathetic but also opens up opportunities for social inclusion for quadriplegic users in VR. \n\n\n## What's next for AccessibleLocomotionWebXR\n\n\nAfter a few performance and animation updates, we will be publishing the component on the A-Frame registry. We will try the component with the quadriplegic users in collaboration with EqualEntry hosts of A11YNYC.\nA projection for Accessible Locomotion XR is the AR component\u2026 We are currently creating a model for VR experience. Eventually, we hope that the methods we introduce here are taken into a broad range of XR experiences and creates an inclusive environment for locomotion.\n\n\n"
}